[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to my blog! To learn more about me, check out my website here."
  },
  {
    "objectID": "posts/cnn-architectures/index.html",
    "href": "posts/cnn-architectures/index.html",
    "title": "CNN architectures",
    "section": "",
    "text": "“A small child is sitting on the ground in a brightly lit playground, surrounded by colorful toy blocks, legos. The child is focused on building a tall structure. The child’s expression is one of deep thought and gentle confusion, holding up several blocks. unsure which one to place next.” Generated by DALL-E 3"
  },
  {
    "objectID": "posts/cnn-architectures/index.html#fundamental-cnn-layers",
    "href": "posts/cnn-architectures/index.html#fundamental-cnn-layers",
    "title": "CNN architectures",
    "section": "Fundamental CNN layers",
    "text": "Fundamental CNN layers\nRemember from our previous blog post where we talk about fully connected layer. If we have, for example, a 32x32x3 image, the first thing we do is stretch or flatten it into a 3072x1 vector. This flattened vector then becomes the input to our layer. The layer computes \\(Wx\\), where \\(W\\) is a weight matrix. If we want 10 output activations (say, for 10 classes), then \\(W\\) would be a 10x3072 matrix. Each row of \\(W\\) can be thought of as a template. The output of this matrix multiplication is a 10x1 vector of activations. Looking a bit closer at how each of those 10 output activations is computed, each individual number in that output vector is the result of taking a dot product between one row of the weight matrix \\(W\\) and the entire input vector \\(x\\). So, if the input \\(x\\) is 3072-dimensional, each output activation is a 3072-dimensional dot product. This means every output neuron is connected to every input neuron, hence “fully connected.”\n\nConvolution layer\nNow, let’s contrast this with the Convolution Layer. A fundamental difference is that the convolution layer aims to preserve the spatial structure of the input image. So, if we have a 32x32x3 image, we don’t flatten it. We treat it as a 3D volume of numbers: 32 in height, 32 in width, and 3 in depth (representing, for example, the red, green, and blue color channels).\n\nThe core operation in a convolution layer involves a filter, also sometimes called a kernel. This filter is also a small volume of numbers. For example, we might have a 5x5x3 filter. The “3” here refers to the depth of the filter. The operation is to convolve the filter with the image. Conceptually, this means we “slide over the image spatially, computing dot products.” We’ll make this much more precise in a moment, but the key idea is that the filter interacts with local regions of the input image. A very important point here is that filters always extend the full depth of the input volume. So, if our input image is 32x32x3, then our filter, say a 5x5 filter, must also have a depth of 3. It will be a 5x5x3 filter. This is critical. The filter isn’t just looking at a 2D patch of one channel, it’s looking at a 3D slice through the entire depth of the input volume at that spatial location. This allows the filter to learn patterns that involve combinations of information across all input channels simultaneously.\nThe input to a convolution layer is typically a batch of images with dimensions N x Cin x H x W, where N is the batch size, Cin is the number of input channels and H and W are the height and width of the input feature maps. The convolution layer itself is defined by a set of filters. If we want Cout output channels (i.e., we want to produce Cout activation maps), we will have Cout filters. Each filter will have dimensions Cin x Kh x Kw, where Kh and Kw are the height and width of the kernel (e.g., 5x5). Note that the depth of each filter, Cin, must match the number of input channels of the volume it’s being convolved with. So, the collection of filters can be thought of as a tensor of shape Cout x Cin x Kh x Kw. There will also be a Cout-dimensional bias vector, one bias term for each of the Cout filters. The output of the convolution layer will then be a batch of output volumes with dimensions N x Cout x H’ x W’. Here, Cout is the number of output channels (equal to the number of filters), and H’ and W’ are the new height and width of the feature maps. The exact values of H’ and W’ will depend on the input H and W, the kernel size Kh and Kw, and also on other hyperparameters like stride and padding, which we will discuss shortly. This framework describes the fundamental operation of a convolution layer. It takes an input volume, applies a set of learned filters to it locally across space, and produces an output volume where each “slice” in depth corresponds to the response of one of those filters\n\nOkay, so now that we understand the mechanics of a single convolution layer, let’s see how they fit into a larger network. Essentially, a ConvNet is a neural network that incorporates Conv layers as its primary building blocks. , especially in the earlier stages responsible for feature extraction. So, we might start with an input volume, say a 32x32x3 image. We pass this through a first CONV layer. For example, this layer might use 6 filters, each of size 5x5x3. Assuming stride 1 and no padding, this would produce an output volume of size 28x28x6. The depth of 6 corresponds to the 6 filters used. This output volume then becomes the input to the next CONV layer. So, the 28x28x6 volume is fed into a second CONV layer. This layer might, for example, use 10 filters, each of size 5x5x6. Notice that the depth of these filters (6) must match the depth of the input volume (6). If these filters are also 5x5, then again assuming stride 1 and no padding, the output of this second CONV layer would be a volume of size 24x24x10. The depth of 10 corresponds to the 10 filters used in this layer. And this process can continue, stacking more CONV layers to learn increasingly complex and abstract features.\nA very important point, which we haven’t explicitly shown in the diagrams until now but is absolutely crucial, is that ConvNets, like other neural networks, need non-linearities. So, a ConvNet is a neural network with Conv layers, with activation functions! Typically, an activation function, most commonly ReLU, is applied element-wise to the output of each CONV layer after the bias has been added. So, the flow would be: Input → CONV (filters + bias) → ReLU → Output Volume. This output volume then feeds into the next CONV → ReLU sequence, and so on. Without these non-linearities, stacking multiple CONV layers would be equivalent to a single, more complex CONV layer, and the network wouldn’t be able to learn the rich hierarchical features we desire.\n\n\nWhat do Conv filters learn?\nSo, a natural question arises: What do these Conv filters actually learn? Let’s think back to our simpler models. With a Linear Classifier, we saw that it learned essentially one template per class. These templates were global, representing an average look for each category. When we moved to a Multi-Layer Perceptron (MLP), specifically a 2-layer neural network, the first layer (W1) learned a bank of whole-image templates. These were still operating on the flattened image, but the network could learn multiple templates that could then be combined by the second layer. These templates were more diverse than the single template per class of a linear classifier.\n\nNow, with ConvNets, the first-layer conv filters learn local image templates. Because the filters are small and slide across the image, they learn to detect small, localized patterns. Empirically, it’s often observed that these first-layer filters learn to detect things like oriented edges, or opposing colors (e.g., a filter that activates strongly when it sees a green region next to a red region, or a horizontal edge. The example shown here on the left is from the first layer of AlexNet, which had 64 filters, each of size 3x11x11 (operating on RGB input). You can see the variety of edge detectors and color blob detectors that have been learned. What about deeper conv layers? Visualizing what filters learn in deeper layers is harder, because they are no longer operating directly on image pixels but on the activation maps produced by previous layers. However, various visualization techniques suggest that deeper conv layers tend to learn larger, more complex structures. They combine the simpler features detected by earlier layers to represent more abstract concepts, for example, parts of objects like eyes, or even more complex textures or object parts, sometimes even letter-like shapes if trained on relevant data. The visualization here on the right, from Springenberg et al. (2015), attempts to show patterns that maximally activate neurons in a 6th layer conv layer of an ImageNet model. You can see more intricate and larger receptive field patterns\n\n\nSpatial dimension\nLet’s now focus on the Spatial Dimensions of the convolution operation. This is about understanding how the height and width of the activation map are determined by the input size and the filter size, as well as other hyperparameters. In general, if the input has a spatial dimension (width or height) of W (or H), and the filter has a spatial dimension of K (or Kh, Kw), and we are using a stride of 1 and no padding, then the output dimension will be W - K + 1. Now, this formula W - K + 1 reveals a Problem: Feature maps shrink with each layer! If we have a deep network with many convolution layers, and each layer reduces the spatial dimensions (e.g., from 32 to 28, then from 28 to 24, and so on), the feature maps can become very small quite quickly. This might be undesirable if we want to maintain spatial resolution for a while, or if we want to build very deep networks without the features vanishing spatially. This shrinking effect is something we often want to control. So, what’s the solution?\nThe Solution to this shrinking problem is to add padding around the input before sliding the filter. Usually, this padding consists of zeros. If we use P pixels of padding on each side, the effective input size becomes W + 2P. Then, applying a filter of size K, the output size becomes (W + 2P) - K + 1. So, our new formula for the output size is W - K + 1 + 2P. A very common setting for padding is to choose P = (K - 1) / 2. This is typically used when the filter size K is odd. If you plug this P into the output size formula W - K + 1 + 2P, you get W - K + 1 + 2 * (K - 1) / 2, which simplifies to W - K + 1 + K - 1, which equals W. This means that with this choice of padding, the output feature map has the same spatial size as the input feature map. This is often called “same” padding or “half” padding, and it’s very useful for building deep networks because it prevents the spatial dimensions from shrinking at each layer.\n\n\nReceptive fields\nNow, let’s talk about another important concept related to stacking convolution layers: Receptive Fields. The receptive field of a neuron in a convolutional network is the region in the input space (e.g., the original image) that a particular neuron “sees” or is affected by. For a single convolution layer with a kernel size K, each element in the output feature map depends on a K x K receptive field in the input to that layer.\n\nWhen we stack multiple convolution layers, the receptive field size grows. Each successive convolution adds K - 1 to the receptive field size (assuming stride 1). Consider the diagram: The purple output neuron in the third layer “sees” a 3x3 region in the orange layer (its direct input). Each of those orange neurons, in turn, sees a 3x3 region in the blue layer. So, the purple neuron’s receptive field in the blue layer is larger. More generally, with L layers, each using a KxK filter (and stride 1), the receptive field size in the original input is 1 + L * (K - 1). It’s important to be careful here: we distinguish between the receptive field in the input (meaning the original image) versus the receptive field in the previous layer. This growth of the receptive field is desirable because it allows neurons in deeper layers to capture information from larger and larger regions of the input image, enabling them to learn more global and abstract features. However, there’s a Problem: If we only use small KxK filters (like 3x3) and stride 1 convolutions, then for large images, we would need many, many layers for each output neuron to “see” the whole image, or at least a significant portion of it. For example, if K=3, each layer adds 2 to the receptive field size. To get a receptive field of, say, 100, you’d need roughly 50 layers. This can lead to very deep and computationally expensive networks if this is the only mechanism for increasing receptive fields. So, how do we address this problem of needing many layers for large receptive fields? One common solution is to downsample inside the network. If we reduce the spatial dimensions of the feature maps at certain points in the network, then subsequent convolution filters, even if they are spatially small (like 3x3), will cover a larger effective area of the original input image.\nOne way to downsample within the network and thus increase the effective receptive field size more quickly is by using Strided Convolution. In general, if the input has dimension W, the filter has dimension K, we’re using Padding P, and a Stride S, then the output dimension is given by the formula: (W - K + 2P) / S + 1. It’s important that (W - K + 2P) is divisible by S for this to work out cleanly without fractional pixels, or you need to decide on a rounding convention (floor or ceil). Most libraries will use a floor operation implicitly if it’s not perfectly divisible. So, strided convolutions give us a way to perform the convolution operation and downsample the feature map simultaneously. This is a very common technique used in many CNN architectures to reduce computational cost and increase receptive field sizes efficiently.\nOkay, let’s provide a Convolution Summary to bring all these definitions and formulas together.\nInput: A volume of size Cin x H x W. Hyperparameters that define the convolution layer:\n\nKernel size: KH x KW (often KH = KW = K, e.g., 3x3, 5x5).\nNumber of filters: Cout (this determines the depth of the output volume).\nPadding: P (number of zeros added to each side of the input spatial dimensions).\nStride: S (how many pixels the filter slides at each step).\n\nThe Weight matrix (or tensor) can be thought of as having dimensions Cout x Cin x KH x KW. This represents Cout filters, each of size Cin x KH x KW.\nThe Bias vector has dimension Cout (one bias per output filter/channel).\nThe Output size will be Cout x H’ x W’, where:\n\nH’ = (H - KH + 2P) / S + 1\nW’ = (W - KW + 2P) / S + 1\n\nSome common settings for these hyperparameters include:\n\nKH = KW: Using small, square filters is very common (e.g., 3x3, 5x5, sometimes 1x1).\nP = (K - 1) / 2: This results in “Same” padding, where the output spatial dimensions match the input (assuming S=1).\nCin, Cout: Often chosen as powers of 2 (e.g., 32, 64, 128, 256, 512) and typically increase as we go deeper into the network.\nK=3, P=1, S=1: A very common 3x3 convolution that preserves spatial resolution.\nK=5, P=2, S=1: A 5x5 convolution that preserves spatial resolution.\nK=1, P=0, S=1: This is a 1x1 convolution, which we’ll discuss separately as it has interesting properties.\nK=3, P=1, S=2: A 3x3 convolution that downsamples the input by a factor of 2 (approximately, depending on exact input size and rounding). This is often used to reduce spatial dimensions.\n\n\n\nPooling layers\nAlright, let’s move on to Pooling Layers. These provide another effective way to downsample feature maps, often used in conjunction with convolution layers. The primary purpose of a pooling layer is to reduce the spatial dimensions of the input volume. Importantly, pooling is applied independently to each depth slice of the input. So, given an input C x H x W, the pooling operation will downsample each 1 x H x W plane separately. The number of channels C remains unchanged by the pooling operation itself.\n\nThe Hyperparameters for a pooling layer are:\n\nKernel Size: This defines the spatial extent of the pooling window (e.g., 2x2).\nStride: This dictates how much the pooling window slides at each step (e.g., a stride of 2 is common with a 2x2 kernel for non-overlapping pooling).\nPooling function: This specifies the operation to perform within each pooling window. Common choices are max pooling or average pooling.\n\nA key property of pooling, especially max pooling, is that it gives some invariance to small spatial shifts in the input. If the exact location of a feature moves slightly within a pooling window, the output of max pooling might remain the same if the maximum value is still captured. Also, critically, pooling layers typically have no learnable parameters. The operation (max or average) is fixed.\nHere’s a Pooling Summary:\nInput: A volume C x H x W.\nHyperparameters:\n\nKernel size: K (e.g., 2 for a 2x2 pooling window).\nStride: S (e.g., 2).\nPooling function: Commonly ‘max’ or ‘avg’.\n\nOutput size: C x H’ x W’, where the formulas for H’ and W’ are the same as for convolution:\n\nH’ = (H - K) / S + 1 (assuming P=0, as padding is less common with pooling, though possible)\nW’ = (W - K) / S + 1\n\nAnd a crucial point: No learnable parameters. A very common setting is max pooling with K=2 and S=2. This effectively gives 2x downsampling of the spatial dimensions, halving the height and width of the feature map."
  },
  {
    "objectID": "posts/cnn-architectures/index.html#normalize-layers",
    "href": "posts/cnn-architectures/index.html#normalize-layers",
    "title": "CNN architectures",
    "section": "Normalize layers",
    "text": "Normalize layers\nSo, taking a broader view, we can identify the primary components of nearly all CNNs, We have our Convolution Layers, Pooling Layers, and typically, at the terminus of the network, one or more Fully-Connected Layers that perform the final classification. Interspersed throughout are the activation functions that introduce non-linearity. We also have regularization techniques like dropout, which we’ll discuss shortly. But now, I want to focus on a component that has become absolutely central to modern deep learning: Normalization Layers. Their introduction has been one of the key factors enabling the training of the very deep and high-performing networks we see today. They address some fundamental issues related to the optimization dynamics of deep models.\nTo develop our intuition, let’s begin with a specific example: Layer Normalization, or LayerNorm. The high-level idea behind it, and indeed behind most normalization layers, is a two-step process. First, you take the activations at some point in the network and you normalize them, typically to have a zero mean and unit variance. This step helps to mitigate the problem of “internal covariate shift,” where the distribution of each layer’s inputs changes during training as the parameters of the previous layers change. This can stabilize and accelerate the training process. However, rigidly enforcing a zero-mean, unit-variance distribution might be suboptimal. Perhaps the network would benefit from activations with a different mean or variance. Therefore, the second step is to introduce two new learnable parameters that allow the network to scale and shift the normalized data. This gives the network the expressive power to, if necessary, learn to reverse the normalization or, more generally, learn the optimal affine transformation for its activations\nConsider a mini-batch of N inputs, where each input x is a D-dimensional vector.\n\\[\n\\begin{align}\n\\mu, \\sigma &: N \\times 1 \\\\\n\\gamma, \\beta &: 1 \\times D \\\\\ny = &\\frac{\\gamma (x - \\mu)}{\\sigma} + \\beta\n\\end{align}\n\\]\nFor Layer Normalization, the key is how the statistics are computed. The mean, \\(\\mu\\), and standard deviation, \\(\\sigma\\), are calculated per batch element. As you can see, their dimension is N x 1. This means that for each individual training example, we compute its mean and standard deviation across all of its D features. Once the data is normalized, we apply the learned parameters: a scaling factor \\(\\gamma\\) and a shifting factor \\(\\beta\\). Both of these are D-dimensional vectors, meaning we learn a unique scale and shift for each feature dimension. These parameters are shared across all examples in the batch and are updated via backpropagation. The final output y is thus the normalized input, scaled by \\(\\gamma\\) and shifted by \\(\\beta\\).\n\nNow, LayerNorm is just one member of a family of normalization techniques. This visualization from the Group Normalization paper provides an excellent conceptual map of the different approaches. The blue region highlights the set of neurons over which the mean and standard deviation are calculated to normalize a single value. The most common variant in CNNs is Batch Normalization. Here, we normalize across the batch dimension (N) and the spatial dimensions (H, W), but we do so independently for each feature channel (C). Layer Normalization, which we just discussed, normalizes across all channel and spatial dimensions for a single example in the batch. It’s agnostic to the batch size, which can be advantageous. Instance Normalization is even more granular, normalizing over only the spatial dimensions for each channel and each batch example independently. This is often used in style transfer. And Group Normalization strikes a balance, normalizing over spatial dimensions and pre-defined groups of channels. The choice between these depends on the specific architecture, task, and practical considerations like batch size"
  },
  {
    "objectID": "posts/cnn-architectures/index.html#dropout",
    "href": "posts/cnn-architectures/index.html#dropout",
    "title": "CNN architectures",
    "section": "Dropout",
    "text": "Dropout\nNow, let’s focus specifically on Dropout. While incredibly influential and effective, especially in fully-connected layers, its use has become somewhat more nuanced with the rise of techniques like Batch Normalization, which itself provides a slight regularizing effect. Nevertheless, understanding Dropout is fundamental. So, what is the mechanism of Dropout? The idea, proposed by Srivastava and his colleagues in 2014 “Dropout: A simple way to prevent neural networks from overfitting”, is conceptually quite simple. During the training phase, for each forward pass through the network, we randomly set the activations of some neurons to zero. The probability of dropping a neuron, or conversely, the probability p of keeping a neuron, is a hyperparameter that we choose. A common value is 0.5, meaning half of the neurons in a given layer are randomly deactivated for each training example that passes through. The key here is that a different set of neurons is dropped for each forward pass. This means that for every mini-batch, the network is effectively a different, “thinned” version of the full architecture.\nThis raises a very natural question: How can randomly removing parts of your model possibly be a good idea? It seems counterintuitive, almost destructive. But there are a couple of powerful interpretations for why it works so well. The first interpretation is that it prevents the co-adaptation of features. In a standard network, it’s possible for a set of neurons to become highly dependent on one another. For instance, in classifying a cat, one neuron might become a very specific “has a tail” detector, and another neuron downstream might learn to only activate strongly when that specific tail-detector is active. This is a fragile dependency. Dropout breaks these dependencies. Since any neuron can be randomly dropped at any time, a given neuron cannot rely on the presence of any single one of its inputs. It is forced to learn more robust features that are redundant and useful in a variety of different contexts. It has to learn to use evidence from many input neurons, making the learned representation less brittle and more generalizable.\nThere is another, perhaps more powerful, interpretation. Dropout can be viewed as an efficient way to train a massive ensemble of neural networks. Every time we apply a different dropout mask, that is, a different pattern of dropped neurons, we are effectively training a unique, thinned sub-network. All of these distinct sub-networks, however, share the same underlying parameters. The scale of this ensemble is simply staggering. For a single fully-connected layer with 4096 units, there are 24096 possible dropout masks. This number is astronomically larger than the number of atoms in the universe. So, during training, we are sampling from this enormous set of models and taking a gradient step for each one. This provides an extremely potent form of model averaging, which is a well-established technique for improving performance and reducing overfitting.\nThis stochastic behavior during training introduces a critical consideration: What do we do at test time? At test time, we want a single, deterministic prediction. We want to leverage the full capacity of the network we’ve trained. So we do not drop any neurons, all the neurons are active. However this create a mismatch. During training the expected output of any given neuron was scaled down because it was only active with the probability of p. If we simply use the full network at test time, the magnitude of the activation will be systematically larger than the network experience during training, To correct for this, we must scale the activation at test time. Specifically, we multiply the output of each layer to which dropout was applied during training by the keep probability p. This ensure that the expected output of any neuron at test time matches the expected output during training, learning to well-calibrated prediction.\nHere’s the “vanilla implementation” of dropout:\np = 0.5\ndef train_step(X):\n  # forward pass for example 3-layer network\n  H1 = np.maximum(0, np.dot(W1, X) + b1)\n  U1 = np.random.rand(*H1.shape) &lt; p # first drop mask\n1  H1 *= U1 # drop!\n  H2 = np.maximum(0, np.dot(W2, H1) + b2)\n  U2 = np.random.rand(*H2.shape) &lt; p # second drop mask\n  H2 *= U2 # drop!\n  out = np.dot(W3, H2) + b3\n\n  # backward pass: compute gradients... (not show)\n  # parameter update... (not show)\n\n\ndef predict(X):\n  # ensembled forward pass\n2  H1 = np.maximum(0, np.dot(W1, X) + b1) * p # scale the activations\n  H2 = np.maximum(0, np.dot(W2, H1) + b2) * p # scale the activations\n  out = np.dot(W3, H2) + b3\n\n1\n\nWe have to distinct mode of operation. During the training step we first compute the activation, then generate a random mask and apply it, effectively setting some neuron to zero, this is the ‘drop in training time’ phase.\n\n2\n\nThen at test time, we perform the standard forward pass, but after each layer computation, we scale the result by a keep probability p, this is the ‘scale at test time’ phase .\n\n\nI should not that this is a “not recommended” implementation. A more common approach today is known as inverted dropout. In inverted dropout, the scaling is performed during training step by dividing the activation by p rather than at the test time. This has the practical advantage that the test time forward pass remain unchanged. Which simplify deployment. The net effect is the same, but the implement detail is important to be aware of."
  },
  {
    "objectID": "posts/cnn-architectures/index.html#activation-functions",
    "href": "posts/cnn-architectures/index.html#activation-functions",
    "title": "CNN architectures",
    "section": "Activation functions",
    "text": "Activation functions\nWe’ve talked about the operations that involve learnable parameters, like convolution and fully-connected layers. The activation function is the piece that follows these linear operations, and its role is profound. What is that role? The fundamental goal of an activation function is to introduce non-linearities into our model. This is not a minor detail; it is the very reason deep networks are powerful. If you were to stack any number of linear layers without any non-linearities in between, the entire network would collapse into a single, equivalent linear transformation. You would have a very deep, very computationally expensive linear classifier, which is no more expressive than the simple linear models It’s the non-linearity that allows the network to approximate arbitrarily complex functions and learn the hierarchical features that are the hallmark of deep learning.\nHere’s a little toy example of a double ReLU function you can play around with to gain a better understanding of why non-linearity is necessary. This uses just 2 ReLU functions, imagine hundreds of these! You could approximate any function by increasing the number of ReLUs and tuning their hyperparameters. The double ReLU function is defined as:\n\\[f(x) = a_1 \\cdot \\max(0, x - b_1) + a_2 \\cdot \\max(0, x - b_2)\\]\n\n\n\n\n\n\nDesign choice for scaling factor placement\n\n\n\nNote that I placed ai outside of the max function (rather than inside), which allows for negative slopes. For this educational demo, this provides more intuitive control and creates more visually interesting non-linear combinations.\n\n\n\nimport {viewof a1, viewof b1, viewof a2, viewof b2, viewof showComponents, chart} from \"https://observablehq.com/d/d00db79bf2aca3bf\"\n\n// Display the controls and chart\nviewof a1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nviewof b1\n\n\n\n\n\n\n\nviewof a2\n\n\n\n\n\n\n\nviewof b2\n\n\n\n\n\n\n\nviewof showComponents\n\n\n\n\n\n\n\nchart\n\n\n\n\n\n\n\n\nLet’s begin with a historically significant activation function: the Sigmoid.\n\\[\n\\sigma(x) = \\frac{1}{1 + e^{-x}}\n\\]\nIt has a very appealing property: it squashes any real-valued input into the range [0, 1]. This made it popular in the early days of neural networks because it provided a nice biological analogy to the “firing rate” of a neuron, which can be thought of as varying between a state of no activity (0) and maximum saturation (1).\n\nimport {sigmoidChart} from \"https://observablehq.com/d/d00db79bf2aca3bf\"\n\nsigmoidChart\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHowever, it harbors a key problem, one that severely hampered the training of deep networks. When you stack many layers of sigmoid neurons, you can encounter an issue with gradients. Let me pose this question to you: looking at the shape of the function, in which regions does the sigmoid have a very small gradient? As the input x become very large, either negative or positive, the sigmoid function saturates. Its output approaches either 1 or 0, and the curve becomes flat. In these flat regions, the local gradient is virtually zero. During backpropagation, the gradients from downstream layers are multiplied by these local gradients. If a neuron is in a saturated state, its local gradient will effectively “kill” or “vanish” the gradient signal passing through it. In a deep network with many sigmoid layers, this effect compounds, leading to the infamous vanishing gradient problem, where gradients in the early layers of the network become so small that the weights are barely updated, and learning grinds to a halt.\n\nimport {reluChart} from \"https://observablehq.com/d/d00db79bf2aca3bf\"\n\nreluChart\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis problem necessitated the search for better activation functions, which led to the widespread adoption of the Rectified Linear Unit, or ReLU. Its definition is elegantly simple: \\(f(x) = \\max(0, x)\\). It simply thresholds the input at zero. This function, introduced in the context of deep learning in the AlexNet paper, was a major breakthrough. It has several compelling advantages. First, and most importantly, it does not saturate in the positive region. For any positive input, the gradient is simply 1, allowing the gradient signal to flow unhindered during backpropagation, thus alleviating the vanishing gradient problem. Second, it is computationally trivial, it’s just a simple comparison to zero, it’s a very cheap operation just basically check the sign bit whether it on or off, which makes both the forward and backward passes very fast. The empirical result of these properties is dramatic: networks using ReLU often converge much faster than those using sigmoid or tanh. The AlexNet authors reported a 6x speedup in convergence on ImageNet. However, ReLU is not without its own set of issues. One issue is that its output is not zero-centered. This can introduce some undesirable dynamics during gradient descent, although this is often mitigated by techniques like Batch Normalization. A more significant annoyance is the Dead ReLU problem. Look at the negative region, for any input x &lt; 0, the output is 0, and importantly, the gradient is also 0. If a neuron, due to a large gradient update or poor initialization, gets pushed into a regime where its input is consistently negative, it will always output zero. The gradient flowing through it will also always be zero. Consequently, the weights feeding into that neuron will never again receive a gradient update. The neuron is effectively “dead” for the remainder of training, having become an inert part of the network.\n\nimport {reluGeluChart} from \"https://observablehq.com/d/d00db79bf2aca3bf\"\n\nreluGeluChart\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe quest to find an activation that combines the benefits of ReLU while mitigating its drawbacks is an active area of research. One prominent successor is the Gaussian Error Linear Unit, or GELU.\n\\[\nf(x) = x \\cdot \\Phi(x)\n\\]\nwhere \\(\\Phi(x)\\) is the cumulative distribution function of the standard Gaussian distribution. This means that GELU weights inputs based on their value, allowing for a smoother transition compared to other activation functions.\nGELU can be thought of as a smoother, probabilistic version of ReLU. As you can see from the plot, it closely tracks ReLU for positive values but smoothly curves below zero. It doesn’t have the hard zero-gradient “kink” that ReLU does. This smoothness around zero is empirically beneficial and can facilitate more stable training. Critically, it does not have a zero gradient for negative inputs, which helps avoid the Dead ReLU problem. However, this comes at a cost: it is more computationally expensive than the simple ReLU. And while it’s less prone to killing gradients, for large negative values, the gradient does still approach zero. GELU has become the standard in many state-of-the-art models, particularly in the domain of Transformers.\nAnd there are a lot activation functions out there like Leaky ReLU introduces a small, fixed slope for negative inputs to prevent neurons from dying. ELU uses an exponential function for negative inputs to push the mean activation closer to zero. And we’ve just discussed GELU. Another interesting one is SiLU, or Swish, which is x times the sigmoid of x, creating a non-monotonic function that dips slightly below zero before rising. The main takeaway here is not to memorize every single one, but to recognize that while ReLU remains a very strong and common default choice, the selection of an activation function is a design decision with trade-offs between performance, computational cost, and training stability.\nNow thinking about a standard CNN architecture… where are these activation functions actually used? They are generally placed immediately after the linear operators in the network. So, you would have a convolution layer, followed by an activation function. Or a fully-connected layer, followed by an activation function. Their role is to take the output of these linear transformations and inject the critical non-linearity before the data is passed to the next layer."
  },
  {
    "objectID": "posts/cnn-architectures/index.html#cnn-architectures",
    "href": "posts/cnn-architectures/index.html#cnn-architectures",
    "title": "CNN architectures",
    "section": "CNN architectures",
    "text": "CNN architectures\nThe next natural step is to see how these pieces are assembled into full-scale, effective CNN architectures. And to understand the evolution of these architectures, there is no better lens than the ImageNet Large Scale Visual Recognition Challenge, or ILSVRC.\n\nWhat you see here are the winning top-5 error rates on the ImageNet challenge from 2010 through 2017. In the pre-deep learning era of 2010 and 2011, the methods were based on shallow feature engineering, and the error rates were quite high, around 28% and 26%. Then, in 2012, something remarkable happened. AlexNet, an 8-layer convolutional neural network, entered the competition and dramatically reduced the error rate to 16.4%. This was the watershed moment that convinced the computer vision community of the power of deep learning. Following this, we see a clear and consistent trend: year after year, the error rates fall, while the network depths steadily increase. We go from 8 layers, to 19, to 22, and then in 2015, a truly massive leap to 152 layers with ResNet, which for the first time achieved an error rate lower than the estimated human performance on this task. This chart is, in essence, a story of the community learning how to successfully build and train progressively deeper neural networks. The revolution, as I mentioned, began in 2012 with AlexNet. This 8-layer network, building on many of the components we’ve discussed like ReLU and Dropout, demonstrated definitively that deep, learned features could vastly outperform hand-engineered ones. It set the stage for all the architectural development that followed. Today, we’re going to pick up the story in 2014. After AlexNet’s success, the immediate research question was, “If 8 layers are good, are more layers better?” The winning entries from 2014, VGGNet and GoogLeNet, answered this with a resounding yes. They pushed network depth from 8 layers to 19 and 22 layers, respectively, and were rewarded with another significant drop in error, from over 11% in the previous year down to the 7% range. Let’s start by taking a closer look at the VGG architecture.\n\nVGG\nVGGNet, from Simonyan and Zisserman at Oxford. The core philosophy behind VGG was to explore the effect of depth using an architecture that was remarkably simple. Their central idea was this: “Small filters, Deeper networks.”\n\nIf you look at the comparison with AlexNet on the left, you’ll see AlexNet used a mix of filter sizes, a large 11x11 filter in the first layer, followed by 5x5 filters. VGGNet, in contrast, made a radical design choice: it exclusively uses very small 3x3 convolutional filters throughout the entire network. This uniformity allowed them to stack these layers very deep, creating the 16 model shown here. The structure is a repeating motif: a block of two or three 3x3 convolutions, followed by a 2x2 max-pooling layer to reduce the spatial dimensions. By sticking to this simple rule, they went from 8 layers to 16, and with this added depth, they achieved a substantial improvement, reducing the top-5 error from 11.7% to 7.3%. This brings us to a critical design question. Why did they make this choice? Why use only these small 3x3 filters? Why not use a 5x5 or a 7x7 filter, which would have a larger receptive field and seemingly be able to capture larger spatial patterns in a single step? Let’s take a moment to consider the implications of this design.\nAlright, let’s analyze this question quantitatively. What is the effective receptive field of stacking three consecutive 3x3 convolution layers, assuming a stride of 1? There are two profound advantages. First, and arguably most important, the stacked approach is deeper and incorporates more non-linearities. Between each of the 3x3 convolutions, we place an activation function, like a ReLU. So, in the stacked version, we apply three non-linearities over that 7x7 receptive field. A single 7x7 convolution layer would only have one non-linearity. This increased non-linearity allows the model to learn more complex and discriminative features, which is a key benefit of depth. The second advantage is a significant reduction in the number of parameters. Let’s assume the number of channels per layer is C. A single 7x7 conv layer would have 7 * 7 * C * C = 49 * C2 parameters. A stack of three 3x3 conv layers has 3 * (3 * 3 * C * C) = 27 * C2 parameters. This is a substantial reduction, making the network more efficient and less prone to overfitting. So, the VGG design gives us more expressive power with fewer parameters, a clear win-win.\nThe VGGNet philosophy, that deeper is better, especially when done efficiently set the stage for what came next. This trend of increasing depth continued, but in 2015, we saw a jump that was qualitatively different from what came before. Kaiming He and his colleagues introduced the Residual Network, or ResNet, which had a staggering 152 layers. This wasn’t just a simple extrapolation; it was a fundamental architectural innovation that enabled training at depths previously thought impossible. This truly marks the “Revolution of Depth.”\n\n\nResnet\nThis architecture was born from a very simple and direct research question: What happens when we just continue stacking deeper and deeper layers on a “plain” convolutional network, like a VGG-style architecture? One might naively assume that performance should just continue to improve, or at worst, plateau. The reality, as we will see, is surprisingly different, and it revealed a fundamental optimization problem that ResNet was designed to solve.\nKaiming He and his colleagues took a plain network architecture, similar in style to VGG, and trained two versions: a “shallower” 20-layer model and a much deeper 56-layer model. Here are the results\n\n\n\nOn the left, we see the test error, and on the right, the training error. The blue line represents the 20-layer model, and the red line represents the 56-layer model.\n\n\nWhat we observe is something quite unexpected. The deeper 56-layer model performs worse than the 20-layer model. But what’s truly puzzling is that it performs worse not only on the test set, but also on the training set. The training error for the 56-layer model is higher than for the 20-layer model. This is a crucial observation. The fact that the deeper model has a higher training error means that this is not a problem of overfitting. If it were overfitting, we would expect the deeper model to achieve a very low training error by memorizing the training data, but then perform poorly on the test set. Here, the deeper model is failing to even fit the training data as well as its shallower counterpart. This phenomenon is known as the degradation problem.\nThis points to a fundamental difficulty. It is a fact that a deeper model, having more parameters, has strictly greater representational power than a shallower model. It can represent any function the shallower model can, plus many more. So, why does it perform worse? The hypothesis put forth by the ResNet authors is that this is not a representation problem, but an optimization problem. While the deeper model can theoretically represent better solutions, it is paradoxically much harder for our optimization algorithms, like stochastic gradient descent, to actually find those good parameter settings. The optimization landscape becomes much more complex and difficult to navigate.\nLet’s formalize this with a thought experiment. Consider a well-trained shallow model. Now, imagine we construct a deeper model. What should this deeper model learn to be, at the absolute minimum, at least as good as the shallow model? Well, there’s a simple solution by construction. The deeper model could simply copy the learned layers from the shallow model for its initial layers, and then set all the additional layers to simply be identity mappings. An identity mapping is a function that just passes its input through unchanged. If the extra layers do nothing, the deeper model will produce exactly the same output as the shallow model, and thus have the same error. Since we know a solution exists that is at least this good, the fact that SGD fails to find it implies that learning the identity mapping with a stack of non-linear layers is surprisingly difficult. This insight is the absolute core of ResNet.\n\nThe conventional approach, what the paper calls “plain” layers, is to have a stack of layers try to learn some desired underlying mapping, H(x). For example, H(x) might be the ideal features for the next stage of the network. The ResNet solution is to reframe the problem. Instead of asking the layers to learn H(x) directly, let’s change what they are learning. We introduce what’s called a “shortcut” or “skip” connection, which takes the input to the block, x, and adds it to the output of the block. The stack of layers is now only asked to learn a residual function, F(x). The final output of the block is H(x) = F(x) + x. Now, consider our identity mapping problem. How can this block learn to be an identity mapping, so that H(x) = x? It’s trivial. The network just needs to learn to set the output of the residual path, F(x), to zero. It can accomplish this by simply driving the weights of the convolutional layers in the block towards zero. This is a much easier optimization target for SGD than trying to learn an identity mapping through a complex stack of non-linear transformations. So, to be explicit, we are changing the objective of the building block. Instead of learning H(x) directly, we use the layers to fit the residual F(x) = H(x) - x. We are hypothesizing that it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping. The skip connection performs the identity mapping, and the stacked layers learn the “correction” or “residual” that needs to be applied. This formulation proved to be the key that unlocked the training of extremely deep neural networks.\n\n\nThe full ResNet architecture is essentially just a stack of these residual blocks. As you can see on the right, the network is composed of repeating modules. Each of these modules, or residual blocks, contains two 3x3 convolutional layers, which echoes the design philosophy of VGG that we just discussed. The crucial difference, of course, is the identity skip connection that bypasses these two layers and is added to their output before the final ReLU activation. The entire network, from start to finish, is constructed by composing these fundamental building blocks one after another.\nNow, a critical detail in any deep CNN is how to handle the changes in spatial resolution and channel depth. A network can’t maintain the same spatial dimensions throughout, as that would be computationally intractable and would fail to build a hierarchy of features. ResNet addresses this in a very systematic way. Periodically, at the beginning of a new “stage” of the network, it does two things simultaneously: it doubles the number of filters, and it downsamples the feature map spatially. This downsampling is achieved not by a pooling layer, but by setting the stride of the first 3x3 convolution in that block to 2. This, of course, creates a dimensionality mismatch for the addition: the identity x has half the spatial resolution and half the channel depth of the output of the convolutional path F(x). To resolve this, when downsampling occurs, the skip connection is also modified. It typically consists of a 1x1 convolution with a stride of 2, which serves to downsample x and project it to the new, higher channel dimension, so that the element-wise addition can be performed. This is a very clean and effective way to manage the tensor dimensions as we go deeper into the network. Finally, there’s one more piece. Before the main stack of residual blocks, the network begins with an initial convolutional layer, sometimes called the “stem.” In the case of ResNet, this is a large 7x7 convolution with a stride of 2, followed by a max-pooling layer. The purpose of this stem is to aggressively reduce the spatial dimensions and quickly extract low-level features like edges and blobs from the input image before it enters the more complex residual stages.\n\n\n\n\nThe elegance of this modular design is its scalability. By simply deciding how many residual blocks to stack in each stage of the network, the authors could easily construct a family of architectures of varying depths. The paper presented models of 18, 34, 50, 101, and the flagship 152-layer network for the ImageNet challenge. This systematic and principled approach to increasing depth was a key contribution. And the results were nothing short of revolutionary. The ability to train these very deep network using residual connections led to a new state-of-the-art. Their 152-layer model won the ILSVRC 2015 classification competition with a top-5 error of just 3.57%, which was a remarkable improvement and the first time a model surpassed the reported human-level performance benchmark on this dataset. The impact of ResNet extended far beyond image classification. The features learned by this architecture were so powerful and general that ResNet-based models swept nearly all major classification and detection competitions in 2015. For several years following its publication, the ResNet architecture became the de facto standard backbone for a vast array of computer vision tasks."
  },
  {
    "objectID": "posts/cnn-architectures/index.html#weight-initialize",
    "href": "posts/cnn-architectures/index.html#weight-initialize",
    "title": "CNN architectures",
    "section": "Weight initialize",
    "text": "Weight initialize\nWe’ve discussed the layers, the activations, and the architectural patterns. Now, we must address the crucial, and often overlooked, topic of Weight Initialization. How we set the initial values of the network’s parameters is not a trivial detail, it can be the difference between a network that trains successfully and one whose gradients either vanish or explode. How should we initialize the weights in our neural network layers? It seems like a minor detail, but as we’ll see, it has profound implications for the trainability of deep models. Let’s explore this with a concrete example.\ndims = [4096] * 7\nhs = []\nx = np.random.randn(16, dims[0])\n\nfor Din, Dout in zip(dims[:-1], dims[1:]):\n  W = 0.01 * np.random.randn(Din, Dout) # small weight init\n  x = np.maximum(0, np.dot(x, W)) # ReLU activation\n  hs.append(x)\n\nHere we have a simple Python snippet that simulates the forward pass through a 6-layer deep network. Each hidden layer has 4096 neurons, and we’re using a ReLU activation function. We’ll start with a naive initialization strategy: initializing the weights from a standard normal distribution, and then scaling them down by a small constant factor, in this case, 0.01. This seems plausible; we want the initial weights to be small to avoid starting in a highly non-linear, saturated regime. But let’s look at what happens. we see histograms of the activations in each of the six layers after a single forward pass with random input data. In the first layer, the activations have a reasonable distribution. But by the second layer, the mean and standard deviation have shrunk considerably. By the third, even more so. As we propagate through the network, the activations progressively collapse towards zero. By the time we reach the final layers, nearly all the activations are zero. What is the consequence of this for learning? If all activations are zero, what will the gradients be during the backward pass? They will also be zero. This is a form of the vanishing gradient problem induced not by the activation function itself, but by poor weight initialization. The signal dies as it propagates forward, and the gradient dies as it propagates backward. The network will not learn.\nOkay, so maybe our initial scaling factor was too small. Let’s try making the weights a bit larger. We’ll change the scaling factor from 0.01 to 0.05. A modest increase.\ndims = [4096] * 7\nhs = []\nx = np.random.randn(16, dims[0])\n\nfor Din, Dout in zip(dims[:-1], dims[1:]):\n  W = 0.05 * np.random.randn(Din, Dout) # large weight init\n  x = np.maximum(0, np.dot(x, W)) # ReLU activation\n  hs.append(x)\n\nThe result is just as catastrophic, but in the opposite direction. Now, looking at the activation statistics, we see the mean and standard deviation exploding as we move through the network. The activations are pushed far into the positive regime of the ReLU. While this doesn’t cause saturation in the same way as a sigmoid, this rapid growth in magnitude leads to extremely large gradients during the backward pass. This is the exploding gradient problem. It can cause the weight updates to be so large that the optimization process becomes unstable, with the loss oscillating wildly or diverging to infinity.\nSo, we’re in a bit of a Goldilocks situation. We need an initialization that is not too small and not too large. The key insight is that the correct scaling factor depends on the size of the layer. Specifically, it depends on the number of input neurons to the layer, which we often call the “fan-in.” The variance of the output of a linear layer is proportional to the variance of the input times the number of input connections times the variance of the weights. To keep the variance of the activations constant as we pass through the network, we need to scale our weight initialization to counteract the effect of the fan-in. A principled way to do this, specifically for networks using ReLU activations, was proposed in the same year as ResNet by Kaiming He and colleagues. Their solution, often called “Kaiming initialization” or “MSRA initialization,” is to scale the weights by the square root of 2 divided by the fan-in (Din).\ndims = [4096] * 7\nhs = []\nx = np.random.randn(16, dims[0])\n\nfor Din, Dout in zip(dims[:-1], dims[1:]):\n  W = np.random.randn(Din, Dout) * np.sqrt(2 / Din)\n  x = np.maximum(0, np.dot(x, W))\n  hs.append(x)\n\nWhen we use Kaiming initialization, the result is remarkable. Looking at the histograms, we see that the distribution of activations remains stable across all six layers. The mean and standard deviation are preserved as the signal propagates through the deep network. This is precisely what we want. It ensures that all layers have a healthy flow of information and receive meaningful gradients, which is essential for successful training. This type of principled initialization has become standard practice and is a critical component for training deep architectures from scratch."
  },
  {
    "objectID": "posts/cnn-architectures/index.html#data-preparation",
    "href": "posts/cnn-architectures/index.html#data-preparation",
    "title": "CNN architectures",
    "section": "Data preparation",
    "text": "Data preparation\n\nData preprocessing\nWe’ve built our network, but now we need to train it. We will now focus on the practical methodologies for how to train CNNs, starting with data preprocessing. For image data, the standard preprocessing step is normalization. Here is the TLDR. The universal practice in modern deep learning is to center and scale the data for each channel independently. This means we first compute the mean and standard deviation of the pixel values across the entire training dataset, but we do this separately for the Red, Green, and Blue channels. This gives us three mean values and three standard deviation values. Then, for every image we feed into the network (both at training and test time), we subtract the corresponding per-channel mean from each pixel and divide by the per-channel standard deviation. This process ensures that the input data for each channel has approximately zero mean and unit variance. This is crucial for stable training, as it puts the data into a well-behaved numerical range, which helps with gradient flow and prevents the first layer from having to learn to adapt to arbitrarily scaled inputs. Note that this requires a pre-computation step on your training set before you begin the main training loop.\n\n\nData augmentation\nWe will now discuss Data Augmentation. This is an extremely powerful and widely used technique for improving the generalization performance of your model, effectively a form of regularization. Before we look at specific data augmentation techniques, I want to highlight a common pattern that underlies many forms of regularization in deep learning. The general pattern is this: during the training phase, we introduce some form of stochasticity or randomness into the process. The model’s output y is a function not only of the weights W and the input x, but also of some random variable z.\n\\[\ny = f_W(x, z)\n\\]\nThen, at test time, we want a deterministic prediction. The strategy here is to marginalize out, or average over, this randomness. We want to compute the expectation of the model’s output over the distribution of the random variable z. In practice, this often involves an approximation, such as sampling or using an analytical trick.\n\\[\ny = f(x) = E_z\\left[f(x, z)\\right] = \\int p(z)f(x, z)dz\n\\]\nWe’ve already seen a perfect example of this pattern: Dropout. During training, the randomness comes from the binary mask that randomly drops activations. At testing, we average out this randomness. The scaling of activations by the keep probability p that we discussed is a clever analytical way to compute the exact expected output of the ensemble of all possible sub-networks. So, training involves random dropping, and testing involves averaging. Data augmentation fits this regularization pattern perfectly. The core idea is to artificially enlarge the training dataset by applying random, label-preserving transformations to the input images. During training, for each image we load, we apply a random transformation—a slight rotation, a crop, a color shift—and then we feed this transformed image to the CNN to compute the loss. This forces the network to learn features that are invariant to these transformations. The source of randomness, z, is the choice of transformation.\n\nLet’s look at some common examples. The simplest and one of the most effective is the horizontal flip. For most object categories in natural images, like this cat, the semantic label is invariant to a horizontal flip. A cat is still a cat when mirrored. So, during training, we can randomly flip each image horizontally with a 50% probability. This effectively doubles the size of our training set and teaches the model that left-right orientation is not a distinguishing feature for this class.\nA more sophisticated and extremely powerful technique involves random crops and scales. The procedure described here is from the original ResNet paper. During training, you first randomly pick a scale L from a given range. You resize the image so its shorter side is L, and then you sample a random 224x224 patch from this resized image. This teaches the model to be robust to both variations in object scale and position within the frame. Then, at test time, we follow the pattern of averaging out the randomness. We perform what is called test-time augmentation (TTA). Instead of a single random crop, we create a deterministic, fixed set of crops. For example, we might resize the image to 5 different scales, and for each scale, we take 10 crops: one from the center, one from each of the four corners, and then the horizontal flips of all five. We run all 50 of these crops through the network and average their final predictions to get a single, robust prediction for the image.\nAnother very common technique is Color Jitter. The exact same object can appear very different under varying lighting conditions. To make our model robust to this, we can randomly perturb the color properties of the image during training. Simple approaches involve randomly adjusting the contrast and brightness of the image. More complex methods can involve perturbations in the PCA space of the RGB values, as was done in the AlexNet paper.\nWe can also think of regularization techniques that operate directly on the image space, analogous to Dropout. Techniques like Cutout, or Random Erasing, involve setting random rectangular regions of the input image to zero (or some other constant value). This forces the network to look at the entire object and not become overly reliant on one specific, salient feature. For example, to recognize this cat, it can’t just rely on seeing the eye; it has to learn to use information from the ears, the fur texture, and the overall shape, because any one of those features might be occluded by the random patch. This works very well, especially for smaller datasets like CIFAR where overfitting is a major concern."
  },
  {
    "objectID": "posts/cnn-architectures/index.html#transfer-learning",
    "href": "posts/cnn-architectures/index.html#transfer-learning",
    "title": "CNN architectures",
    "section": "Transfer learning",
    "text": "Transfer learning\nThis is arguably one of the most important concepts in the practical application of deep learning today: Transfer Learning. This brings us to a very pragmatic question. Training these large models, like the ResNets we just discussed, on a dataset like ImageNet requires immense computational resources and vast amounts of labeled data. So, what do you do in a more common scenario? What if you don’t have a lot of data? Can you still leverage the power of these deep CNNs for your specific problem? The answer is a definitive yes, and the mechanism for doing so is Transfer Learning. The fundamental intuition behind transfer learning in computer vision is that the features learned by a network trained on a large, diverse dataset are often useful for other, related tasks.\n\nIf we visualize the learned filters from the very first convolutional layer of AlexNet, we see that they are not random noise. The network has learned to detect fundamental visual primitives like oriented edges, color blobs, and other Gabor-like patterns. These low-level features are not specific to the 1000 classes of ImageNet; they are generic building blocks for visual understanding. They are likely to be useful for almost any computer vision task. This principle holds true even as we go deeper into the network. If we take the feature vector from one of the last layers of a pre-trained network—in this case, the 4096-dimensional vector before the final classifier—and we look for nearest neighbors in this feature space, we see something remarkable. The feature space is semantically organized. A test image of a flower is closest to other images of flowers. An elephant is closest to other elephants. An aircraft carrier is closest to other aircraft carriers. This demonstrates that the network has learned a rich, high-level representation of the visual world that captures semantic similarity. The core idea of transfer learning is to leverage this pre-existing knowledge.\nSo, here is the standard workflow for transfer learning. Step one is performed once by the broader research community. A very large model, like a ResNet, is trained on a massive, diverse dataset like ImageNet, which contains millions of images across a thousand categories. This is a computationally intensive process that can take weeks on many GPUs. The result is a set of “pre-trained” weights. Now, let’s say you have a new task, for which you only have a small dataset. Perhaps you want to classify between 10 different types of flowers, and you only have a few hundred examples. The strategy is as follows: you take the pre-trained network, and you freeze the weights of all the convolutional layers. You treat this part of the network as a fixed feature extractor. Then, you remove the original final fully-connected layer (which was trained to classify 1000 ImageNet classes) and replace it with a new, randomly initialized fully-connected layer that has the correct number of outputs for your new task (e.g., 10 outputs for 10 flower classes). You then train only this new final layer on your small dataset. Since you are only training a small number of parameters, this is much less prone to overfitting.\nNow, consider a different scenario. What if you have a bigger dataset for your new task? Perhaps you have tens of thousands of images. In this case, you have enough data to do more than just train the final layer. The strategy here is to initialize the entire network with the pre-trained weights, replace the final layer as before, and then finetune the entire model. This means you allow backpropagation to update all the weights in the network, but you typically do so with a much smaller learning rate than you would use for training from scratch. This allows the pre-trained features to be gently adapted and specialized for your new task, often leading to better performance than simply freezing the feature extractor.\nSo, what is the key takeaway for any applied deep learning you do in the future? If you have a dataset of interest, and it’s smaller than the massive, web-scale datasets, you should almost always leverage transfer learning. You should find a model pre-trained on a large, similar dataset (ImageNet is the default for natural images) and then transfer that knowledge to your specific task using the strategies we’ve discussed. You don’t need to train these large models yourself. Deep learning frameworks like PyTorch and platforms like Hugging Face provide a “Model Zoo” of pre-trained models that you can download and use with just a few lines of code. This is an incredibly powerful paradigm that has democratized access to high-performing computer vision models."
  },
  {
    "objectID": "posts/cnn-architectures/index.html#hyperparameter-selection",
    "href": "posts/cnn-architectures/index.html#hyperparameter-selection",
    "title": "CNN architectures",
    "section": "Hyperparameter selection",
    "text": "Hyperparameter selection\nWe’ve built the model, we have the data, but there are still many “knobs” to turn, learning rates, weight decay, dropout probabilities, architectural choices. This brings us to the crucial, and often challenging, task of Hyperparameter Selection. This is often more of an art guided by scientific principles than a rigid science itself. I’m going to provide you with a practical, step-by-step workflow for approaching this problem. Here is a sequence of steps that will serve you well.\nStep 1: First, some initial sanity checks. Check your initial loss. Before you even start training, do a single forward pass and make sure the loss is what you’d expect. For a softmax classifier on a C-class problem, the initial loss should be around log(C). If it’s not, something is likely wrong with your initialization or loss function implementation.\nStep 2: Overfit a small sample. Take a tiny subset of your data, maybe just a few mini-batches, and try to train your network to 100% accuracy on that small set. The goal here is to prove that your model and your optimization setup are capable of learning something. If you can’t even overfit a tiny slice of data, you have a bug somewhere.\nStep 3: Find a learning rate that makes the loss go down. Once you can overfit a small sample, take your full dataset, turn on a small amount of regularization (weight decay), and experiment with a wide range of learning rates—from 1e-1 down to 1e-5. All you’re looking for here is a learning rate that causes the loss to drop reliably within the first hundred or so iterations. This gives you a reasonable starting point.\nOnce you have a viable learning rate, you can move on to a more systematic search.\nStep 4: Perform a coarse search over a grid of hyperparameters. Now you’ll be tuning not just the learning rate, but also regularization strength, perhaps dropout probability, and other architectural choices. Sample these values, and for each combination, train for just a few epochs—maybe 1 to 5. You’re just trying to identify promising regions in the hyperparameter space.\nStep 5: Refine the grid. Take the best-performing hyperparameter settings from your coarse search and perform a more focused search in that narrower region, and this time, train the models for a longer duration.\nStep 6: which is crucial throughout this whole process, is to look at the loss and accuracy curves. These plots are your primary diagnostic tool for understanding what’s happening during training.\nLet’s look at some examples.\n\n\nHere you see the training accuracy in blue and the validation accuracy in orange. Both curves are steadily increasing, and there’s a relatively small gap between them. This is the ideal scenario. It tells you that your model is still learning and has not yet plateaued. The prescription here is simple: you just need to train for longer.\n\n\n\n\n\n\nNow, consider this case. What is happening here? The training accuracy continues to climb, but the validation accuracy, after an initial increase, starts to decrease. This is the classic signature of overfitting. The large and growing gap between the training and validation accuracy means your model is learning to memorize the training set, but this knowledge is not generalizing to unseen data. The solution here is to increase regularization. This could mean adding more weight decay, increasing the dropout rate, or using more aggressive data augmentation. Alternatively, if possible, getting more training data is often the most effective remedy for overfitting.\n\n\n\n\n\n\nAnd finally, what about this scenario? Here, the training and validation accuracy are very close to each other, but both have flattened out at a suboptimal level. There’s no gap, which means the model is not overfitting. This is a sign of underfitting. Your model does not have enough capacity to capture the complexity of the data. The potential solutions are to train longer (though it looks like it’s already converged), or more likely, you need to use a bigger, more powerful model—for instance, moving from a ResNet-18 to a ResNet-34.\n\n\n\n\nStep 7: This process is inherently iterative. You follow these steps, you look at your loss curves, you diagnose the problem, and that diagnosis informs the next action. Often, after refining your grid and training longer (Step 5), you’ll look at the curves (Step 6), and they will tell you that you’re now overfitting. That’s a GOTO Step 5: you go back, refine your regularization parameters, and train again. It’s a cycle of experimentation and analysis.\nWhen you’re performing these hyperparameter searches, how should you sample the values? The traditional approach is Grid Search, where you define a fixed grid of values for each parameter. However, a paper by Bergstra and Bengio showed that Random Search is almost always more efficient.\n\nThe reason is that not all hyperparameters are equally important. As shown in these diagrams, some parameters have a much larger impact on performance than others. With a grid layout, you are testing each “unimportant” parameter value multiple times, which is wasteful. With a random layout, you are effectively testing a unique value for each hyperparameter in every trial. This allows you to explore the hyperparameter space much more effectively for the same computational budget, making it more likely you’ll find a good combination."
  },
  {
    "objectID": "posts/neural-network-and-backprobagation/index.html",
    "href": "posts/neural-network-and-backprobagation/index.html",
    "title": "Neural network and backpropagation",
    "section": "",
    "text": "“Deep learning” this is the term you hear everywhere now, and it’s really transformed not just computer vision, but many areas of artificial intelligence. And the progress, especially in the last few years, has been absolutely outstanding. Let’s just take a moment to appreciate some of the incredible capabilities that deep learning models have unlocked, particularly in the realm of image generation and understanding.\n\n\n\nImages generated by DALL-E 2 on Sam Altman tweet and https://openai.com/dall-e-2/\n\n\nThese images are generated by DALL-E 2. These are not photographs; they are synthesized by an AI from text prompts. The level of detail, the coherence of the scenes, and the creativity are just remarkable. These models are clearly understanding complex relationships between concepts and visual elements. In 2022 Ramesh et al. released “Hierarchical Text-Conditional Image Generation with CLIP Latents” give us a glimpse into how some of these text-to-image models work. So basically it involves an image encoder, a text encoder, a prior model that learns to associate these representations, and then a decoder to generate the image, the CLIP objective, which learns joint embeddings of images and text, plays a crucial role here.\n\n\n\nImages from “Improving image generation with better captions” paper (2023)\n\n\nMore recently, we’ve seen DALL-E 3. These models are getting exceptionally good at interpreting nuanced textual descriptions. Notice the image on the right, the model not only generates the individual elements but also composes them into a coherent scene with multiple interacting characters, and it even captures the specified “graphic novel” style. You can see how it even annotates which parts of the image correspond to which parts of the prompt.\n\n\n\nThinking with images (image from https://openai.com/index/introducing-o3-and-o4-mini/)\n\n\nThis isn’t just about image generation. Now with o3 and o4 mini, these model can integrate images directly into their chain of thought which they call “Visual reasoning in action”, they can reason, think about the image. This shows a deep level of visual reasoning.\n\nThinking with images allows you to interact with ChatGPT more easily. You can ask questions by taking a photo without worrying about the positioning of objects, whether the text is upside down or there are multiple physics problems in one photo. Even if objects are not obvious at first glance, visual reasoning allows the model to zoom in to see more clearly.\n\n\n\n\nSegment Anything Model (SAM)\n\n\nWe also have models like the Segment Anything Model (SAM). SAM is remarkable because it can generate segmentation masks for any object in an image, just from a prompt or a click. Look at the density of masks it can produce, it can segment hundreds, even thousands of objects and parts of objects within a single image. This has huge implications for image editing, robotics, and a wide range of computer vision tasks.\n\nAnd then we move into video. This is an example from Sora, a recent model from OpenAI that generates video from text. The quality, coherence over time, and realism are truly groundbreaking. Sora can do things like animating images that were themselves generated by models like DALL-E. It can also do video-to-video editing, where you provide an input video and a text prompt, and it modifies the video accordingly. The model understands the content of the video and can plausibly alter its style or elements based on text.\n\n\n\nBase compute\n\n\n\n4x compute\n\n\n\n32x compute\n\n\nAnd a key theme, as with many of these large generative models, is the impact of more compute. These video show the output of Sora for the same prompt, but with increasing amounts of compute used for generation. You can clearly see how the quality, detail, and realism improve significantly as more computational resources are applied. This scaling hypothesis that bigger models and more data, trained with more compute, lead to better performance has been a driving force in deep learning.\nSo, these examples are just a taste of what deep learning is achieving. It’s creating tools that can understand, generate, and manipulate visual information in ways that were science fiction just a decade ago. The common thread underlying all these incredible advancements is the use of neural networks, often very deep ones, trained on massive datasets. And that’s what we’re here to understand today: what are these neural networks, and how do we train them?"
  },
  {
    "objectID": "posts/neural-network-and-backprobagation/index.html#deep-learning-today",
    "href": "posts/neural-network-and-backprobagation/index.html#deep-learning-today",
    "title": "Neural network and backpropagation",
    "section": "",
    "text": "“Deep learning” this is the term you hear everywhere now, and it’s really transformed not just computer vision, but many areas of artificial intelligence. And the progress, especially in the last few years, has been absolutely outstanding. Let’s just take a moment to appreciate some of the incredible capabilities that deep learning models have unlocked, particularly in the realm of image generation and understanding.\n\n\n\nImages generated by DALL-E 2 on Sam Altman tweet and https://openai.com/dall-e-2/\n\n\nThese images are generated by DALL-E 2. These are not photographs; they are synthesized by an AI from text prompts. The level of detail, the coherence of the scenes, and the creativity are just remarkable. These models are clearly understanding complex relationships between concepts and visual elements. In 2022 Ramesh et al. released “Hierarchical Text-Conditional Image Generation with CLIP Latents” give us a glimpse into how some of these text-to-image models work. So basically it involves an image encoder, a text encoder, a prior model that learns to associate these representations, and then a decoder to generate the image, the CLIP objective, which learns joint embeddings of images and text, plays a crucial role here.\n\n\n\nImages from “Improving image generation with better captions” paper (2023)\n\n\nMore recently, we’ve seen DALL-E 3. These models are getting exceptionally good at interpreting nuanced textual descriptions. Notice the image on the right, the model not only generates the individual elements but also composes them into a coherent scene with multiple interacting characters, and it even captures the specified “graphic novel” style. You can see how it even annotates which parts of the image correspond to which parts of the prompt.\n\n\n\nThinking with images (image from https://openai.com/index/introducing-o3-and-o4-mini/)\n\n\nThis isn’t just about image generation. Now with o3 and o4 mini, these model can integrate images directly into their chain of thought which they call “Visual reasoning in action”, they can reason, think about the image. This shows a deep level of visual reasoning.\n\nThinking with images allows you to interact with ChatGPT more easily. You can ask questions by taking a photo without worrying about the positioning of objects, whether the text is upside down or there are multiple physics problems in one photo. Even if objects are not obvious at first glance, visual reasoning allows the model to zoom in to see more clearly.\n\n\n\n\nSegment Anything Model (SAM)\n\n\nWe also have models like the Segment Anything Model (SAM). SAM is remarkable because it can generate segmentation masks for any object in an image, just from a prompt or a click. Look at the density of masks it can produce, it can segment hundreds, even thousands of objects and parts of objects within a single image. This has huge implications for image editing, robotics, and a wide range of computer vision tasks.\n\nAnd then we move into video. This is an example from Sora, a recent model from OpenAI that generates video from text. The quality, coherence over time, and realism are truly groundbreaking. Sora can do things like animating images that were themselves generated by models like DALL-E. It can also do video-to-video editing, where you provide an input video and a text prompt, and it modifies the video accordingly. The model understands the content of the video and can plausibly alter its style or elements based on text.\n\n\n\nBase compute\n\n\n\n4x compute\n\n\n\n32x compute\n\n\nAnd a key theme, as with many of these large generative models, is the impact of more compute. These video show the output of Sora for the same prompt, but with increasing amounts of compute used for generation. You can clearly see how the quality, detail, and realism improve significantly as more computational resources are applied. This scaling hypothesis that bigger models and more data, trained with more compute, lead to better performance has been a driving force in deep learning.\nSo, these examples are just a taste of what deep learning is achieving. It’s creating tools that can understand, generate, and manipulate visual information in ways that were science fiction just a decade ago. The common thread underlying all these incredible advancements is the use of neural networks, often very deep ones, trained on massive datasets. And that’s what we’re here to understand today: what are these neural networks, and how do we train them?"
  },
  {
    "objectID": "posts/neural-network-and-backprobagation/index.html#what-is-neural-network",
    "href": "posts/neural-network-and-backprobagation/index.html#what-is-neural-network",
    "title": "Neural network and backpropagation",
    "section": "What is neural network?",
    "text": "What is neural network?\nWe’re going to build these up step-by-step. And perhaps surprisingly, we’ll start with something very familiar. Think about our original linear classifier. Before, when we talked about linear score functions, we had this simple equation: \\(f(x) = Wx\\). Here \\(x\\) is our input vector, say an image flattened into a D-dimensional vector so \\(x\\) is in \\(\\mathbb{R}^D\\). And \\(W\\) is our weight matrix, of size C x D, where C is the number of classes. This matrix \\(W\\) projects our D-dimensional input into a C-dimensional space of class scores. This is the simplest possible “network” if you like - a single linear layer. Now, how do we make this more powerful? How do we build a neural network with, say, 2 layers? We take our original linear score function, \\(Wx\\), and we are going to insert something in the middle. Now our new equation look like this:\n\\[\nf(x) = W_2  \\max(0, W_1x)\n\\]\nWe’re still starting with our input \\(x\\). We first multiply it by a weight matrix, let’s call it \\(W_1\\). So we compute \\(W_1x\\). If x is \\(\\mathbb{R}^D\\), then \\(W_1\\) will be a matrix of size H x D. H here represents the number of neurons or hidden units in this first layer. So, \\(W_1x\\) gives us an H-dimensional vector. Then we apply a non-linear function. Here we’re using \\(max(0, ...)\\) which we will talk about this for a moment. The output of this non-linearity is now an H dimensional vector, we then multiply this vector by a second weight matrix, \\(W_2\\). This \\(W_2\\) matrix will have dimensions C x H, where C is still our number of output classes. So, \\(W_2\\) takes the H-dimensional output of the first layer and maps it down to our C class scores. In practice we will usually add a learnable bias at each layer as well. So, more accurately, the operations would look like \\(W_1x + b_1\\) and \\(W_2h + b_2\\) (where h is the output of the first layer). We often absorb the bias into the weight matrix by augmenting \\(x\\) with a constant 1, as we’ve discussed before, or handle it as a separate bias vector. For now, we’ll keep the notation a bit cleaner by omitting explicit biases, but remember they’re typically there.\n\nConsider this example, We have some data points in a 2D space. The red points form a cluster in the center, and the blue points surround them. Can we separate these red and blue points with a linear classifier? No, you can’t draw a single straight line in this x, y space that perfectly separates the red from the blue. A linear classifier, by definition, can only learn linear decision boundaries. But what if we could transform our input space? This is where the idea of feature transformations comes in, and it’s closely related to what neural networks do. Imagine we apply a function \\(f(x, y)\\) that maps our original Cartesian coordinates (x, y) to polar coordinates (\\(r(x,y)\\), \\(\\theta(x,y)\\)). So, \\(r\\) is the radius from the origin, and \\(\\theta\\) is the angle. Now in this new \\((r, \\theta)\\) space, shown on the right. The red points, which were close to the origin, now have small \\(r\\) values. The blue points, which were further out, now have larger \\(r\\) values. In this transformed space, the red and blue points are linearly separable! We can now draw a simple horizontal line (a constant \\(r\\) value) that separates them.\nThis is the core idea. The non-linearities in a neural network allow the network to learn these kinds of powerful feature transformations. The first layer \\(W_1x\\) followed by the \\(\\max(0, ...)\\) non-linearity effectively computes a new representation of the input. And then the next layer \\(W_2\\) operates on this transformed representation. By stacking these layers with non-linearities, the network can learn increasingly complex and abstract features that make the original problem (like classification) easier, often making it linearly separable in some high-dimensional learned feature space."
  },
  {
    "objectID": "posts/neural-network-and-backprobagation/index.html#neural-network-architectures",
    "href": "posts/neural-network-and-backprobagation/index.html#neural-network-architectures",
    "title": "Neural network and backpropagation",
    "section": "Neural network architectures",
    "text": "Neural network architectures\nYou’ll hear these kinds of networks, \\(f = W_2  \\max(0, W_1x)\\), referred to by a few names. While “Neural Network” is a very broad term, the specific architecture we’re discussing here is often more accurately called a “fully-connected network”. This is because, as we’ll see when we visualize them, every neuron (or unit) in one layer is connected to every neuron in the subsequent layer. Another term you might encounter, especially in older literature, is “multi-layer perceptron” or MLP. For our purposes, when we talk about a basic neural network with these stacked layers of matrix multiplies and non-linearities, we’re generally talking about a fully-connected network. And remember, we usually add learnable biases at each layer.\nNow, there’s nothing stopping us from making these networks deeper. We can extend this to 3 layers, or indeed many more. Our 2-layer network was \\(f = W_2  \\max(0, W_1x)\\). A 3-layer neural network simply inserts another layer of weights and non-linearity:\n\\[\n\\begin{align}\nf &= W_3  \\max(0, W_2  \\max(0, W_1x)) \\\\\nx &\\in \\mathbb{R}^D, W_1 \\in \\mathbb{R}^{H_1 \\times D}, W_2 \\in \\mathbb{R}^{H_2 \\times H_1}, W_3 \\in \\mathbb{R}^{C \\times H_2}\n\\end{align}\n\\]\nA quick note on terminology: when we say an “N-layer neural network,” we typically mean a network with N layers of weights, or N-1 hidden layers plus an output layer. So, our 2-layer Neural Network here has one hidden layer (the W_1 transformation produces its activations), and our 3-layer Neural Network has two hidden layers (activations produced after \\(W_1\\) and after \\(W_2\\)). The input is sometimes called the input layer, but it doesn’t usually involve learnable weights or non-linearities in the same way.\n\nWe can visualize this as a form of hierarchical computation. So what’s the advantage of this hidden layer h? Think back to our linear classifier, \\(f = Wx\\). Each row of \\(W\\) could be interpreted as a “template” for one of the classes. For CIFAR-10, we’d learn 10 templates. Now, with a 2-layer neural network, the first weight matrix W_1 (e.g., 100x3072) can be thought of as learning many more templates – in this example, 100 templates. These aren’t necessarily templates for the final classes directly. Instead, they are like intermediate features or visual primitives. The hidden layer \\(h\\) then represents the extent to which each of these 100 learned “templates” or features is present in the input image. Then, the second weight matrix \\(W_2\\) (10x100) learns how to combine these 100 intermediate features to produce the final scores for the 10 classes. So, instead of learning just 10 direct templates, we learn, say, 100 more foundational templates, and these templates can be shared and re-weighted by W_2 to form the decision for each class. This gives the network much more flexibility and expressive power. It can learn a richer set of features and then learn complex combinations of those features."
  },
  {
    "objectID": "posts/neural-network-and-backprobagation/index.html#activation-functions",
    "href": "posts/neural-network-and-backprobagation/index.html#activation-functions",
    "title": "Neural network and backpropagation",
    "section": "Activation functions",
    "text": "Activation functions\nNow, a key question arises from this construction, why do we want non-linearity? Let’s consider the function we just built: \\(f = W_2  \\max(0, W_1x)\\). What if we didn’t have that \\(\\max(0, ...)\\) non-linearity? What if our function was just \\(f = W_2  W_1  x\\)? Well, if \\(W_1\\) is a matrix and \\(W_2\\) is a matrix, then their product, \\(W_2  W_1\\), is just another matrix. Let’s call it \\(W_{prime}\\). So, \\(f = W_{prime}  x\\). This is just a linear classifier again! Stacking multiple linear transformations without any non-linearity in between doesn’t actually increase the expressive power of our model beyond a single linear transformation. It collapses back down to a linear function. The non-linearity is what allows us to learn much more complex functions. So, what are some common choices for these activation functions?\n\nThe one we’ve been using, \\(\\max(0, x)\\), is called ReLU (Rectified Linear Unit), and you can see its plot in the top left, highlighted in red. For any negative input, it outputs zero. For any positive input, it outputs the input itself. It’s simple, computationally efficient, and works remarkably well in practice. In fact, ReLU is a good default choice for most problems and is very widely used. But there are many other activation functions people have explored like Leaky ReLU This can sometimes help with issues where ReLU units “die” or Sigmoid This function squashes its input into the range (0, 1). It was historically very popular, especially in the early days of neural networks, because its output can be interpreted as a probability or a firing rate of a neuron. However, it suffers from vanishing gradients for very large positive or negative inputs, which can slow down learning in deep networks. Tanh this function squashes its input into the range (-1, 1). It’s zero-centered, which can sometimes be advantageous over sigmoid. Like sigmoid, it also suffers from vanishing gradients at the extremes. And a lot of ReLU variations like ELU, GELU, SiLU, or what ever it is just ending with LU. But you shouldn’t expect much from those activation functions, you might get a slightly better result when using this activation function over one another depend on the problem you’re trying to solve, but trust me just stick with ReLU and it will work pretty much with most of the problems you might come up with."
  },
  {
    "objectID": "posts/neural-network-and-backprobagation/index.html#a-note-on-network-size-and-regularization",
    "href": "posts/neural-network-and-backprobagation/index.html#a-note-on-network-size-and-regularization",
    "title": "Neural network and backpropagation",
    "section": "A note on network size and regularization",
    "text": "A note on network size and regularization\nOkay, so we have these building blocks: layers composed of a linear transformation followed by a non-linear activation function. Now, let’s think about how we arrange these layers to form different neural network architectures.\n\nYou’ll often see neural networks visualized like this, with nodes and connections. We have a “3-layer Neural Net” or a “2-hidden-layer Neural Net”. The circles on the far left represent the input layer – these are just our raw input features, say the pixel values of an image. The next set of circles in the middle is the hidden layer. Each unit here computes a weighted sum of the inputs, adds a bias, and then passes it through an activation function (like ReLU). And finally, the circles on the right form the output layer, which produces our final class scores, or perhaps a regression value. The output of the first hidden layer becomes the input to the second hidden layer, which then feeds into the output layer. The key thing to notice in these diagrams are the lines connecting the nodes. These layers are typically Fully-connected layers. This means that every neuron in the input layer is connected to every neuron in the first hidden layer. And every neuron in the first hidden layer is connected to every neuron in the second hidden layer, and so on, up to the output layer. Each of these connections has an associated weight.\nSo, when we say an N-layer Neural Net, we generally mean there are N layers of weights and N layers of activations being computed or N-1 hidden layers plus an output layer. The input itself isn’t usually counted as a layer in this sense, as it doesn’t perform a computation with learnable weights. Let’s look at an example feed-forward computation for a neural network, to make this more concrete.\n1f = lambda x: 1.0/(1.0 + np.exp(-x))\n2x = np.random.randn(3, 1)\n3h1 = f(np.dot(W1, x) + b1)\n4h2 = f(np.dot(W2, h1) + b2)\n5out = np.dot(W3, h2) + b3\n\n1\n\nwe define our activation function f. In this example, they’re using a sigmoid\n\n2\n\nwe have some input x, which is a 3x1 vector.\n\n3\n\nget the activations of the first hidden layer, h1. W_1would be a 4x3 weight matrix, and b1 would be a 4x1 bias vector then we performs the matrix multiplication and add the bias b1. Finally we apply the activation function f element-wise to the result. So h1 will be a 4x1 vector.\n\n4\n\nsimilarly, for the second hidden layer activations, h2\n\n5\n\nfinally for the output neuron, out, W3 would be a 1x4 matrix b3 a 1x1 bias. Notice that for the output layer, an activation function isn’t always applied, or a different one might be used depending on the task for example softmax for multi-class classification, sigmoid for binary classification, or no activation for regression.\n\n\nThis step-by-step process, from input to output, is called the forward pass. Now, it might seem like these networks are incredibly complex, but you might be surprised to learn that a full implementation of training a 2-layer Neural Network needs only about 20 lines of Python code using NumPy. This is, of course, a simplified example, but it captures all the essential components\nimport numpy as np\nfrom numpy.random import rand\n\n1N, D_in, H, D_out = 64, 1000, 100, 10\nx, y = randn(N, D_in), randn(N, D_out)\nw1, w2 = randn(D_in, H), randn(H, D_out)\n\n2for t in range(2000):\n  h = 1 / (1 + np.exp(-x.dot(w1)))\n  y_pred = h.dot(w2)\n  loss = np.square(y_pred - y).sum()\n  print(t, loss)\n\n3  grad_y_pred = 2.0 * (y_pred - y)\n  grad_w2 = h.T.dot(grad_y_pred)\n  grad_h = grad_y_pred.dot(w2.T)\n  grad_w1 = x.T.dot(grad_h * h * (1 - h))\n\n4  w1 -= 1e-4 * grad_w1\n  w2 -= 1e-4 * grad_w2\n\n1\n\nwe define the network, set up our hyperparameters. We then create some random input data X and target labels y, we initialize our weight matrices w1 and w2\n\n2\n\nnext we perform forward pass\n\n3\n\nafter the forward pass, we need to calculate the analytical gradients. This is the core of how we’ll update our weights. We’re essentially applying the chain rule to work backward from the loss and find how w1 and w2 affect that loss. We’ll dive much deeper into this process, called backpropagation, very soon.\n\n4\n\nAnd the final step in our training loop is to perform the gradient descent update. We simply take our current weights and subtract the gradient (multiplied by a small learning rate, here 1e-4) to move in the direction that reduces the loss.\n\n\nAnd that’s it! In about 20 lines, we have a complete training procedure for a 2-layer neural network. Now, an obvious question that arises when designing these networks is how do we go about setting the number of layers and their sizes? This is a fundamental question in neural network design, and it relates to the “capacity” of the model\n\nThe diagrams in the first row illustrate the decision boundary learned by a two layer neural network with a varying number of neurons in its single hidden layer, trying to classify some 2D points. With only 3 hidden neurons, the network can learn a relatively simple decision boundary. When we increase to 6 hidden neurons, the network has more capacity. It can learn a more complex, wigglier decision boundary, fitting the data a bit better. And with 20 hidden neurons, it can learn an even more intricate boundary, potentially capturing finer details of the data distribution. The general principle here is that more neurons = more capacity. A network with more neurons (and more layers, which we’ll get to) can approximate more complex functions. It has more parameters, more knobs to turn, so it can fit more complicated patterns in the data. Now, if more neurons give more capacity, you might be tempted to think: “Great! I’ll just make my network huge, and it will fit the training data perfectly!”. But there’s a catch, and that’s overfitting. A network with too much capacity might learn the training data perfectly, including all its noise but fail to generalize to new and unseen data.\nSo how do we control this? One might think “Okay, I’ll just make my network smaller to prevent overfitting.” However, the general wisdom in the deep learning community now is: Do not use the size of the neural network as your primary regularizer. Use stronger regularization instead. What does this mean? It’s generally better to use a larger network that has the potential to learn the true underlying function well, and then control overfitting using explicit regularization techniques like L2 regularization, dropout, or data augmentation. Look at these plots at the bottom. They show the decision boundary for a network of a fixed, reasonably large size, but with different strengths of L2 regularization, controlled by \\(\\lambda\\). The idea is that it’s often better to make your network “big enough” (in terms of layers and neurons) to have sufficient capacity to represent the complexity of the true underlying function, and then use regularization techniques to prevent it from merely memorizing the training data. Trying to find the “perfect” small size for your network is often harder and less effective than using a larger network with good regularization."
  },
  {
    "objectID": "posts/neural-network-and-backprobagation/index.html#how-to-compute-gradients",
    "href": "posts/neural-network-and-backprobagation/index.html#how-to-compute-gradients",
    "title": "Neural network and backpropagation",
    "section": "How to compute gradients?",
    "text": "How to compute gradients?\nOkay, so now we have our neural network, which is essentially a more complex, nonlinear score function. For a 2-layer network, our scores s are given by \\(f(x; W_1, W_2) = W_2  \\max(0, W_1x)\\). This function takes our input x and our learnable parameters \\(W_1\\) and \\(W_2\\) and produces class scores. Once we have these scores, the rest of the framework we developed for linear classifiers still applies. We need a loss function to tell us how good these scores are. For example, we can use the Hinge Loss (or SVM loss) on these predictions:\n\\[\nL_i = \\sum_{j \\neq y_i} \\max(0, s_j - s_{y_i} + 1)\n\\]\nThis is exactly the same hinge loss we saw before. It penalizes the network if the score \\(s_j\\) for an incorrect class \\(j\\) is not sufficiently lower than the score \\(s_{y_i}\\) for the correct class \\(y_i\\) (by a margin of 1). We could just as easily use the Softmax loss (cross-entropy loss) here. The choice of loss function depends on the specific problem and desired properties, just like with linear models. Next, we also typically include a regularization term to prevent overfitting. A common choice is L2 regularization: \\(R(W) = \\sum_k W_k^2\\). This penalizes large values in our weight matrices.\nAnd finally, the total loss \\(L\\) is the average of the data loss \\(L_i\\) over all N training examples, plus our regularization terms.\n\\[\nL = \\frac{1}{N}  \\sum_{i=1}^N L_i + \\lambda R(W_1) + \\lambda R(W_2)\n\\]\nNotice that now we have regularization terms for each of our weight matrices, \\(W_1\\) and \\(W_2\\) and if we had more layers, we’d regularize all their weights as well. The \\(\\lambda\\) hyperparameter controls the strength of this regularization. So the overall structure is very familiar, 1) define a score function. 2) define a loss function. 3) add regularization. Our goal is still the same find the weight \\(W_1\\), \\(W_2\\) that minimize this total loss \\(L\\). But now, our score function f is more complex. It’s a nested composition of functions with matrix multiplies and non-linearities. This brings us to the next big challenge: How to compute gradients?\nTo use gradient descent, we need to compute the partial derivatives of the total loss \\(L\\) with respect to each of our parameters. That is, we need \\(\\frac{\\partial L}{\\partial W_1}, \\frac{\\partial L}{\\partial W_2}\\), if we can compute these gradients then we can update \\(W_1\\) and \\(W_2\\) using our standard gradient descent rule: \\(W_1 = W_1 - learning\\_rate  \\frac{\\partial L}{\\partial W_1}\\), and similarly for \\(W_2\\). So how do we get these gradients? One idea which turns out to be a bad idea is to try and derive \\(\\frac{\\partial L}{\\partial W}\\) on paper, analytically, and for the entire complex function. Think back to our linear classifier with SVM loss. Even for that relatively simple case, where \\(s = f(x;W) = Wx\\), and \\(L_i = \\sum \\max(0, s_j - s_{y_i} + 1)\\), deriving the full gradient \\(\\frac{\\partial L}{\\partial W}\\) was already a bit involved. We had to expand it all out, as shown here, and then take derivatives.\n\\[\n\\begin{align}\nL_i &= \\sum_{j \\neq y_i} \\max(0, s_j - s_{y_i} + 1) \\\\\n    &= \\sum_{j \\neq y_i} \\max(0, W_{j,:} \\cdot x + W_{y_i, :} \\cdot x + 1) \\\\\nL &= \\frac{1}{N} \\sum_{i=1}^N L_i + \\lambda \\sum_k W_k^2 \\\\\n&= \\frac{1}{N} \\sum_{i=1}^N \\sum_{j \\neq y_i} \\max(0, W_{j,:} \\cdot x + W_{y_i, :} \\cdot x + 1) + \\lambda \\sum_k W_k^2 \\\\\n\\nabla_W L &= \\nabla_W \\left(\\frac{1}{N} \\sum_{i=1}^N \\sum_{j \\neq y_i} \\max(0, W_{j,:} \\cdot x + W_{y_i, :} \\cdot x + 1) + \\lambda \\sum_k W_k^2 \\right)\n\\end{align}\n\\]\nNow imagine doing this for our 2-layer neural network, where \\(s\\) itself is \\(W_2 * \\max(0, W_1 x)\\). The expression for \\(L\\) in terms of the raw inputs \\(x\\) and weights \\(W_1\\), \\(W_2\\) would become much, much more complicated. It would be very tedious. It involves a lot of matrix calculus, and you’d need a lot of paper (or whiteboard space!). It’s very easy to make mistakes. What if we want to change our loss function? Say we initially derived everything for Hinge loss, and now we want to try Softmax loss. We’d have to re-derive everything from scratch! That’s not flexible at all. This approach is simply not feasible for very complex models. Modern deep neural networks can have tens, hundreds, or even thousands of layers, with various types of operations. Deriving the full analytical gradient for such a beast by hand is practically impossible and incredibly error-prone. So, we need a more systematic, more modular, and more scalable way to compute these gradients. And that leads us to a better idea: Computational graphs + Backpropagation.\nThe core idea here is to break down our complex function from input \\(x\\) and weights \\(W\\) all the way to the final loss \\(L\\) into a sequence of simpler, elementary operations. We can represent this sequence as a computational graph\n\nLet’s look at the example, which is for our familiar linear classifier with hinge loss and regularization.\n\nWe start with our inputs \\(x\\) and our weights \\(W\\).\nThe first operation is computing the scores: \\(s = Wx\\). This is represented by a node. It takes \\(W\\) and \\(x\\) as inputs and produces scores \\(S\\).\nThese scores \\(S\\) then feed into the hinge loss function \\(L_i = \\sum \\max(0, s_j - s_{y_i} + 1)\\). This is another node in our graph.\nSeparately, our weights \\(W\\) also feed into a regularization function \\(R(W)\\).\nFinally, the output of the hinge loss (\\(L_i\\), which would then be averaged over the batch) and the output of the regularization term \\(R(W)\\) are added together (the + node) to give us our final total loss \\(L\\).\n\nSo, we’ve decomposed our complex loss calculation into a directed acyclic graph of basic operations. Each node in this graph takes some inputs and produces an output. The beauty of this approach is that if we know how to compute the local gradient for each elementary operation for example how does the output of the \\(Wx\\) node change if \\(W\\) changes a little?, we can then use the chain rule from calculus to systematically “backpropagate” the gradient of the final loss \\(L\\) all the way back to our input parameters \\(W\\). This “backpropagation” algorithm is the workhorse that allows us to efficiently compute gradients for arbitrarily complex neural networks. It’s modular because we only need to know the local derivative for each type of node (matrix multiply, addition, ReLU, sigmoid, loss function, etc.). And it’s systematic, meaning we can implement it once, and it will work for any network architecture we can express as a computational graph. This approach is essential for handling the complexity of modern deep learning models.\nThink about Convolutional Network like AlexNet, which was a groundbreaking architecture for image recognition. Trying to write down the full analytical derivative of the loss with respect to all those different sets of weights in AlexNet would be an absolute nightmare. It’s just too complex. And it gets even crazier. Consider something like a Neural Turing Machine. This is a type of neural network architecture that’s augmented with an external memory component that it can learn to read from and write to. The idea is to give the network capabilities closer to a traditional computer program. The computational graph for a Neural Turing Machine, especially if you unroll its operations over time becomes incredibly deep and complex. Each “time step” of the machine’s operation adds another layer to this graph. Again, attempting to derive gradients by hand for such a system is completely out of the question. So, for all these sophisticated architectures – from convolutional networks to these very deep and recurrent models – we absolutely need a robust, general, and efficient method for computing gradients. And that method is backpropagation, operating on these computational graphs."
  },
  {
    "objectID": "posts/neural-network-and-backprobagation/index.html#backpropagation",
    "href": "posts/neural-network-and-backprobagation/index.html#backpropagation",
    "title": "Neural network and backpropagation",
    "section": "Backpropagation",
    "text": "Backpropagation\nSo the solution backpropagation. This is the algorithm that will allow us to compute these gradients efficiently and systematically. It’s essentially a practical application of the chain rule from calculus on a computational graph. Let’s illustrate backpropagation with a simple example. Consider the function \\(f(x, y, z) = (x + y) * z\\). Our goal will be to compute the partial derivatives of \\(f\\) with respect to \\(x\\), \\(y\\), and \\(z\\), that is, \\(\\frac{\\partial f}{\\partial x}\\), \\(\\frac{\\partial f}{\\partial y}\\), and \\(\\frac{\\partial f}{\\partial z}\\). First, let’s represent this function as a computational graph\n\nWe have inputs \\(x\\) and \\(y\\). These are fed into an addition node. Let’s call the output of this addition \\(q\\). So, \\(q = x + y\\). Then, this intermediate value q and our third input \\(z\\) are fed into a multiplication node. The output of this multiplication node is our final function value \\(f\\). So, \\(f = q z\\). This graph breaks the computation down into two simple steps: an addition and a multiplication. Now, let’s assign some concrete values to our inputs to trace the computation. Let’s say, for example, \\(x = -2\\), \\(y = 5\\), and \\(z = -4\\), so \\(q = 3\\) and \\(f = -12\\). This is the forward pass: we compute the values of all nodes in the graph from inputs to output.\nNow, for backpropagation, we need the local gradients for each node in our graph. That is, for each operation, we need to know how its output changes with respect to its inputs. Consider the first operation: \\(q = x + y\\), \\(\\frac{\\partial q}{\\partial x} = 1\\) so if \\(x\\) changes by a small amount, \\(q\\) changes by that same amount, similarly \\(\\frac{\\partial q}{\\partial y} = 1\\). These are the local gradients for the addition gate. Next, consider the second operation: \\(f = q z\\). \\(\\frac{\\partial f}{\\partial q} = z\\) so if \\(q\\) changes by a small amount \\(dq\\), \\(f\\) changes by \\(z \\cdot dq\\). And \\(\\frac{\\partial f}{\\partial z} = q\\), if z changes by a small amount \\(dz\\), \\(f\\) changes by \\(q \\cdot dz\\). So, we have the local derivatives for each individual operation. Ultimately, what we want are the gradients of the final output \\(f\\) with respect to the initial inputs \\(x\\), \\(y\\), and \\(z\\): \\(\\frac{\\partial f}{\\partial x}\\), \\(\\frac{\\partial f}{\\partial y}\\) and \\(\\frac{\\partial f}{\\partial z}\\).\nOkay, we’re ready to start the backward pass. We work from the output \\(f\\) back towards the inputs \\(x, y, z\\). The very first gradient we consider is the gradient of the final output \\(f\\) with respect to itself, \\(\\frac{\\partial f}{\\partial f}\\). This is always 1, by definition. This ‘1’ is often called the initial “upstream gradient”, it’s the gradient that starts the whole backward flow. You can see it annotated on the graph in read next to \\(f\\).\nNow let’s move back one step, we want to find \\(\\frac{\\partial f}{\\partial z}\\). \\(z\\) is an input to the multiplication gate \\(f = qz\\) The local gradient of \\(f\\) with respect to \\(z\\) is \\(q\\). From our forward pass, we know \\(q = 3\\). So, \\(\\frac{\\partial f}{\\partial z} = 3\\). The upstream gradient \\(\\frac{\\partial f}{\\partial f}\\) (which is 1) is multiplied by the local gradient \\(\\frac{\\partial f}{\\partial z}\\) (which is q). So, \\(1 q = q = 3\\). This value 3 is now the gradient of the final output \\(f\\) with respect to the node \\(z\\).\nNext, we want \\(\\frac{\\partial f}{\\partial q}\\). \\(q\\) is the other input to the multiplication gate \\(f = qz\\). The local gradient of \\(f\\) with respect to \\(q\\) is \\(z\\). From our forward pass, \\(z = -4\\). So, \\(\\frac{\\partial f}{\\partial q} = -4\\). Again the upstream gradient \\(\\frac{\\partial f}{\\partial f}\\) (which is 1) multiplied by the local gradient \\(\\frac{\\partial f}{\\partial q}\\) (which is z). So, \\(1 \\cdot z = z = -4\\). This \\(-4\\) is the gradient of the final output \\(f\\) with respect to the intermediate node \\(q\\). This value will be crucial as we continue propagating backward, because \\(q\\) itself depends on \\(x\\) and \\(y\\).\nNow we need to go further back to get \\(\\frac{\\partial f}{\\partial y}\\). \\(y\\) is an input to the addition gate \\(q = x + y\\). The node \\(y\\) influences \\(f\\) through \\(q\\). This is where the chain rule comes into play. The chain rule tells us:\n\\[\n\\frac{\\partial f}{\\partial y}= \\frac{\\partial f}{\\partial q} \\frac{\\partial q}{\\partial y}\n\\]\n\\(\\frac{\\partial f}{\\partial q}\\) is the gradient of the final output \\(f\\) with respect to \\(q\\). We just calculated this as \\(-4\\). This is often called the upstream gradient coming into the \\(q\\) node from later parts of the graph. \\(\\frac{\\partial q}{\\partial y}\\) is the local gradient of the \\(q\\) node with respect to its input \\(y\\). From \\(q = x + y\\), we know \\(\\frac{\\partial q}{\\partial y} = 1\\). So, \\(\\frac{\\partial f}{\\partial y} = -4 \\cdot 1 = -4\\). This \\(-4\\) is now the gradient of the final output \\(f\\) with respect to the input \\(y\\).\nFinally, let’s get \\(\\frac{\\partial f}{\\partial x}\\) Similar to \\(y, x\\) is an input to the addition gate \\(q = x + y\\), and it influences \\(f\\) through \\(q\\), we apply the chain rule here \\(\\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial q} \\frac{\\partial q}{\\partial x}\\). So, \\(\\frac{\\partial f}{\\partial x} = -4 \\cdot 1 = -4\\). This \\(-4\\) is the gradient of the final output \\(f\\) with respect to the input \\(x\\).\nSo, to summarize, we’ve found:\n\\[\n\\begin{align}\n\\frac{\\partial f}{\\partial f} &= 1 \\\\\n\\frac{\\partial f}{\\partial z} &= q = 3 \\\\\n\\frac{\\partial f}{\\partial q} &= z = -4 \\\\\n\\frac{\\partial f}{\\partial y} &= \\frac{\\partial f}{\\partial q}  \\frac{\\partial q}{\\partial y}  = -4 \\cdot 1 = -4 \\\\\n\\frac{\\partial f}{\\partial x} &= \\frac{\\partial f}{\\partial q} \\frac{\\partial q}{\\partial x}  = -4 \\cdot 1 = -4\n\\end{align}\n\\]\nLet’s generalize this idea of backpropagation through a single computational gate.\n\nImagine we have some generic function, or “gate,” \\(f\\). This gate takes some inputs, let’s say \\(x\\) and \\(y\\), and it produces an output \\(z\\). This \\(f\\) could be an addition, a multiplication, a ReLU, a sigmoid, any elementary operation. For this gate to be part of a backpropagation process, we need to know its local gradients. These are the partial derivatives of the gate’s output \\(z\\) with respect to each of its inputs, \\(x\\) and \\(y\\) So, we need to be able to compute \\(\\frac{\\partial z}{\\partial x}\\) and \\(\\frac{\\partial z}{\\partial y}\\) . These gradients tell us how a small change in x (or y) directly affects z, assuming all other inputs to this specific gate are held constant. Now, during the backward pass, this gate \\(f\\) will receive an upstream gradient. This upstream gradient is the derivative of the final loss function \\(L\\) which is at the very end of our entire computational graph with respect to the output of this gate, \\(z\\). So, we receive \\(\\frac{\\partial L}{\\partial z}\\). This \\(\\frac{\\partial L}{\\partial z}\\) tells us how much the final loss \\(L\\) changes if the output \\(z\\) of this particular gate changes by a small amount. This gradient has been propagated backward from later parts of the graph. Our goal, when backpropagating through this gate, is to compute the Downstream gradients. These are the derivatives of the final loss \\(L\\) with respect to the inputs of this gate, \\(x\\) and \\(y\\). So, we want to calculate \\(\\frac{\\partial L}{\\partial x}\\) and \\(\\frac{\\partial L}{\\partial y}\\). How do we do this? We use the chain rule. To find \\(\\frac{\\partial L}{\\partial x}\\), we multiply the upstream gradient \\(\\frac{\\partial L}{\\partial z}\\) by the local gradient \\(\\frac{\\partial z}{\\partial x}\\):\n\\[\n\\frac{\\partial L}{\\partial x}= \\frac{\\partial L}{\\partial z} * \\frac{\\partial z}{\\partial x}\n\\]\nThis tells us how much the final loss \\(L\\) changes if the input \\(x\\) to this gate changes by a small amount. And similarly for the input \\(y\\). So, for any gate in our computational graph, if we know its local gradient and the upstream gradient coming into its output we can compute the downstream gradient which will then become the upstream gradients for the gate that produced \\(x\\) and \\(y\\). This process is then repeated for every gate in the graph, working backwards from the final loss. Each gate receives an upstream gradient from its successor(s) in the forward pass. It uses this, along with its own local gradients, to compute downstream gradients. These downstream gradients are then passed further back to its predecessor(s). This recursive application of the chain rule is what allows us to efficiently compute the gradient of the overall loss function with respect to all parameters and inputs in the network. This pattern of upstream gradient times local gradient gives downstream gradient is the fundamental computation performed at each node during backpropagation.\nOkay, let’s look at another example This time, the function is\n\\[\nf(w, x) = \\frac{1}{1 + e^{-(w_0x_0 + w_1x_1 + w_2)}}\n\\]\nThis should look familiar! It’s the sigmoid function applied to a linear combination of inputs \\(w_0x_0 + w_1x_1 + w_2\\). This is exactly what one neuron in a neural network might compute if it’s using a sigmoid activation. Our goal will be to find the gradients of this f with respect to \\(w_0, x_0, w_1, x_1\\), and \\(w_2\\). And if we work it out like the previous example we would have this graph\n\n\nSigmoid function: \\(\\sigma(x) = \\frac{1}{1 + e^{-x}}\\)\n\nNow, let’s take a step back and look at a section of this graph. The sequence of operations we performed multiply by -1 then exp then add 1, then take reciprocal (1/x) – this entire chain, highlighted in the blue box, is actually the computation of the Sigmoid function. The input to this entire sigmoid block was the value 1.00 and the output of the sigmoid block was 0.73. An important point here is that the computational graph representation may not be unique. We can choose to break down functions into very fine-grained elementary operations, as we did, or we can group common sequences of operations into a single “macro” gate. The key is to choose a representation where the local gradients at each node can be easily expressed. So, instead of backpropagating through each of those four individual steps, we could have treated the entire sigmoid function as a single gate. To do that, we’d need the sigmoid local gradient. The derivative of the sigmoid function with respect to its input \\(x\\) is a well-known result:\n\\[\n\\frac{d \\sigma(x)}{d x} = \\frac{e^{-x}}{\\left(1 + e^{-x}\\right)^2} = \\left( \\frac{1 + e^{-x} - 1}{1 + e^{-x}}\\right) \\left(\\frac{1}{1 + e^{-x}}\\right) = \\left(1 - \\sigma(x)\\right) \\sigma(x)\n\\]\nThis is a very convenient form because it expresses the derivative in terms of the function’s own output value. So, if we treat the sigmoid as a single gate. The input to this sigmoid gate was \\(x\\_in = 1.00\\). The output was \\(\\sigma(x\\_in) = 0.73\\). The upstream gradient coming into the output of the sigmoid gate was 1.00. The local gradient of the sigmoid function, evaluated at its input \\(x\\_in=1.00\\), is \\(\\sigma(1.00) (1 - \\sigma(1.00))\\). Since \\(\\sigma(1.00)\\) is approximately \\(0.73\\), the local gradient is \\(0.73 \\cdot (1 - 0.73) = 0.73 \\cdot 0.27 \\approx 0.1971\\). Then we calculate the downstream gradient which is \\(upstream\\_gradient \\cdot (1-\\sigma(1)) \\sigma(1)\\) and this evaluates to approximately 0.2. And if you look back at our step-by-step backpropagation, the gradient we computed at the input of the multiply by -1 gate which was the start of our sigmoid block was indeed 0.20! This demonstrates the modularity of backpropagation. We can define common operations like sigmoid, ReLU, tanh, etc., as single “black-box” gates. As long as we know how to compute their output during the forward pass and their local gradient during the backward pass, we can easily plug them into any computational graph. Modern deep learning frameworks are built around this idea of a library of predefined layers or operations, each knowing its forward and backward computation. This ability to abstract away complex operations into single gates with well-defined local gradients is incredibly powerful and makes implementing backpropagation for complex architectures much more manageable."
  },
  {
    "objectID": "posts/neural-network-and-backprobagation/index.html#backpropagation-in-practice",
    "href": "posts/neural-network-and-backprobagation/index.html#backpropagation-in-practice",
    "title": "Neural network and backpropagation",
    "section": "Backpropagation in practice",
    "text": "Backpropagation in practice\nNow, let’s look for some general patterns in how these gradients flow through different types of gates. Understanding these can give you a good intuition for how backpropagation works.\n\nFirst, let’s consider the add gate: \\(z = x + y\\). The add gate acts as a gradient distributor. It takes the upstream gradient and simply passes it along (distributes it) to both of its inputs unchanged, because the local gradients are 1.\nNext, the mul gate (multiplication gate): \\(z = x y\\). The mul gate acts as a swap multiplier. The gradient \\(\\frac{dL}{dx}\\) is the upstream gradient times the other input \\(y\\). And \\(\\frac{dL}{dy}\\) is the upstream gradient times the other input \\(x\\). It’s like the inputs get swapped when they multiply the upstream gradient.\nWhat about a copy gate? A copy gate is when a single input \\(x\\) is used in multiple places further down the graph. During backpropagation, this means gradients from multiple paths will flow back to x. A copy gate (or a “fan-out” point) acts as a gradient adder. It sums up all the incoming upstream gradients. This is a direct consequence of the multivariate chain rule when a variable influences the loss through multiple paths.\nFinally, consider a max gate: \\(z = \\max(x, y)\\). The max gate acts as a gradient router. It takes the upstream gradient and routes it entirely to the input that won (had the maximum value) during the forward pass. The gradient for the input that lost is zero. This makes sense if \\(x\\) wasn’t the max, then small changes in x (as long as it stays not the max) won’t affect the output \\(z\\) at all.\nThese patterns of distributor for add, swap multiplier for mul, adder for copy, and router for max are very useful to keep in mind when thinking about how gradients propagate through a network. ReLU, for example, \\(\\max(0, x)\\), is a special case of the max gate. If \\(x &gt; 0\\), the gradient passes through. If \\(x &lt;= 0\\), the gradient becomes zero.\nSo, we have this conceptual understanding of backpropagation using computational graphs. How do we actually implement this? One way is to write “Flat” code. This means we explicitly write out the sequence of operations for the forward pass and then, in reverse order, write out the corresponding operations for the backward pass. Let’s look at our sigmoid example again\n\n\n\n\n\n\ndef f(w0, x0, w1, x1, w2):\n  s0 = w0 * x0  # Forward pass\n  s1 = w1 * x1\n  s2 = s0 + s1\n  s3 = s2 + w2\n  L = sigmoid(s3)\n\n\n  grad_L = 1.0 # Backward pass\n  grad_s3 = grad_L * (1 - L) * L\n  grad_w2 = grad_s3\n  grad_s2 = grad_s3\n  grad_s0 = grad_s2\n  grad_s1 = grad_s2\n  grad_w1 = grad_s1 * x1\n  grad_x1 = grad_s1 * w1\n  grad_w0 = grad_s0 * x0\n  grad_x0 = grad_s0 * w0\n\n\n\nDuring this forward pass, we would typically store all the intermediate values s0, s1, s2, s3 and the final output L, because we’ll need them for the backward pass. Now, for the Backward pass to compute the gradients we work backward from the last operation in the forward pass. And that’s it! We’ve computed all the gradients we needed. Notice how the backward pass code mirrors the forward pass code but in reverse order. Each line in the forward pass that computes some variable v will have a corresponding set of lines in the backward pass that compute grad_v (the gradient of the final loss with respect to v) and then use grad_v to compute gradients for the inputs to that operation. This “flat” style of implementation is straightforward for simple functions. However, for very deep or complex networks, writing out all these steps explicitly can become tedious and error-prone. We’d prefer a more modular approach.\nA much better approach for practical deep learning systems is a modularized implementation using a well-defined forward / backward API for each type of gate or operation. The idea is that each elementary operation (like multiplication, addition, sigmoid, ReLU, convolution, etc.) is implemented as an object or a set of functions that knows two things: 1) how to compute its output given its inputs (the forward pass). 2) how to compute the gradients with respect to its inputs, given the gradient with respect to its output (the backward pass). Here’s an example of what this might look like, using syntax similar to what you’d find in PyTorch’s autograd.Function.\nclass Multiply(torch.autograd.Function):\n  @staticmethod\n  def forward(ctx, x, y):\n1    ctx.save_for_backward(x, y)\n    z = x * y\n    return z\n\n  @staticmethod\n2  def backward(ctx, grad_z):\n    x, y = ctx.saved_tensors\n    grad_x = y * grad_z\n    grad_y = x * grad_z\n    return grad_x, grad_y\n\n1\n\nWe need to cache some values for use in backward\n\n2\n\ngrad_z is upstream gradient\n\n\nThis is the pattern. Every operation (or “layer” in a deep learning framework) will have a forward method that computes its output and stashes away anything needed for the backward pass, and a backward method that takes the upstream gradient and uses the stashed values and local derivatives to compute and return the downstream gradients."
  },
  {
    "objectID": "posts/neural-network-and-backprobagation/index.html#backpropagation-with-vectors-and-matrices",
    "href": "posts/neural-network-and-backprobagation/index.html#backpropagation-with-vectors-and-matrices",
    "title": "Neural network and backpropagation",
    "section": "Backpropagation with vectors and matrices",
    "text": "Backpropagation with vectors and matrices\nSo far, we’ve mostly been talking about scalar inputs and outputs for these gates. But in neural networks, we usually deal with vectors, matrices and tensors. All the examples we’ve worked through like \\(f(x,y,z) = (x+y)z\\) and the sigmoid example \\(f(w,x) = \\sigma(w_0x_0 + w_1x_1 + w_2)\\) involved scalar inputs and scalar intermediate values. But in real neural networks, our inputs x are often vectors (e.g., an image flattened into a vector), our weights W are matrices, and the outputs of layers are vectors or higher-dimensional tensors. So, the question is what about vector-valued functions? How does backpropagation work then? To answer this, let’s quickly recap vector derivatives.\nScalar to Scalar, if we have a scalar input \\(x \\in \\mathbb{R}\\) and a scalar \\(y \\in \\mathbb{R}\\), then we have the regular derivative \\(\\frac{dy}{dx}\\). This tells us: if x changes by a small amount, how much will y change? The derivative itself is also a scalar \\(\\frac{dy}{dx} \\in \\mathbb{R}\\).\nVector to Scalar, now what if our input \\(x \\in \\mathbb{R}^N\\) (a vector) and \\(y \\in \\mathbb{R}\\) (a scalar). The derivative is now the Gradient \\(\\frac{\\partial y}{\\partial x} \\in \\mathbb{R}\\) where n-th component is \\(\\left(\\frac{\\partial y}{\\partial x}\\right)_n = \\frac{\\partial y}{\\partial x_n}\\). This gradient answers the question for each element of \\(x\\), if it changes by a small amount, then how much will \\(y\\) change? Our loss function \\(L\\) is a scalar, and our weights \\(W\\) (and inputs x to various layers) are often vectors or matrices. So, when we compute \\(\\frac{\\partial L}{\\partial W}\\), we are computing a gradient.\nVector to Vector, what if both our input \\(x \\in \\mathbb{R}^N\\) (a vector) and our output \\(y \\in \\mathbb{R}^M\\) (a vector). The derivative is now the Jacobian matrix \\(\\frac{\\partial y}{\\partial x} \\in \\mathbb{R}^{N \\times M}\\). This is an \\(M \\times N\\) matrix, where the element at row \\(m\\), column \\(n\\) is \\(\\left(\\frac{\\partial y}{\\partial x}\\right)_{n,m} = \\frac{\\partial y_m}{\\partial x_n}\\). This Jacobian answers the question for each element of \\(x\\), if it changes by a small amount, then how much will each element of y change? Many layers in a neural network can be thought of as vector-to-vector functions (e.g., a fully connected layer maps an input vector to an output vector of activations). The chain rule still applies, but now it involves matrix-vector multiplications or Jacobian-vector products. Let’s see how this plays out.\n\nWe still have our gate \\(f\\) which takes inputs, say \\(x\\) and \\(y\\), and produces an output \\(z.\\) Crucially, the final Loss \\(L\\) is still a scalar! We are always trying to minimize a single scalar value representing how bad our network’s predictions are. During the backward pass, this gate will receive an Upstream gradient. This is the gradient of the scalar Loss L with respect to the vector output z of this gate. So, it’s \\(\\frac{\\partial L}{\\partial z}\\). Since \\(L\\) is a scalar and \\(z\\) is a D_z-dimensional vector, this upstream gradient \\(\\frac{\\partial L}{\\partial z}\\) will also be a D_z-dimensional vector. It tells us: “For each element of \\(z\\), how much does that element influence the final Loss \\(L\\)?”. Now, the “local gradients” for this gate \\(f\\) describe how its output z changes with respect to its inputs \\(x\\) and \\(y\\). \\(\\frac{\\partial z}{\\partial x}\\): Since \\(z\\) is D_z-dim and \\(x\\) is D_x-dim, this is the Jacobian matrix of \\(z\\) with respect to \\(x\\) and same with \\(y\\). The shapes of these Jacobians:\n\n\\(\\frac{\\partial z}{\\partial x}\\) will be a D_x x D_z matrix (derivative of each component of \\(z\\) w.r.t. each component of \\(x\\)).\n\\(\\frac{\\partial z}{\\partial y}\\) will be a D_y x D_z matrix.\n\nThe Downstream gradients we want to compute are \\(\\frac{\\partial L}{\\partial x}\\) (a D_x-dim vector) and \\(\\frac{\\partial L}{\\partial y}\\) (a D_y-dim vector). Using the chain rule:\n\\[\n\\begin{align}\n\\frac{\\partial L}{\\partial x} &= \\frac{\\partial z}{\\partial x} \\frac{\\partial L}{\\partial z}  \\\\\n\\frac{\\partial L}{\\partial y} &= \\frac{\\partial z}{\\partial y} \\frac{\\partial L}{\\partial z}\n\\end{align}\n\\]\nAn important principle to remember: Gradients of variables with respect to the loss have the same dimensions as the original variable.\n\n\\(x\\) is a D_x-dimensional vector. Its gradient \\(\\frac{\\partial L}{\\partial x}\\) is also a D_x-dimensional vector.\n\\(y\\) is a D_y-dimensional vector. Its gradient \\(\\frac{\\partial L}{\\partial y}\\) is also a D_y-dimensional vector.\n\\(z\\) is a D_z-dimensional vector. Its upstream gradient \\(\\frac{\\partial L}{\\partial z}\\) is also a D_z-dimensional vector\n\nThis makes intuitive sense: for every element in a variable (like a weight matrix or an activation vector), we need a corresponding gradient value that tells us how changing that specific element affects the final scalar loss. This consistency of shapes is critical for implementing backpropagation correctly in code\nLet’s walk through a concrete example of backprop with vectors, using an element-wise ReLU function, \\(f(x) = \\max(0,x)\\)\n\nWe have a 4D input vector x as shown in the image, and the function max(0,x) is applied element-wise, and the output vector z. Now, for the backward pass, let’s assume we’ve received the upstream gradient \\(\\frac{\\partial L}{\\partial z}\\) This \\(\\frac{\\partial L}{\\partial z}\\) is also a 4D vector. Let’s say it’s \\(\\left[4, -1, 5, 9\\right]^T\\). This tells us, for example, that if the first element of \\(z\\) changes by a small amount, the loss \\(L\\) changes by 4 times that amount. To compute the downstream gradient \\(\\frac{\\partial L}{\\partial x}\\), we need the Jacobian matrix \\(\\frac{\\partial z}{\\partial x}\\). Since \\(z_i = \\max(0, x_i)\\), the output \\(z_i\\) only depends on the corresponding input \\(x_i\\). It does not depend on \\(x_j\\) for \\(j \\neq i\\). This means the Jacobian matrix will be diagonal.\n\nIf \\(x_i &gt; 0\\), then \\(z_i = x_i\\), so \\(\\frac{\\partial z_i}{\\partial x_i} = 1\\).\nIf \\(x_i \\leq 0\\), then \\(z_i = 0\\), so \\(\\frac{\\partial z_i}{\\partial x_i} = 0\\).\n\nSo, the Jacobian \\(\\frac{\\partial z}{\\partial x}\\) is a 4x4 diagonal matrix. Now, we compute \\(\\frac{\\partial L}{\\partial x}\\) = \\(\\frac{\\partial z}{\\partial x}\\) \\(\\frac{\\partial L}{\\partial z}\\). This is a matrix-vector multiplication\nNotice something important here. For element-wise operations like ReLU, the Jacobian is sparse, and specifically, it’s diagonal. The off-diagonal entries are always zero. Because of this, we Never explicitly form the full Jacobian matrix in practice for these types of operations. That would be very inefficient, especially for high-dimensional vectors. Storing a large D x D diagonal matrix is wasteful if only D elements are non-zero. Instead, we use implicit multiplication. We know that multiplying by a diagonal matrix is equivalent to an element-wise product with the diagonal elements\n\\[\n\\left(\\frac{\\partial L}{\\partial x}\\right)_i =\n\\begin{cases}\n\\left(\\frac{\\partial L}{\\partial z}\\right)_i & \\text{ if } x_i &gt; 0 \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\nThis is far more efficient than forming the full Jacobian and doing a matrix-vector multiply. Most deep learning frameworks will implement backpropagation through element-wise operations like this, using element-wise masking or multiplication. So, while the concept of Jacobians is important for understanding the general case, for many common operations, we can exploit their structure to make the backward pass much more efficient.\nNow, what about when our variables are not just vectors, but matrices or even higher-dimensional tensors? So let’s extend this to backprop with matrices (or tensors). The good news is that the core principles remain exactly the same. We still have our gate f, Our inputs x and y can now be matrices (or higher-order tensors). Let’s say x is D_x x M_x and y is D_y x M_y. The output z is also a matrix, say D_z x M_z. The Loss L is still a scalar! This is always true. And remember, \\(\\frac{\\partial L}{\\partial x}\\) will always have the same shape as x. So \\(\\frac{\\partial L}{\\partial x}\\) will be a D_x x M_x matrix of gradients.\n\nThe local gradients are still conceptually Jacobian matrices, but now they relate elements of input matrices to elements of the output matrix. The chain rule, downstream = local * upstream, still holds. However, explicitly forming these Jacobians when \\(x\\), \\(y\\), and \\(z\\) are matrices would involve thinking about derivatives of matrices with respect to matrices, which can lead to 4th-order tensors and becomes very cumbersome to write down and implement directly. Instead, what usually happens in practice is that we consider the matrix operations themselves (like matrix multiplication, element-wise operations on matrices, etc.) and derive the gradient expressions for those specific operations directly, often by thinking about how a small change in an input matrix element affects an element in the output matrix, and then summing up contributions to the scalar loss. The result is usually an expression for the gradient matrix \\(\\frac{\\partial L}{\\partial X}\\) that has the same shape as \\(X\\) and can be computed efficiently. The dimensions of these abstract Jacobians would be:\n\n\\(\\frac{\\partial z}{\\partial x}\\) relating a (D_x x M_x) input to a (D_z x M_z) output.\n\\(\\frac{\\partial z}{\\partial y}\\) relating a (D_y x M_y) input to a (D_z x M_z) output.\n\nThese would be very high-dimensional objects if we thought of them explicitly. The key takeaway is that even though the underlying math involves Jacobians of matrix-valued functions, for common operations like matrix multiplication, element-wise functions, convolutions, etc., we have pre-derived, efficient formulas for computing the gradient \\(\\frac{\\partial L}{\\partial X}\\) given \\(\\frac{\\partial L}{\\partial Z}\\) and the original inputs, and these formulas maintain the correct shapes.\nLet’s look at a very common example: matrix multiplication.\n\nWe have an input matrix \\(x\\) of size N x D, and a weight matrix \\(w\\) of size D x M. The output \\(y\\) is their product: \\(y = x w\\). This output \\(y\\) will be an N x M matrix. The formula for an element \\(y_{n,m}\\) of the output is \\(y_{n,m} = \\sum_d (x_{n,d} w_{d,m})\\). This is the standard definition of matrix multiplication – the dot product of the n-th row of \\(x\\) with the m-th column of \\(w.\\) We are given the upstream gradient \\(\\frac{\\partial L}{\\partial y}\\), which is an N x M matrix, having the same shape as \\(y\\). Our goal is to find \\(\\frac{\\partial L}{\\partial x}\\) (an N x D matrix) and \\(\\frac{\\partial L}{\\partial w}\\) (a D x M matrix). Now, if we were to think about the Jacobians explicitly, the Jacobian \\(\\frac{\\partial y}{\\partial x}\\) would relate an (N x D) input to an (N x M) output, the Jacobian \\(\\frac{\\partial y}{\\partial z}\\) would relate a (D x M) input to an (N x M) output. These would be 4D tensors! For example, if N=64 (batch size), D=M=4096 (common hidden layer sizes), then NxD is roughly 250,000, and NxM is also roughly 250,000. So the Jacobian \\(\\frac{\\partial y}{\\partial x}\\) would have (ND)  (N*M) elements, which is about (2.5e5)^2 = 6.25e10 elements. If each is a 4-byte float, that’s 250 GB of memory just for one Jacobian! Clearly, we must work with them implicitly! We cannot afford to form these giant Jacobian tensors. So, let’s reason about it element by element. What parts of \\(y\\) are affected by one element of \\(x\\)? Let’s consider \\(x_{n,d}\\). The element \\(x_{n,d}\\) affects the whole n-th row of y (e.g., \\(y_{n,:}\\)), this is because \\(x_{n,d}\\) participates in the calculation of \\(y_{n,m}\\) for all values of \\(m\\). So, to find \\(\\frac{\\partial y}{\\partial x_{n,d}}\\), we need to sum up the influence of \\(x_{n,d}\\) on each \\(y_{n,m}\\) and then how each \\(y_{n,m}\\) influences \\(L\\). Using the chain rule:\n\\[\n\\frac{\\partial y}{\\partial x_{n,d}} = \\sum_m{ \\frac{\\partial L}{\\partial y_{n,m}} \\frac{\\partial y_{n,m}}{\\partial x_{n,d}}}\n\\]\nNow, let’s look at the local derivative \\(\\frac{\\partial y_{n,m}}{\\partial x_{n,d}}\\). From \\(y_{n,m} = \\sum_k{ x_{n,k}  w_{k,m}}\\), if we vary \\(x_{n,d}\\), how does \\(y_{n,m}\\) change? Well, \\(x_{n,d}\\) is multiplied by \\(w_{d,m}\\) to contribute to \\(y_{n,m}\\). So\n\\[\n\\begin{align}\n\\frac{\\partial y_{n,m}}{\\partial x_{n,d}} &= w_{d,m} \\\\\n\\frac{\\partial L}{\\partial x_{n,d}} &= \\sum_m \\frac{\\partial L}{\\partial y_{n,m}} w_{d,m}\n\\end{align}\n\\]\nAnd that sum is exactly the formula for the (n,d)-th element of the matrix product \\(\\frac{\\partial L}{\\partial y} w^T\\). So, in matrix form:\n\\[\n\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y}  * w^T\n\\]\nLet’s check shapes\n\\[\n\\overbrace{\\frac{\\partial L}{\\partial x}}^{N \\times D}\n= \\underbrace{\\frac{\\partial L}{\\partial y}}_{N\\times M}\\;\n  \\overbrace{w^{\\!T}}^{M\\times D}\n\\]\nBy similar logic for \\(\\frac{\\partial L}{\\partial w}\\) , we can derive:\n\\[\n\\overbrace{\\frac{\\partial L}{\\partial w}}^{ D \\times M } =\n\\underbrace{x^T}_{D \\times N}\n\\overbrace{\\frac{\\partial L}{\\partial y}}^{N \\times M}\n\\]\nAnd there’s a nice mnemonic: they are the only way to make the shapes match up! If you remember the input and output shapes, and that you need to use \\(\\frac{\\partial L}{\\partial y}\\) and the other input matrix (or its transpose), you can often re-derive these just by making the dimensions work out for matrix multiplication. So, for a matrix multiply gate, given the upstream gradient \\(\\frac{\\partial L}{\\partial y}\\) , we can directly compute the downstream gradients \\(\\frac{\\partial L}{\\partial x}\\) and \\(\\frac{\\partial L}{\\partial w}\\) using these matrix multiplication formulas, without ever forming the explicit giant Jacobian. This is how it’s done in practice."
  },
  {
    "objectID": "posts/regularization_and_optimization/index.html",
    "href": "posts/regularization_and_optimization/index.html",
    "title": "Regularization and Optimization",
    "section": "",
    "text": "“An explorer seen from behind, standing on a high rocky ridge looking through binoculars. The figure overlooks a vast valley rendered as a topographical map with intricate, glowing contour lines. At the bottom of this valley, a radiant orb pulses with energy. Two paths descend the landscape: one is a chaotic, jagged red line, and the other is a smooth river of glowing blue light flowing directly to the orb,” generated by DALL-E 3"
  },
  {
    "objectID": "posts/regularization_and_optimization/index.html#we-have-a-loss-function-now-what",
    "href": "posts/regularization_and_optimization/index.html#we-have-a-loss-function-now-what",
    "title": "Regularization and Optimization",
    "section": "We have a loss function, now what?",
    "text": "We have a loss function, now what?\nIn the last blog post we started by framing our core problem Image Classification. This is a fundamental task in computer vision. Given an image, our goal is to assign it one label from a predefined set of categories, the model needs to output the correct label. And we talked about why this is so challenging, right? It’s not trivial for a computer. We have immense variability due to viewpoint changes, different illumination conditions, deformations (cats are particularly good at this!), occlusion where parts of the object are hidden, clutter in the background, and of course, massive intraclass variation, think of all the different breeds and appearances of cats. To tackle this, we introduced the data-driven approach. Instead of trying to explicitly code rules for every variation, we collect a large dataset of images and their labels. We then use machine learning to learn the patterns. We talk about a powerful, parametric approach the Linear Classifier. Here, the idea is to learn a set of parameters, or weights, denoted by \\(W\\) (and a bias term \\(b\\)). For an input image \\(x\\), which we typically flatten into a vector, our score function is \\(f(x, W) = Wx + b\\). This function computes scores for each class. We looked at this from a few different perspectives. The Algebraic Viewpoint: just a matrix multiplication and a bias addition. The Visual Viewpoint: where each row of W can be seen as a template for a class. The model is trying to learn what an “ideal” cat or dog template looks like. And the Geometric Viewpoint: where each row of W defines a hyperplane, and the classifier is essentially carving up the high-dimensional image space with these hyperplanes.\nOkay, so we have these scores from our linear classifier. The question is: How good are these scores? How good is our current set of parameters \\(W\\)? For this, we introduced the concept of a loss function. A loss function, often denoted \\(L_i\\) for a single example, quantify how unhappy we are with the prediction of that example. Given a dataset of N examples \\({(x_i, y_i)}\\) where \\(x_i\\) is the image and \\(y_i\\) is its true integer label, we can compute the total loss. Typically the overall loss \\(L\\) is the average of the individual loss \\(L_i\\) over all the training example. This loss function is our guide. It tells us how well our current classifier is performing. A high loss means our classifier is doing poorly; a low loss means it’s doing well. So we have a way to score images and a way to measure how good those score are. The next natural question are: how do we find the parameters \\(W\\) that minimize this loss? And how do we make sure our model doesn’t just memorize the training data but actually learn to generalize? And that precisely where we’re headed today with Regularization and Optimization We’ll start by looking at regularization.\nSo far, here’s our loss function:\n\\[\nL(W) = \\frac{1}{N} \\sum_{i = 1}^N L_i(f(x_i, W), y_i)\n\\]\nCurrently our loss function just has the data loss. This term, the average of \\(L_i\\) over our \\(N\\) training examples, measure how well our model’s prediction, \\(f(x_i, W)\\), match the true labels, \\(y_i\\) on the training data. Our goal is to make this data loss small. However, if we only focus on minimizing the data loss, we can run into a problem called overfitting. Our model might become too good at fitting the training data, including its noise and then fail to generalize to new, unseen data."
  },
  {
    "objectID": "posts/regularization_and_optimization/index.html#keeping-model-honest",
    "href": "posts/regularization_and_optimization/index.html#keeping-model-honest",
    "title": "Regularization and Optimization",
    "section": "Keeping model honest",
    "text": "Keeping model honest\nThis is where regularization comes in. We modify our loss function to include an additional term, often denoted as \\(R(W)\\), which is the regularization penalty. So, our full loss function now become the sum of the data loss and this regularization term, scaled by a hyperparameter lambda (\\(\\lambda\\))\n\\[\nL(W) = \\frac{1}{N} \\sum_{i = 1}^N L_i(f(x_i, W), y_i) + \\lambda R(W)\n\\]\nThe data loss part as before, pushes the model to fit the training data. The new regularization term, \\(R(W)\\), is designed to penalize model complexity. Its job is essentially to prevent the model from doing too well on the training data, or more precisely, fitting the training data in an overly complex way. Lambda, the regularization strength, control the trade-off between these two terms: fitting the data well versus keeping the model simple.\nThis preference for simpler models, as we discussed, is nicely encapsulated by Occam’s Razor. The principle, attributed to William of Ockham, states that among multiple competing hypotheses, the simplest one is generally the best. Our regularization term \\(R(W)\\) is our way of mathematically encoding a preference for certain types of “simpler” weight matrices \\(W\\)."
  },
  {
    "objectID": "posts/regularization_and_optimization/index.html#a-penalty-for-complexity",
    "href": "posts/regularization_and_optimization/index.html#a-penalty-for-complexity",
    "title": "Regularization and Optimization",
    "section": "A penalty for complexity",
    "text": "A penalty for complexity\nSo what are some common form of this regularization term \\(R(W)\\)? Here are a few classic examples:\n\\[\nR(W) = \\sum_{k} \\sum_{l} W^{2}_{k,l}\n\\]\nL2 regularization, also known as weight decay or Ridge regression in other contexts. Here, \\(R(W)\\) is the sum of the squares of all the individual weight elements \\(W_{k,l}\\). This penalty discourages very large weights. It prefers to distribute weights among many features rather than having a few features with very large weights. It leads to diffuse, small weight vector\n\\[\nR(W) = \\sum_{k} \\sum_{l} |W_{k,l}|\n\\]\nL1 regularization, also known as Lasso. Here, R(W) is the sum of the absolute values of the weights. L1 regularization has an interesting property: it tends to produce sparse weight vectors, meaning many of the weights will become exactly zero. This can be useful for feature selection, as it effectively tells you which input features the model deems unimportant.\n\\[\nR(W) = \\sum_{k} \\sum_{l} \\beta W^{2}_{k,l} + |W_{k,l}|\n\\]\nElastic Net regularization is simply a linear combination of L1 and L2 regularization. This tries to get the best of both worlds, offering a balance between the diffuse weights of L2 and the sparsity of L1. The \\(\\beta\\) here would be another hyperparameter controlling the mix.\nThese are some of the most common, traditional forms of regularization that directly penalize the magnitudes of the weights. But the concept of regularization is broader than just L1 and L2 penalty on weights. There are many other techniques, particularly in deep learning, that have a regularizing effect, even if they don’t explicitly appear as an R(W) term added to the loss. For examples,\n\nDropout: This involves randomly setting some neuron activations to zero during training. It prevents co-adaptation of neurons and encourages more robust feature learning\nBatch Normalization: This normalized the activations between mini-batch. While primarily introduced to help with optimization and training stability, it also has a slight regularizing effect.\nAnd there are others like Stochastic Depth(randomly drop entire layer during training) or fractional pooling, which introduce stochasticity or constrains during training to improve regularization.\n\nSo, while we often start by thinking about L1 or L2, keep in mind that regularization is a general concept of adding something to your training process to prevent overfitting and improve performance on unseen data.\nNote that one of the reason that I think really interesting about why we regularize is to improve optimization by adding curvature. This is a more subtle point, especially relevant for L2 regularization. Sometimes, the loss landscape can be very flat in certain directions, making optimization difficult. Adding an L2 penalty (which is a quadratic bowl shape) can add curvature to the loss surface, potentially making it easier for optimization algorithms like gradient descent to find a good minimum."
  },
  {
    "objectID": "posts/regularization_and_optimization/index.html#finding-the-bottom-of-the-valley",
    "href": "posts/regularization_and_optimization/index.html#finding-the-bottom-of-the-valley",
    "title": "Regularization and Optimization",
    "section": "Finding the bottom of the valley",
    "text": "Finding the bottom of the valley\nSo, we’ve defined what it means for a set of weights W to be “good”, it should result in a low values for the total loss \\(L\\). The big question that remain is: How do we find the best \\(W\\)? This is not a trivial task, especially \\(W\\) is consist of millions or even billions, of parameters in modern neural networks. We’re searching in an incredibly high-dimensional space. And this leads us to our major topic for today: Optimization\n\nTo build some intuition, let’s think about a simpler analogy. Imagine this beautiful mountain landscape. You can think of the terrain here as representing our loss function. The east-west and north-south directions could represent, say, two of our weights, W1 and W2 (though in reality, we have many more dimensions). And the altitude at any point (W1, W2) represents the value of our loss function L(W) for those particular weight settings. Our goal, then, is to find the lowest point in this valley – the point where the loss is minimized. So if we imagine ourselves, or this intrepid hiker, standing somewhere in the landscape, what’s our strategy for getting to the bottom of the valley? We’re trying to find the minimum. This is precisely what optimization algorithms are designed to do: to provide a systematic way to navigate this loss landscape and find a good set of parameters \\(W\\). Now, how might we go about this? What strategy could we employ to find this minimum? Let’s consider a few approaches, starting with a very simple one, perhaps naive, one.\n\nStrategy 1: Random search\nThis is generally a very bad idea for optimizing a complex models, but it’s a simple baseline to start with. The idea is straightforward\n# assume X_train is the data where each column is an example (e.g. 3073 x 50,000)\n# assume Y_train are the labels (e.g. 1D array of 50,000)\n# assume the function L evaluates the loss function\n\nbestloss = float('inf')  # Python assigns the highest possible float value\nfor num in xrange(1000):\n    W = np.random.randn(10, 3073) * 0.0001  # generate random parameters\n    loss = L(X_train, Y_train, W)  # get the loss over the entire training set\n    if loss &lt; bestloss:  # keep track of the best solution\n        bestloss = loss\n        bestW = W\n    print('in attempt %d the loss was %f, best %f' % (num, loss, bestloss))\nin attempt 0 the loss was 9.401632, best 9.401632\nin attempt 1 the loss was 8.959668, best 8.959668\nin attempt 2 the loss was 9.044034, best 8.959668\nin attempt 3 the loss was 9.278948, best 8.959668\nin attempt 4 the loss was 8.857370, best 8.857370\nin attempt 5 the loss was 8.943151, best 8.857370\nin attempt 6 the loss was 8.605604, best 8.605604\n... (truncated: continues for 1000 lines)\nAs you can see from the example printout, with each attempt, we got a loss value. Sometimes we find a better \\(W\\), sometimes we don’t. We just keep trying random configurations and hope to stumble upon a good one. This is, as you might imagine, highly inefficient. The space of possible W matrices is astronomically vast. Just randomly sampling points in this space is like trying to find a specific grain of sand on all the beaches of the world by randomly picking up grains. But, let’s humor ourselves. Suppose we run this random search for a while and get our bestW. How well does it actually perform on unseen data?\n# assume X_test is [3073 x 10000A], y_test [10000 x 1]\nscores = bestW.dot(X_test) # 10 x 10000, the class scores for all test examples\ny_pred = np.argmax(scores, axis=0)\nnp.mean(y_pred == y_test)\n0.1555\nAnd the result? We get 15.5% accuracy! Now, for a 10-class classification problem like CIFAR-10, random guessing would give you 10% accuracy. So, 15.5% is… well, it’s better than random! One might sarcastically say “not bad!” given the simplicity of the approach. However, if we compare this to the state-of-the-art (SOTA) for image classification on datasets like CIFAR-10, which can be well above 90%, even approaching 99.7% for sophisticated models, 15.5% is clearly terrible. It highlights that simply guessing random parameters is not a viable strategy for training effective machine learning models. So, random search is a non-starter for any serious application. We need a more intelligent way to navigate the loss landscape. We need a strategy that uses information about the landscape itself to guide the search. This brings us to our next, much more sensible, strategy. We want to “follow the slope.”\n\n\nStrategy 2: Follow the slope\nGoing back to our mountain analogy, if you’re standing on a hillside and want to get to the bottom of the valley, what’s the most intuitive thing to do? You look around, feel for the direction where the ground slopes downwards most steeply, and you take a step in that direction. Right? You follow the path of steepest descent. This intuitive idea of “slope” has a precise mathematical counterpart.\nIn 1-dimension, if we have a function \\(f(x)\\), the concept of slope is captured by derivative \\(\\frac{df}{dx}\\). As you’ll recall from calculus, the derivative is defined as\n\\[\n\\frac{df(x)}{dx} = \\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{h}\n\\]\nIt tells us how much the function \\(f(x)\\) changes for an infinitesimally small change in \\(x\\). A positive derivative means the function is increasing, and a negative derivative means it’s decreasing. Now our loss function \\(L(W)\\) is not a function of a single variable; it’s a function of many variables (all the elements of our weight matrix \\(W\\)). So, we’re in a multiple-dimension scenario. In this case, the equivalent of the derivative is the gradient. The gradient of \\(L\\) with respect to \\(W\\) often denoted as \\(\\nabla L(W)\\) or \\(\\nabla_W L\\), is a vector of partial derivatives. Each elements of this gradient vector tells us the slope of the loss function along the direction of the corresponding weight. For example \\(\\frac{\\nabla L}{\\nabla W_{ij}}\\) tells us how the loss \\(L\\) change if we make a tiny change to the weight \\(W_{ij}\\), keeping other weights constant.\nSo, the gradient vector \\(\\nabla L(W)\\) points in the direction in which the loss function L(W) increases the fastest. How do we find the slope in any arbitrary direction? If we have some direction vector \\(u\\), the slope in that direction is given by the dot product of that direction vector \\(u\\) with the gradient vector \\(\\nabla L(W)\\). And most importantly for our goal of minimizing the loss: the direction of steepest descent – the direction in which the loss function decreases the fastest it’s simple but happens to be true is the negative gradient, i.e., \\(-\\nabla L(W)\\). So, if we can compute this gradient vector, we have a clear instruction: to reduce the loss, take a small step in the direction opposite to the gradient. This is the fundamental idea behind one of the most important optimization algorithms in machine learning.\nThis method is called the Numeric Gradient. And it has a couple of important properties: 1) It’s Slow! We need to loop over all dimensions (all parameters) and perform a full loss computation for each one. For models with millions of parameters, this is completely impractical for training. 2) It’s Approximate. Because we’re using a finite h instead of the true limit as h approaches zero, what we calculate is an approximation of the true gradient. The choice of h is also a bit tricky – too small and you hit numerical precision issues; too large and the approximation is poor. So, while the numeric gradient is conceptually simple and directly follows from the definition of a derivative, it’s not how we typically train our models due to its inefficiency. However, it’s an incredibly useful tool for debugging our more efficient gradient calculation methods, which we’ll get to. If you implement a more complex way to calculate the gradient (like backpropagation), you can compare its output to the numeric gradient on a small example to check if your implementation is correct. This is called a gradient check. But for actually doing the optimization, we need something much faster. The loss function \\(L(W)\\) is, after all, just a mathematical function of \\(W\\). Can’t we use calculus to find an exact, analytical expression for the gradient?\nThe key insight is that our loss function \\(L\\) is just a function of \\(W\\)\n\\[\n\\begin{align}\nL   &= \\frac{1}{N} \\sum_{i=1}^{N} L_i + \\sum_{k} W_{k}^2 \\\\\nL_i &= \\sum_{i \\neq y_i} \\max(0, s_j - s_{y_i} + 1) \\text{ or } L_i = -\\log(\\frac{e^{s_{y_i}}}{\\sum_{j}e^{s_j}}) \\\\\ns  &= f(x,W) = Wx \\\\\n&\\text{ want } \\nabla_W L\n\\end{align}\n\\]\nSo you can see \\(L\\) ultimately a mathematical function of \\(W\\), the input \\(x\\) and the true label \\(y_i\\) are fix constant from our dataset, the only thing we’re changing is \\(W\\). What we want is the gradient of this entire expression L with respect to \\(W\\) denoted as \\(\\nabla_W L\\). Instead of numerically approximating this gradient by adjusting each \\(W_k\\), we should be able to use the rules of calculus.\n\n\n\nNewton and Leibniz\n\n\nGiven that we have these well-defined mathematical functions, we can use calculus to compute an analytic gradient. This means we can derive a direct mathematical formula for \\(\\nabla_W\\). This is where our friend Newton and Leibniz, the inventors of calculus, come into play. Their work allow us to find these gradient efficiently and exactly (up to machine precision), without any of this iteration perturbation. Computing the analytic gradient involves applying the chain rule repeatedly, because our loss function is a composition of several functions (linear score function, then the loss calculation like hinge or softmax, then averaging, then adding regularization). For complex, deep neural networks, this process of applying the chain rule systematically is known as backpropagation which we will talk about very soon in other post. For now, let’s appreciate that deriving an analytic formula for the gradient is the efficient and exact way to go. It’s much faster than the numerical gradient because we just evaluate one formula, rather than N+1 evaluations of the loss function. So now instead of numerically calculating \\(dW\\) element by element, if we had derived the analytic gradient, \\(dW\\) would be given by some function that depends on our input data and the current \\(W\\). We would plug our data and current \\(W\\) into this derived formula, and it would directly give us the entire gradient matrix \\(dW\\) in one go.\nSo we have two ways to think about computing gradient:\n\nNumerical gradient:\n\nIt’s approximate (due to the finite h).\nIt’s very slow (requires N+1 evaluations of the loss function for N parameters).\nHowever, it’s generally easy to write the code for, as it directly implements the definition of a derivative.\n\nAnalytic gradient:\n\nIt’s exact (up to machine precision).\nIt’s very fast (typically a single pass of computation once the formula is derived).\nHowever, deriving and implementing the formula, especially for complex models, can be error-prone. It’s easy to make a mistake in the calculus or in the code.\n\n\nSo, what does this lead us to in practice? In practice: Always use the analytic gradient for training your models because of its speed and exactness. However, because it’s error-prone to implement, you should check your implementation with the numerical gradient. This crucial debugging step is called a gradient check. How does gradient check work? You implement your analytic gradient. Then for a small test case like a small \\(W\\) and a few data point, you also compute the numerical gradient. You then compare the two results. If they are very close, you can be reasonably confident that your analytic gradient implementation is correct. If they differ significantly, you have a bug in your analytic gradient derivation or code."
  },
  {
    "objectID": "posts/regularization_and_optimization/index.html#sgd",
    "href": "posts/regularization_and_optimization/index.html#sgd",
    "title": "Regularization and Optimization",
    "section": "SGD",
    "text": "SGD\nOkay, so now we have an efficient and exact way to compute the gradient \\(\\nabla_W L\\), which tells us the direction of steepest ascent. To minimize the loss, we want to go in the opposite direction. This brings us to the core algorithm for optimization in deep learning: Gradient Descent. The core idea is remarkably simple, here is a simple implementation of “Vanilla Gradient Decent”.\nwhile True:\n  weights_grad = evaluate_gradient(loss_fun, data, weights)\n  weights += - step_size * weights_grad # perform parameter update\nSo basically we start with some initial guess for our weights, then we enter a loop that (conceptually) runs “True” or until some stopping criterion is met. Inside the loop, the first crucial step is weights_grad = evaluate_gradient(loss_fun, data, weights). This is where we compute the analytic gradient of our loss function (loss_fun) with respect to the current weights, using our training data. This weights_grad is our \\(\\nabla_W L\\). Next is parameter update, we take the weights_grad multiply it by - step_size the negative sign is because we want to move in the direction opposite to the gradient (the direction of steepest descent). The step_size (also known as the learning rate) is a small positive scalar hyperparameter. It controls how far we step in that negative gradient direction. If it’s too large, we might overshoot the minimum. If it’s too small, training will be very slow. We then add this scaled negative gradient to our current weights to get the updated weights. And we repeat. We re-evaluate the gradient at the new weights, take another step, and so on, iteratively moving “downhill” on the loss surface.\n\n\n\n\n\nThis is Vanilla Gradient Descent. The “vanilla” part implies there are more sophisticated variants, which we’ll get to. One important detail in this vanilla version is that evaluate_gradient typically involves computing the gradient over the entire training dataset to get the true gradient of \\(L\\). This can be very computationally expensive if our dataset is large. This brings us to Stochastic Gradient Descent (SGD), or more commonly in practice, Minibatch Gradient Descent.\n\\[\n\\begin{align}\nL(W) &= \\frac{1}{N} \\sum_{i=1}^{N} L_i(x_i, y_i, W) + \\lambda R(W)\\\\\n\\nabla_W L &= \\frac{1}{N} \\sum_{i = 1}^N \\nabla_W L_i(x_i, y_i, W) + \\lambda \\nabla_W R(W)\n\\end{align}\n\\]\nThe problem is that computing this full sum over all N examples for \\(\\nabla_W L_i\\) is expensive when \\(N\\) is large. If you have millions of training examples, calculating the gradient across all of them just to make one tiny step with your weights is very inefficient. You’d be spending a lot of computation for a potentially very small update. The core idea of Stochastic Gradient Descent is to approximate this sum using a small random subset of the data called a minibatch. Instead of summing the gradients over all N examples, we sum them over just a small batch of, say, 32, 64, or 128 examples. Common minibatch sizes often range from 32 to 256, sometimes larger, depending on memory constraints and the specific problem. So, the gradient we compute on a minibatch is not the true gradient of the total loss L, but it’s an estimate or an approximation. Since the minibatch is sampled randomly, this estimate is unbiased on average, and it’s much, much faster to compute.\n# Vanilla Minibatch Gradient Descent\nwhile True:\n  data_batch = sample_training_data(data, 256) # sample 256 examples\n  weights_grad = evaluate_gradient(loss_fun, data_batch, weights)\n  weights += - step_size * weights_grad # perform parameter update\nThe key difference from Vanilla Gradient Descent is the first line inside the loop: data_batch = sample_training_data(data, 256). Here, instead of using the full data, we randomly sample a data_batch (e.g., 256 examples). This approach is “stochastic” because each gradient estimate is noisy and depends on the specific random minibatch chosen. However, by taking many such noisy steps, we still tend to move towards the minimum of the loss function, often much faster in terms of wall-clock time than full batch gradient descent because each step is so much cheaper. The term “Stochastic Gradient Descent (SGD)” technically refers to the extreme case where the minibatch size is 1 – you update the weights after seeing just one example. In practice, when people say “SGD” in deep learning, they almost always mean minibatch gradient descent. Using minibatches not only speeds up training but can also sometimes help the optimization process escape poor local minima or saddle points due to the noise in the gradient estimates, leading to better generalization.\nSo, SGD with minibatches is the workhorse for training almost all large-scale deep learning models. However, it’s not without its own set of challenges and nuances.\n\nProblem #1 with SGD: Dealing with ill-conditioned loss landscapes\n\nConsider a scenario where our loss function landscape looks like this: a long, narrow valley, or a ravine. The ellipses here represent level sets of the loss function, and the smiley face is at the minimum we want to reach. The question is: What if the loss changes quickly in one direction and slowly in another? For example, along the w2 direction (vertically), the valley is very steep – the loss changes rapidly. But along the w1 direction (horizontally), the valley is very shallow and elongated – the loss changes slowly. If we start at the red dot and apply standard SGD, what will happen? What gradient descent does is it always points in the direction of steepest descent. In such a ravine, the steepest direction is mostly perpendicular to the valley floor, pointing across the steep walls. So, SGD will tend to take large steps that oscillate back and forth across the narrow valley (the w2 direction, where the gradient is large). Because of these large oscillations, if we use a single learning rate, it has to be small enough to prevent divergence in this steep direction. However, this small learning rate means that progress along the shallow direction of the valley (the w1 direction, where the gradient component is small) will be agonizingly slow. The result is very slow progress along the shallow dimension, and a lot of jitter or oscillation along the steep direction, as shown by the red zigzag path. We’re making very inefficient progress towards the true minimum. This kind of behavior is characteristic of loss functions that are ill-conditioned.\n\n\nProblem #2 with SGD: Local Minima and Saddle Points\nThe loss functions for deep neural networks are highly non-convex. This means they can have many local minima – points where the loss is lower than all its immediate neighbors, but not necessarily the global minimum (the lowest possible loss overall). They can also have saddle points.\n\n\n\nSaddle point, image source: https://en.wikipedia.org/wiki/Saddle_point\n\n\nAt a local minimum, or at a saddle point, the gradient is zero (or very close to zero). If the gradient is zero, then weights_grad in our update rule weights += - step_size * weights_grad is zero. This means the weights stop updating. Gradient descent gets stuck! It thinks it has found the bottom of a valley, but it might just be a small dip, or a point that’s flat in some directions but goes up in others.\nNow, for a long time, people were very concerned about local minima being a major impediment to training deep networks. The fear was that SGD would get trapped in a poor local minimum and never find a good solution. However, more recent research, like the paper by Dauphin et al. (2014) cited here, suggests that in very high-dimensional spaces (which is where our weight matrices W live), saddle points are actually much more common than local minima. For a point to be a local minimum, the loss function needs to curve upwards in all dimensions around that point. For it to be a local maximum, it needs to curve downwards in all dimensions. For it to be a saddle point, it needs to curve upwards in some dimensions and downwards in others. In high dimensions, it’s statistically much more likely to have a mix of curvatures than for all curvatures to go in the same direction. So, while getting stuck is a problem, it’s often saddle points, rather than bad local minima, that are the primary culprits for slowing down or halting SGD. At a saddle point, the gradient is zero, but it’s not a minimum.\nSo, that’s the second major issue: points with zero or near-zero gradient (local minima and, more commonly, saddle points) can trap or significantly slow down SGD.\n\n\nProblem #3 with SGD: Noisy Gradients\nThis problem is inherent in the “S” of SGD – the stochasticity. As we discussed, our loss \\(L(W)\\) is an average over all \\(N\\) training examples (plus regularization, which we’ll ignore for this point for simplicity). And it’s true gradient is:\n\\[\n\\nabla_W L = \\frac{1}{N} \\sum_{i = 1}^N \\nabla_W L_i(x_i, y_i, W)\n\\]\nHowever, with minibatch SGD, we don’t compute this full sum. We compute the gradient on a small minibatch. This minibatch gradient is an estimate of the true gradient. Because the minibatch is a random sample, this estimate will be noisy. It won’t point exactly in the direction of the true steepest descent. If you take many different minibatches at the same point \\(W\\), you’ll get slightly different gradient vectors.\n\n\n\n\n\nThe noisy gradients mean that our steps won’t be as smooth as the idealized animation we saw earlier. They’ll be a bit erratic. While this noise can sometimes be helpful (e.g., for escaping sharp local minima or some saddle points), it also means that the convergence path can be jittery, and finding the exact minimum can be harder. The optimization path might dance around the minimum without settling perfectly.\nSo, these three problems – ill-conditioned landscapes (ravines), local minima/saddle points, and noisy gradients – are key challenges that standard SGD faces. This has motivated the development of more advanced optimization algorithms that try to mitigate these issues. One of the first and most important enhancements is the idea of momentum."
  },
  {
    "objectID": "posts/regularization_and_optimization/index.html#momentum",
    "href": "posts/regularization_and_optimization/index.html#momentum",
    "title": "Regularization and Optimization",
    "section": "Momentum",
    "text": "Momentum\nThe first major improvement we’ll discuss is adding Momentum to SGD. If SGD gets to a flat region where the gradient is zero, it stops. Momentum, like a ball rolling downhill, has inertia. It might be able to “roll through” small local minima or flat regions of saddle points because it has built up speed from previous gradients. In that zigzagging scenario, the gradient components across the ravine tend to cancel out on average over iterations due to the oscillation. However, the small gradient components along the valley consistently point in the same direction. Momentum helps to average out these oscillations and amplify the consistent movement along the valley floor. You can see the blue line (SGD+Momentum) makes much more direct progress towards the minimum compared to the black zigzagging SGD line. The noise in minibatch gradients causes SGD to jitter. Momentum helps to smooth out these noisy updates by taking a running average of the gradients. This leads to a more stable and often faster convergence towards the minimum\n\n\n\n\n\nNow, for SGD + Momentum. The key idea is to continue moving in the general direction as the previous iterations. We introduce a “velocity” term, v, which accumulates a running mean of the gradients.\n\\[\n\\begin{align}\nv_{t+1} &=  \\rho v_t + \\nabla f(x_t) \\\\\nx_{t+1} &= x_t - \\alpha v_{t+1}\n\\end{align}\n\\]\n\n\\(v_{t}\\) is the velocity vector from the previous step. It’s initialized to zero.\n\\(\\rho\\) is the momentum coefficient (or friction term). It’s typically a value like 0.9 or 0.99. It determines how much of the previous velocity is retained. If ρ=0, we recover standard SGD (almost, as we’ll see the learning rate is applied differently).\nIn the first equation, we update the velocity: we take the old velocity \\(v_t\\), scale it down by \\(\\rho\\), and then add the current gradient \\(\\nabla f(x_t)\\). So, the velocity v is essentially an exponentially decaying moving average of the past gradients.\nIn the second equation, we update the parameters \\(x\\) by taking a step in the direction of this new velocity \\(v_t+1\\), scaled by the learning rate \\(\\alpha\\). Crucially, we are now stepping with the velocity, not directly with the current gradient.\n\nThe intuition is that we build up “velocity” as a running mean of gradients. If gradients consistently point in the same direction, the velocity in that direction grows. If gradients oscillate, those components of the velocity tend to be dampened. The term \\(\\rho\\) acts like friction. If \\(\\rho\\) is close to 1 (e.g., 0.99), we have high momentum, and the velocity persists for a long time. If \\(\\rho\\) is smaller (e.g., 0.5), it’s like having more friction, and the velocity relies more on recent gradients. The paper by Sutskever et al. (2013) is a classic reference highlighting the importance of momentum in deep learning\nHere’s how this might look in code:\nvx = 0 # initialize velocity to zero\nwhile True:\n  dx = compute_gradient(x) # current gradient\n  vx = rho * vx + dx       # update velocity\n  x -= learning_rate + vx  # update parameters\nMomentum is a very powerful and widely used technique. But it’s not the only improvement over vanilla SGD. Other methods try to adapt the learning rate on a per-parameter basis, which can be particularly helpful for those ill-conditioned ravine-like loss surfaces."
  },
  {
    "objectID": "posts/regularization_and_optimization/index.html#smarter-steps-with-adaptive-optimizer",
    "href": "posts/regularization_and_optimization/index.html#smarter-steps-with-adaptive-optimizer",
    "title": "Regularization and Optimization",
    "section": "Smarter steps with adaptive optimizer",
    "text": "Smarter steps with adaptive optimizer\nOkay, so SGD with Momentum helps to accelerate in consistent directions and dampen oscillations. But what about that problem of different curvatures in different dimensions – the ravines? Momentum helps, but can we do even better? This leads us to optimizers that try to adapt the learning rate on a per-parameter basis. One popular example is RMSProp. Recall SGD+Momentum: we compute a velocity vx and update x using learning_rate * vx. Now, for RMSProp, developed by Tieleman and Hinton:\n1grad_square = 0\nwhile True:\n  dx = compute_gradient(x)\n2  grad_square = decay_rate * grad_square + (1 - decay_rate) * dx * dx\n3  x -= learning_rate * dx / (np.sqrt(grad_squared) + 1e-7)\n\n1\n\nWe maintain a variable grad_squared. This variable will keep track of an exponentially decaying average of the squared gradients for each parameter. It’s initialized to zero.\n\n2\n\nThis is an exponentially weighted moving average. decay_rate is a hyperparameter, typically something like 0.9 or 0.99. We’re taking the previous grad_squared, decaying it, and adding the newly computed dx * dx (element-wise square of the current gradient), scaled by 1 - decay_rate. So, grad_squared accumulates information about the typical magnitude of recent gradients for each parameter.\n\n3\n\nthis is the parameter update rule, we take our square root of our accumulated grad_squared this gives us something like the Root Mean Square of recent gradients, add a small epsilon (like 1e-7) for numerical stability to prevent division by zero if grad_squared is tiny, and then we divide the current gradient dx by this term.\n\n\nThe effect of this division by np.sqrt(grad_squared) is crucial. If a particular parameter has consistently had large gradients in the past (meaning its corresponding element in grad_squared is large), then np.sqrt(grad_squared) will be large, and the effective step size for that parameter learning_rate / (np.sqrt(grad_squared) + epsilon) will be smaller. Conversely, if a parameter has consistently had small gradients (so grad_squared for it is small), then np.sqrt(grad_squared) will be small, and its effective step size will be larger. This effectively gives us per-parameter learning rates or adaptive learning rates. The learning rate is adapted for each parameter based on the history of its gradient magnitudes.\nWhat happens with RMSProp? Specifically, how does this help with that ravine problem? What happens is that progress along “steep” directions is damped, and progress along “flat” directions is accelerated. Think back to our ravie. In the steep direction (e.g., w2), the gradients dx are large. This means dx * dx will be large, and grad_squared will accumulate to a large value for that dimension. Consequently, when we divide dx by np.sqrt(grad_squared), we significantly reduce the step size in that steep direction. This helps to prevent the oscillations. In the flat, shallow direction (e.g., w1), the gradients dx are small. dx * dx will be small, and grad_squared will be small for that dimension. When we divide dx by a small np.sqrt(grad_squared), the effective step size for that direction is relatively larger (or at least not as suppressed). This helps to make faster progress along the shallow valley. So, RMSProp automatically adjusts the learning rate for each parameter, making bigger steps in directions where gradients have been small and smaller steps in directions where gradients have been large.\n\n\n\n\n\n\n\nblue line: RMSProp, red line: SGD+Momentum, black line: SGD\n\n\nRMSProp is a very effective optimizer and was a popular choice for quite some time. It directly addresses the issue of different learning rates being needed for different parameter dimensions. Now, what if we could combine the benefits of momentum with the adaptive learning rates of RMSProp? That leads us to Adam.\nHere’s a look at the core of the Adam optimizer, presented as “Adam (almost)” because it’s missing one small but important detail we’ll get to. This was introduced by Kingma and Ba in their 2015 ICLR paper.\nfirst_moment = 0\nsecond_moment = 0\nwhile True:\n  dx = compute_gradient(x)\n1  first_moment = beta1 * first_moment + (1 - beta1) * dx\n2  second_moment = beta2 * second_moment + (1 - beta2) * dx * dx\n3  x -= learning_rate * first_moment / (np.sqrt(second_moment) + 1e-7)\n\n1\n\nThis is an exponentially decaying moving average of the gradient itself. beta1 is a decay rate (e.g., 0.9). This looks very much like the velocity update in SGD+Momentum! It’s capturing the “momentum” aspect which is an estimate of the mean of the gradients.\n\n2\n\nThis is an exponentially decaying moving average of the squared gradients. beta2 is another decay rate (e.g., 0.999). This looks exactly like the grad_squared update in RMSProp! It’s capturing information about the variance (or uncentered second moment) of the gradients.\n\n3\n\nHere, we’re using the first_moment (our momentum term) in the numerator, and we’re dividing by the square root of the second_moment (our RMSProp-like adaptive scaling term) in the denominator.\n\n\nIt’s sort of like RMSProp with momentum. It’s trying to get the best of both worlds: the acceleration benefits of momentum and the per-parameter adaptive learning rates. Remember, first_moment and second_moment are initialized to zero. What’s the implication of this?\nAt the very first timestep (and for the first few timesteps), because first_moment and second_moment start at zero and the decay rates beta1 and beta2 are typically close to 1 (e.g., 0.9, 0.999), these moving average estimates will be biased towards zero. They haven’t had enough gradient information to “warm up” to their true values. This can cause the initial steps to be undesirably small. This is where the “almost” part is resolved. The full form of Adam includes a bias correction step to address this initialization bias.\nfirst_moment = 0\nsecond_moment = 0\nfor t in range(1, num_iterations): # t is the timestep\n  dx = compute_gradient(x)\n  first_moment = beta1 * first_moment + (1 - beta1) * dx\n  second_moment = beta2 * second_moment + (1 - beta2) * dx * dx\n\n  # Bias correction\n  first_unbias = first_moment / (1 - beta1**t)\n  second_unbias = second_moment / (1 - beta2**t)\n\n  x -= learning_rate * first_unbias / (np.sqrt(second_unbias) + 1e-7)\nHere t is the current iteration number (timestep). At the beginning with small t, beta1**t and beta2**t are close to beta1 and beta2. So, (1 - beta1**t) and (1 - beta2**t) are small numbers, which effectively scales up the first_moment and second_moment estimates, counteracting their initial bias towards zero. As t gets large, beta1**t and beta2**t approach zero (since beta1, beta2 &lt; 1), so (1 - beta1**t) and (1 - beta2**t) approach 1, and the bias correction has less and less effect. The parameter update then uses these bias-corrected first_unbias and second_unbias estimates. This bias correction is for the fact that the first and second moment estimates start at zero and would otherwise be biased, especially during the early stages of training.\n\n\n\n\n\n\n\nblue line: RMSProp, red line: SGD+Momentum, black line: SGD, purple: Adam\n\n\nThe Adam optimizer, with its combination of momentum-like behavior and RMSProp-like adaptive scaling, plus this bias correction, has proven to be very effective and robust across a wide range of deep learning tasks and architectures. The original paper suggests default values for the hyperparameters:\n\nbeta1 = 0.9 (for the first moment/momentum)\nbeta2 = 0.999 (for the second moment/RMSProp-like scaling)\nlearning_rate typically in the range of 1e-3 (0.001) or 5e-4 (0.0005).\nThe epsilon for numerical stability is often 1e-7 or 1e-8.\n\nAnd indeed, Adam with these default settings (beta1=0.9, beta2=0.999, and a learning_rate like 1e-3) is often a great starting point for many models! It’s frequently the first optimizer people try, and it often works quite well out of the box without extensive hyperparameter tuning, though tuning the learning rate is still important. Adam is a fantastic general-purpose optimizer. However, like all things, it’s not perfect, and some subtleties have emerged, particularly regarding how it interacts with weight decay (L2 regularization).\nThis leads us to AdamW: an Adam Variant with Weight Decay. The question to consider is: How does regularization interact with the optimizer? (e.g., L2)\nRemember that our full loss function is typically \\(L_{data} + \\lambda R(W)\\). If \\(R(W)\\) is L2 regularization, then its gradient is \\(2\\lambda W.\\) This gradient of the regularization term gets added to the gradient of the data loss term to form the total dx = compute_gradient(x). So, how does this interaction play out with an adaptive optimizer like Adam? The answer, perhaps unsurprisingly, is: It depends! Specifically, it depends on how the L2 regularization (weight decay) is implemented with respect to the adaptive learning rate mechanism. In Standard Adam (and many other adaptive gradient methods like RMSProp or AdaGrad), the L2 regularization term \\(\\lambda W\\) is typically added to the gradient of the data loss before the moment calculations. So, dx = gradient_of_data_loss + gradient_of_L2_regularization. This combined dx is then used during the moment calculations (for first_moment and second_moment). What this means is that the L2 regularization gradient gets scaled by the adaptive learning rates (the 1/sqrt(second_moment) term). If second_moment is large for a particular weight (meaning its gradients have been large), then the effective weight decay for that parameter is also reduced. This coupling means that the strength of the weight decay is not uniform and can be different for different parameters, and can change over time in a way that might not be optimal. Larger gradients lead to smaller effective weight decay.\nAdamW introduced by Loshchilov and Hutter, 2017, in “Decoupled Weight Decay Regularization” proposes a different approach. Instead of adding the L2 regularization gradient into dx before the moment calculations, AdamW performs weight decay after the moment updates, directly on the weights themselves. So, the first_moment and second_moment are computed using only the gradient of the data loss. Then, the weight decay is applied as a separate step, effectively by subtracting learning_rate * weight_decay_coefficient * weights from the weights after the Adam step that uses first_unbias and second_unbias. So, AdamW is now often preferred over standard Adam when L2 regularization is used, as it implements weight decay in a way that is often more effective and closer to its original intention. Many deep learning libraries now offer AdamW as a distinct optimizer."
  },
  {
    "objectID": "posts/regularization_and_optimization/index.html#learning-rate-schedules",
    "href": "posts/regularization_and_optimization/index.html#learning-rate-schedules",
    "title": "Regularization and Optimization",
    "section": "Learning rate schedules",
    "text": "Learning rate schedules\nNow, let’s turn our attention to a hyperparameter that is critical for all of them: the learning rate, often referred to as step_size. In the vanilla gradient descent update, weights += - step_size * weights_grad, the learning rate determines how big of a step we take in the negative gradient direction. All the optimizers we’ve discussed – SGD, SGD+Momentum, RMSProp, Adam, AdamW all have the learning rate as a crucial hyperparameter. Choosing the right learning rate is often one of the most important parts of getting good performance from a deep learning model.\n\n\n\nHow learning rate effect our training?\n\n\nSo, the question is: Which one of these learning rates is best to use? Is there a single magic value? The answer is: In reality, all of these could be good learning rates… at different stages of training. This is a key insight. A single, fixed learning rate throughout the entire training process might not be optimal. Early in training, when you’re far from a good solution, a relatively larger learning rate can help you make rapid progress across the loss landscape. However, as you get closer to a minimum, that same large learning rate might cause you to overshoot and bounce around the minimum, preventing you from settling into the very bottom of the valley. In this later stage, a smaller learning rate is often beneficial to fine-tune the weights and converge more precisely. This idea that the optimal learning rate might change during training leads directly to the concept of learning rate schedules, also known as learning rate decay. We don’t just pick one learning rate and stick with it; we dynamically adjust it as training progresses.\nOne common strategy is Step Decay.\n\n\n\ntraining loss epoch, notice the distinct drops in loss at certain epochs\n\n\nThe strategy here is to reduce the learning rate at a few fixed points during training. For example, a common schedule for training ResNets on ImageNet is to start with a certain learning rate, and then multiply it by 0.1 after, say, 30 epochs, then again by 0.1 after 60 epochs, and perhaps one more time after 90 epochs. You can see that after the learning rate is reduced (indicated by the arrows), the loss often plateaus for a bit and then finds a new, lower level. This happens because with a smaller learning rate, the optimizer can more finely navigate the bottom of the loss valley it has reached\nStep decay is quite effective, but it requires choosing when to drop the learning rate and by how much. Are there smoother ways to decay the learning rate? Yes! Another very popular schedule is Cosine Decay.\n\n\n\nCosine Decay (or cosine annealing)\n\n\nThe formula is:\n\\[\n\\alpha_t = \\frac{1}{2} \\alpha_0 \\left(1 + \\cos\\left(\\frac{t}{T} \\pi\\right)\\right)\n\\]\nWhere:\n\n\\(\\alpha_0\\) is initial learning rate\n\\(\\alpha_t\\) is the learning rate at epoch \\(t\\)\n\\(T\\) is the total number of epochs you plan to train for\n\nThe plot shows how this learning rate evolves over epochs. It starts at \\(\\alpha_0\\) and smoothly decreases, following a cosine curve, until it reaches near zero at the end of training (epoch \\(T\\)). This smooth decay is often found to work very well in practice and is less sensitive to the exact choice of drop points compared to step decay. Many recent state-of-the-art models use cosine annealing, sometimes with warm restarts which we’re not covering in detail here but involves periodically resetting the learning rate and re-annealing, as in the Loshchilov and Hutter paper. And on the right is an example of what the training loss might look like when using a cosine decay schedule. You see a generally smooth decrease in loss over a larger number of epochs, without the sharp drops associated with step decay. It just gradually refines the solution as the learning rate smoothly anneals.\n\n\n\nleft: Linear Decay, right: Inverse Square Root Decay\n\n\nAnother simple schedule is Linear Decay, the formula is:\n\\[\n\\alpha_t = \\alpha_0 (1 - \\frac{t}{T})\n\\]\nThe learning rate decreases linearly from \\(\\alpha_0\\) down to 0 over \\(T\\) epochs. This is also a common choice, used in famous models like BERT, for example. The plot shows this straight-line decrease.\nAnd one more example: Inverse Square Root Decay. The formula is:\n\\[\n\\alpha_t = \\frac{\\alpha_0}{\\sqrt{t}}\n\\]\nThe learning rate decreases proportionally to the inverse square root of the epoch number \\(t\\). This schedule makes the learning rate drop relatively quickly at the beginning and then more slowly later on. This was famously used in the original “Attention is All You Need” Transformer paper.\n\n\n\nLinear Warmup\n\n\nThere’s one more important trick related to learning rates, especially when using large initial learning rates or large batch sizes: Linear Warmup. The idea is that high initial learning rates can sometimes make the loss explode right at the beginning of training, especially if the initial weights are far from good and produce very large gradients. The model can become unstable. To prevent this, a common practice is to linearly increase the learning rate from 0 (or a very small value) up to your target initial learning rate \\(\\alpha_0\\) over the first few thousand iterations (e.g., the first ~5,000 iterations or the first epoch). The plot shows this: the learning rate starts at 0, ramps up linearly to a peak (this is our \\(\\alpha_0\\)), and then a decay schedule like cosine or step takes over. This “warmup” phase allows the model to stabilize a bit with small updates before the more aggressive learning rate kicks in. It’s a very common and effective technique, particularly in training large models like Transformers. There’s also an interesting empirical rule of thumb often cited (e.g., from Goyal et al.’s paper “Accurate, Large Minibatch SGD”): If you increase the batch size by N, you should also scale the initial learning rate by N (or \\(\\sqrt{N}\\) in some cases), often in conjunction with a warmup. The intuition is that larger batches give more stable gradient estimates, so you can afford to take larger steps.\nSo, managing the learning rate is not just about picking a single value, but often about defining a schedule that includes an initial phase (like warmup) and a decay phase. This is a critical part of the “art” of training deep neural networks.\nSo, in practice:\n\nAdam(W) is a good default choice in many cases. It often works reasonably well even with a constant learning rate (though a schedule is still generally better) and requires less manual tuning of the learning rate compared to SGD with momentum. AdamW is preferred if you’re using L2 regularization.\nSGD+Momentum can outperform Adam but may require more tuning of the learning rate and its schedule. There’s a bit of an ongoing debate and empirical evidence suggesting that with careful tuning, SGD+Momentum can sometimes find solutions that generalize better than those found by Adam, especially for certain types of vision models. However, “careful tuning” is the operative phrase."
  },
  {
    "objectID": "posts/2024-11-12-random-forest/index.html",
    "href": "posts/2024-11-12-random-forest/index.html",
    "title": "Random Forest, Bagging, Boosting",
    "section": "",
    "text": "What’s up! It’s been a long time since the last post, i’m quite lazy recently, but from know i will try to write more blog post though. I’ve revisited the Titanic dataset, this time through the lens of ensemble learning techniques. Previously I wrote about this dataset in this blog, but now, let’s dive into why random forests and gradient boosting machine are particularly suitable for tabular data.\nYou might ask, “Why not just use logistic regression?” While it seems simple, logistic regression can be surprisingly difficult to get right especially with transformation, interactions, and outlier handling. Random forests, on the other hand, offers resilience and robustness that are hard to match, which I’ll explain today.\nTo start, building a random forest is insightful help demystify the intricacies of machine learning algorithm. I’ll also touch on bagging and boosting, giving a clear view of their strengths\nOn a practical note, a helpful tip I’ve stumbled upon is using fastai’s import to efficiently bringing in essential libraries like Numpy an pandas. Here’s the snippet to simplify your setup:\nfrom fastai.imports import *\nnp.set_printoptions(linewidth=130)\nThese tools and techniques have enhanced my learning journey, and I’m excited to share these insights with you. Alright without any further ado let’s get right into it."
  },
  {
    "objectID": "posts/2024-11-12-random-forest/index.html#decision-tree",
    "href": "posts/2024-11-12-random-forest/index.html#decision-tree",
    "title": "Random Forest, Bagging, Boosting",
    "section": "Decision Tree",
    "text": "Decision Tree\n\nData Processing\nFirst off, ensure that you have the Titanic dataset downloaded, Here’s the quick setup:\n\nimport zipfile, kaggle\n\npath = Path('titanic')\nkaggle.api.competition_download_cli(str(path))\nzipfile.ZipFile(f'{path}.zip').extractall(path)\n\ndf = pd.read_csv(path/'train.csv')\ntst_df = pd.read_csv(path/'test.csv')\nmodes = df.mode().iloc[0]\n\nDownloading titanic.zip to /home/monarch/workplace/random_forest\n\n\n100%|██████████| 34.1k/34.1k [00:00&lt;00:00, 365kB/s]\n\n\n\n\n\n\n\n\nI’ve previously detailed the intricacies of processing the Titanic dataset in a separate blog post which you might find useful. For now, let’s breeze through some basic data processing steps without going into too much detail:\n\ndef proc_data(df):\n    df['Fare'] = df.Fare.fillna(0)\n    df.fillna(modes, inplace=True)\n    df['LogFare'] = np.log1p(df['Fare'])\n    df['Embarked'] = pd.Categorical(df.Embarked)\n    df['Sex'] = pd.Categorical(df.Sex)\n\nproc_data(df)\nproc_data(tst_df)\n\nOur next task involves organizing the data by identifying continuous and categorical variables, along with dependent variable we’re predicting\n\ncats=[\"Sex\",\"Embarked\"]\nconts=['Age', 'SibSp', 'Parch', 'LogFare',\"Pclass\"]\ndep=\"Survived\"\n\nNow, a brief look at how Pandas handles categorical variables. Let’s consider the Sex column:\n\ndf.Sex.head()\n\n0      male\n1    female\n2    female\n3    female\n4      male\nName: Sex, dtype: category\nCategories (2, object): ['female', 'male']\n\n\nIt’s fascinating, although it appears unchanged(still just Male and Female), it’s now a category with a predefine list. Behind the magic, Pandas cleverly assigns numerical codes for these categories for efficient processing:\n\ndf.Sex.cat.codes.head()\n\n0    1\n1    0\n2    0\n3    0\n4    1\ndtype: int8\n\n\nIt’s actually turned them into numbers. This transformation sets the stage for our decision tree modeling\n\n\nBinary Split\nA random forest is essentially an ensemble of decision trees, and each tree is constructed from a series of binary split. But what exactly is a binary split?\nImagine taking all the passengers on the Titanic and dividing them into males and females to examine their survival rates.\n\nimport seaborn as sns\n\nfig,axs = plt.subplots(1,2, figsize=(11,5))\nsns.barplot(data=df, y=dep, x=\"Sex\", ax=axs[0]).set(title=\"Survival rate\")\nsns.countplot(data=df, x=\"Sex\", ax=axs[1]).set(title=\"Histogram\");\n\n\n\n\n\n\n\n\nwe see a stark difference: about a 20% survival rate for males and 75% for females, there are roughly twice as many males as females. If you base a model solely on sex, predicting survival becomes surprisingly effective: men likely didn’t survive, while woman likely did this division by sex exemplifies a binary split - it simple divide the data into two distinct groups.\nTo test the efficacy of this basic model, we first split our data into training and test dataset and encode our categorical variables.\n\nfrom numpy import random\nfrom sklearn.model_selection import train_test_split\n\nrandom.seed(42)\ntrn_df,val_df = train_test_split(df, test_size=0.25)\ntrn_df[cats] = trn_df[cats].apply(lambda x: x.cat.codes)\nval_df[cats] = val_df[cats].apply(lambda x: x.cat.codes)\n\nNext, let’s create function to to extract independent variables (xs) and the dependent variable (y).\n\ndef xs_y(df):\n    xs = df[cats+conts].copy()\n    return xs,df[dep] if dep in df else None\n\ntrn_xs,trn_y = xs_y(trn_df)\nval_xs,val_y = xs_y(val_df)\n\nFrom here we make predictions:\n\nfrom sklearn.metrics import mean_absolute_error\npreds = val_xs.Sex==0\nmean_absolute_error(val_y, preds)\n\n0.21524663677130046\n\n\nA 21.5% error rate isn’t too shabby for such a simple model. Can we do better? Let’s try another variable such as Fare which is continuous.\n\ndf_fare = trn_df[trn_df.LogFare&gt;0]\nfig,axs = plt.subplots(1,2, figsize=(11,5))\nsns.boxenplot(data=df_fare, x=dep, y=\"LogFare\", ax=axs[0])\nsns.kdeplot(data=df_fare, x=\"LogFare\", ax=axs[1]);\n\n\n\n\n\n\n\n\nThe boxenplot shows that those who survived generally paid higher fares.\nSo here’s another model LogFare greater than 2.7:\n\npreds = val_xs.LogFare&gt;2.7\nmean_absolute_error(val_y, preds)\n\n0.336322869955157\n\n\nOh, much worse\nTo evaluate binary split uniformly, regardless of the datatype, We measure how similar the dependent variable values are within each split. We aim for standard deviations within groups, multiplied by group sizes to account for impact differences.\n\ndef _side_score(side, y):\n    tot = side.sum()\n    if tot&lt;=1: return 0\n    return y[side].std()*tot\n\ndef score(col, y, split):\n    lhs = col&lt;=split\n    return (_side_score(lhs,y) + _side_score(~lhs,y))/len(y)\n\nSo for example, if we split by Sex, is greater than or less than 0.5.That’ll create two groups, males and females, and that gives us this score.\n\nscore(trn_xs[\"Sex\"], trn_y, 0.5)\n\n0.4078753098206398\n\n\nAnd if we do LogFare greater than or less than 2.7, it gives us this score.\n\nscore(trn_xs[\"LogFare\"], trn_y, 2.7)\n\n0.4718087395209973\n\n\nLower scores indicates better splits, with Sex outperforming LogFare. But how can we find a best split point i mean we have to try ourself right? In every values and see if the score improve or not right, well that was pretty inefficient. It would be nice if we could find some automatic wway to do al that. Well, of course we can. If we want to find the best split point for Age, and try each one in turn, and see what score we get, if we made a binary split on that level of Age. So here’s a list of all the possible binary split threshold of Age\n\ncol = trn_xs[\"Age\"]\nunq = col.unique()\nunq.sort()\nunq\n\narray([ 0.42,  0.67,  0.75,  0.83,  0.92,  1.  ,  2.  ,  3.  ,  4.  ,  5.  ,  6.  ,  7.  ,  8.  ,  9.  , 10.  , 11.  , 12.  ,\n       13.  , 14.  , 14.5 , 15.  , 16.  , 17.  , 18.  , 19.  , 20.  , 21.  , 22.  , 23.  , 24.  , 24.5 , 25.  , 26.  , 27.  ,\n       28.  , 28.5 , 29.  , 30.  , 31.  , 32.  , 32.5 , 33.  , 34.  , 34.5 , 35.  , 36.  , 36.5 , 37.  , 38.  , 39.  , 40.  ,\n       40.5 , 41.  , 42.  , 43.  , 44.  , 45.  , 45.5 , 46.  , 47.  , 48.  , 49.  , 50.  , 51.  , 52.  , 53.  , 54.  , 55.  ,\n       55.5 , 56.  , 57.  , 58.  , 59.  , 60.  , 61.  , 62.  , 64.  , 65.  , 70.  , 70.5 , 74.  , 80.  ])\n\n\nLet’s go through all of them. For each of them calculate the score and then Numpy and Pytorch have an argmin() function, which tells you what index into that list is the smallest.\n\nscores = np.array([score(col, trn_y, o) for o in unq if not np.isnan(o)])\nunq[scores.argmin()]\n\n6.0\n\n\nHere’s the scores.\n\nscores\n\narray([0.48447755, 0.48351588, 0.48158676, 0.48061929, 0.47964987, 0.480937  , 0.48347294, 0.48171397, 0.47987776, 0.47884826,\n       0.47831672, 0.47949847, 0.47957573, 0.48092137, 0.48130659, 0.48200571, 0.48163287, 0.48124801, 0.48151498, 0.48183316,\n       0.48105614, 0.48202484, 0.48178211, 0.48337829, 0.48439618, 0.48501782, 0.48545475, 0.48556795, 0.48550856, 0.48554074,\n       0.48550094, 0.48504976, 0.48480161, 0.48561331, 0.4852559 , 0.48513473, 0.48529147, 0.48530156, 0.48543741, 0.48569729,\n       0.48571309, 0.48571467, 0.4856701 , 0.48563657, 0.48579877, 0.48579767, 0.4858019 , 0.48580095, 0.48580002, 0.48580178,\n       0.48580211, 0.48579777, 0.4857996 , 0.48580236, 0.48579236, 0.48580043, 0.48580303, 0.4858034 , 0.4857613 , 0.4855666 ,\n       0.48579394, 0.48580506, 0.48580434, 0.48580707, 0.48579364, 0.48580788, 0.48581017, 0.48580597, 0.48581077, 0.48576815,\n       0.48580167, 0.48545792, 0.48567909, 0.48542059, 0.48557468, 0.48492654, 0.4852198 , 0.48548666, 0.48590271, 0.48601112,\n       0.48447755, 0.48543732])\n\n\nCreate a function to calculate this for any column:\n\ndef min_col(df, nm):\n    col,y = df[nm],df[dep]\n    unq = col.dropna().unique()\n    scores = np.array([score(col, y, o) for o in unq if not np.isnan(o)])\n    idx = scores.argmin()\n    return unq[idx],scores[idx]\n\nmin_col(trn_df, \"Age\")\n\n(6.0, 0.47831671750899085)\n\n\nRevealing that is at 6.0 for Age. So now we can just go through and calculates the score for the best split point for each column.\n\ncols = cats+conts\n{o:min_col(trn_df, o) for o in cols}\n\n{'Sex': (0, 0.4078753098206398),\n 'Embarked': (0, 0.478833425731479),\n 'Age': (6.0, 0.47831671750899085),\n 'SibSp': (4, 0.4783740258817423),\n 'Parch': (0, 0.4805296527841601),\n 'LogFare': (2.4390808375825834, 0.4620823937736595),\n 'Pclass': (2, 0.4604826188580666)}\n\n\nAnd if we do that, we find that the lowest score is Sex. So that is how to calculate the best binary split. So we now know that the model we created earlier with Sex is the best single binary split model we can find.\nAnd this simple thing we just did which is finding a single binary split, actually is a type of model, it has a name too, it’s called OneR. And OneR model it turned out in a review of machine learning methods in the 90s is one of the best, if not the best. It’s not a bad idea to always start creating a baseline of OneR, a decision tree with a single binary split.\n\n\nCreating a Tree\n“OneR” is probably not going to cut it for a lot of things, though it’s surprisingly effective, but maybe we could go a step further. And the other step further we could go is by creating a maybe “TwoR”. What if we took each of those groups, males and females in the Titanic dataset, and split each of these into two other groups? So split the males into two groups and split the females into two groups. To do that, we can repeat the exact same piece of code we just did, but let’s remove sex from it:\n\ncols.remove(\"Sex\")\nismale = trn_df.Sex==1\nmales,females = trn_df[ismale],trn_df[~ismale]\n\nThen, run the same piece of code that we just did before, but just for the males:\n\n{o:min_col(males, o) for o in cols}\n\n{'Embarked': (0, 0.387558187041091),\n 'Age': (6.0, 0.37398283710105873),\n 'SibSp': (4, 0.38758642275862637),\n 'Parch': (0, 0.3874704821461953),\n 'LogFare': (2.803360380906535, 0.38048562317581447),\n 'Pclass': (1, 0.3815544200436083)}\n\n\nThis provides a “OneR” rule for how to predict which males survived the Titanic, Interestingly, age turns out to be the biggest predictor for males whether they were greater than or less than 6 determined their survival odds\nSimilarity, for females:\n\n{o:min_col(females, o) for o in cols}\n\n{'Embarked': (0, 0.4295252982857326),\n 'Age': (50.0, 0.4225927658431646),\n 'SibSp': (4, 0.42319212059713585),\n 'Parch': (3, 0.4193314500446157),\n 'LogFare': (4.256321678298823, 0.413505983329114),\n 'Pclass': (2, 0.3335388911567602)}\n\n\nThe passenger class Pclass, or whether they were in first class or not, was the biggest predictor of survival.\nThis process generates a decision tree - a serries of binary splits that gradually categorize our data so that in the leaf nodes, we derive strong predictions about survival\nWe can continue these steps for each of the four groups manually with a couple of extra lines of code, or we can use a decision tree classifier. This class automates the process we just outlined:\n\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\n\nm = DecisionTreeClassifier(max_leaf_nodes=4).fit(trn_xs, trn_y);\n\nAnd one very nice thing it has is it can draw the tree for us. So here’s a tiny little draw_tree function:\n\nimport graphviz\n\ndef draw_tree(t, df, size=10, ratio=0.6, precision=2, **kwargs):\n    s=export_graphviz(t, out_file=None, feature_names=df.columns, filled=True, rounded=True,\n                      special_characters=True, rotate=False, precision=precision, **kwargs)\n    return graphviz.Source(re.sub('Tree {', f'Tree {{ size={size}; ratio={ratio}', s))\n\ndraw_tree(m, trn_xs, size=10)\n\n\n\n\n\n\n\n\nAnd you can see here it’s going to first of all split on sex. Now, it looks a bit weird to say sex is less than or equal to 0.5, but remember our binary characteristics are coded as zero or one. This is just an easy way to denote males versus females.\nFor females, the next split is based on their class. For males, age is the dedicating factor. This creates our four leaf nodes. For instance, of the females in the first class, 116 survived, and only 4 didn’t showing that being a wealthy woman on the Titanic was quite advantageous. On the other hand, among adult males, 68 survived while 350 perished, illustrating the peril they faced.\nThis quick summary showcases why decision trees are favoured in exploratory data analysis; they provide a clear picture of key variables driving the dataset and their predictive power\nOne additional point is the Gini measure, a way of evaluating how good a split is, which i’ve illustrated in the code below:\n\ndef gini(cond):\n    act = df.loc[cond, dep]\n    return 1 - act.mean()**2 - (1-act).mean()**2\n\nTo understand this mathematically: if \\(p\\) is a probability of an instance being classified as a positive class, and \\((1 - p)\\) for the negative class, \\(p^2\\) denotes the chance of both randomly selected instances being positive and \\((1-p)^2\\) being negative. The term \\((1-p^2 - (1-p)^2)\\) gives us the probability of misclassification, subtracting the chances of correctly classifying instances.\nHere’s an example of Gini calculation for gender:\n\ngini(df.Sex=='female'), gini(df.Sex=='male')\n\n(0.3828350034484158, 0.3064437162277842)\n\n\nHere, act.mean()**2 is the probability that two randomly selected individual both survived, and (1 - act.mean())**2 that both did not. Lower Gini impurity suggests a strong skew in survival outcomes, which can be insightful for decision making or predicting survival likelihood based on gender.\nDecision trees thus provide not only visual insights but quantitative ways to discern what’s happening within your dataset.\n\nmean_absolute_error(val_y, m.predict(val_xs))\n\n0.2242152466367713\n\n\nSo that was for the “OneR” version. For the decision tree with four leaf nodes, the mean absolute error was 0.224, which is actually a bit worse. This outcome suggest that due to the small size of the dataset, the “OneR” method was impressively effective, and enhancements weren’t substantial enough to be discerned among the randomness of such a small validation set.\nTo take it further, let’s implement a decision tree with a minimum of 50 samples per leaf node:\n\nm = DecisionTreeClassifier(min_samples_leaf=50)\nm.fit(trn_xs, trn_y)\ndraw_tree(m, trn_xs, size=12)\n\n\n\n\n\n\n\n\nThis indicates that each leaf will contain at least 50 samples, in this context passengers on the Titanic. For example, suppose you’ve identified that 67 people were female, first-class, and under 28. That’s the point where the tree ceases splitting further\nLet’s evaluate this decision tree:\n\nmean_absolute_error(val_y, m.predict(val_xs))\n\n0.18385650224215247\n\n\nWith an absolute error of 0.183, this approach shows a bit of improvement.\nAn interesting aspect of decision trees is the minimal preprocessing required you may have noticed this advantage. There was no need for dummy variables for category features, and although you can create them, it isn’t necessary. Decision trees can manage without these adjustments. We only took the logarithm of the fare to enhance the visual appearance of our graph but the split would operate identically on the original scale, focusing only on data ordering\nMoreover, decision trees are indifferent to outliers, long-tailed distributions, and categorical variables: they handle all these situations effectively.\nThe take away here is that for tabular data, starting with a decision tree-based approach is prudent. It helps create baselines because they are remarkably resilient and offer a robust performance without intricate tuning"
  },
  {
    "objectID": "posts/2024-11-12-random-forest/index.html#random-forest",
    "href": "posts/2024-11-12-random-forest/index.html#random-forest",
    "title": "Random Forest, Bagging, Boosting",
    "section": "Random Forest",
    "text": "Random Forest\nNow, what if we wanted to make this more accurate? Could we grow the tree further? We could, but with only 50 samples in these leaves, further splitting would result in the leaf nodes having so little data that their predictions wouldn’t be very meaningful. Naturally, there are limitation to how accurate a decision tree can be. so, what we can do? Enter a fascinating strategy called bagging.\nHere’s the procedure of bagging:\n\nRandomly choose a subset of data rows (a “bootstrap replicate” of the learning set).\nTrain a model using this subset.\nSave that model, then go back to step 1 and repeat several times.\nThis will give you multiple trained models. Predict with all models, and then average their predictions to make the final prediction.\n\nThe core insight of bagging is that although models trained on data subsets will make more errors than a model trained on the full dataset, these errors aren’t correlated across models. Different models will make different errors, and when averaged, those errors offset each other. Thus, average the predictions of all the model sharpens the final prediction with more models providing finer estimations.\nIn essence, a random forest averages the predictions of numerous decision trees, which are generated randomly varying parameters such as training dataset or tree parameters. Bagging is a particular approach to “ensembling” or combining results from multiple models.\nLet’s create one in a few lines. Here’s a function to generate a decision tree:\n\ndef get_tree(prop=0.75):\n    n = len(trn_y)\n    idxs = random.choice(n, int(n*prop))\n    return DecisionTreeClassifier(min_samples_leaf=5).fit(trn_xs.iloc[idxs], trn_y.iloc[idxs])\n\nHere, prop denotes the data proportion used, say 75% each time with n as the sample size. Random samples idxs are selected based on the specified proportion, and a decision tree is built from this subset.\nLet’s get 100 trees and compile them into a list:\n\ntrees = [get_tree() for t in range(100)]\nall_probs = [t.predict(val_xs) for t in trees]\navg_probs = np.stack(all_probs).mean(0)\n\nmean_absolute_error(val_y, avg_probs)\n\n0.2272645739910314\n\n\nBy collecting predictions from these trees, stacking them, and averaging their predictions, we have our random forest.\nRandom forests are remarkably simple yet powerful. A key feature is that they also randomly select subset of columns to build decision trees, changing the column subset with each node split. The idea is to maintain randomness, yet retain usefulness. For more efficient implementation, we use RandomForestClassifier:\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(100, min_samples_leaf=5)\nrf.fit(trn_xs, trn_y);\nmean_absolute_error(val_y, rf.predict(val_xs))\n\n0.18834080717488788\n\n\nHere, we specify the number of trees and samples per leaf, then fit the classifier. While our mean absolute error might not surpass a single decision tree due to dataset constraints, it remains robust\nOne can inspect the built decision trees to identify split columns. Monitoring column improvements in Gini across decision trees yields a feature importance plot:\n\npd.DataFrame(dict(cols=trn_xs.columns, imp=m.feature_importances_)).plot('cols', 'imp', 'barh');\n\n\n\n\n\n\n\n\nFeature importance plots demonstrate a feature’s significance by indicating how frequently and effectively it was used for splits. The Sex variable emerges as most significant, follow by Pclass, with other variables less crucial. And this is another reason, by the way, why the random forest isn’t really particularly helpful, because it’s just a easy split to do, basically all the matter is what class you are in and whether you’re male of female.\nRandom Forests, due to their versatility with data distribution and categorical variable handling, allow immediate and insightful datasets analyses. For large datasets, they quickly reveal key features, facilitating further focused analysis."
  },
  {
    "objectID": "posts/2024-11-12-random-forest/index.html#what-else-can-we-do-with-random-forest",
    "href": "posts/2024-11-12-random-forest/index.html#what-else-can-we-do-with-random-forest",
    "title": "Random Forest, Bagging, Boosting",
    "section": "What else can we do with Random Forest",
    "text": "What else can we do with Random Forest\nThere are other things that you can do with Random Forests and the Titanic dataset is a small one, so it doesn’t highlight the full power of Random Forests. For a bigger and more numerically interesting dataset, let’s consider the auction price of heavy industrial equipment. This dataset is from The Blue Book for Bulldozers Kaggle competition. I highly recommended taking a peek at the overview and the dataset on the competition page before we start.\n\nPreparing Stuff\n\nDownloading the Dataset\n\n\nImport stuff click to show the code\nfrom fastbook import *\nfrom pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype\nfrom fastai.tabular.all import *\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom dtreeviz.trees import *\nfrom IPython.display import Image, display_svg, SVG\n\nimport warnings\nwarnings.simplefilter('ignore', FutureWarning)\n\npd.options.display.max_rows = 20\npd.options.display.max_columns = 8\n\n\nPick a path to download the dataset:\n\ncomp = 'bluebook-for-bulldozers'\npath = URLs.path(comp)\npath\n\nPath('/home/monarch/.fastai/archive/bluebook-for-bulldozers')\n\n\nUse the Kaggle API to download the data to the specified path and extract it:\n\nfrom kaggle import api\n\nif not path.exists():\n    path.mkdir(parents=true)\n    api.competition_download_cli(comp, path=path)\n    shutil.unpack_archive(str(path/f'{comp}.zip'), str(path))\n\npath.ls(file_type='text')\n\nDownloading bluebook-for-bulldozers.zip to /home/monarch/.fastai/archive/bluebook-for-bulldozers\n\n\n100%|██████████| 48.4M/48.4M [00:07&lt;00:00, 6.52MB/s]\n\n\n\n\n\n(#7) [Path('/home/monarch/.fastai/archive/bluebook-for-bulldozers/Machine_Appendix.csv'),Path('/home/monarch/.fastai/archive/bluebook-for-bulldozers/Test.csv'),Path('/home/monarch/.fastai/archive/bluebook-for-bulldozers/TrainAndValid.csv'),Path('/home/monarch/.fastai/archive/bluebook-for-bulldozers/Valid.csv'),Path('/home/monarch/.fastai/archive/bluebook-for-bulldozers/ValidSolution.csv'),Path('/home/monarch/.fastai/archive/bluebook-for-bulldozers/median_benchmark.csv'),Path('/home/monarch/.fastai/archive/bluebook-for-bulldozers/random_forest_benchmark_test.csv')]\n\n\nI’ll now walk you through the dataset. If you examine the Data tab on the competition page, here are the key fields found in train.csv:\n\nSalesID: The unique identifier of the sale.\nMachineID: the unique identifier of the machine. A machine can be sold multiple times.\nsaleprice: The auction sale price of the machine (only provided in train.csv)\nsaledate: The date the sale occurred.\n\nWe begin by reading the training set into Pandas DataFrame. It’s generally advisable to specify low_memory=False unless Pandas runs out of memory and throws an error. By default, low_memory is True, instructing Pandas to process data in chucks, which may lead to inconsistent column data types and subsequent data processing or modeling errors.\n\ndf = pd.read_csv(path/'TrainAndValid.csv', low_memory=False)\ndf.columns\n\nIndex(['SalesID', 'SalePrice', 'MachineID', 'ModelID', 'datasource',\n       'auctioneerID', 'YearMade', 'MachineHoursCurrentMeter', 'UsageBand',\n       'saledate', 'fiModelDesc', 'fiBaseModel', 'fiSecondaryDesc',\n       'fiModelSeries', 'fiModelDescriptor', 'ProductSize',\n       'fiProductClassDesc', 'state', 'ProductGroup', 'ProductGroupDesc',\n       'Drive_System', 'Enclosure', 'Forks', 'Pad_Type', 'Ride_Control',\n       'Stick', 'Transmission', 'Turbocharged', 'Blade_Extension',\n       'Blade_Width', 'Enclosure_Type', 'Engine_Horsepower', 'Hydraulics',\n       'Pushblock', 'Ripper', 'Scarifier', 'Tip_Control', 'Tire_Size',\n       'Coupler', 'Coupler_System', 'Grouser_Tracks', 'Hydraulics_Flow',\n       'Track_Type', 'Undercarriage_Pad_Width', 'Stick_Length', 'Thumb',\n       'Pattern_Changer', 'Grouser_Type', 'Backhoe_Mounting', 'Blade_Type',\n       'Travel_Controls', 'Differential_Type', 'Steering_Controls'],\n      dtype='object')\n\n\nThat’s many columns to scour! Start by exploring the dataset to familiarize yourself with the data content in each column. Soon we’ll focus on the most compelling bits.\nWith ordinal columns, it’s beneficial to specify meaningful order. These columns contain strings with an inherent sequence. For example, check out the ProducSize levels:\n\ndf['ProductSize'].unique()\n\narray([nan, 'Medium', 'Small', 'Large / Medium', 'Mini', 'Large', 'Compact'], dtype=object)\n\n\nInstruct Pandas about the relevant order of these levels:\n\nsizes = 'Large','Large / Medium','Medium','Small','Mini','Compact'\n\ndf['ProductSize'] = df['ProductSize'].astype('category')\ndf['ProductSize'].cat.set_categories(sizes, ordered=True)\n\n0            NaN\n1         Medium\n2            NaN\n3          Small\n4            NaN\n           ...  \n412693      Mini\n412694      Mini\n412695      Mini\n412696      Mini\n412697      Mini\nName: ProductSize, Length: 412698, dtype: category\nCategories (6, object): ['Large' &lt; 'Large / Medium' &lt; 'Medium' &lt; 'Small' &lt; 'Mini' &lt; 'Compact']\n\n\nIn this dataset, Kaggle suggests using Root Mean Square Log Error (RMSLE) as the metric for comparing actual versus predicted auction prices.\n\ndep_var = 'SalePrice'\ndf[dep_var] = np.log(df[dep_var])\n\nThis transformation ensures that the target variables is in format suitable for modeling.\n\n\nData Preparation\nThe first piece of data preparation we need to to do is enrich our representation of dates. The fundamental basis of the decision tree that we just discussed is bisection (dividing a group into two). We look at the ordinal variables and divide the dataset based on whether the variable’s value is greater (ow lower) than a threshold, and we look at the categorical variables and divide the dataset based on whether the variable’s level is a particular level. This algorithm divides the dataset based on both original and categorical data\nBut how does this apply to a common data type, the date? You might want to tree at date as an ordinal value because it is meaningful to say that one date is greater than other. However, dates are a bit different from most ordinal values in that some dates are qualitatively different from others, which is often relevant to the systems we are modeling.\nTo help our algorithm handle dates intelligently, we’d like our model to know ore than whether a date is more recent or less recent than other. We might want our model to make decisions based on that date’s day of the week, on whether a day is holiday, on what month it is in, and so forth. To accomplish this, we replace every date column with a set of date metadata columns, such as holiday, day of the week, and month. These columns provide categorical data that we suspect will be useful.\nFastai comes with a function to do this for us that mean we only need to pass in a column name that contains dates:\n\ndf = add_datepart(df, 'saledate')\n\n# do the same for the test set\ndf_test = pd.read_csv(path/'Test.csv', low_memory=False)\ndf_test = add_datepart(df_test, 'saledate')\n\nWe can see that there are now many new columns in our DataFrame:\n\n' '.join(o for o in df.columns if o.startswith('sale'))\n\n'saleYear saleMonth saleWeek saleDay saleDayofweek saleDayofyear saleIs_month_end saleIs_month_start saleIs_quarter_end saleIs_quarter_start saleIs_year_end saleIs_year_start saleElapsed'\n\n\nThis a solid first step, but we need further data cleaning. For this, we will use fastai objects called TabularPandas and TabularProc.\nAnother aspect of preparatory processing is ensuring we can handle strings and missing data. We will use fastai’s class TabularPandas, which wraps a Pandas DataFrame and offers some conveniences. when we say it “wraps” a DataFrame, it means taking a Pandas DataFrame as input and adding additional specifically useful for machine-learning tasks with tabular data. To populate a TabularPandas, we will utilize two TabularProcs: Categorify and FillMissing.\nTabularProcs are unique data transformation process used in fastai designed to prepare you data to ML models. We introduce two specific TabularProcs here:\n\nCategorify: convert categorical columns text or non numeric data into numeric categories. For instance, a column Color with values like “Red”, “Blue”, “Green” could be encoded as 1, 2, 3.\nFillMissing: Manages missing data in your dataset. it replaces missing values with the column’s median value and creates a new boolean column to flag rows that originally had missing values.\n\nHow TabularProc differs from regular transforms:\n\nReturns the exact same object that’s passed to it, after modifying the object in place, which optimizes memory efficiency especially with large datasets.\nExecutes the transformation immediately when the data is first passed in rather than delaying until the data is accessed.\n\nIn practical terms, when using TabularPandas with TabularProcs:\n\nStart with your raw data in a Pandas DataFrame.\nWrap this DataFrame with TabularPandas.\nApply TabularProcs (Categorify and FillMissing)\nThese procs instantly process all your data, converting categories to numbers and filling in missing values.\nThe outcome is a dataset ready for machine learning models, with all categorical data converted and missing values addressed.\n\nThis methodology streamlines the data preparation process, ensure consistent data processing ready for model training or inference.\n\nprocs = [Categorify, FillMissing]\n\nTabularPandas will also manage the dataset split into training and validation sets for us.\n\n\nHandling a Time Series\nWhen dealing with time series data, randomly selecting a subset of data points for training and validation is not sufficient, as sequence of data is vital. The test set represents a future six-month period starting from May 2012, thus not overlapping with the training set. This setup is intentional because the competition sponsor aims to evaluate the model’s predictive capability selected from a later time than your training dataset.\nThe provided Kaggle training data concludes in April 2012. Therefore, we’ll construct to focused training dataset comprising data from before November 2011 and establish a validation set with data from after November 2011.\nThis is achieved using np.where, which helps in obtaining indices for specific conditions:\n\ncond = (df.saleYear&lt;2011) | (df.saleMonth&lt;10)\ntrain_idx = np.where( cond)[0]\nvalid_idx = np.where(~cond)[0]\n\nsplits = (list(train_idx),list(valid_idx))\n\nTabularPandas requires knowledge of which columns are continuous and which are categorical. We can simplify this with the cont_cat_split helper function:\n\ncont,cat = cont_cat_split(df, 1, dep_var=dep_var)\nto = TabularPandas(df, procs, cat, cont, y_names=dep_var, splits=splits)\n\nThis setup turns TabularPandasinto something akin to a fastai Dataset object, with accessible train and valid attributes:\n\nlen(to.train),len(to.valid)\n\n(404710, 7988)\n\n\nIt’s possible to view the dataset’s categorical variables still represented as strings:\n\nto.show(3)\n\n\n\n\n\nUsageBand\nfiModelDesc\nfiBaseModel\nfiSecondaryDesc\nfiModelSeries\nfiModelDescriptor\nProductSize\nfiProductClassDesc\nstate\nProductGroup\nProductGroupDesc\nDrive_System\nEnclosure\nForks\nPad_Type\nRide_Control\nStick\nTransmission\nTurbocharged\nBlade_Extension\nBlade_Width\nEnclosure_Type\nEngine_Horsepower\nHydraulics\nPushblock\nRipper\nScarifier\nTip_Control\nTire_Size\nCoupler\nCoupler_System\nGrouser_Tracks\nHydraulics_Flow\nTrack_Type\nUndercarriage_Pad_Width\nStick_Length\nThumb\nPattern_Changer\nGrouser_Type\nBackhoe_Mounting\nBlade_Type\nTravel_Controls\nDifferential_Type\nSteering_Controls\nsaleIs_month_end\nsaleIs_month_start\nsaleIs_quarter_end\nsaleIs_quarter_start\nsaleIs_year_end\nsaleIs_year_start\nauctioneerID_na\nMachineHoursCurrentMeter_na\nSalesID\nMachineID\nModelID\ndatasource\nauctioneerID\nYearMade\nMachineHoursCurrentMeter\nsaleYear\nsaleMonth\nsaleWeek\nsaleDay\nsaleDayofweek\nsaleDayofyear\nsaleElapsed\nSalePrice\n\n\n\n\n0\nLow\n521D\n521\nD\n#na#\n#na#\n#na#\nWheel Loader - 110.0 to 120.0 Horsepower\nAlabama\nWL\nWheel Loader\n#na#\nEROPS w AC\nNone or Unspecified\n#na#\nNone or Unspecified\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n2 Valve\n#na#\n#na#\n#na#\n#na#\nNone or Unspecified\nNone or Unspecified\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\nStandard\nConventional\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n1139246\n999089\n3157\n121\n3.0\n2004\n68.0\n2006\n11\n46\n16\n3\n320\n1.163635e+09\n11.097410\n\n\n1\nLow\n950FII\n950\nF\nII\n#na#\nMedium\nWheel Loader - 150.0 to 175.0 Horsepower\nNorth Carolina\nWL\nWheel Loader\n#na#\nEROPS w AC\nNone or Unspecified\n#na#\nNone or Unspecified\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n2 Valve\n#na#\n#na#\n#na#\n#na#\n23.5\nNone or Unspecified\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\nStandard\nConventional\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n1139248\n117657\n77\n121\n3.0\n1996\n4640.0\n2004\n3\n13\n26\n4\n86\n1.080259e+09\n10.950807\n\n\n2\nHigh\n226\n226\n#na#\n#na#\n#na#\n#na#\nSkid Steer Loader - 1351.0 to 1601.0 Lb Operating Capacity\nNew York\nSSL\nSkid Steer Loaders\n#na#\nOROPS\nNone or Unspecified\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\nAuxiliary\n#na#\n#na#\n#na#\n#na#\n#na#\nNone or Unspecified\nNone or Unspecified\nNone or Unspecified\nStandard\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n1139249\n434808\n7009\n121\n3.0\n2001\n2838.0\n2004\n2\n9\n26\n3\n57\n1.077754e+09\n9.210340\n\n\n\n\n\nHowever, all underlying data has been converted to numeric form:\n\nto.items.head(3)\n\n\n\n\n\n\n\n\nSalesID\nSalePrice\nMachineID\nModelID\n...\nsaleIs_year_start\nsaleElapsed\nauctioneerID_na\nMachineHoursCurrentMeter_na\n\n\n\n\n0\n1139246\n11.097410\n999089\n3157\n...\n1\n1.163635e+09\n1\n1\n\n\n1\n1139248\n10.950807\n117657\n77\n...\n1\n1.080259e+09\n1\n1\n\n\n2\n1139249\n9.210340\n434808\n7009\n...\n1\n1.077754e+09\n1\n1\n\n\n\n\n3 rows × 67 columns\n\n\n\nCategorical columns undergo transformation by substituting each unique category with a number. These numbers are assigned consecutively as they first appear, implying no intrinsic value to these numbers, unless ordered categories (like ProductSize) pre-specify the sequence. You can check the mapping through the classes attribute:\n\nto.classes['ProductSize']\n\n['#na#', 'Compact', 'Large', 'Large / Medium', 'Medium', 'Mini', 'Small']\n\n\nA neat feature in fastai is the ability to save processed data, which can be time-consuming. Saving the data allows you to resume further work without repeating the preprocessing steps. Fastai utilizes Python’s pickle system for this purpose:\n\nsave_pickle(path/'to.pkl',to)\n\nto retrieve it later you’ll simply do:\n\nto = load_pickle(path/'to.pkl')\n\nWith preprocessing complete, we’re set to create a decision tree.\n\n\n\nDecision Tree Ensembles\nLet’s consider how we find the right questions to ask when creating decision trees. Fortunately we don’t have to do this manually computer are designed for this purpose! Here’s a simple overview of training a decision tree:\n\nLoop through each column of the dataset in turn.\nFor each column, loop through each possible level of that column in turn.\nTry splitting the data into two groups, based on whether they are greater than or less than that value (or if it is a categorical variable, based on whether they are equal to or not equal to that level of that categorical variable).\nFind the average sale price for each of those two groups, and see how close that is to the actual sale price of each of the items of equipment in that group. That is, treat this as a very simple “model” where our predictions are simply the average sale price of the item’s group.\nAfter looping through all of the columns and all the possible levels for each, pick the split point that gave the best predictions using that simple model.\nWe now have two different groups for our data, based on this selected split. Treat each of these as separate datasets, and find the best split for each by going back to step 1 for each group.\nContinue this process recursively, until you have reached some stopping criterion for each group—for instance, stop splitting a group further when it has only 20 items in it.\n\nTo implement this, start by defining your independent and dependent variables:\n\nxs,y = to.train.xs,to.train.y\nvalid_xs,valid_y = to.valid.xs,to.valid.y\n\n\nxs.head()\n\n\n\n\n\n\n\n\nUsageBand\nfiModelDesc\nfiBaseModel\nfiSecondaryDesc\n...\nsaleDay\nsaleDayofweek\nsaleDayofyear\nsaleElapsed\n\n\n\n\n0\n2\n963\n298\n43\n...\n16\n3\n320\n1.163635e+09\n\n\n1\n2\n1745\n529\n57\n...\n26\n4\n86\n1.080259e+09\n\n\n2\n1\n336\n111\n0\n...\n26\n3\n57\n1.077754e+09\n\n\n3\n1\n3716\n1381\n0\n...\n19\n3\n139\n1.305763e+09\n\n\n4\n3\n4261\n1538\n0\n...\n23\n3\n204\n1.248307e+09\n\n\n\n\n5 rows × 66 columns\n\n\n\nOnce your data is numeric and lacks missing values, you can create a decision tree:\n\nm = DecisionTreeRegressor(max_leaf_nodes=4)\nm.fit(xs, y);\n\nHere, we’ve instructed sklearn to create four leaf nodes. To visualize what the model has learned, we can display the tree:\n\ndraw_tree(m, xs, size=10, leaves_parallel=True, precision=2)\n\n\n\n\n\n\n\n\nUnderstanding this visualization helps in graphing decision tree:\n\nTop node: Represents the entire dataset before any splits. Average sale price (log) is 10.10, with a mean squared error of 0.48.\nFirst split: Based on coupler_system.\n\n\nLeft branch: coupler_system &lt; 0.5 (360,847 records, avg. 10.21)\nRight branch: coupler_system &gt; 0.5 (43,863 records, avg. 9.21)\n\n\nSecond split (on left branch): Based on YearMade.\n\n\nLeft sub-branch: YearMade &lt;= 1991.5 (155,724 records, avg. 9.97)\nRight sub-branch: YearMade &gt; 1991.5 (205,123 records, avg. 10.4)\n\n\nLeaf nodes: The bottom row, where no more splits occur.\n\nWe can display this information using Terence Parr’s dtreeviz library to enhance visualization:\n\nsamp_idx = np.random.permutation(len(y))[:500]\ndtreeviz(m, xs.iloc[samp_idx], y.iloc[samp_idx], xs.columns, dep_var,\n        fontname='DejaVu Sans', scale=1.6, label_fontsize=10,\n        orientation='LR')\n\n\n\n\n\n\n\n\nThis visualization illuminates data distribution, showcasing issues like bulldozers dated to the year 1000, likely placeholders for missing data. For modeling precision, these can be substituted with 1950 to improve visualization clarity without significantly influencing model results:\n\nxs.loc[xs['YearMade']&lt;1900, 'YearMade'] = 1950\nvalid_xs.loc[valid_xs['YearMade']&lt;1900, 'YearMade'] = 1950\n\nThis update clarifies the tree visualization while maintaining the models integrity. After making this change, re-evaluate the decision tree:\n\nm = DecisionTreeRegressor(max_leaf_nodes=4).fit(xs, y)\n\ndtreeviz(m, xs.iloc[samp_idx], y.iloc[samp_idx], xs.columns, dep_var,\n        fontname='DejaVu Sans', scale=1.6, label_fontsize=10,\n        orientation='LR')\n\n\n\n\n\n\n\n\nNow, let’s leverage the decision tree algorithm to generate a more complex model. This time, we’ll refrain from specifying any stopping criteria, such as max_leaf_nodes:\n\nm = DecisionTreeRegressor()\nm.fit(xs, y);\n\nTo evaluate our model’s performance, we’ll define a function to compute the root mean squared error(RMSE) which was the scoring criterion in this competition:\n\ndef r_mse(pred,y): return round(math.sqrt(((pred-y)**2).mean()), 6)\ndef m_rmse(m, xs, y): return r_mse(m.predict(xs), y)\nm_rmse(m, xs, y)\n\n0.0\n\n\nThe output is 0.0. At the first glance, it appears that our model is flawless. But hold on, we need to evalueate the validation set to check for overfitting:\n\nm_rmse(m, valid_xs, valid_y)\n\n0.332239\n\n\nThe validation set RMSE is 0.332239, indicating potential overfitting. Let’s investigating further\n\nm.get_n_leaves(), len(xs)\n\n(324338, 404710)\n\n\nIt turns out our model has nearly as many leaves as data point! This occurs because sklearn’s default setting allow continual splitting until there’s just one item per leaf node. We can address this by adjusting the stopping rule to require each leaf node to have at least 25 auction records:\n\nm = DecisionTreeRegressor(min_samples_leaf=25)\nm.fit(to.train.xs, to.train.y)\nm_rmse(m, xs, y), m_rmse(m, valid_xs, valid_y)\n\n(0.243049, 0.308857)\n\n\nThis results in a more balanced model. Let’s verify the new number of leaves:\n\nm.get_n_leaves()\n\n12432\n\n\nDecision trees are adept at modelling data due to their adaptability to nonlinear relationships and variable interactions. Nonetheless, a compromise exist between generallizability (achieved with smaller trees) and training accuracy (achieved with larger trees)\nHow do wee balance these strengths? We’ll explore further after covering essential aspect handling categorical variables.\nIn deep learning, categorical variables are often one-hot encoded and fed into embedding layers. However, decision trees lack embedding layers so how can we leverage untreated categorical variables efficiently? let’s consider a use-case with product codes.\nSuppose we have an auction dataset with product codes (categorical variables) and sale prices. “Product X” for instance, consistently sells at a premium. Decision trees split data based on features optimally partition the target variable. A split distinguishing “Product X” from others creates:\n\nGroup A: containing product X\nGroup B: containing all other products\n\nThis chose arises because “Product X” is notably pricier, leading Group A to have a higher average price than Group B. This split provides valuable insights for price prediction, prompting the algorithm to prefer it. The decision tree isolates “Product X” quickly, allowing precise price predictions while evaluating other products’ prices.\nOne-hot encoding is another option; it transforms a single categorical column into multiple binary columns, each representing a category level. Pandas offers the get_dummies method which does just that.\nHowever, there’s little evidence that one-hot encoding enhances results. Thus, we tend to avoid it when unnecessary, as it complicates data handling.\n\n\nCreating a Random Forest\nCreating a random forest involves a process similar to crafting a decision tree, but with added flexibility through parameters that determine the number of trees, data point subset size(rows), and field subset size(columns):\n\ndef rf(xs, y, n_estimators=40, max_samples=200_000,\n       max_features=0.5, min_samples_leaf=5, **kwargs):\n    return RandomForestRegressor(n_jobs=-1, n_estimators=n_estimators,\n        max_samples=max_samples, max_features=max_features,\n        min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y)\n\nHere’s an explanation of the parameters used in the function:\n\nn_estimators: specifies the number of tree in the forest.\nmax_samples: indicates how many rows to sample when training each tree.\nmax_features: sets the number of columns to sample at each split (e.g., 0.5 means using half of the columns).\nmin_samples_leaf: determines the minimum number of samples required in the leaf node, controlling the tree depth.\n\nAdditionally, n_jobs=-1 ensures that all available CPUs are utilized for parallel tree building. This function allows quick experimentation with different configurations.\nInitiating the random forest model is straightforward:\n\nm = rf(xs, y);\n\nBy using multiple trees rather than a single DecisionTreeRegressor, the validation RMSE significantly improves:\n\nm_rmse(m, xs, y), m_rmse(m, valid_xs, valid_y)\n\n(0.171371, 0.233223)\n\n\nA distinctive feature of random forests is the resilience hyperparameter configurations, particularly max_features.\n\n\n\n\n\n\nNote\n\n\n\nWhen we say random forests show resilience to hyperparameter configurations, it means that the algorithm performs well across a range of different hyperparameter settings. It doesn’t require very precise tuning to achieve good results, making it a flexible option in many applications.\n\n\nThe N_estimators parameter can be set to as high as value as feasible, the more trees, the greater the accuracy potential\nFor visualizing effects of varying max_features with increasing tree counts, refer to sklearn’s documentation which provides insightful plots.\n\nThe image demonstrates:\n\nBlue line: represents minimal features usage.\nGreen line: represents maximal feature usage (full feature set). Subsets of features combined with numerous trees usualy yield the lowest error.\n\nTo explore the impact of n_estimators analyze predictions from each individual tree within the forest (accessible via the estimators_ attribute):\n\npreds = np.stack([t.predict(valid_xs) for t in m.estimators_])\n\n\nr_mse(preds.mean(0), valid_y)\n\n0.233223\n\n\nThis calculation, preds.mean(0), parallels the overall random forest prediction. Observe RMSE progression as trees are added:\n\nplt.plot([r_mse(preds[:i+1].mean(0), valid_y) for i in range(40)]);\n\n\n\n\n\n\n\n\nDespite improved RMSE in training, the validation set’s performance may deteriorate due to potential overfitting or time discrepancies. This challenge is addressable by leveraging the out-of-bag (OOB) error methodology in random forests, offering valuable insights.\nIn the next section, we’ll delve deeper into creating a random forest and optimizing it’s performance.\n\n\nOut of Bag Error\nIn a random forest, each tree is trained on different subset of data. Consequently, there’s a unique opportunity: each tree has an implicit validation set composed of the data rows not selected for its training, know as out-of-bag (OOB) data.\nOOB error is particularly useful when dealing with a limited dataset, as it offers a measure of model generalization without needing to withhold data for a separate validation set. These OOB predictions are stored in the oob_prediction_ attribute. Remember, these are compared with training labels, as the OOB calculation involves the training set:\n\nr_mse(m.oob_prediction_, y)\n\n0.211234\n\n\nThe OOB error frequently appears lower than the validation set error, hinting that other factors might contribute to the validation error, hinting that other factors might contribute to the validation error outside mere generalization discrepancies. We’ll delve into these causes soon.\n\n\nModel Interpretation\nInterpreting models trained on tabular data presents valuable insights. Higher understanding can be sought in ares like:\n\nHow confident are we in our predictions using a particular row of data?\nFor predicting with a particular row of data, what were the most important factors, and how did they influence that prediction?\nWhich columns are the strongest predictors, which can we ignore?\nWhich columns are effectively redundant with each other, for purposes of prediction?\nHow do predictions vary, as we vary these columns?\n\nRandom forests are adept at addressing these questions. Let’s start with evaluating confidence in predictions!\nModel predictions are an average of individual tree predictions, providing an estimated value. But how can we gauge the confidence of this estimate? One simplistic approach is using the standard deviations of tree predictions - higher deviations imply less confidence, suggesting that caution is needed, especially in scenarios where tree predictions are inconsistent.\nIn creating the random forest, predictions over the validations set were obtained using Python’s list comprehension:\n\npreds = np.stack([t.predict(valid_xs) for t in m.estimators_])\n\n\npreds.shape\n\n(40, 7988)\n\n\nThis results in a prediction for each tree across all validation set auctions (40 trees, 7,988 auctions). With this data, compute the standard deviation of predictions for each auction:\n\npreds_std = preds.std(0)\npreds_std[:5]\n\narray([0.2000169 , 0.08355874, 0.113672  , 0.2747    , 0.12065141])\n\n\nThe standard deviations highlight varying levels of confidence across auctions. A lower deviation signals stronger agreement among trees, leading to higher confidence. Conversely, higher deviations indicate disagreement, pointing towards lower confidence. In practical applications like auction bidding, this information is useful; you might reconsider bidding when predictions show low certainty.\n\nFeature Importance\nKnowing a model’s predictive accuracy is critical, but equally important is understanding how those predictions are made. Feature importance offers valuable insight into this process. Sklearn’s random forest model provides feature importance scores via the feature_importance_ attributes. Here’s a simple function load these scores into a DataFrame and sort them\n\ndef rf_feat_importance(m, df):\n    return pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}\n                       ).sort_values('imp', ascending=False)\nfi = rf_feat_importance(m, xs)\nfi[:10]\n\n\n\n\n\n\n\n\ncols\nimp\n\n\n\n\n57\nYearMade\n0.166375\n\n\n30\nCoupler_System\n0.113599\n\n\n6\nProductSize\n0.103802\n\n\n7\nfiProductClassDesc\n0.078686\n\n\n3\nfiSecondaryDesc\n0.054542\n\n\n54\nModelID\n0.052919\n\n\n65\nsaleElapsed\n0.050521\n\n\n31\nGrouser_Tracks\n0.041514\n\n\n12\nEnclosure\n0.039451\n\n\n32\nHydraulics_Flow\n0.035355\n\n\n\n\n\n\n\nEvaluating the features importances reveals that a few columns significantly contribute to the model’s predictions, most notably, YearMade and ProductSize.\nTo visualize these importance, plotting them can clarify their relative value:\n\ndef plot_fi(fi):\n    return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)\n\nplot_fi(fi[:30]);\n\n\n\n\n\n\n\n\n\n\nRemoving Low-Importance Variables\nA subset of columns might suffice to maintain accuracy while enhancing simplicity by discarding low-importance variables. Let’s retain only those with an importance score above 0.005:\n\nto_keep = fi[fi.imp&gt;0.005].cols\nlen(to_keep)\n\n22\n\n\nRetrain the model using this refined feature set:\n\nxs_imp = xs[to_keep]\nvalid_xs_imp = valid_xs[to_keep]\n\nm = rf(xs_imp, y)\nm_rmse(m, xs_imp, y), m_rmse(m, valid_xs_imp, valid_y)\n\n(0.180965, 0.231633)\n\n\nThe models accuracy remain consistent, yet fewer columns necessitate examination:\n\nlen(xs.columns), len(xs_imp.columns)\n\n(66, 22)\n\n\nSimplifying a model is often the initial step in enhancing it having 78 columns can be overwhelming for deep analysis. Particularly, a learner, more interpretable model is simpler to deploy and manage.\nRevisiting the feature importance plot provides clearer insights:\n\nplot_fi(rf_feat_importance(m, xs_imp));\n\n\n\n\n\n\n\n\nWhile interpreting, redundancy may arise as seen with ProductGroup and ProductGroupDesc. Attempting to remove such redundant features can further streamline interpretation.\n\n\nRemoving Redundant Variables\nWe’ll begin by clustering columns to identify pairs that are closely aligned often suggesting redundancy:\n\ncluster_columns(xs_imp)\n\n\n\n\n\n\n\n\nThe chart generated from clustering will reveal which columns were merged early on. Notably, pairs like ProductGroup with ProductGroupDesc, saleYear with saleElapsed, and fiModelDesc with fiBaseModel are likely correlated to the point of redundancy.\nNext, we will attempt to simplify the model by removing these related features. We begin by defining a function to quickly train a random forest and capture the out-of-bag(OOB) score. This score, ranging from 1.0 for perfection to near-zero, provides a relative comparison metric as we remove redundant columns:\n\ndef get_oob(df):\n    m = RandomForestRegressor(n_estimators=40, min_samples_leaf=15,\n        max_samples=50000, max_features=0.5, n_jobs=-1, oob_score=True)\n    m.fit(df, y)\n    return m.oob_score_\n\nFirst, we’ll confirm our baseline score with all columns:\n\nget_oob(xs_imp)\n\n0.8760739540611289\n\n\nNext, test the impact of removing each potentially redundant variable individually:\n\n{c:get_oob(xs_imp.drop(c, axis=1)) for c in (\n    'saleYear', 'saleElapsed', 'ProductGroupDesc','ProductGroup',\n    'fiModelDesc', 'fiBaseModel',\n    'Hydraulics_Flow','Grouser_Tracks', 'Coupler_System')}\n\n{'saleYear': 0.8742959821922331,\n 'saleElapsed': 0.8698149904307536,\n 'ProductGroupDesc': 0.8755334280543031,\n 'ProductGroup': 0.8745495772129529,\n 'fiModelDesc': 0.8743458666758965,\n 'fiBaseModel': 0.8748827464781819,\n 'Hydraulics_Flow': 0.8762012623754625,\n 'Grouser_Tracks': 0.8755826405754699,\n 'Coupler_System': 0.8758570604637711}\n\n\nWe’ll also explore the effect of dropping one columns from each identified pair:\n\nto_drop = ['saleYear', 'ProductGroupDesc', 'fiBaseModel', 'Grouser_Tracks']\nget_oob(xs_imp.drop(to_drop, axis=1))\n\n0.8743053306321846\n\n\nEncouragingly, the model’s performance remains largely unchanged. We will now finalize this reduce dataset:\n\nxs_final = xs_imp.drop(to_drop, axis=1)\nvalid_xs_final = valid_xs_imp.drop(to_drop, axis=1)\n\nsave_pickle(path/'xs_final.pkl', xs_final)\nsave_pickle(path/'valid_xs_final.pkl', valid_xs_final)\n\nFor later retrieval, you can load these condensed datasets with:\n\nxs_final = load_pickle(path/'xs_final.pkl')\nvalid_xs_final = load_pickle(path/'valid_xs_final.pkl')\n\nLet’s verify that the RMSE remains consistent after this reduction:\n\nm = rf(xs_final, y)\nm_rmse(m, xs_final, y), m_rmse(m, valid_xs_final, valid_y)\n\n(0.182663, 0.231313)\n\n\nBy concentrating on key variables and eliminating redundancies, we’ve streamlined our model significantly. Now, let’s further explore how these influential variables affect predictions using partial dependence plots.\n\n\nPartial Dependence\nAlright, let’s get a feel for these predictions. Imagine checking out the menu at a restaurant. Before ordering, you’d want to know what’s popular, right? We do the same thing with our data. For ProductSize, we count how many times each size appears using something like Pandas’ value_counts method and then plot this on a bar chart. Here’s our code in action:\n\np = valid_xs_final['ProductSize'].value_counts(sort=False).plot.barh()\nc = to.classes['ProductSize']\nplt.yticks(range(len(c)), c);\n\n\n\n\n\n\n\n\nTurns out, the biggest “dish” on our menu is labeled Compact but look at #na#, fastai’s way of showing missing values. No big surprise there!\nWhat about YearMade? This time, instead of a bar chart, we whip out a histogram.\n\nax = valid_xs_final['YearMade'].hist()\n\n\n\n\n\n\n\n\nApart from 1950, which we used as placeholder for unknown years, most machines were crafted post-1990. Vintage anyone?\nPartial dependence plots help us see what would happen to the sale price if one feature changed while everything else stayed the same.\nFor YearMade, we can’t just average sale prices by year because many things change over time. Instead, we replace every year value with a single year, like 1950, and calculate the average predicted sale price. We repeat this for each year, up to 2011, to see how YearMade alone affects price.\nThen, we plot the results:\n\nfrom sklearn.inspection import PartialDependenceDisplay\nfig,ax = plt.subplots(figsize=(12, 4))\nPartialDependenceDisplay.from_estimator(m, valid_xs_final, ['YearMade', 'ProductSize'],\n                                        grid_resolution=20, ax=ax)\n\n\n\n\n\n\n\n\nFor YearMade, after 1990, there’s a clear pattern: prices rise as the year increase. This make sense because older items depreciate.\nThe plot for ProductSize show that the group with missing values has the lowest prices. Understanding why these values are missing is crucial, as sometimes they can be good predictors, or they could indicate an issue like data leakage\n\n\nData Leakage\nIn the world of data mining, there’s a tricky issue known as data leakage, described in detail by Shachar Kaufman, Saharon Rosset, and Claudia Perlich in their paper, Leakage in Data Mining: Formulation, Detection, and Avoidance. They define it as the unintentional introduction of information about the target of a data mining problem that shouldn’t be available to mine from. To put it simply, it’s like saying ‘it rains on rainy days,’ where the model mistakenly uses the target itself as an input.\nData leakage can be subtle, appearing in various forms, and one such form is through missing values. Here are the straightforward steps to spot data leakage:\n\nAssess whether your model’s accuracy seems too perfect. If it feels too good to be true, leakage might be playing a part.\nEvaluate the significant predictors. If they don’t add up in a practical sense, then something might be off.\nAnalyze the partial dependence plots. If they yield nonsensical results, you could be facing a leakage issue.\n\nAdditionally, tools like tree interpreters can aid in understanding which factors are influencing specific predictions.\nAvoiding data leakage demands meticulous attention through all phases of data handling—from collection to preparation. The key is adopting a “learn-now, predict-later” approach, ensuring that models are built without any preview of the answers.\n\n\nTree Interpreter\nBefore we go in please make sure you’re already have treeinterpreter and waterfallcharts installed if not run this in your terminal\npip install treeinterpreter\npip install waterfallcharts\nAt the start of this section, we said that we wanted to be able to answer five questions:\n\n\n\n\n\n\nNote\n\n\n\n\nHow confident are we in our predictions using particular row of data?\nFor predicting with a particular row of data, what were the most important factors, and how did they influence that predictions?\nWhich columns are the strongest predictors, which can we ignore?\nWhich columns are effectively redundant with each other, for purpose of prediction?\nHow do predictions vary, as we vary these columns?\n\n\n\nWe’ve addressed four of these, leaving only the second question. To tackle this, we’ll use the treeinterpreter library, along with the waterfallcharts library for visualization.\n\nfrom treeinterpreter import treeinterpreter\nfrom waterfall_chart import plot as waterfall\n\nWhile we’ve computed feature importances across entire random forest, we can apply a similar concept to a single row of data. This approach examines the contribution of each variable to improving the model at each branch of every tree, then sums these contributions per variables for a specific data point.\nFor example, if we’re analyzing a particular auction item predicted to be expensive, we can understand why by examining that single row of data. We’ll process it through each decision tree, observing the split used at each point and calculating the increase or decrease in addition compared to the parent node. This process is repeated for every tree, summing up the total change in importance by split variable.\n\n\n\n\n\n\nNote\n\n\n\nFor example, if you’re predicting house prices:\n\nThe bias might be the average house price in your dataset.\nA positive contribution from the “number of bedrooms” feature would indicate that having more bedrooms increased the predicted price.\nA negative contribution from the “distance from city center” feature might indicate that being further from the city center decreased the predicted price.\n\n\n\nLet’s select the first few rows of our validation set:\n\nrow = valid_xs_final.iloc[:5]\n\nWe can then use treeinterpreter:\n\nprediction,bias,contributions = treeinterpreter.predict(m, row.values)\n\nHere, prediction is the random forest’s prediction, bias is the prediction based on the mean of the dependent variable, and contributions shows how each feature (independent variable) in your input data contributed to moving the prediction away from the bias. The sum of contributions plus bias equals the prediction for each row\n\nprediction[0], bias[0], contributions[0].sum()\n\n(array([10.06313964]), 10.104746057831763, -0.04160642242374439)\n\n\nTo visualize the contributions clearly, we can use waterfall plot:\n\nwaterfall(valid_xs_final.columns, contributions[0], threshold=0.08,\n          rotation_value=45,formatting='{:,.3f}');\n\n\n\n\n\n\n\n\nThis plot demonstrates how positive and negative contributes from all independent variables sum up to create the final prediction, show in the rightmost column labeled net.\nThis type of information is particularly valuable in production environments, rather than during model development. It can provide users of your data product with insightful information about the underlying reasoning behind the predictions.\nHaving explored these classic machine learning techniques, we’re now ready to see how deep learning can contribute to solving this problem"
  },
  {
    "objectID": "posts/2024-11-12-random-forest/index.html#extrapolation-and-neuron-networks",
    "href": "posts/2024-11-12-random-forest/index.html#extrapolation-and-neuron-networks",
    "title": "Random Forest, Bagging, Boosting",
    "section": "Extrapolation and Neuron Networks",
    "text": "Extrapolation and Neuron Networks\nRandom forests, like all machine learning or deep learning algorithms, don’t always generalize well to new data. Lets explore this issue, particularly focusing on the extrapolation problem that random forests face.\n\nThe Extrapolation Problem\nConsider a simple task: making prediction from 40 data points showing a slightly noisy linear relationship. We’ll create this data and visualize it:\n\nnp.random.seed(42)\nx_lin = torch.linspace(0,20, steps=40)\ny_lin = x_lin + torch.randn_like(x_lin)\nplt.scatter(x_lin, y_lin);\n\n\n\n\n\n\n\n\nWe need to reshape our data for sklearn, which expect a matrix of independent variables:\n\nxs_lin = x_lin.unsqueeze(1)\nx_lin.shape,xs_lin.shape\n\n(torch.Size([40]), torch.Size([40, 1]))\n\n\n\nx_lin[:,None].shape\n\ntorch.Size([40, 1])\n\n\nNow, let’s create a random forest using the first 30 rows for training:\n\nm_lin = RandomForestRegressor().fit(xs_lin[:30],y_lin[:30])\n\nWe’ll test the model on the full dataset and visualize the results:\n\nplt.scatter(x_lin, y_lin, 20)\nplt.scatter(x_lin, m_lin.predict(xs_lin), color='red', alpha=0.5);\n\n\n\n\n\n\n\n\nHere’s where we encounter a significant issue: our predictions outside the training data domain are consistently too low. This happens because a random forest average value of the rows in a leaf. Consequently, a random forest can’t predict values outside the rage of its training data.\nThis limitation is particularly problematic for data with time-based trends, like inflation, where future predictions are needed. Your predictions will systematically be too low.\nThe problem isn’t limited to time variables, though. Random forest struggle to extrapolate beyond the types of data they’ve seen in a more general sense. That’s wy it’s crucial to ensure our validation set doesn’t contain out-of-domain data\n\n\nFinding Out-of-Domain Data\nIdentifying whether your test set is distributed differently from your training data can be challenging. Interestingly, we can use a random forest to help us with this task. Here’s how:\nInstead of predicting our actual dependent variable, we’ll try to predict whether a row belongs to the validation set or the training set. Let’s combine our training and validation sets, create a new dependent variable representing the dataset origin, and build a random forest:\n\ndf_dom = pd.concat([xs_final, valid_xs_final])\nis_valid = np.array([0]*len(xs_final) + [1]*len(valid_xs_final))\n\nm = rf(df_dom, is_valid)\nrf_feat_importance(m, df_dom)[:6]\n\n\n\n\n\n\n\n\ncols\nimp\n\n\n\n\n6\nsaleElapsed\n0.910266\n\n\n11\nSalesID\n0.073707\n\n\n14\nMachineID\n0.012246\n\n\n0\nYearMade\n0.000813\n\n\n9\nfiModelDesc\n0.000535\n\n\n5\nModelID\n0.000471\n\n\n\n\n\n\n\nThis reveals three columns that differ significantly between the sets: saleElapsed, SalesID and MachineID. saleElapsed directly encoded the date, while SalesID and MachineID likely represent incrementing identifiers over time.\nLet’s compare the RMSE of our original model with versions that exclude these columns:\n\nm = rf(xs_final, y)\nprint('orig', m_rmse(m, valid_xs_final, valid_y))\n\nfor c in ('SalesID','saleElapsed','MachineID'):\n    m = rf(xs_final.drop(c,axis=1), y)\n    print(c, m_rmse(m, valid_xs_final.drop(c,axis=1), valid_y))\n\norig 0.231001\nSalesID 0.230214\nsaleElapsed 0.235865\nMachineID 0.231447\n\n\nIt appears that we can remove SalesID and MachineID without losing accuracy:\n\ntime_vars = ['SalesID','MachineID']\nxs_final_time = xs_final.drop(time_vars, axis=1)\nvalid_xs_time = valid_xs_final.drop(time_vars, axis=1)\n\nm = rf(xs_final_time, y)\nm_rmse(m, valid_xs_time, valid_y)\n\n0.228264\n\n\nRemoving these variables slightly improves the model’s accuracy and should make it more resilient over time, easier to maintain, and understand.\nSometimes, using only recent data can help. Let’s try using data from the most recent years:\n\nxs['saleYear'].hist();\n\n\n\n\n\n\n\n\n\nfilt = xs['saleYear']&gt;2004\nxs_filt = xs_final_time[filt]\ny_filt = y[filt]\n\n\nm = rf(xs_filt, y_filt)\nm_rmse(m, xs_filt, y_filt), m_rmse(m, valid_xs_time, valid_y)\n\n(0.176448, 0.228537)\n\n\nThis yields a slightly improvement, demonstrating that using your entire dataset isn’t always the best approach; sometimes subset can perform better.\nI recommend building a model with is_valid as the dependent variable for all datasets. This can uncover subtle domain shift issues that might otherwise go unnoticed.\nNext, we’ll explore whether using a neural network can further improve our results\n\n\nUsing Neural Networks\nTo build a neural network model, we’ll follow a similar approach to our random forest setup. First, let’s replicate the steps for creating the TabularPandas object:\n\ndf_nn = pd.read_csv(path/'TrainAndValid.csv', low_memory=False)\ndf_nn['ProductSize'] = df_nn['ProductSize'].astype('category')\ndf_nn['ProductSize'].cat.set_categories(sizes, ordered=True)\ndf_nn[dep_var] = np.log(df_nn[dep_var])\ndf_nn = add_datepart(df_nn, 'saledate')\n\nWe can utilize the column selection from our random forest model for the neural network:\n\ndf_nn_final = df_nn[list(xs_final_time.columns) + [dep_var]]\n\nNeural networks handle categorical columns differently than decision trees. Embedding are an effective method for categorical variables in neural nets. Fastai determines which columns should be treated as categorical by comparing the number of distinct levels to the max_card parameter. We’ll use 9,000 as our max_card to avoid unnecessarily large embeddings:\n\ncont_nn,cat_nn = cont_cat_split(df_nn_final, max_card=9000, dep_var=dep_var)\n\nIt’s crucial to ensure that saleElapsed isn’t treated as a categorical variable as we need to predict auction sale prices in the feature. Let’s verify the continuous variable\n\n\n\n\n\n\nNote\n\n\n\nAs a continuous variable, saleElapsed can capture trends over time. If it were treated as a categorical variable, you’d lose the ability to interpolate or extrapolate between known values, which is crucial for prediction.\nWhen you’re predicting auction sale prices for future dates, you’ll be dealing with ‘saleElapsed’ values that weren’t in your training data. If ‘saleElapsed’ were categorical, your model wouldn’t know how to handle these new values.\n\n\n\ncont_nn\n\n['saleElapsed']\n\n\nNow, let’s examine the cardinality of our chosen categorical variables:\n\ndf_nn_final[cat_nn].nunique()\n\nYearMade                73\nCoupler_System           2\nProductSize              6\nfiProductClassDesc      74\nfiSecondaryDesc        177\nModelID               5281\nEnclosure                6\nHydraulics_Flow          3\nfiModelDesc           5059\nfiModelDescriptor      140\nHydraulics              12\nProductGroup             6\nDrive_System             4\nTire_Size               17\nTrack_Type               2\ndtype: int64\n\n\nWe notice two “model” variables with similar high cardinalities, suggesting potential redundancy. To reduce the embedding matrix size. Let’s assess the impact of removing one of these model columns on our random forest:\n\nxs_filt2 = xs_filt.drop('fiModelDescriptor', axis=1)\nvalid_xs_time2 = valid_xs_time.drop('fiModelDescriptor', axis=1)\nm2 = rf(xs_filt2, y_filt)\nm_rmse(m2, xs_filt2, y_filt), m_rmse(m2, valid_xs_time2, valid_y)\n\n(0.178386, 0.229505)\n\n\ngiven the minimal impact, We’ll remove fiModelDescriptor from our neural network predictors:\n\ncat_nn.remove('fiModelDescriptor')\n\nWhen creating our TabularPandas object for the neural network, we need to add normalization, which is crucial for neural networks but unnecessary for random forests:\n\nprocs_nn = [Categorify, FillMissing, Normalize]\nto_nn = TabularPandas(df_nn_final, procs_nn, cat_nn, cont_nn,\n                      splits=splits, y_names=dep_var)\n\nSince tabular models and data generally don’t require much GPU RAM, we can use larger batch sizes:\n\ndls = to_nn.dataloaders(1024)\n\nFor regression models, it’s advisable to set y_range. Let’s find the min and max of our dependent variable:\n\ny = to_nn.train.y\ny.min(),y.max()\n\n(8.465899467468262, 11.863582611083984)\n\n\nNow we can create the Learner for our tabular model. We’ll use MSE as the loss function and increase the default layer sizes to 500 and 250 for our large dataset:\n\nlearn = tabular_learner(dls, y_range=(8,12), layers=[500,250],\n                        n_out=1, loss_func=F.mse_loss)\nlearn.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.00013182566908653826)\n\n\n\n\n\n\n\n\n\nWe’ll train with fit_one_cycle for a few epochs:\n\nlearn.fit_one_cycle(5, 1e-2)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.061921\n0.067224\n00:05\n\n\n1\n0.051130\n0.056330\n00:04\n\n\n2\n0.046388\n0.054012\n00:03\n\n\n3\n0.041853\n0.054157\n00:03\n\n\n4\n0.040173\n0.052207\n00:03\n\n\n\n\n\nLet’s compare the result to our earlier random forest using the r_mse function:\n\npreds,targs = learn.get_preds()\nr_mse(preds,targs)\n\n\n\n\n\n\n\n\n0.228488\n\n\nThe neural network performs better than the random forest, although it take longer to train and requires more careful hyprerparameter tuning\nWe’ll save our model for future use:\n\nlearn.save('nn')\n\nPath('models/nn.pth')\n\n\nTo further improve generalization, we can use ensemble learning, which evolves averaging predictions from several models."
  },
  {
    "objectID": "posts/2024-11-12-random-forest/index.html#ensembling",
    "href": "posts/2024-11-12-random-forest/index.html#ensembling",
    "title": "Random Forest, Bagging, Boosting",
    "section": "Ensembling",
    "text": "Ensembling\nThe success of random forests is rooted in the principle that while individual trees have errors, these errors are not correlated. With enough trees, the average of these errors should approach zero. We can apply similar reasoning to combine predictions from different algorithms.\nIn our case, we have two distinct models: a random forest and a neural network. Their different approaches likely result in different types of errors. Therefore, averaging their predictions could potentially outperform either model individually.\nIt’s worth nothing that a random forest is itself an ensemble, By combining it with a neural network, we’re creating an ensemble of ensembles! While ensembling may not revolutionize your modeling process, it can provide a welcome boost to your exiting model.\nOne small challenge we face is the different output types from our Pytorch and sklearn models. Pytorch gives a rank-2 tensor (a column matrix), while sklearn produces a rank-1 array (a vector). We can address this using squeeze to remove unit axes and to_np to convert to Numpy array\n\nrf_preds = m.predict(valid_xs_time)\nens_preds = (to_np(preds.squeeze()) + rf_preds) /2\n\nThis ensemble approach yield better result than either model individually:\n\nr_mse(ens_preds,valid_y)\n\n0.222895"
  },
  {
    "objectID": "posts/2024-11-12-random-forest/index.html#boosting",
    "href": "posts/2024-11-12-random-forest/index.html#boosting",
    "title": "Random Forest, Bagging, Boosting",
    "section": "Boosting",
    "text": "Boosting\nWhile our previous ensembling approach used bagging (combination many models trained on different data subsets by averaging), another important technique is boosting, where models are added instead of averaged.\nBoosting works as follow:\n\nTrain a small, underfitting model on you dataset.\nCalculate this model predictions for the training set.\nSubtract these predictions from the actual targets to get the “residuals”(the error for each training point).\nReturn to step 1, but use the residuals as the new training targets.\nRepeat this process until reaching a stopping criterion(e.g., maximum number of trees or worsening validation set error).\n\nIn this approach, each new tree attempts to fit the combined error of all previous trees. As we continually create new residuals by subtracting each new tree’s predictions from the previous residuals, these residuals progressively decrease.\nTo make predictions with a boosted tree ensemble, we calculate predictions from each tree and sum them. This approach has many variations and names, including Gradient Boosting Machines (GBMs) and Gradient Boosted Decision Trees (GBDTs). XGBoost is currently the most popular implementation.\nUnlike random forests, boosting can lead to overfitting. In random forests, adding more trees doesn’t cause overfitting because each tree is independent. However, in a boosted ensemble, more trees continuously improve the training error, potentially leading to overfitting on the validation set."
  },
  {
    "objectID": "posts/2024-11-12-random-forest/index.html#key-takeaway",
    "href": "posts/2024-11-12-random-forest/index.html#key-takeaway",
    "title": "Random Forest, Bagging, Boosting",
    "section": "Key takeaway",
    "text": "Key takeaway\nWe have discussed two approaches to tabular modeling: decision tree ensembles and neural networks. We’ve also mentioned two different decision tree ensembles: random forests, and gradient boosting machines. Each is very effective, but each also has compromises:\n\nRandom forests are the easiest to train, because they are extremely resilient to hyperparameter choices and require very little preprocessing. They are very fast to train, and should not overfit if you have enough trees. But they can be a little less accurate, especially if extrapolation is required, such as predicting future time periods.\nGradient boosting machines in theory are just as fast to train as random forests, but in practice you will have to try lots of different hyperparameters. They can overfit, but they are often a little more accurate than random forests.\nNeural networks take the longest time to train, and require extra preprocessing, such as normalization; this normalization needs to be used at inference time as well. They can provide great results and extrapolate well, but only if you are careful with your hyperparameters and take care to avoid overfitting.\n\nWe suggest starting your analysis with a random forest. This will give you a strong baseline, and you can be confident that it’s a reasonable starting point. You can then use that model for feature selection and partial dependence analysis, to get a better understanding of your data.\nFrom that foundation, you can try neural nets and GBMs, and if they give you significantly better results on your validation set in a reasonable amount of time, you can use them. If decision tree ensembles are working well for you, try adding the embeddings for the categorical variables to the data, and see if that helps your decision trees learn better.\nAlright guys, it’s been a long post huh? Thanks for reading all of those, catch you on the flip side, and I’ll see you… next time!"
  },
  {
    "objectID": "posts/2024-07-28-from-neuron-to-gradient/index.html",
    "href": "posts/2024-07-28-from-neuron-to-gradient/index.html",
    "title": "Looking inside neural networks",
    "section": "",
    "text": "Hey there. It’s been a couple of week since my last post - blame exams and obsessive quest to tweak every configuration setting for my workflow (which is turned into a week-long habit hole - i regret nothing). But today, I’m excited to dive back into the world of AI and share my latest escapades from Lesson 3 of the FastAI course taught by the indomitable Jeremy Horawd. Spoiler alert: it’s packed with enough neural wonders to make your brain do a happy dance.\nIn the coming post, I’ll guide you through:\n\nPicking of right AI model that’s just right for you\nDissecting the anatomy of these models (paramedics not required)\nThe inner workings of neuron networks\nThe Titanic competition\n\nSo, hold onto your neural nets and let’s jump right into it, shall we?"
  },
  {
    "objectID": "posts/2024-07-28-from-neuron-to-gradient/index.html#introduction",
    "href": "posts/2024-07-28-from-neuron-to-gradient/index.html#introduction",
    "title": "Looking inside neural networks",
    "section": "",
    "text": "Hey there. It’s been a couple of week since my last post - blame exams and obsessive quest to tweak every configuration setting for my workflow (which is turned into a week-long habit hole - i regret nothing). But today, I’m excited to dive back into the world of AI and share my latest escapades from Lesson 3 of the FastAI course taught by the indomitable Jeremy Horawd. Spoiler alert: it’s packed with enough neural wonders to make your brain do a happy dance.\nIn the coming post, I’ll guide you through:\n\nPicking of right AI model that’s just right for you\nDissecting the anatomy of these models (paramedics not required)\nThe inner workings of neuron networks\nThe Titanic competition\n\nSo, hold onto your neural nets and let’s jump right into it, shall we?"
  },
  {
    "objectID": "posts/2024-07-28-from-neuron-to-gradient/index.html#choosing-the-right-model",
    "href": "posts/2024-07-28-from-neuron-to-gradient/index.html#choosing-the-right-model",
    "title": "Looking inside neural networks",
    "section": "Choosing the Right Model",
    "text": "Choosing the Right Model\nWe’ll explore how to choose an image model that’s efficient, reliable, and cost-effective—much like selecting the perfect gadget. I’ll walk you through a practical example comparing two popular image models by training a pet detector model.\nLet’s start by setting up our environment.\n\nfrom fastai.vision.all import *\nimport timm\n\n\npath = untar_data(URLs.PETS)/'images'\n\n\ndls = ImageDataLoaders.from_name_func(\n    \".\",\n    get_image_files(path),\n    valid_pct=0.2,\n    seed=42,\n    label_func=RegexLabeller(pat=r'^([^/]+)_\\d+'),\n    item_tfms=Resize(224)\n)\n\n\n\n\n\n\n    \n      \n      100.00% [811712512/811706944 01:13&lt;00:00]\n    \n    \n\n\nLet’s break down what’s happening here. We’re using The Oxford-IIIT Pet dataset, fetched with a nifty little URL constant provide by FastAI. If you’re staring at the pattern pat=r'^([^/]+)\\_\\d+' like it’s some alien script, fear not! It’s just a regular expression used to extract label from filenames using fastai RegexLabeller\nHere’s the cheat sheet for the pattern:\n\n^ asserts the start of a string.\n([^/]+) matches one or more characters that are not forward slash and captures them as a group.\n_ matches an underscore.\n\\d+ matches one ore more digits.\n\nNow, let’s visualize our data:\n\ndls.show_batch(max_n=4)\n\n\n\n\n\n\n\n\nAnd, it’s training time! We start with a ResNet34 architecture:\n\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(3)\n\nDownloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n\n100%|██████████| 83.3M/83.3M [00:00&lt;00:00, 147MB/s] \n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.491942\n0.334319\n0.105548\n00:26\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.454661\n0.367568\n0.112991\n00:32\n\n\n1\n0.272869\n0.274704\n0.081867\n00:33\n\n\n2\n0.144361\n0.246424\n0.073072\n00:33\n\n\n\n\n\nAfter about two minutes, we reached a 7% error rate—not too shabby! However, there’s one catch: while ResNet34 is dependable like a classic family car, it isn’t the fastest option out there. To really amp things up, we need to find a more advanced, high-performance model.\n\nExploring the Model Landscape\nThe PyTorch image model library offers a wide range of architectures—not quite a zillion, but enough to give you plenty of options. Many of these models are built on mathematical functions like ReLUs (Rectified Linear Units), which we’ll discuss in more detail later. Ultimately, choosing the right model comes down to three key factors:\n\nSpeed\n\nMemory Usage\n\nAccuracy\n\n\n\nThe “Which Image Model is Best?” Notebook\nI highly recommend taking a look at Jeremy Howard’s excellent notebook, “Which image models are best?”. It’s a valuable resource for finding the best architecture for your needs. If you find it helpful, do check it out and consider giving it an upvote—Jeremy’s insights are solid.\nI’ve also included a copy of the plot below for quick reference. Enjoy exploring the model landscape!\n\nPlotly = require('https://cdn.plot.ly/plotly-latest.min.js');\ndf_results = FileAttachment(\"results-imagenet.csv\").csv()\ndf = FileAttachment(\"benchmark-infer-amp-nhwc-pt111-cu113-rtx3090.csv\").csv()\n\ndf_merged = {\n  let df_results_processed = df_results.map(r =&gt; ({ ...r, model_org: r.model, model: r.model.split('.')[0] }));\n\n  const dfColumns = Object.keys(df[0]);\n  const dfResultsColumns = Object.keys(df_results_processed[0]);\n\n  return df.flatMap(d =&gt; {\n    const matches = df_results_processed.filter(r =&gt; r.model === d.model);\n    return matches.map(match =&gt; {\n      let mergedRow = {};\n      dfColumns.forEach(col =&gt; {\n        if (dfResultsColumns.includes(col) && col !== 'model') { mergedRow[`${col}_x`] = d[col]; } else { mergedRow[col] = d[col]; }\n      });\n      dfResultsColumns.forEach(col =&gt; {\n        if (dfColumns.includes(col) && col !== 'model') { mergedRow[`${col}_y`] = match[col]; } else { mergedRow[col] = match[col]; }\n      });\n      return mergedRow;\n    });\n  });\n}\n\ndf_final = df_merged\n  .map(d =&gt; ({...d, secs: 1 / Number(d.infer_samples_per_sec)}))\n  .map(d =&gt; {\n    const familyMatch = d.model.match(/^([a-z]+?(?:v2)?)(?:\\d|_|$)/);\n    let family = familyMatch ? familyMatch[1] : '';\n    if (d.model.includes('in22')) family += '_in22';\n    if (d.model.match(/resnet.*d/)) family += 'd';\n    return {...d, family: family};\n  })\n  .filter(d =&gt; !d.model.endsWith('gn'))\n  .filter(d =&gt; /^re[sg]netd?|beit|convnext|levit|efficient|vit|vgg|swin/.test(d.family));\n{\n  const uniqueFamilies = [...new Set(df_final.map(d =&gt; d.family))];\n  const colorScale = uniqueFamilies.map((family, index) =&gt; { return `hsl(${index * 360 / uniqueFamilies.length}, 70%, 50%)`; });\n  const traces = uniqueFamilies.map((family, index) =&gt; {\n    const familyData = df_final.filter(d =&gt; d.family === family);\n    return {\n      name: family,\n      x: familyData.map(d =&gt; d.secs),\n      y: familyData.map(d =&gt; Number(d.top1)),\n      mode: 'markers',\n      type: 'scatter',\n      marker: { size: familyData.map(d =&gt; Math.pow(Number(d.infer_img_size), 2) / 5700), color: colorScale[index], },\n      text: familyData.map(d =&gt; `${d.model}&lt;br&gt; family=${d.family}&lt;br&gt; secs=${d.secs.toFixed(8)}&lt;br&gt; top1=${Number(d.top1).toFixed(3)}&lt;br&gt; size=${d.param_count_x}&lt;br&gt; infer_img_size=${d.infer_img_size}`),\n      hoverinfo: 'text',\n      hoverlabel: { bgcolor: colorScale[index] }\n    };\n  });\n  const layout = {\n    title: 'Inference',\n    width: 795,\n    height: 750,\n    autosize: true,\n    xaxis: { title: 'secs', type: 'log', autorange: true, gridcolor: 'rgb(233,233,233)', },\n    yaxis: { title: 'top1', range: [65, 90], gridcolor: 'rgb(233,233,233)', },\n    plot_bgcolor: 'rgb(240,240,255)',\n    showlegend: true,\n    legend: { title: {text: 'family'}, itemclick: 'toggle', itemdoubleclick: 'toggleothers' },\n    hovermode: 'closest'\n  };\n  const config = {responsive: true};\n  const plot = DOM.element('div');\n  Plotly.newPlot(plot, traces, layout, config);\n  return plot;\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere’s a breakdown of the plot from the notebook:\n\nThe X-axis represents seconds per sample (the lower, the better performance).\nThe Y-axis reflects the accuracy (higher is preferable).\n\nIn an ideal scenario, you would choose models that are located in the upper left corner of the plot. Although ResNet34 is a reliable choice—like a pair of trusty jeans—it’s no longer considered state-of-the-art. It’s time to explore the ConvNeXT models!\nBefore you get started, ensure that you have the timm package installed. You can install it using pip or conda:\npip install timm\nor\nconda install timm\nAfter that, let’s search for all available ConvNeXT models.\n\ntimm.list_models(\"convnext*\")\n\n\n\n['convnext_atto',\n 'convnext_atto_ols',\n 'convnext_base',\n 'convnext_femto',\n 'convnext_femto_ols',\n 'convnext_large',\n 'convnext_large_mlp',\n 'convnext_nano',\n 'convnext_nano_ols',\n 'convnext_pico',\n 'convnext_pico_ols',\n 'convnext_small',\n 'convnext_tiny',\n 'convnext_tiny_hnf',\n 'convnext_xlarge',\n 'convnext_xxlarge',\n 'convnextv2_atto',\n 'convnextv2_base',\n 'convnextv2_femto',\n 'convnextv2_huge',\n 'convnextv2_large',\n 'convnextv2_nano',\n 'convnextv2_pico',\n 'convnextv2_small',\n 'convnextv2_tiny']\n\n\nFound one? Awesome! Now, let’s put it to the test. We’ll specify the architecture as a string when we call vision_learner, Why previous time when we use ResNet34 we don’t need to pass it as string? you say! That’s because ResNet34 was built in fastai library so you just need to call it but with ConvNext you have to pass the arch as a string for it to work, alright let’s see what it look like:\n\narch = 'convnext_tiny.fb_in22k'\nlearn = vision_learner(dls, arch, metrics=error_rate).to_fp16()\nlearn.fine_tune(3)\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.123377\n0.240116\n0.081191\n00:27\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.260218\n0.225793\n0.071719\n00:34\n\n\n1\n0.199426\n0.169573\n0.059540\n00:33\n\n\n2\n0.132157\n0.166686\n0.056834\n00:33\n\n\n\n\n\n\n\nResults Are In!\nThe training time increased slightly—by about 3 to 4 seconds—but here’s the exciting part: the error rate dropped from 7.3% to 5.6%!\nNow, those model names might seem a bit cryptic at first glance. Here’s a quick guide to help you decode them:\n\nNames like Tiny, Small, Large, etc.: These indicate the model’s size and resource requirements.\nfb_in22k: This means the model was trained on the ImageNet dataset with 22,000 image categories by Facebook AI Research (FAIR).\n\nIn general, ConvNeXT models tend to outperform others in accuracy for standard photographs of natural objects. In summary, we’ve seen how choosing the right architecture can make a significant difference by balancing speed, memory usage, and accuracy. Stay tuned as we dive even deeper into the intricacies of neural networks next!"
  },
  {
    "objectID": "posts/2024-07-28-from-neuron-to-gradient/index.html#whats-in-the-model",
    "href": "posts/2024-07-28-from-neuron-to-gradient/index.html#whats-in-the-model",
    "title": "Looking inside neural networks",
    "section": "What’s in the Model?",
    "text": "What’s in the Model?\nAlright, you see? Our model did better, right? Now, you’ve probably wondering, how do we turn this awesome piece of neural magic into an actual application? They key is to save the trained model so that users won’t have to wait for the training time.\nTo do that, we export our learner with the following command, creating a magical file called model.pkl:\n\nlearn.export('model.pkl')\n\nFor those of you who’ve followed my previous blog posts, you’ll recall that when I deploy an application on HuggingFace Spaces, I simply load the model.pkl file. This way, the learner functions almost identically to the trained learn object—and the best part is, you no longer have to wait forever!\nNow, you might be wondering, “What exactly did we do here? What’s inside this model.pkl file?”\n\nDissecting the model.pkl File\nLet’s take a closer look. The model.pkl file is essentially a saved learner, and it contains two main components:\n\nPre-processing Steps: These include all the procedures needed to transform your raw images into a format that the model can understand. In other words, it stores the information from your DataLoaders (dls), DataBlock, or any other pre-processing pipeline you’ve set up.\nThe Trained Model: This is the core component—a trained model that’s ready to make predictions.\n\nTo inspect its contents, we can load the model back up and examine it.\n\nm = learn.model\nm\n\n\n\nSequential(\n  (0): TimmBody(\n    (model): ConvNeXt(\n      (stem): Sequential(\n        (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n        (1): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n      )\n      (stages): Sequential(\n        (0): ConvNeXtStage(\n          (downsample): Identity()\n          (blocks): Sequential(\n            (0): ConvNeXtBlock(\n              (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n              (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=96, out_features=384, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (norm): Identity()\n                (fc2): Linear(in_features=384, out_features=96, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n            (1): ConvNeXtBlock(\n              (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n              (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=96, out_features=384, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (norm): Identity()\n                (fc2): Linear(in_features=384, out_features=96, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n            (2): ConvNeXtBlock(\n              (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n              (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=96, out_features=384, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (norm): Identity()\n                (fc2): Linear(in_features=384, out_features=96, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n          )\n        )\n        (1): ConvNeXtStage(\n          (downsample): Sequential(\n            (0): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n            (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n          )\n          (blocks): Sequential(\n            (0): ConvNeXtBlock(\n              (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n              (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=192, out_features=768, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (norm): Identity()\n                (fc2): Linear(in_features=768, out_features=192, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n            (1): ConvNeXtBlock(\n              (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n              (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=192, out_features=768, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (norm): Identity()\n                (fc2): Linear(in_features=768, out_features=192, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n            (2): ConvNeXtBlock(\n              (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n              (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=192, out_features=768, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (norm): Identity()\n                (fc2): Linear(in_features=768, out_features=192, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n          )\n        )\n        (2): ConvNeXtStage(\n          (downsample): Sequential(\n            (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n            (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n          )\n          (blocks): Sequential(\n            (0): ConvNeXtBlock(\n              (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n              (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (norm): Identity()\n                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n            (1): ConvNeXtBlock(\n              (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n              (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (norm): Identity()\n                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n            (2): ConvNeXtBlock(\n              (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n              (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (norm): Identity()\n                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n            (3): ConvNeXtBlock(\n              (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n              (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (norm): Identity()\n                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n            (4): ConvNeXtBlock(\n              (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n              (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (norm): Identity()\n                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n            (5): ConvNeXtBlock(\n              (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n              (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (norm): Identity()\n                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n            (6): ConvNeXtBlock(\n              (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n              (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (norm): Identity()\n                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n            (7): ConvNeXtBlock(\n              (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n              (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (norm): Identity()\n                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n            (8): ConvNeXtBlock(\n              (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n              (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (norm): Identity()\n                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n          )\n        )\n        (3): ConvNeXtStage(\n          (downsample): Sequential(\n            (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n            (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n          )\n          (blocks): Sequential(\n            (0): ConvNeXtBlock(\n              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (norm): Identity()\n                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n            (1): ConvNeXtBlock(\n              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (norm): Identity()\n                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n            (2): ConvNeXtBlock(\n              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (norm): Identity()\n                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n          )\n        )\n      )\n      (norm_pre): Identity()\n      (head): NormMlpClassifierHead(\n        (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n        (norm): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n        (flatten): Flatten(start_dim=1, end_dim=-1)\n        (pre_logits): Identity()\n        (drop): Dropout(p=0.0, inplace=False)\n        (fc): Identity()\n      )\n    )\n  )\n  (1): Sequential(\n    (0): AdaptiveConcatPool2d(\n      (ap): AdaptiveAvgPool2d(output_size=1)\n      (mp): AdaptiveMaxPool2d(output_size=1)\n    )\n    (1): fastai.layers.Flatten(full=False)\n    (2): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): Dropout(p=0.25, inplace=False)\n    (4): Linear(in_features=1536, out_features=512, bias=False)\n    (5): ReLU(inplace=True)\n    (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (7): Dropout(p=0.5, inplace=False)\n    (8): Linear(in_features=512, out_features=37, bias=False)\n  )\n)\n\n\n\n\nWhat’s All This Stuff?\nAlright, there’s a lot to digest here. Basically, the model is structured in layers upon layers. Here’s the breakdown:\nTimmBody: this contains most of the model architecture. Inside the TimmBody. You’ll find:\n\nModel: The main model components.\nStem: The initial layers that process the raw input.\nStages: There are further broken down into multiple blocks, each packed with convolutional layers. normalization layers, and more.\n\n\n\nLet’s Peek Inside a Layer\nTo dig deeper into what these layers contain, you can use a really convenient Pytorch method called get_submodule:\n\nl = m.get_submodule('0.model.stem.1')\nl\n\nLayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n\n\nAs you can see it return a LayerNorm2d layer. Wondering what this LayerNorm2d thing is all about? It comprises a mathematical function for normalization and bunch of parameters:\n\nprint(list(l.parameters()))\n\n\n\n[Parameter containing:\ntensor([ 1.2546e+00,  1.9191e+00,  1.2191e+00,  1.0385e+00, -3.7148e-04,\n         7.6571e-01,  8.8668e-01,  1.6324e+00,  7.0477e-01,  3.2892e+00,\n         7.8641e-01, -1.7453e-03,  1.0006e+00, -2.0514e-03,  3.2976e+00,\n        -1.2112e-03,  1.9842e+00,  1.0206e+00,  4.4522e+00,  2.5476e-01,\n         2.7248e+00,  9.2616e-01,  1.2374e+00,  4.3668e-03,  1.7875e+00,\n         5.4292e-01,  4.6268e+00,  1.1599e-02, -5.4437e-04,  3.4510e+00,\n         1.3520e+00,  4.1267e+00,  2.6876e+00,  4.1197e+00,  3.4007e+00,\n         8.5053e-01,  7.3569e-01,  3.9801e+00,  1.2851e+00,  6.3985e-01,\n         2.6897e+00,  1.1181e+00,  1.1699e+00,  5.5318e-01,  2.3341e+00,\n        -3.0504e-04,  9.7000e-01,  2.3409e-03,  1.1984e+00,  1.7897e+00,\n         4.0138e-01,  4.5116e-01,  9.7186e-01,  3.9881e+00,  6.5935e-01,\n         6.8778e-01,  9.8614e-01,  2.7053e+00,  1.2169e+00,  7.6268e-01,\n         3.3019e+00,  1.6200e+00,  9.5547e-01,  2.1216e+00,  6.2951e-01,\n         4.0349e+00,  8.9246e-01, -2.9147e-03,  4.0874e+00,  1.0639e+00,\n         1.3963e+00,  1.6683e+00,  4.6571e-04,  7.6833e-01,  8.8542e-01,\n         6.4305e-01,  1.3443e+00,  7.1566e-01,  5.4763e-01,  2.0902e+00,\n         1.1952e+00,  3.0668e-01,  2.9682e-01,  1.4709e+00,  4.0830e+00,\n        -7.8233e-04,  1.1455e+00,  3.8835e+00,  3.5997e+00,  4.8206e-01,\n         2.1703e-01, -1.6550e-04,  6.4791e-01,  3.0069e+00,  3.0463e+00,\n         4.6374e-03], device='cuda:0', requires_grad=True), Parameter containing:\ntensor([-9.8183e-02, -4.0191e-02,  4.1647e+00, -8.9313e-03,  3.7929e-03,\n        -2.7139e-02, -3.1174e-02, -7.9865e-02, -1.4053e-01, -6.3492e-02,\n         3.2160e-01, -3.3837e-01, -5.6851e-02, -4.0384e-03, -4.7630e-02,\n        -2.6376e-02, -4.0858e-02, -4.0886e-02,  8.7548e-03, -2.4149e-02,\n         8.5088e-03, -1.6333e-01, -4.0154e+00,  5.2989e-01, -5.3410e-01,\n         2.8046e+00,  3.5663e-02, -1.0321e-02, -1.1255e-03, -1.1721e-01,\n        -1.3768e-01,  1.8840e-02, -9.5614e-02, -1.3149e-01, -1.9291e-01,\n        -6.8939e-02, -3.6672e-02, -1.2902e-01,  1.5387e-01,  3.6398e-03,\n        -6.6185e-02,  5.8841e-02, -9.1987e-02, -1.1453e+00, -5.4502e-02,\n        -5.3649e-03, -1.8238e-01,  2.3167e-02,  3.8862e-02, -5.9394e-02,\n        -4.1380e-02, -5.6917e-02, -4.3903e-02, -1.2954e-02, -1.1092e-01,\n         7.0337e-03, -3.9300e-02, -1.5816e-01, -9.8132e-02, -1.8553e-01,\n        -1.1112e-01, -1.8186e-01, -3.4278e-02, -2.6474e-02,  1.4192e+00,\n        -3.1935e-02, -4.3245e-02, -2.7030e-01, -4.6695e-02, -6.4756e-04,\n         2.6561e-01,  1.8779e-01,  6.9716e-01, -3.0647e-01,  8.1973e-02,\n        -1.0845e+00,  1.4999e-02, -4.4244e-02, -8.0861e-02, -6.8972e-02,\n        -1.3070e-01, -1.7093e-02, -1.9623e-02, -3.9345e-02, -6.9878e-02,\n         1.2335e-02, -5.9947e-02, -3.5691e-02, -7.9831e-02, -7.4387e-02,\n        -9.5232e-03, -3.7763e-01, -1.1987e-02, -2.5113e-02, -6.2690e-02,\n        -3.0666e-04], device='cuda:0', requires_grad=True)]\n\n\nAnother example: Let’s inspect a layer deeper inside:\n\nl = m.get_submodule('0.model.stages.0.blocks.1.mlp.fc1')\nprint(l)\nprint(list(l.parameters()))\n\n\n\nLinear(in_features=96, out_features=384, bias=True)\n[Parameter containing:\ntensor([[ 0.0227, -0.0014,  0.0404,  ...,  0.0016, -0.0453,  0.0083],\n        [-0.1439,  0.0169,  0.0261,  ...,  0.0126, -0.1044,  0.0565],\n        [-0.0655, -0.0327,  0.0056,  ..., -0.0414,  0.0659, -0.0401],\n        ...,\n        [-0.0089,  0.0699,  0.0003,  ...,  0.0040,  0.0415, -0.0191],\n        [ 0.0019,  0.0321,  0.0297,  ..., -0.0299, -0.0304,  0.0555],\n        [ 0.1211, -0.0355, -0.0045,  ..., -0.0062,  0.0240, -0.0114]],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([-0.4049, -0.7419, -0.4234, -0.1651, -0.3027, -0.1899, -0.5534, -0.6270,\n        -0.3008, -0.4253, -0.5996, -0.4107, -0.2173, -1.7935, -0.3170, -0.1163,\n        -0.4483, -0.2847, -0.4343, -0.4945, -0.4064, -1.1403, -0.6754, -1.7236,\n        -0.2954, -0.2655, -0.2188, -0.3913, -0.4148, -0.4771,  0.2366, -0.7542,\n        -0.5851, -0.1821, -1.5273, -0.3625, -2.4688, -2.3461, -0.6110, -0.4114,\n        -0.6963, -0.5764, -0.5878, -0.0318, -2.0354, -0.2859, -0.3954, -0.8404,\n        -2.2399, -1.0874, -0.2296, -0.9002, -0.7585, -0.8834, -0.3753, -0.4548,\n        -0.3836, -0.4048, -2.0231, -1.0264, -0.4106, -1.1566, -0.2225, -0.4251,\n        -0.2496, -0.4224, -0.0975, -1.4017, -0.6887, -0.4370, -0.2931, -0.4643,\n        -0.4959, -1.2535, -1.0720, -1.2966, -0.6276, -1.4162, -2.3081, -2.4540,\n        -0.4258, -0.9987, -0.4638, -0.3147, -0.2417, -0.8744, -0.2828, -1.4208,\n        -0.3257, -0.3202, -0.0603, -0.1894, -0.2496, -0.6130, -0.2975, -2.1466,\n        -0.4129, -0.3677, -1.9813, -0.3814, -0.3785, -0.2294, -0.3698, -0.3256,\n        -0.5585, -2.4192, -0.4589, -1.7748, -0.3995, -0.4092, -0.3517, -0.5331,\n        -1.6535, -1.8190,  0.6264, -0.4059,  0.5873, -2.2074, -0.2438, -2.4539,\n        -0.2283, -0.6865,  0.6988,  0.6476, -0.6445, -0.3452, -0.3276, -0.5700,\n        -0.5173, -0.2775, -0.4089, -0.3020, -0.4872, -0.4952, -0.4072, -0.4356,\n        -0.5102, -0.4128, -2.0918, -0.2826, -0.5830, -1.5835,  0.6139, -0.8504,\n        -0.4669, -2.1358, -0.3418, -0.3767, -0.3345, -0.3960, -0.3886, -0.5667,\n        -0.2225, -1.3059, -0.4600, -0.3927, -0.4667, -0.4214, -0.4755, -0.2866,\n        -1.5805, -0.1787, -0.4367, -0.3172,  1.5731, -0.4046, -0.4838, -0.2576,\n        -0.5612, -0.4264, -0.2578, -0.3175, -0.4620, -1.9552, -1.9145, -0.3960,\n         0.3988, -2.3519, -0.9688, -0.2831, -1.9001, -0.4180,  0.0159, -1.1109,\n        -0.4921, -0.3177, -1.8909, -0.3101, -0.8136, -2.3345, -0.3845, -0.3847,\n        -0.1974, -0.4445, -1.6233, -2.5485, -0.3176, -1.2715, -1.1479,  0.6149,\n        -0.3748, -0.3949, -2.0747, -0.4657, -0.3780, -0.4957, -0.3282, -1.9219,\n        -2.0019, -0.5307, -0.2554, -1.1160, -0.3517, -2.2185, -1.1393,  0.5364,\n        -0.3217, -2.0389, -0.4655,  0.1850, -0.5830, -0.3128,  0.6180, -0.2125,\n        -2.3538, -0.9699, -0.9785, -0.3667, -0.4502, -1.9564, -0.2662, -1.1755,\n        -0.4198, -0.9024, -0.3605, -0.5172, -1.1879, -0.4190, -0.4770, -1.5560,\n        -0.4011, -0.6518, -0.4818, -0.2423,  0.6909, -0.5081, -0.4304, -0.6068,\n        -0.4000, -0.3329, -0.3596, -1.6108, -0.2371, -0.2467, -0.4545,  0.1807,\n        -0.3227, -0.3918, -0.3515, -0.3755, -1.2178, -0.3999, -0.3578, -0.2882,\n        -1.7483, -0.2363, -0.1599, -0.2640, -0.9769, -1.3065, -0.4148, -0.2663,\n        -0.3933, -0.4627, -0.2174,  0.2140, -0.5733, -0.2766, -0.3659, -0.5172,\n        -0.3484, -0.3362, -0.6445,  0.6866, -0.3738, -0.2902, -2.0863, -0.4882,\n        -0.2597, -1.0496, -1.6616, -0.3398, -0.5111, -0.5659, -0.3027, -0.5048,\n        -0.2877, -0.2841, -0.1982, -0.6910, -0.2873, -2.1121, -0.8927, -0.2301,\n        -1.5013, -0.4734, -2.2292, -0.4022, -0.2926, -0.4199,  0.6646, -0.3047,\n        -0.1688, -0.3749, -0.6433, -2.3348, -0.3101, -1.2730, -0.8193, -1.0593,\n        -0.0934, -1.6387,  0.3426, -0.8484, -0.4910, -0.5001, -1.0631, -0.3534,\n        -1.1564, -0.3842, -0.3172, -0.6432, -0.9083, -0.6567, -0.6490,  0.6337,\n        -0.2662, -1.3202, -1.1623, -1.2032, -2.0577, -0.3001, -1.3596, -0.4612,\n        -0.5024, -0.4950, -0.3156, -0.3272, -0.2669, -0.4279, -0.3296, -0.3011,\n        -1.6635,  0.6434, -0.9455,  0.6099, -0.4234,  0.3917, -0.4944, -0.4284,\n        -0.2587, -0.4952, -2.1991, -0.2601, -0.3934, -0.4565, -0.5816, -0.3487,\n        -0.7372, -0.3589, -0.4894, -2.0105,  0.4557, -0.8055, -1.7748, -0.3512,\n        -0.5359, -0.2101, -0.3955, -0.4782, -1.1457, -0.3974, -2.2115, -0.2838],\n       device='cuda:0', requires_grad=True)]\n\n\nWhat do these numbers mean, you ask? Essentially, they represent the learned parameters of the model— the weights that have been fine-tuned during training. These weights form the “secret sauce” that enables the model to distinguish between, say, a basset hound and a tabby cat.\nNext, we’ll dive into how neural networks function behind the scenes, exploring the mechanisms that transform these parameters into powerful predictions."
  },
  {
    "objectID": "posts/2024-07-28-from-neuron-to-gradient/index.html#how-neural-networks-really-work",
    "href": "posts/2024-07-28-from-neuron-to-gradient/index.html#how-neural-networks-really-work",
    "title": "Looking inside neural networks",
    "section": "How Neural Networks Really Work",
    "text": "How Neural Networks Really Work\nTo answer the burning question from before, let’s dive into the marvels of neural networks. Yes, Jeremy Howard has an amazing notebook called “How does a neural net really work?” that’s perfect for beginners. But, I’m here to give you a walkthrough with a dash of humor!\nMachine learning models are like very smart shape-fitting artists. They find pattern in data and learn to recognize them. We’ll start simple - with a quadratic function. Let’s see how it all works:\n\n\nCode\nimport plotly\nimport plotly.express as px\nimport torch\nimport numpy as np\nfrom IPython.display import display, HTML\n\n # Tomas Mazak's workaround for MathJax in VSCode\nplotly.offline.init_notebook_mode()\ndisplay(HTML(\n    '&lt;script type=\"text/javascript\" async src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_SVG\"&gt;&lt;/script&gt;'\n)) \ndef plot_function(f, title=None, min=-2.1, max=2.1):\n    x = torch.linspace(min, max, steps=100)\n    y = f(x)\n    return px.line(x=x, y=y, title=title)\n\n\n\ndef f(x): return 3 * x**2 + 2 * x + 1\nplot_function(f, title=r\"$3x^2 + 2x + 1$\")\n\n\nfunction f(x, a, b, c) { return a*x**2 + b*x + c }\nx = { return Array.from({ length: 40 }, (_, i) =&gt; -2 + (i * (2 - (-2)) / (40 - 1))); }\ny = x.map(element =&gt; f(element, a, b, c))\n{\n  var trace1 = { x: x, y: y, mode: 'lines', name: 'quadratic'};\n  var data = [trace1];\n  var layout = { title: \"3x²+ 2x + 1\", xaxis: { title: 'x', zeroline: false, }, yaxis: { title: \"y\", } };\n  const div = DOM.element('div');\n  Plotly.newPlot(div, data, layout);\n  return div;\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat we want to do here is straightforward: suppose we don’t know the exact mathematical function, and we’re trying to reconstruct it from some data. Here’s the actual function, and our goal is to approximate it using a variety of quadratic equations.\nCreating Quadratics on Demand\nIn Python, the partial function lets us fix certain parameters of a function to generate different variations. It’s like having a playlist of your favorite songs with the flexibility to change the lyrics whenever you want!\n\nfrom functools import partial\n\ndef quad(a, b, c, x): return a * x**2 + b * x + c\ndef mkquad(a, b, c): return partial(quad, a, b, c)\n\n\nIntroducing Noise\nIn real life, data never fits perfectly to a function. There’s always some noise, it’s often as messy and unpredictable as a doctor’s illegible handwriting. Let’s add some noise to our data:\n\ndef noise(x, scale): return np.random.normal(scale=scale, size=x.shape)\ndef add_noise(x, mult, add): return x * (1 + noise(x, mult)) + noise(x, add)\n\n\nnp.random.seed(42)\nx = torch.linspace(-2, 2, steps=40)\ny = add_noise(f(x), 0.15, 1.5)\npx.scatter(x=x, y=y)\n\n\ndata = FileAttachment(\"dataponts.csv\").csv()\nx_data = data.map(item =&gt; item.x);\ny_data = data.map(item =&gt; item.y);\n{\n  var trace1 = { x:  x_data , y:  y_data , mode: 'markers', name: 'data ponts'};\n  var data = [trace1];\n  var layout = { title:\"\", xaxis: { title: 'x', }, yaxis: { title:\"y\", } };\n  const div = DOM.element('div');\n  Plotly.newPlot(div, data, layout);\n  return div;\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis noisy data is inspired by the quadratic function but comes with a sprinkle of randomness.\nPlot Quadratics with Sliders: Interactive Fun\nEver played with sliders to adjust stuff? Here’s your chance to do the same with quadratics. You can tweak the coefficients a, b, and c to fit the noisy data manually.\n\nviewof a = Inputs.range([-1, 4], {label: \"a\", step: 0.1})\nviewof b = Inputs.range([-1, 4], {label: \"b\", step: 0.1})\nviewof c = Inputs.range([-1, 4], {label: \"c\", step: 0.1})\n{\n  var trace1 = { x: x, y: y, mode: 'lines', name: 'quadratic'};\n  var trace2 = { x:  x_data , y:  y_data , mode: 'markers', name: 'data ponts'};\n  var data = [trace1, trace2];\n  var layout = { title: `Interactive Quadratics`, xaxis: { title: 'x', zeroline: false, }, yaxis: { title: `${a}x² + ${b}x + ${c}`, } };\n  const div = DOM.element('div');\n  Plotly.newPlot(div, data, layout);\n  return div;\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBut who wants to be a human slider forever? We need a more scientific approach to measure how well our function fits the data. Enter loss functions - the unsung heroes of machine learning.\n\n\nMeet the Mean Squared Error (MSE)\nMSE stands for Mean Squared Error. It’s a way to measure how far off our predictions are from the actual values. Here’s how you define it:\n\ndef mse(preds, acts): return ((preds - acts)**2).mean()\n\nNow, let’s use MSE to evaluate our quadratics. This function will calculate the loss (how bad our predictions are) and give us a number we can use to improve our model.\n\nfunction mse(preds, acts) {\n  const squared_error = [];\n  for (let i=0; i &lt; preds.length; i++) {\n    const error = preds[i] - acts[i];\n    squared_error.push(error**2);\n  }\n  const mse = squared_error.reduce((acc, curr) =&gt; acc+curr, 0) / preds.length;\n  return mse;\n}\n_y = x.map(element =&gt; f(element, _a, _b, _c))\nviewof _a = Inputs.range([-1, 4], {label: \"a\", step: 0.1})\nviewof _b = Inputs.range([-1, 4], {label: \"b\", step: 0.1})\nviewof _c = Inputs.range([-1, 4], {label: \"c\", step: 0.1})\n{\n  const loss = mse(_y, y_data)\n  var trace1 = { x: x, y: _y, mode: 'lines', name: 'quadratic'};\n  var trace2 = { x:  x_data , y:  y_data , mode: 'markers', name: 'data ponts'};\n  var data = [trace1, trace2];\n  var layout = { title: `Loss: ${loss.toFixed(4)}`, xaxis: { title: 'x', zeroline: false, }, yaxis: { title: `${_a}x² + ${_b}x + ${_c}`, } };\n  const div = DOM.element('div');\n  Plotly.newPlot(div, data, layout);\n  return div;\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWith Mean Squared Error (MSE), you can objectively assess whether a model’s fit is improving without relying solely on visual inspection. Instead of manually adjusting parameters—which can be tedious and inefficient—we can automate the process using calculus.\n\n\nThe Power of Derivatives\nOne straightforward approach might be to manually tweak each parameter and observe how the loss, which quantifies the model’s prediction error, changes. However, there’s a far more efficient method: by computing the derivative of the loss function with respect to the parameters. These derivatives, also known as gradients, indicate the direction and rate at which the loss changes. This information is crucial for guiding the optimization process.\n\n\nLeveraging PyTorch\nFortunately, PyTorch automates the calculation of these derivatives, greatly simplifying the optimization process. For example, consider a function called quad_mse, which computes the Mean Squared Error between our observed noisy data and a quadratic model defined by parameters [a, b, c]. This function serves as a foundation for adjusting the model parameters in an informed and efficient way.\n\ndef quad_mse(params):\n    f = mkquad(*params)\n    return mse(f(x), y)\n\nThis function takes the coefficients (a, b, c), creates a quadratic function, and then returns the MSE of the predicted values against the actual noisy data.\n\nquad_mse([1.5, 1.5, 1.5])\n\ntensor(6.7798, dtype=torch.float64)\n\n\nWe get a MSE of 6.78, and yes, it’s a tenser (just a fancy array with some extra Pytorch powers). Let’s make it easier to hand:\n\nabc = torch.tensor([1.5, 1.5, 1.5])\nabc.requires_grad_()\n\ntensor([1.5000, 1.5000, 1.5000], requires_grad=True)\n\n\nNow, our tensor is ready to calculate gradients for these coefficients whenever used in computations. Pass this to quad_mse to verify:\n\nloss = quad_mse(abc)\nprint(loss)\n\ntensor(6.7798, dtype=torch.float64, grad_fn=&lt;MeanBackward0&gt;)\n\n\nAs expected, we get that magical tensor value 6.78. Nothing fancy yet? Hold on. We now tell Pytorch to store the gradients:\n\nloss.backward()\n\nNo fireworks, but something profound just happened. Run this:\n\nprint(abc.grad)\n\ntensor([-7.6934, -0.4701, -2.8031])\n\n\nVoila! You’ve got the gradients or slopes. They tell us how much the loss changes if you tweak each parameter-perfect for finding the optimal values.\n\n\nUpdating Parameters Using Gradients\nTo bring our loss down, we adjust the parameters in the direction that reduces the loss. Essentially, we descend down the gradient:\n\nwith torch.no_grad():\n    abc -= abc.grad * 0.01\n    loss = quad_mse(abc)\nprint(loss)\n\ntensor(6.1349, dtype=torch.float64)\n\n\nThis operation subtracts a small proportion of the gradient from each parameter, resulting in an updated set of parameters and a reduction of the loss from 6.78 to 6.13.\nNote that using the context manager with torch.no_grad() disables gradient computation for the weight and bias update step, as this update does not require gradient tracking.\n\n\nAutomating Gradient Descent\nInstead of performing updates manually, you can automate the process using a loop to handle multiple iterations of gradient descent.\n\nfor i in range(5):\n    loss = quad_mse(abc)\n    loss.backward()\n    with torch.no_grad():\n        abc -= abc.grad * 0.01\n        print(f\"Step {i}; loss: {loss:.2f} \")\n        abc.grad.zero_()  # Clear the gradient after each step\n\nStep 0; loss: 6.13 \nStep 1; loss: 5.05 \nStep 2; loss: 4.68 \nStep 3; loss: 4.37 \nStep 4; loss: 4.10 \n\n\n\nabc\n\ntensor([1.9329, 1.5305, 1.6502], requires_grad=True)\n\n\nAfter about five gradient descent iterations, the parameters have adjusted incrementally toward their optimal values. These parameters continuously update to minimize the loss and capture the underlying patterns in your data.\n\n\nWelcome to Optimization: The Role of Gradient Descent\nThe process of fine-tuning parameters to reduce prediction error is known as optimization, with gradient descent being one of the most widely used methods. Nearly all machine learning models—including complex neural networks—rely on some variant of this technique.\n\n\nThe Importance of ReLUs\nSimple quadratic functions are often insufficient for modeling real-world data, which tends to exhibit far greater complexity. When distinguishing subtle visual features in images, for example, a more sophisticated approach is required.\nThis is where the Rectified Linear Unit (ReLU) comes in. As an activation function, ReLU serves as a fundamental building block for constructing highly flexible models capable of capturing intricate patterns.\n\ndef rectified_linear(m, b, x):\n    y = m * x + b\n    return torch.clip(y, min=0.0)\n\nThis function is a simple line y = mx + b. The torch.clip() function takes anything blow zero and flatlines it at zero. Essentially, this turns any negative output into zero, while keeping positive values unchanged.\nHere’s what the ReLU looks like:\n\nplot_function(partial(rectified_linear, 1, 1))\n\n\nfunction rectified_linear(m, b, x) {\n  const y = m*x + b;\n  return Math.max(y, 0);\n}\nviewof m = Inputs.range([-1, 4], {label: \"m\", step: 0.1})\nviewof b_ = Inputs.range([-1, 4], {label: \"b\", step: 0.1})\n{\n  const _y = x.map(element =&gt; rectified_linear(m, b_, element));\n  var trace1 = {\n    x: x,\n    y: _y,\n    mode: 'lines',\n    name: 'ReLU',\n  };\n  var data = [trace1];\n  var layout = {\n    title: \"Rectified Linear Unit\",\n    xaxis: { title: \"x\", },\n    yaxis: { title: \"y\", range: [-1, 4] }\n  };\n  const div = DOM.element('div');\n  Plotly.newPlot(div, data, layout);\n  return div;\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImagine a line rising up at a 45-degree angle until it hits zero-at which point it surrenders to the great oblivion blow it. Now, you can adjust the coefficients m (slope) and b (intercept) and watch the magic happen.\n\n\nThe Power of Double ReLU: Fun With Functions\nWhy stop at one ReLU when you can have double the fun with two?\n\ndef double_relu(m1, b1, m2, b2, x):\n    return rectified_linear(m1, b1, x) + rectified_linear(m2, b2, x)\n\nThis function combines two ReLUs. Let’s plot this end see what unfolds:\n\nplot_function(partial(double_relu, 1, 1, -1, 1))\n\n\nfunction double_relu(m1,b1,m2,b2,x) {\n  return rectified_linear(m1,b1,x) + rectified_linear(m2,b2,x)\n}\nviewof m1 = Inputs.range([-2, 4], {label: \"m1\", step: 0.1})\nviewof b1 = Inputs.range([-2, 4], {label: \"b1\", step: 0.1})\nviewof m2 = Inputs.range([-2, 4], {label: \"m2\", step: 0.1})\nviewof b2 = Inputs.range([-2, 4], {label: \"b2\", step: 0.1})\n{\n  const _y = x.map(element =&gt; double_relu(m1,b1,m2,b2, element));\n  var trace1 = {\n    x: x,\n    y: _y,\n    mode: 'lines',\n    name: 'Double ReLU',\n  };\n  var data = [trace1];\n  var layout = {\n    title: \"Double Rectified Linear Unit\",\n    xaxis: { title: \"x\", },\n    yaxis: { title: \"y\", range: [-1, 4] }\n  };\n  const div = DOM.element('div');\n  Plotly.newPlot(div, data, layout);\n  return div;\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou’ll notice a downward slope that hooks upward into another slope. Tweak the coefficients m1, b1, m2, and b2, and watch the slopes and hooks dance around!\n\n\nInfinity Flexible ReLUs\nThink this is fun? Imagine adding a million ReLUs together. In face, you can add as many as you want to create function as wiggly and complex as you desire.\nBehold the power of ReLUs! With enough ReLUs, you can match any data pattern with incredible precision. you want a function that isn’t just 2D but spreads across multiply dimensions? You got it! ReLUs can do 3D, 4D, 5D…, nD.\n\n\nNeed Parameters? We’ve got Gradient Descent\nBut we need parameters to make magic happen, right? Here’s where gradient descent swoops in to save the day. By continuously tweaking these coefficients based on our loss calculations, we gradually descend towards the perfect parameter set.\n\n\nThe Big Picture: Adding ReLus and Gradient Descent === Deep Learning\nBelieve it or not, this is the essence of deep learning. Everything else-every other tweak is just about making this process faster and more efficient, sparking those “a-ha!” moments.\nQuoting Jeremy Howard:\n\n\n“Now I remember a few years ago when I said something like this in a class, somebody on the forum was like, this reminds me of that thing about how to draw an owl. Okay, step one: draw two circles; step two: daw the rest of the owl”\n\nThis explanation highlights the fundamental components of deep learning. At its core, deep learning involves leveraging activation functions like ReLUs, optimizing parameters through gradient descent, and processing data samples to generate predictions. Essentially, when you stack layers of ReLUs and systematically adjust parameters using gradient descent, the network learns to map inputs to outputs—much like an image of an owl being recognized by the computer.\nWhenever the concepts feel overwhelming, remember that the process boils down to these basics: using gradient descent to fine-tune parameters and combining numerous activation functions to capture complex patterns in your data.\nAnd that’s the essence of deep learning—simple building blocks working together to create sophisticated models. Stay curious and explore how each component contributes to the overall process."
  },
  {
    "objectID": "posts/history-of-computer-vision-and-deep-learning/index.html",
    "href": "posts/history-of-computer-vision-and-deep-learning/index.html",
    "title": "A brief history of computer vision and deep learning",
    "section": "",
    "text": "“A split scene: on one side, an old master painter (think Da Vinci) painting a realistic portrait, and on the other, a futuristic robot painting a digital image using code or neural networks as its brush strokes”, generated by DALL·E 3\nMy goal over the next ten weeks or so is to have a deep, foundational understanding of the principles and practices that are driving the state-of-the-art in visual intelligence. So to begin our journey, I find it useful to first situate what we will be studying within a broader intellectual landscape. We can start with the most encompassing field: Artificial Intelligence."
  },
  {
    "objectID": "posts/history-of-computer-vision-and-deep-learning/index.html#our-place-on-the-ai-map",
    "href": "posts/history-of-computer-vision-and-deep-learning/index.html#our-place-on-the-ai-map",
    "title": "A brief history of computer vision and deep learning",
    "section": "Our place on the AI map",
    "text": "Our place on the AI map\nAI is the grand, overarching ambition. It’s the quest to build machines that can perform tasks that have historically required human intelligence (tasks like reasoning, planning, and perception). It’s a field with a long and rich history, full of profound philosophical questions and formidable engineering challenges.\n\n\n\nWhere are we at? A broad view of the field of AI - Image inspired by Justin Johnson\n\n\nNow, AI is an enormous domain. Within it, we can delineate several major sub-disciplines. Two of the most significant are Machine Learning and Computer Vision. Machine learning is a specific approach to achieving AI. Instead of explicitly programming a machine with a set of handcrafted rules to solve a task, the machine learning paradigm is to develop algorithms that allow machine to learn the rules by itself, by analyzing data. This shift from rule-based system to data-driven system is a fundamental concept that we will return to again and again. Then we have Computer Vision. This is the scientific and engineering discipline dedicated to a different goal: enabling machines to see. That is, to take in visual information from the world, from images, from video and to derive understanding from it. These two fields have a significant and ever-growing intersection. While there exists a body of classical computer vision work that does not rely on machine learning, think of the techniques from computational geometry or signal processing but the most powerful and prevalent methods in modern computer vision are fully rooted in machine learning.\nNow let’s zoom in one level deeper. Within Machine Learning, a particular subfield has emerged over the last decade or so that has completely revolutionized the landscape. And that is Deep Learning. Deep learning is a specific class of machine learning algorithms. The defining characteristic is the use of neural networks with many layers, hence “deep” networks. These architectures, as we will go into great detail, have proven to be exceptionally effective at learning intricate patterns and hierarchical representations from vast amounts of data.\nThis brings us to the core focus of our discussion. The intersection of Deep Learning and Computer Vision. The red area on the diagram above is where we will spend our time. Our objective is to understand and implement deep learning architectures and methodologies that are purpose-built to solve computer vision problems. This convergence is responsible for nearly all of the dramatic breakthroughs in visual perception you may have seen in recent years.\nHowever, it’s crucial to understand that while our focus is on vision, deep learning is not exclusively a tool for computer vision. It is a general-purpose computational paradigm that has had a similar transformative impact on other fields of AI. For example, another major subfield is Natural Language Processing, or NLP, which deals with enabling computers to understand and generate human language. And a closely related field is Speech Recognition, which focuses on converting spoken language into text. Both NLP and Speech have been fundamentally reshaped by the application of deep learning models.\nWe can further expand our map to include fields like Robotics. Robotics is an inherently integrated discipline. A truly autonomous robot must perceive its environment (which is a core computer vision problem) and then decide how to act, which often evolves from experience(a machine learning problem). Therefore, robotics draws heavily from both computer vision and machine learning and increasingly, deep learning is the unifying methodology.\nMathematics, particularly linear algebra, probability, and calculus, provides the formal language and the core tools we use to define and optimize our models. Neuroscience and Psychology provide the biological inspiration for our network architectures and offer insights into the nature of intelligence itself. We also have Physics because we need to understand optics and image formation and how images are actually formed. We need to understand Biology and Psychology how the animal brain physically sees and processes visual information. And of course, all of this is built upon the substrate of Computer Science which gives us the algorithms, data structures, and high-performance computing systems necessary to make these computationally intensive ideas a reality.\nFinally, it’s imperative to recognize that none of these fields exists in a vacuum. They are built upon and draw inspiration from a wide array of fundamental scientific disciplines. So while we will live in that red intersection of deep learning and computer vision, I want you to maintain this broader perspective. The work we do here connects to a rich and interdisciplinary tapestry of human knowledge."
  },
  {
    "objectID": "posts/history-of-computer-vision-and-deep-learning/index.html#why-vision-from-first-eye-to-billion-cameras",
    "href": "posts/history-of-computer-vision-and-deep-learning/index.html#why-vision-from-first-eye-to-billion-cameras",
    "title": "A brief history of computer vision and deep learning",
    "section": "Why vision? From first eye to billion cameras",
    "text": "Why vision? From first eye to billion cameras\nAlright, so that gives you the sense of the intellectual landscape so let’s begin with the history. And to truly appreciate the motivation of our field i find it instructive to go back… quite a long way.\n\n\n\nEarly multicellular life and the dawn of vision. All images from Wikipedia / CC‑BY. Left: Burgess Shale trilobite fossil preserving antennae and legs. Top right: Dickinsonia costata, a quilted Ediacaran organism of unknown affinity. Bottom right: Artistic reconstruction of Opabinia, the five‑eyed Cambrian critter that helped ignite interest in the Cambrian explosion.\n\n\nRoughly 540 million years ago, our planet experienced a period of unprecedentedly rapid diversification of complex, multicellular life. This is known as the Cambrian Explosion. And a leading scientific hypothesis for what acted as the primary catalyst for this “big bang” of evolution… was the advent of vision.\nThe development of the first primitive eyes created an enormous new set of evolutionary pressures. For the very first time, organisms could actively hunt, evade predators, and navigate their environment with a richness of information that was previously unimaginable. In a very real sense the ability to see changed the rules of life on Earth, and may have been the driving force behind the development and much of the biological complexity we see today.\n\n\n\nOctopus camera-type eye, insect compound eye, chameleon turret eye, human binocular eye.\n\n\nAnd the legacy of that ancient innovation is all around us. Vision is a powerful example of convergent evolution. It has been independently invented by nature dozens of times across the tree of life. From the compound eyes of insects, which excel at detecting motion, to the incredibly sophisticated camera-like eyes of octopus, to the remarkable independently moving eyes of a chameleon… and of course, to our own visual system. The fact that evolution has arrived at the solution of “the eye” so many times underscores it profound utility as a mechanism for interacting with the world\nFor most of history vision was a purely biological phenomenon. But humanity has long been obsessed with capturing what we see, with creating an external record of our vision perception.\n\n\n\nThe Camera Obscura. Top right: first published picture of camera obscura, in Gemma Frisius’ 1545. Bottom right: Leonardo da Vinci, 16th Century AD. Left: camera obscura in Encyclopedia, 18th Century (all images from From Wikipedia, the free encyclopedia)\n\n\nThis quest leads us to one of the most foundational principles in the history of imaging: The Camera Obscura, which is Latin for “dark chamber”. As early as the 16th century, and with principles understood even in antiquity, scholars and artists recognized that if you have a darkened enclosure with a small aperture, an inverted image of the external scene is projected into the opposite wall. This is the fundamental principle upon which all photography and even modern cameras is built. It represents the first critical step in humanity’s attempt to externalize the scene of sight.\nNow, if we fast-forward from the simple pinhole in a dark room to the 21st century, the consequence of that is… staggering.\n\n\n\nComputer vision is now everywhere. First row, left to right: [1], [2], [3], [4]. Second row, left to right: [1], [2], [3], [4]. Third row, left to right: [1], [2], [3], [4]\n\n\nThe reason we have a field called computer vision today is, in large part, because the sensors of vision (cameras) are utterly ubiquitous. They are in our pocket, in our cars, in our homes, attached to drones, flying through the air, and even roving the surface of other planets.\nThe proliferation of inexpensive, high-resolution digital cameras has resulted in an unprecedented deluge of visual data. More images are now captured every two minutes than were captured in the entire 19th century. This vast sea of pixels is the raw material, the fuel, that powers the deep learning models we will talk about a lot.\nSo this brings us to a critical question. We have this deep, biological imperative for vision, and we have this modern technological reality of ubiquitous cameras generating near-infinite data. Given this perfect storm of motivation and raw material… how did the scientific engineering discipline of Computer Vision actually come to be? Where did we, as a field, come from?"
  },
  {
    "objectID": "posts/history-of-computer-vision-and-deep-learning/index.html#neuroscience-lights-the-way",
    "href": "posts/history-of-computer-vision-and-deep-learning/index.html#neuroscience-lights-the-way",
    "title": "A brief history of computer vision and deep learning",
    "section": "Neuroscience lights the way",
    "text": "Neuroscience lights the way\nThe story often begins not in computer science but in neuroscience. In 1959, two neuroscientists, David Hubel and Torsten Wiesel, conducted a series of now-famous experiments for which they would later win the Nobel Prize. They sought to understand the architecture of the mammalian visual system. They did this by inserting microelectrodes into the primary visual cortex—the first cortical area to receive input from the eyes of an anesthetized cat. They then presented the cat with very simple visual stimuli on a screen—things like bars of light, dots, or oriented edges.\n\n\n\nHubel & Wiesel’s classic 1959 cat–visual‑cortex experiment (image inspired by Justin Johnson)\n\n\nWhat they discovered was remarkable. They found that individual neurons in the brain region were not responding to complex concepts like “a mouse” or “a food bowl”. Rather, they were highly specialized feature detectors. They identified two principal classes of cells. First, simple cells. A given simple cell would fire vigorously as shown in the top response graph, only when a bar of light with a very specific orientation appeared at a very specific location in the visual field. If the orientation was wrong, or if the stimulus was just a dot, the neuron remained silent. Then they found complex cells. These cells also respond to oriented edges, but they were invariant to the precise location of that edge within their receptive field. As you can see on the diagram, the bar can move, or translate, and the complex cell continues to fire. Many were also tuned to the direction of motion.\nThis discovery was profoundly influential. It provided the first biological evidence for a hierarchical visual processing system. Where the initial stages are dedicated to detecting simple, local features like oriented edges. This idea of building up complex recognition from a hierarchy of simple feature detectors is a cornerstone of modern computer vision, and as we will see, it is the fundamental architectural principle behind convolutional neural networks.\nJust a few years later, inspired in part by this new understanding of biological vision, the field of computer vision had its genesis. Larry Robert’s 1963 PhD thesis MIT is widely considered to be the seminal work.\n\n\n\nRoberts’s 1963 “block world” vision pipeline (image from epicsysinc)\n\n\nHis system aimed to solve what seems like a simple problem: understanding the 3D geometry of simple “block world” scenes from a single 2D image. His approach was a pipeline. First take the original image. Second compute a “differentiated picture” which is a computational method for finding sharp changes in intensity in other words, an edge detector. This is a direct computational analog of what Hubel and Wiesel’s simple cell was doing. Finally, from this edge map, he would select feature points like corners and junctions and use geometric reasoning to infer the 3D shape. This was the start: a non-learning, rule-based system that decomposed vision into a series of explicit steps: find edges, find junctions, infer geometry.\nThis early success bred a great deal of optimism. So much so that in 1966, a group at MIT, led by Seymour Papert, proposed what is now famously known as “The Summer Vision Project”. The idea was, now we’ve got digital cameras, now they can detect edges, and Hubel and Wiesel told us how the brain works so basically what he wanted to do is hang a couple undergrads put them to work over the summer and after the summer we show it we should be able to construct a significant portion of visual system. The ambition was, in essence, to largely solve the problem of vision in a single summer by breaking it down into sub-problems. This, of course, turned out to be a profound underestimation of the problem’s difficulty. Now it’s clearly the computer vision was not solved and nearly 60 years later we’re still plugging away trying to achieve this what they thought they could do it in a summer with few undergrads. But it speaks to the excitement and perceived tractability of the field in its infancy.\nFollowing this period of excitement and subsequent realization of the problem’s true depth, the field entered a phase of more systematic, theoretical thinking. The most influential of this era was David Marr.\n\n\n\nStages of Visual Representation, David Marr, 1970s. Bottom right: Recognition via Parts (1970s)\n\n\nIn the 1970s, Marr proposed a comprehensive framework for how a visual system should be structured. He argued for a staged, bottom-up pipeline. You start with an input image just an array of pixel intensities. The first stage is to compute what he called the Primal Sketch. This is a representation of 2D image structure, identifying primitive elements like zero-crossing, edges, bars, and blobs. Again, you see the direct intellectual lineage from Hubel and Wiesel. From the Primal Sketch, the system would then compute the 2.5-D Sketch. This is a viewer-centric representation that captures local surface orientation and depth discontinuities. It’s not a full 3D model, but rather a map of how surfaces are angled relative to the observer. Finally from the 2.5-D Sketch, the system would construct a full, object-centered 3-D Model Representation, describing the shapes and their spatial arrangement in a way that is independent from the viewpoint. This framework was immensely influential and guided vision research for many years.\nMarr’s ideas spurred a great deal of research into how one might actually represent these 3D models. One popular idea from the 1970s was “Recognition via Parts”. One formulation of this was the idea of Generalized Cylinders proposed by Brooks and Binfold. The concept is to represent complex objects as a composition of simple, parameterized volumetric primitives like cylinders. A human figure can be modeled as an articulated collection of these cylinders. Another related idea was that of Pictorial Structure, from Fischler and Elshlager. Here, an object is represented as a collection of parts arranged in a deformable configuration, like nodes, connected by springs. This captures both the appearance of the parts and their plausible spatial relationships. Both of these are instantiations of the core idea that object recognition proceeds by identifying constituent parts and their arrangement.\n\n\n\nRecognition via Edge Detection (1980s) - Edge detection algorithms by John Canny, 1986\n\n\nThroughout the 1980s, much of the field’s energy was focused on perfecting the very first stage of Marr’s pipeline: edge detection. The thinking was that if we could just produce a perfect line drawing of the world from an image, as you see on the right, the subsequent steps of recognition would be much more tractable. This led to seminal work on edge detection algorithms, most famously by John Canny in 1986, whose algorithm is still baseline today, and also by David Lowe, whom we will encounter again later. The field became very good at turning images of things like those razors into learn edge maps.\nNow, zooming out to the broader context of artificial intelligence during this period… something important was happening. The field was entering what became known as an “AI winter”. The massive enthusiasm and, critically, the government funding for AI research began to dwindle. This was largely the dominant paradigm of the time, so-called “Expert Systems” which tried to encode human expertise in vast, handcrafted rule-bases had failed on their very grandiose promise. However, this didn’t mean that research stopped. Instead, the subfield of AI, like computer vision, NLP, and robotics, continued to mature. They grew into more distinct disciplines, focusing on their own specific problems and developing their own specialized techniques, often with less of the grand, unifying ambition of the early AI pioneers.\nBut in the meantime.. while this entire arc of “classical” computer vision was unfolding, from Hubel and Wiesel to Marr to edge detector… another set of ideas, also with roots in neuroscience and cognitive science, was developing in parallel. And it is this other thread of history that will ultimately lead us to the “deep learning” part."
  },
  {
    "objectID": "posts/history-of-computer-vision-and-deep-learning/index.html#learning-to-find-faces-in-a-crowd",
    "href": "posts/history-of-computer-vision-and-deep-learning/index.html#learning-to-find-faces-in-a-crowd",
    "title": "A brief history of computer vision and deep learning",
    "section": "Learning to find faces in a crowd",
    "text": "Learning to find faces in a crowd\nThroughout the 1970s and 80s, cognitive scientists were conducting experiments that revealed just how complex and sophisticated the human system truly is, often in ways that these early models couldn’t account for.\n\n\n\nIrving Biederman’s experiment in the early 1970s.\n\n\nOne such piece of work comes from Irving Biederman in the early 1970s. He represented subjects with images like the one you see on the left—a coherent, real-world scene. Unsurprisingly, people can recognize this scene and its constituent objects almost instantaneously. But then he would show them an image like the one on the right, which contains the exact image patches, but jumped into a non-sensical configuration. Recognition of the individual objects in this jumbled scene is significantly slower and more difficult. This simple but elegant experiment demonstrates a crucial point: our visual system doesn’t just recognize isolated parts. It relies heavily on the global context and the plausible spatial arrangement of those parts. The “whole” is more, and is processed differently than, the sum of its parts. This posed a significant challenge to a purely bottom-up, part-based recognition pipeline.\nAnother line of inquiry focused on the sheer speed of human vision. A common experimental paradigm used to study this is called Rapid Serial Visual Perception, or RSVP. The setup is simple: a subject fixates on a cross at the center of a screen, and images are flashed in very rapid succession often for only a few tens of milliseconds each.\n\n\n\n\n\n\nEEG signal corresponding to the brain’s responses\n\n\n\n\n\n\n\n\nIn 1996, Thorpe and colleagues used this RSVP paradigm in conjunction with electroencephalography, or EEG, which measures electrical activity in the brain with very high temporal resolution. They flashed images of animals and non-animals and asked subjects to perform a simple categorization task. What they found, as you can see on the plot, was outstanding. The EEG signal corresponds to the brain’s response to “animal” images, shown in darkest line, significantly diverged from the signal for “non-animal” images, shown in lightest line, at approximately 150 milliseconds after the image was presented. 150 milliseconds. To put that in perspective, a single blink of an eye takes about 300 to 400 milliseconds. This implies that the core computation underlying object recognition (from photons hitting the retina to a high-level semantic distinction) happens in a fraction of a blink. This is a critical insight that will strongly inform the design of the deep neural network we will talk about later.\nAnd where in the brain is this happening? The advent of functional Magnetic Resonance Imaging, or fMRI, in the 1990s allowed researchers to start answering this question. While fMRI has poor temporal resolution, it has good spatial resolution, allowing us to see what brain regions are active during a task. Seminal work by Nancy Kanwisher and her colleagues called “The fusiform face area: a cortical region specialized for the perception of faces” identified specific regions in human brain that show preferential activation for specific high-level categories. For instance they discovered a region in the fusiform gyrus, which they termed the Fusiform Face Area or FFA, that responds quickly to faces than to other objects like houses. Conversely, they found another region, the Parahippocampal Place Area or PPA, that shows opposite preference: it responds strongly to scenes like houses, but not to faces. This provided concrete evidence for semantic organization and specialization within the higher level of the visual cortex.\nSo taking stock of these findings from neuroscience and cognitive science, a clear picture emerges. Visual recognition is a fundamental, core competency of visual intelligence. And the biological solution to this problem is incredibly fast, it exploited global context, and it appears to culminate in specialized representations for semantically meaningful categories. This understanding began to shift the focus of the computer vision community itself.\n\n\n\nToward face recognition (~1997–2001). Top row: output of segmentation methods (normalized‑cuts in 1997) showing region grouping into visually coherent “blobs”. Middle of timeline: SIFT (1999) introduces keypoint detection and robust local feature description based on invariant orientation and scale matching. Bottom-right: Viola–Jones face detection framework (2001) applies a cascade of Haar‑like feature classifiers to group evidence into fast, reliable face detections in real time\n\n\nComing out of the AI winter and into the 1990s, the field began to move away from pure edge detection and towards tackling the recognition problem more directly. One prominent approach was what we called “Recognition via Grouping”. The idea here is that a critical step towards recognition is to segment the image into perceptually meaningful regions. A landmark algorithm in this era was Normalized Cuts developed by Jianbo Shi and Jitendra Malik in 1997. As you can see, it takes an input image and groups pixels into coherent segments, effectively partitioning the image into a foreground object and a background. The underlying principle is based on graph theory, finding a cut in the pixel graph that minimizes a particular normalized cost. The thinking was, if we can achieve a good segmentation, recognition of the isolated object becomes a much simpler problem.\nThen as we moved into the 2000s, another paradigm emerged that would become incredibly dominant: “Recognition via Matching”. The quintessential work here is David Lowe’s Scale-Invariant Feature Transform, or SIFT, from 1999. The core innovation of SIFT was a procedure to find a set of local, high distinctive keypoints in an image and to describe them in a way that is invariant to transformation like changes in scale, image rotation, and to some extent, illumination. Recognition then becomes a task of matching these keypoint descriptors between a query image, and a database of known objects. As you can see here, the algorithm can robustly find corresponding points to the stop sign, even though it’s viewed from a different angle and at a different scale. For about a decade, feature-based methods like SIFT were the state-of-the-art for many object recognition tasks.\nAnd right at the turn of the millennium, in 2001, we see a truly landmark achievement that pointed to the future. This was the face detector developed by Paul Viola and Michael Jones. This was one of the first truly robust and real-time objective detections. It was so effective that it was quickly incorporated into consumer digital cameras, enabling the auto-focus-on-faces feature that we now take for granted. What was so revolutionary about the Viola-Jones detector was that it was one of the most highly successful applications of machine learning to a core computer vision problem. Instead of a human engineer meticulously designing feature to find faces, their algorithm learned a cascade of very simple rectangular features using a machine learning algorithm called AdaBoost, trained on a large dataset of positive examples (faces) and negative examples (non-faces). This was a critical turning point. It demonstrated, in a practical and impactful way, the power of a data-driven, learning-based approach over purely hand-engineered systems. And it’s this learning-based philosophy that, when taken to its extreme, will lead us to the deep learning revolution."
  },
  {
    "objectID": "posts/history-of-computer-vision-and-deep-learning/index.html#the-rise-fall-and-return-of-neural-networks",
    "href": "posts/history-of-computer-vision-and-deep-learning/index.html#the-rise-fall-and-return-of-neural-networks",
    "title": "A brief history of computer vision and deep learning",
    "section": "The rise, fall, and return of Neural Networks",
    "text": "The rise, fall, and return of Neural Networks\nSo, we have now traced this timeline of computer vision up to the mid-2000s, We’ve seen the influence of neuroscience, the Marr paradigm, the focus on features like SIFT, and the nascent rise of machine learning. Now to understand what happens next, to understand the “deep learning” revolution we need to pause this timeline rewind all the way to the beginning, and pick up a completely different intellectual thread that was developing in parallel. This second thread also begins in the late 1950s, concurrent with Hubel and Wiesel’s discovery, in 1958, a psychologist named Frank Rosenblatt developed the Perceptron. The Perceptron was a simple computational model of a single biological neuron. It took a set of inputs, multiplied each by a corresponding weight, summed them up and if that sum exceeded a certain threshold it would output a “1”, otherwise “0”. It was simple, linear classifiers. And crucially, Rosenblatt devised a learning rule to automatically adjust the weights based on training examples.\nHowever, this early enthusiasm for Perceptrons was dealt a severe blow in 1969 with the publication of the book Perceptron by Marvin Minsky and Seymour Papert. In this highly influential critique, they rigorously analyzed the mathematical properties of the single-layer Perceptron. They famously showed that there are certain, seemingly simple functions that a Perceptron is fundamentally incapable of learning. The canonical example is the logical XOR function.\n\n\n\nThe perceptron’s inability to solve XOR and its critique by Minsky & Papert.\n\n\nAs you can see, the XOR function is true if one of its true inputs is true, but not both. If you plot the four possible input pairs, you find that you can not draw a single straight line to separate the ‘1’ outputs from ‘0’ outputs. Because the Perceptron is a linear classifier, it is mathematically impossible for it to solve this non-linearly problem. This critique was so powerful that it led to a significant decline in funding and research into neural networks, contributing to that first “AI winter” we discussed.\nDespite this, some research continued, and in 1980, Kunihiko Fukushima in Japan developed a model called Neocognitron in a paper “Neocognitron: A Self-organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position”. This is a truly remarkable piece of work, because it’s arguably the direct architectural ancestor of modern convolutional neural networks. The Neocognitron was explicitly and directly inspired by Hubel and Wiesel’s hierarchical of the visual cortex. It consisted of multiple layers, alternating between what Fukushima called S-cells and C-cells. The S-cells or simple cells, perform pattern matching using operations that are mathematically equivalent to what we now call convolution. The C-cells or complex cells, then provided spatial invariance by performing an operation analogous to what we now call pooling or subsampling. This is the fundamental architectural motif of a modern ConvNet. However, the Neocognitron had a critical limitation: it lacked a principled, end-to-end training algorithm. It was largely trained layer-by-layer with an unsupervised learning rule, and much of it was still hand-designed.\nThe missing piece of the puzzle arrived in 1986. In a landmark paper “Learning representations by back-propagating errors”, David Rumelhart, Geoffrey Hinton, and Ronald Williams popularized the backpropagation algorithm. Backpropagation is, in essence, an efficient method for computing the gradient of a loss function with respect to the weights of a multi-layered neural network. It’s a clever application of the chain rule from calculus. This algorithm provided the key that Minsky and Papert had pointed out was missing: a way to assign credit, or blame, to each neuron in each network, allowing one to systematically adjust the weights to improve performance. For the first time, it was possible to successfully train perceptrons with multiple layers, enabling them to learn non-linear function like XOR.\n\n\n\nTop: Lenet, applied backprop algorithm to a Neocognitron-like architecture, learned to recognize handwritten digits. Bottom: Unsupervised pre-training technique by Hinton, Bengio, and others\n\n\nNow, we see the synthesis, in 1998, Yann LeCun and his colleagues took the Neocognitron architecture (with its alternating layers of convolution and pooling) and applied the backpropagation algorithm to train it from end-to-end on a real world task: recognizing handwritten digits. The resulting model, known as LeNet-5, was a tremendous success. It archived state-of-the-art performance and was deployed commercially by AT&T to read handwritten checks. If you look at this architecture diagram, it is strikingly similar to the convolutional neural networks we use today. This was a powerful proof of concept, demonstrating that these neurally-inspired, trained architectures could solve real, practical problems.\nThis success spurred a small dedicated community of researchers throughout the 2000s to explore what was then beginning to be called “Deep Learning”. The central idea was to build networks that deeper and deeper, with the hypothesis that more layers would allow the learning of more complex and hierarchical features. However, this was not yet a mainstream topic. Training these very deep networks proved to be extremely difficult due to the optimization challenges like the vanishing gradient problem. Researchers like Hinton, Bengio, and others developed clever techniques, like the unsupervised pre-training shown here, to try to initialize these deep networks in a better way before fine-tuning them with backpropagation."
  },
  {
    "objectID": "posts/history-of-computer-vision-and-deep-learning/index.html#the-dataset-that-changed-everything",
    "href": "posts/history-of-computer-vision-and-deep-learning/index.html#the-dataset-that-changed-everything",
    "title": "A brief history of computer vision and deep learning",
    "section": "The dataset that changed everything",
    "text": "The dataset that changed everything\nAlright. So, the Viola-Jones face detector in 2001 gave us a powerful glimpse into the future, showing what was possible when you replaced hand-engineered rules with data-driven machine learning. This trend toward learning-based approaches and the need to rigorously evaluate them, led to another critical development in the field.\n\n\n\nLeft: The Caltech 101 images. Right: PASCAL Visual Object Challenge\n\n\nAnd that was the creation of standardized, large-scale benchmark datasets. Before the 2000s, it was common for researcher to test their algorithms on their own private, often small, collections of images. This made a direct, quantitative comparison of different methods exceptionally difficult. The establishment of datasets like Caltech101 in 2004, and later the PASCAL Visual Object Challenge, which ran from 2005 to 2012, was a major step in transforming computer vision into a more rigorous empirical science. PASCAL was particularly influential because it went beyond simple image classification. It challenged algorithms to perform more complex tasks like object detection drawing a bounding box around an object and semantic segmentation. These shared benchmarks created a common ground, a competitive arena, where the entire community could measure progress.\nStill, deep learning remained something of a niche topic within a broader machine learning and computer vision community. And there was a fundamental reason for this. Even with these new algorithm tricks, these deep high-capacity models were incredibly data-hungry. They require vast amounts of labeled data to learn meaningful representations and to avoid overfitting. And in the mid-2000s, there was simply no good dataset to work on. The existing benchmarks, like Caltech101, were orders of magnitude too small to truly unlock the potential of these models. The algorithms were simply ahead of the data. And that brings us to the final, critical ingredient that would ignite the deep learning explosion. The ImageNet dataset.\n\n\n\nImageNet Large Scale Visual Recognition Challenge\n\n\nConceived and led by Fei-Fei Li, starting in 2007, the ImageNet project was an effort of unprecedented scale. The goal is to map out the entire noun hierarchy of WorldNet and populate it with millions of clean, annotated images. The result was a dataset with over 14 million images, spanning more than 20,000 categories. Crucially, in 2010, the project launched the ImageNet Large Scale Visual Recognition Challenge, or ILSVRC. This competition focused on a subset of the data: 1,000 object classes, with roughly 1.3 million training images. The task was straightforward: given an image, produce a list of five object labels, and you get credit if the correct label is in your list. This dataset and this annual challenge provide a perfect crucible. It was a dataset massive and complex enough to finally demonstrate the power of data-hungry deep learning models, and a competition that would pit them directly against the state-of-the-art classical computer vision system of the day. The stage was now set for a revolution.\nSo, ImageNet sets the stage. Now let’s look at the performance on this challenge over the years. The bar chart above shows the top-5 error rate. That means the model gets five guesses, and if the correct label isn’t in those five, it’s an error. In 2010, the winning entry from Lin et al. had an error of 28.2%. In 2011, Sanchez & Perronnin improved this to 25.8%. These were typically system based on more traditional computer vision pipelines(hand-crafted features like SIFT or HoG), followed by machine learning classifiers like SVMs. Good progress, but still very high error rate. Then look at 2012. A massive drop to 16.4% with Krizhevsky et al.’s model, which we now famously know as AlexNet. We’ll talk a lot about AlexNet. The trend continues, 2013, Zeiler & Fergus: 11.7%, 2014, we see two big ones: VGG (Simonyan & Zisserman) at 7.3% and GoogLeNet (Szegedy et al.) at 6.7%. And then, a really significant milestone in 2015: ResNet (He et al.) achieved 3.6% error. Now, why is that 3.6% so significant? Look over the far right. Andrej Karpathy, when he was a PhD at Stanford and several others including Fei-Fei, did a study (Russakovsky et al. IJCV 2015) to benchmark human performance on the subset of ImageNet. And a well-trained human annotator gets around 5.1% top-5 error. So, by 2015, deep learning models were, for the first time, surpassing human-level performance on this specific, very challenging task! The progress didn’t stop there, 2016, 2017 saw even lower error rates with models like SENet\nNow, let’s zero in on that pivotal moment, AlexNet, 2012. You see the red arrow pointing squarely at that 2012 bar. That 28% down to 16% was not an incremental improvement; it was a paradigm shift. This was the moment deep learning, specifically deep convolutional neural networks, truly announced its arrival and demonstrated its power to the broader computer vision community. AlexNet in 2012 right after Deep learning(2016) and ImageNet(2009), this isn’t a coincidence. The availability of a large dataset like ImageNet, coupled with the increasing computational power of GPUs, allowed deep learning models, which had been around conceptually for a while(you see LeNet from ’98, Neocognitron from ’80) , to finally be trained effectively at scale. AlexNet’s success fundamentally changed the direction of computer vision research. Almost overnight, people shifted from feature engineering to learning features directly from data using deep neural networks. And the rest, as they say, is history, as subsequent years on that chart. So, ImageNet provided the challenge, and AlexNet provided the breakthrough deep learning solution. The combination really superchanged the field, and it’s why we’re here talking about these powerful models."
  },
  {
    "objectID": "posts/history-of-computer-vision-and-deep-learning/index.html#a-revolution-in-pixels",
    "href": "posts/history-of-computer-vision-and-deep-learning/index.html#a-revolution-in-pixels",
    "title": "A brief history of computer vision and deep learning",
    "section": "A revolution in pixels",
    "text": "A revolution in pixels\nOkay, we’ve seen how AlexNet in 2012 was a watershed moment for deep learning in computer vision, dramatically improving performance on ImageNet. Now let’s look at what happened after 2012\n\n\n\nLeft: Publications at top Computer Vision conferences. Right: arXiv papers per month\n\n\nThe graph on the left shows the number of paper submissions and acceptances to CVPR, which is one of the, if not the, top computer vision conferences. You can see a steady growth from 1985 up to around 2010-2012. But then, look what happens after 2012, especially the submissions. It just takes off, almost exponentially! We’re talking about going from around 2000, submissions to over 7000-8000 in just a few years. Now, the graph on the right shows the number of Machine Learning and AI papers uploaded to arXiv per month. arXiv, for those who don’t know, is a preprint server where researchers can upload their papers before or alongside peer review. This allows for very rapid dissemination of ideas. Again, we have a relative modest until around 2012-2013, and then it just skyrockets. We’re looking at thousands of ML/AI paper per month now. This isn’t just computer vision; it’s the broader AI field, but computer vision and deep learning are huge drivers of this trend.\nSo, we have an explosion of papers and research. But what kind of research? What were people working on? Let’s look at the winner of the ImageNet challenge each year following AlexNet:\n\nYear 2010(NEC-UIUC): Before the deep learning craze really hit ImageNet, this was what a state-of-the-art system looked like. You had a ‘Dense descriptor grid’ using features like HOG and LBP, then some ‘Coding’ (like local coordinate coding), ‘Pooling’ (Spatial Pyramid Matching - SPM), and finally a ‘Linear SVM’ for classification. This is a classic, handcrafted feature pipeline.\nYear 2012 (SuperVision, aka AlexNet): We’ve talked about this, Krizhevsky, Sutskever, and Hinton. It’s a stack of layers(convolutions, pooling, fully connected layers). This is a deep convolutional neural network, learning features directly from data.\nYear 2014 (GoogLeNet and VGG): Two years later, and we see even more sophisticated architectures.\n\nGoogleNet (from Google, Szegedy et al.) the idea was to have filters of different sizes operating in parallel. It was also very deep but computationally quite efficient.\nVGG (from Oxford, Simonyan & Zisserman) took a different approach: very simple, uniform architecture, just stacking 3x2 convolutions and 2x2 pooling layers deeper and deeper.\n\nYear 2015 (MSRA, aka ResNet): This was another huge leap, from Microsoft Research Asia(He et al.). This is ResNet, or Residual Network. They introduced ‘skip connections’ or ‘residual connections’ which allowed them to train networks that were incredibly deep, even over 100 or 1000 layers, which was previously impossible due to vanishing gradient problems. This architecture, or variants of it, became the backbone for many, many subsequent models.\n\nSo, in just a few years, we went from handcrafted pipelines to relatively shallow (by today’s standards) CNNs, to very deep and complex architectures, each pushing the boundaries of performance and what we thought was possible.\nNow let’s look at what these models can actually do.\n\n\n\nDeep learning is now everywhere\n\n\nOn the far left, we have examples of Image Classification from AlexNet back in 2012. For each image, the model outputs a list of probabilities for different classes, and here we see the top predictions. These aren’t just simple ‘cat’ or ‘dog’ classifications; the model is identifying specific types of objects, often in challenging, cluttered scenes. And these are real images, not just sanitized datasets. This was a clear demonstration of the power of these learned features. In the middle, we see an application called Image Retrieval. The idea here is: given a query image, can the system find visually and semantically similar images from a large database? These are just two fundamental computer vision tasks, classification and retrieval. But the success of deep learning, starting around 2012, has meant it’s now being applied to virtually every area of computer vision: object detection, segmentation, image captioning, image generation, video analysis, 3D reconstruction and so much more.\nContinuing with the theme of understanding humans and dynamic scenes at the bottom, we have Pose Recognition also known as human pose estimation. The goal here is to identify the key joints of a person’s body like elbows, wrists, knees, ankles, head, shoulders. You can see in these examples (from Toshev and Szegedy, 2014, “DeepPose”, one of the first deep learning approaches for this) that the model can accurately locate these joints even with varied clothing, complex poses, and different backgrounds. This is fundamental for a deeper understanding of human actions, for animation, and augmented reality, and more.\nAnd the reach of deep learning extends far beyond everyday scenes, videos, or games. It’s making significant impacts in highly specialized scientific and medical domains. On the far right, we have Whale Recognition. This might seem niche, but it’s important for ecological studies and conservation This particular image refers to a Kaggle challenge here is the link to the competition page, where participants build models to automatically identify individual whales from photograph. Deep learning is very good at these kinds of fine-grained visual recognition tasks\n\n\n\nTop left: Image Captioning Vinyals et al, 2015 Karpathy and Fei-Fei, 2015. Top left: Krishna et al., ECCV 2016 the “Visual Genome” dataset and the work on generating scene graphs. Bottom: The Neural Style Transfer Algorithm (Gatys et al. 2016), which stylizes a photograph in the style of a given artwork\n\n\nNow, this is where things get really interesting. We’re moving beyond just recognizing objects or pixels, and into the realm of understanding and describing images using natural language. This is Image Captioning. The task is, given an image, to automatically generate a human-like sentence that describes what’s happening in the image. These captions are remarkably accurate and fluent. This typically involves a combination of a Convolutional Neural Network (RNN), often an LSTM, to ‘generate’ the sentence word by word, conditioned on those visual features. The work here is from Vinyals et al. (from Google) and Karpathy and Fei-Fei (from Stanford), both published around 2015, were seminal works in this area, showing how to effectively combine CNNs and RNNs for this task. This was huge step toward machines that can not only see but also communicate what they see.\nImage captioning gives us a sentence. But can we get a even deeper understanding of the relationships and interactions within an image? On the right you see an image with objects detected, and blow it we see something more structured: a scene graph. This moves us towards a much more comprehensive understanding of visual scenes. The work from Krishna et al. ECCV 2016, refers to the “Visual Genome” dataset and the work on generating a scene graph, which provides a dense, structured annotation of images, capturing objects, attributes, and relationships. This is crucial for tasks like visual question answering, where the model answers questions about an image and more complex reasoning about visual content.\nSo far, we’ve mostly seen deep learning used for understanding or analyzing images. But what about creating them? Or manipulating the artistic ways? At the bottom we see something called Neural Style Transfer pioneered by Gatys et al. in 2016. Here, you take two images, a content image here is the houses on the street and a style image like a famous painting like Van Gogh’s The Starry Night. The algorithm then synthesizes a new image that has the content of the first image but is rendered in the style of the second. So you get the houses looking as if they were painted by Van Gogh, or in a stained-glass style. This is done by optimizing an image to match content features from one image and style features (correlations between activations in different layers) from another, using a pre-trained CNN.\nContinuing with generative models, we’ve shown artistic generation. But what about generating entirely new, photorealistic images from scratch? And this points to that capability, especially referencing Generative Adversarial Networks or GANs.\n\n\n\nSliced Wasserstein distance (SWD) between the generated and training images (Section 5) and multi-scale structural similarity (MS-SSIM) among the generated images for several training setups at 128 × 128. And (a-g) CELEBA examples corresponding to rows in Table 1. (h) the converged result\n\n\nThis is from Karras et al., for their work on “Progressive Growing of GANs for improved Quality” GANs, introduced by Ian Goodfellow and his colleagues in 2014, work by having two networks compete against each other. A Generator network tries to create realistic images for example from random noise. And a Discriminator network tried to distinguish between real images (from training set) and images created by the generator. Through this adversarial process, the generator gets better and better at creating images that can fool the discriminator, and the discriminator gets better at telling them apart. The “Progressive Growing of GANs” technique, developed by Karras and his team at NVIDIA was a major breakthrough. It allowed for the generation of much higher-resolution and more stable results than previously possible. They started by generating very small images like 4x4 pixels and then progressively added layers to both the generator and discriminator to produce larger and more detailed images like 8x8, 16x16, all the way up to 1024x1024. You can see from the image above, faces that are not real people, yet they look entirely plausible. This ability to synthesize photorealistic imagery has huge implications for art, design, entertainment, data augmentation, and of course, also raises important ethical considerations about ‘deepfakes’ and misinformation. But these generative capabilities truly underscore how far deep learning has come since 2012, from classifying images to creating entirely new visual realities.\nWe’ve seen some incredible generative capabilities, like GANs creating photorealistic faces. But what if we could guide that generation with more than just random noise or style images? What if we could tell the model exactly what we want it to create, using natural language?\n\n\n\nRamesh et al, “DALL·E: Creating Images from Text”, 2021, images from https://openai.com/blog/dall-e/\n\n\nThis brings us to one of the most mind-blowing developments in recent years: Text-to-image Generation. These are not images found on the internet these were created by an AI model based purely on that text description. And they are remarkably good! You see various interpretations, some look more like a cut avocado half turned into a chair, others are more abstract but clearly evoke both “armchair”and ‘avocado’. What’s so powerful about this (and models like DALL-E, Imagen, Stable Diffusion, etc.) is the compositionality and zero-shot generalization. The model has likely never seen an “armchair in the shape of an avocado” during its training. But it knows what armchairs are, it knows what avocados are, and it understands how to combine these concepts based on the textual relationships. The images above are from Ramesh et al, 2021, for DALL-E, a groundbreaking model from OpenAI. This kind of model is typically a very large transformer-based architecture, trained on massive datasets of image-text pairs. It learns to associate visual concepts with textual descriptions and can then generate novel images by combining these learned concepts in new ways. This ability to translate complex, even whimsical, textual prompts into coherent and creative visual outputs is a huge leap.\nThis isn’t just about fun images. It has profound implications for creative industries, design, content creation, and even help us understand how these large models represent and manipulate concepts. We’ve gone from classifying what’s in an image to generating entirely new visual realities from abstract textual descriptions. It’s truly an exciting time for AI and vision."
  },
  {
    "objectID": "posts/history-of-computer-vision-and-deep-learning/index.html#the-spark-and-the-fuel",
    "href": "posts/history-of-computer-vision-and-deep-learning/index.html#the-spark-and-the-fuel",
    "title": "A brief history of computer vision and deep learning",
    "section": "The spark and the fuel",
    "text": "The spark and the fuel\nSo, we’ve spent a lot of time looking at this incredible explosion of deep learning applications from 2012 to the present. We’ve seen progress in classification, detection, segmentation, captioning, generation, and so much more. A natural question arises: Why now? What were the key ingredients that came together to make this revolution possible?\n\nThere are three main reasons. The Computation, Deep learning models, especially the large ones we’ve been discussing, are incredibly computationally intensive to train. They require billions or even trillions of calculations. The Algorithms, while many core ideas of neural networks have been around for decades, there have been significant algorithm innovations. These include new architectures like ResNet, Transformers, better optimization techniques, new activation functions, regularization methods, and so on, which have made it possible to train much deeper and more complex models effectively. And finally the Data, datasets like ImageNet, which we discussed, were crucial. Deep learning models are data-hungry, they learn by seeing millions of examples. The internet, social media, and large-scale data collection efforts have provided this fuel.\n\n\n\nthe GFLOP per Dollar graph. ‘GFLOP’ stands for Giga Floating Point Operations Per Second. It’s a measure of computational power (how many billions of calculations a processor can do in one second). The graph shows this metric per dollar, so it’s a measure of cost-effectiveness\n\n\nLet’s focus on the Computation aspect. Look at the dramatic difference between CPUs and GPUs starting around 2007-2008 with GPUs like the GeForce 8800 GTX which was one of the first to support general-purpose computing via CUDA. Around 2010-2012 we had GeForce GTX 580, this is the era when AlexNet was developed. Alex Krizhevsky trained AlexNet on NVIDIA GPUs, and their parallel processing capabilities were absolutely critical for training such a large network in a reasonable amount of time. Then we have the so-called “Deep learning Explosion” starting around 2012-2013, precisely when GPU performance and accessibility were taking off. Later GPUs like GTX 1080 Ti, RTX 2080 Ti, RTX 3090, and RTX 3080 continued this trend, offering massive parallel computation at increasingly better price points (or at least, significantly more power for a high-end card).\nBut the story doesn’t end there with the arrival of GPU (Tensor Core) which is a special hardware for deep learning. Starting with NVIDIA’s Volta architectures, GPUs began to include dedicated hardware nits specifically designed to accelerate the types of matrix multiplication and accumulation operations that are the heart of deep learning computations. These Tensor Cores can perform mixed-precision matrix math (e.g., multiplying FP16 matrices and accumulating in FP32) much, much faster than general-purpose FP32 units.\nThis is a fantastic example of a positive feedback loop:\n\nDeep learning shows promise.\nResearchers start using GPUs for their parallel processing capabilities.\nThe demand for deep learning computation grows.\nHardware manufacturers (like NVIDIA) see this massive market and start designing specialized hardware units like Tensor Cores to further accelerate deep learning workloads.\nThis new, even more powerful hardware enables researchers to train even larger, more complex models, pushing the boundaries of AI further.\n\nSo it’s not just that the GPU happened to be good for deep learning; the hardware itself has evolved because of deep learning, making it even more powerful and efficient for these tasks. This co-evolution of algorithms, software, and hardware is a key characteristic of the current AI boom.\nNow let’s zoom out and look at the broader AI’s explosive growth and impact. This isn’t just an academic phenomenon, it’s having a massive real-world impact.\n\ndata = FileAttachment(\"./data/attendance-major-artificial-intelligence-conferences.csv\").csv()\n\n// Group data by conference\naaai_data = data.filter(d =&gt; d.Entity === \"AAAI\")\ncvpr_data = data.filter(d =&gt; d.Entity === \"CVPR\")\niclr_data = data.filter(d =&gt; d.Entity === \"ICLR\")\nicml_data = data.filter(d =&gt; d.Entity === \"ICML\")\nneurips_data = data.filter(d =&gt; d.Entity === \"NeurIPS\")\ntotal_data = data.filter(d =&gt; d.Entity === \"Total\")\n\n// Create the Plotly chart\nPlotly = require(\"plotly.js-dist@2\")\n\nchart = {\n  const traces = [\n    {\n      x: aaai_data.map(d =&gt; d.Year),\n      y: aaai_data.map(d =&gt; d[\"Number of attendees\"]),\n      type: 'scatter',\n      mode: 'lines+markers',\n      name: 'AAAI',\n      line: { color: '#9467bd', width: 3 },\n      marker: { size: 8 },\n      hovertemplate: '%{fullData.name}: %{y:,}&lt;extra&gt;&lt;/extra&gt;'\n    },\n    {\n      x: cvpr_data.map(d =&gt; d.Year),\n      y: cvpr_data.map(d =&gt; d[\"Number of attendees\"]),\n      type: 'scatter',\n      mode: 'lines+markers',\n      name: 'CVPR',\n      line: { color: '#1f77b4', width: 3 },\n      marker: { size: 8 },\n      hovertemplate: '%{fullData.name}: %{y:,}&lt;extra&gt;&lt;/extra&gt;'\n    },\n    {\n      x: iclr_data.map(d =&gt; d.Year),\n      y: iclr_data.map(d =&gt; d[\"Number of attendees\"]),\n      type: 'scatter',\n      mode: 'lines+markers',\n      name: 'ICLR',\n      line: { color: '#ff7f0e', width: 3 },\n      marker: { size: 8 },\n      hovertemplate: '%{fullData.name}: %{y:,}&lt;extra&gt;&lt;/extra&gt;'\n    },\n    {\n      x: icml_data.map(d =&gt; d.Year),\n      y: icml_data.map(d =&gt; d[\"Number of attendees\"]),\n      type: 'scatter',\n      mode: 'lines+markers',\n      name: 'ICML',\n      line: { color: '#8c564b', width: 3 },\n      marker: { size: 8 },\n      hovertemplate: '%{fullData.name}: %{y:,}&lt;extra&gt;&lt;/extra&gt;'\n    },\n    {\n      x: neurips_data.map(d =&gt; d.Year),\n      y: neurips_data.map(d =&gt; d[\"Number of attendees\"]),\n      type: 'scatter',\n      mode: 'lines+markers',\n      name: 'NeurIPS',\n      line: { color: '#2ca02c', width: 3 },\n      marker: { size: 8 },\n      hovertemplate: '%{fullData.name}: %{y:,}&lt;extra&gt;&lt;/extra&gt;'\n    },\n    {\n      x: total_data.map(d =&gt; d.Year),\n      y: total_data.map(d =&gt; d[\"Number of attendees\"]),\n      type: 'scatter',\n      mode: 'lines+markers',\n      name: 'Total',\n      line: { color: '#d62728', width: 3, dash: 'dash' },\n      marker: { size: 8 },\n      hovertemplate: '%{fullData.name}: %{y:,}&lt;extra&gt;&lt;/extra&gt;'\n    }\n  ];\n\n  const layout = {\n    title: {\n      text: 'Conference Attendance Over Time',\n      font: { size: 20 }\n    },\n    xaxis: {\n      title: 'Year',\n      showgrid: true,\n      gridcolor: 'rgba(0,0,0,0.1)'\n    },\n    yaxis: {\n      title: 'Number of Attendees',\n      showgrid: true,\n      gridcolor: 'rgba(0,0,0,0.1)',\n      tickformat: ',d'\n    },\n    legend: {\n      x: 0.02,\n      y: 0.98,\n      bgcolor: 'rgba(255,255,255,0.8)',\n      bordercolor: 'rgba(0,0,0,0.2)',\n      borderwidth: 1\n    },\n    hovermode: 'x unified',\n    plot_bgcolor: 'white',\n    paper_bgcolor: 'white',\n    margin: { l: 80, r: 40, t: 80, b: 80 }\n  };\n\n  const config = {\n    displayModeBar: true,\n    displaylogo: false,\n    modeBarButtonsToRemove: ['pan2d', 'lasso2d', 'select2d'],\n    responsive: true\n  };\n\n  const div = DOM.element(\"div\");\n  div.style.width = \"100%\";\n  div.style.height = \"500px\";\n\n  Plotly.newPlot(div, traces, layout, config);\n\n  return div;\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis chart show the number of attendance at major AI conferences like CVPR(Computer Vision), NeurIPS(Neural Information Processing System, a top ML conference), ICML (International Conference on Machine Learning), AAAI (Association for the Advancement of Artificial Intelligence), ICLR (International conference on Learning Representations), and others, from around 2010 to 2024. Look at what happens around 2012-2015 onwards. The attendance for many of these conferences, especially those focused on machine learning and computer vision (like CVPR, NeurlPS, ICML, ICLR) just explodes. We’re talking about conferences going from a few thousand attendees to over 20,000, sometimes even more, in just a few years. This signifies a huge influx of researchers, students, and industry practitioners into the field. The source is from Our World in Data.\n\naiData = FileAttachment(\"./data/enterprise-ai-revenue.csv\").csv()\n\naiChart = {\n  const trace = {\n    x: aiData.map(d =&gt; d.Year),\n    y: aiData.map(d =&gt; d.Revenue),\n    type: 'bar',\n    name: 'AI Market Revenue',\n    marker: {\n      color: '#4285f4',\n      opacity: 0.8,\n      line: {\n        color: '#1a73e8',\n        width: 1\n      }\n    },\n    text: aiData.map(d =&gt; `$${(+d.Revenue).toFixed(2)}`),\n    textposition: 'outside',\n    textfont: {\n      color: '#333',\n      size: 11\n    },\n    hovertemplate: '&lt;b&gt;%{x}&lt;/b&gt;&lt;br&gt;' +\n                   'Revenue: $%{y:,.2f} million USD&lt;br&gt;' +\n                   '&lt;extra&gt;&lt;/extra&gt;'\n  };\n\n  const layout = {\n    title: {\n      text: 'Enterprise Artificial Intelligence Market Revenue Worldwide 2016-2025',\n      font: { size: 18 },\n      x: 0.5\n    },\n    xaxis: {\n      title: 'Year',\n      showgrid: false,\n      tickmode: 'array',\n      tickvals: aiData.map(d =&gt; d.Year),\n      ticktext: aiData.map(d =&gt; d.Year &lt; 2025 ? `${d.Year}*` : `${d.Year}*`)\n    },\n    yaxis: {\n      title: 'Revenue in million U.S. dollars',\n      showgrid: true,\n      gridcolor: 'rgba(0,0,0,0.1)',\n      tickformat: ',.0f',\n      range: [0, Math.max(...aiData.map(d =&gt; +d.Revenue)) * 1.1]\n    },\n    plot_bgcolor: 'white',\n    paper_bgcolor: 'white',\n    margin: { l: 80, r: 40, t: 100, b: 80 },\n    showlegend: false\n  };\n\n  const config = {\n    displayModeBar: true,\n    displaylogo: false,\n    modeBarButtonsToRemove: ['pan2d', 'lasso2d', 'select2d'],\n    responsive: true\n  };\n\n  const div = DOM.element(\"div\");\n  div.style.width = \"100%\";\n  div.style.height = \"500px\";\n\n  Plotly.newPlot(div, [trace], layout, config);\n\n  return div;\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNext is Enterprise application AI revenue, the bar chart shows the revenue generated from enterprise applications of AI, in billions of U.S. dollars, from 2016 projected out to 2025. Even starting in 2016, there’s already noticeable revenue. But the projected growth is staggering. It goes from a few hundred billion dollars, and then projected to over thirty trillion dollars by 2025. This shows that AI is not just research or startups, it’s being deployed in established businesses across various sectors generating significant economic value. The source is from cloudlevante"
  },
  {
    "objectID": "posts/history-of-computer-vision-and-deep-learning/index.html#beyond-the-benchmark",
    "href": "posts/history-of-computer-vision-and-deep-learning/index.html#beyond-the-benchmark",
    "title": "A brief history of computer vision and deep learning",
    "section": "Beyond the Benchmark",
    "text": "Beyond the Benchmark\nWe’ve had a whirlwind tour through the incredible achievements of deep learning in computer vision since 2012. We’ve seen models classify, detect, segment, caption, and even generate incredibly realistic images. Now, it’s crucial to bring us back to reality and acknowledge that while the successes are profound, there’s still a lot of work to be done.\n\nDespite the successes, computer vision still has a long way to go\n\nWe’ve achieved incredible feats on specific benchmarks, but true human-level visual intelligence, with common sense, robustness, and ethical considerations, is still a grand challenge. This isn’t to diminish the progress, but to inspire you for the future.\n\n\n\nSource: https://www.washingtonpos t.com/technology/2019/ 10/22/ai-hiring-face-scan ning-algorithm-increasingly-decides-whether-you-deserve-job/ and https://www.hirevue.com/platform/ online-video-interviewing-software\n\n\nIn fact, while computer vision can do immense good, it also has the potential to cause harm if not developed and deployed carefully. As future engineers and scientists in this field, it’s vital to be aware of those risks. Consider this concerned example Harmful Stereotypes, specifically related to gender classification. The table on the left shows the accuracy of gender classifiers from major tech companies like Microsoft, FACE++, IBM on different demographic groups. The largest gap column, while accuracy for lighter males and females is very high, it significantly drops for darker-skinned individuals, especially darker females. This means these systems are biased. Why does this happen? Often due to the lack of diverse and representative training data, or biases inherent in the data collection process. The averaged faces below visually represent these biased training sets. This is a critical issue, AI systems, if trained on biased data, will perpetuate and even amplify existing societal biases.\nOn the right, we see that can Affect people’s lives. The headline from The Washington Post: “A face-scanning algorithm increasingly decides whether”you deserve the job”. This refers to companies like HireVue, which use AI-powered video analysis in job interviews to assess candidates. The system analyzes facial expression, speech patterns, and other cues. While the intent might be to standardize hiring, outside experts call it “profoundly disturbing”. Imagine an algorithm, potentially biased, making decisions about your career prospects. This highlights that computer vision systems, when deployed in high-stakes environments like hiring, criminal justice, or healthcare, must be rigorously tested for fairness, transparency, and accuracy across all demographics. The ethical implications are enormous, and we, as a community, have a responsibility to address them.\nBut it’s not all about potential harm. We also need to recognize the immense potential for good. Computer vision can save lives. Consider the challenge of how to take care of seniors while keeping them safe? This is a growing societal problem with an aging global population. Computer vision offers a promising non-invasive solution. Imagine a camera system in a senior’s home, it can help early symptom detection of COVID-19 by monitoring cough, breathing changes, fever-like symptoms through thermal imaging. It can monitor patients with mild symptoms by reducing the need for frequent in-person visits. It can help manage chronic conditions like detecting changes in gait for mobility issues, monitoring sleep patterns, diet, or overall activity levels.\nThese systems are versatile and, crucially, scalable. They can be low-cost compared to continuous human care and can be burden-free for the seniors themselves, allowing them to maintain independence while providing peace of mind to their families and caregivers. This is a powerful example of how computer vision, when designed ethically and thoughtfully can be a force for immense societal benefit.\nBut even with these powerful applications, there are fundamental limitations in reasoning and common sense that remind us just how far we still have to go. This brings us to a classic, and still deeply relevant, thought experiment in computer vision.\n\nBack in 2012, Andrej Karpathy (who you may know as a former Director of AI at Tesla and a key figure in the field) wrote a blog post called “The state of Computer Vision and AI: we are really, really far away.” about the image you see above. He argued that it perfectly illustrated challenge facing AI. He called the state of computer vision at the time “pathetic” in the face of what this image requires. To truly understand the humor and the story in this photo, a computer would need to go far beyond just identifying pixels. It would need to synthesize an incredible amount of world knowledge.\nFirst it needs to understand the complex scene geometry. It has to recognize people, but also realize that some of them are reflections in a mirror, not separate individuals.\nSecond it needs to grasp physical interaction and object affordance. It has to identify the object as a weight scale, understand that the person is standing on it to measure their weight, and then notice that then President Obama has his foot slyly placed on the back of the scale. This requires understanding that applying force to a scale alters its measurement a basic concept of physics.\nBut the real challenge, the part that truly tests intelligence, is reasoning about minds. The system would need to infer that the person on the scale is unaware of Obama’s prank because of his pose and limited field of view. It would need to anticipate the person’s imminent confusion when he sees the inflated number. Add it’s a deeply social, psychological, and physical understanding, all from a single 2D image of RGB pixels.\nSo, that was 2012. Now, let’s fast forward to the present day, over a decade into the deep learning revolution. Did we solve it? This very question resurfaced in 2023. When asked about the original Obama image, Karpathy’s response was telling:\n\nWe tried and it solves it :o.\n\nFor a moment, it seems like the problem was solved. But the story gets more complex. Karpathy immediately followed up with his own skepticism:\n\nI still didn’t believe it could be true.\n\nThe reason for his doubt is a critical concept in modern AI: data contamination. The Obama photo is famous. It, along with Karpathy’s original blog post and thousands of articles explain the joke, and almost certainly part of the massive datasets used to train today’s large vision-language models. So, when the model “explains” the joke, is it truly reasoning from first principles, or is it performing an act of incredibly sophisticated retrieval? Is it recreating an explanation it has already seen, or is it generating one from scratch? Maybe the image might be leaked into the training set. This ambiguity is perfectly captured by Karpathy’s own words:\n\nThe waters are muddied…\n\nAnd this is where we stand today, truly beyond the benchmark. The lines are blurring. Our models have become so powerful that we are no longer just asking “Is it accurate?” but the much harder question: “Does it understand?” The challenge is no longer simply about building a better classifier, but about building a system with verifiable reasoning, untangling true intelligence from phenomenal memory.\nThe road ahead is still long, but the problems we face are no longer just about recognizing pixels. They are about navigating ambiguity, context, and common sense which is the very fabric of intelligence itself. The canvas is far from finished, but the picture we are beginning to paint is more intricate and fascinating than we could have ever imagined."
  },
  {
    "objectID": "posts/linear_classification/index.html",
    "href": "posts/linear_classification/index.html",
    "title": "Image classification with linear classifiers",
    "section": "",
    "text": "Image classification is truly a core task in computer vision. It’s the simplest and most fundamental problem, and mastering it forms the basis for more complex tasks like object detection and segmentation."
  },
  {
    "objectID": "posts/linear_classification/index.html#what-makes-a-cat-a-cat",
    "href": "posts/linear_classification/index.html#what-makes-a-cat-a-cat",
    "title": "Image classification with linear classifiers",
    "section": "What makes a cat a cat?",
    "text": "What makes a cat a cat?\nLet’s explicitly define image classification. The task is: given an input image, assign it one label from a predefine set of possible categories. If we have an image of a cat. Our system would label ‘cat’. Crucially for this task, we assume we are given a set of possible labels, like {dog, cat, truck, plane, …}. The model job is to pick the most appropriate label from this list for any given input image. We’re not asking it for generate new label or descriptions, just to categorize.\nNow, this simple task of image to label hides a fundamental challenge in computer vision, what we call Semantic Gap. With an image what you see is a beautiful, tabby cat maybe with green eyes, you immediately understand it’s a living creature a pet, specifically a cat. But what the computer sees is a grid of numbers. The semantic gap is precisely this challenge: how do we bridge the enormous gap between these raw numerical pixel values (what computer sees) and the high-level semantic concepts that we human effortlessly understand? That’s the core problem we’re trying to solve in image classification, and indeed, in much of computer vision.\nNow let’s look at why this is so hard for a computers. The world is messy, and images with a lot of variability.\nOne of the primitive challenge is Viewpoint variation. Take our friendly cat again. You see if from one angle. But what if the camera moved slightly? Or what if you’re looking at a different photo of a same cat from a completely different side, or from above, blow? The camera illustrated around the cat show different potential viewpoints. From these different angles, even if it’s the exact same physical cat, the resulting image (the grid of pixel values) will be drastically different. For us it’s still ‘a cat’. For a computer it’s a completely new set of numbers. Our model need to learn that all these wildly different arrangements of pixels still correspond to the same underlying object. This invariance viewpoints is a huge challenge.\n\n\n\nChallenges: Illumination\n\n\nAnother major hurdle is Illumination. The way an object is lit dramatically changes its appearance in an image. All of these are cat, but the pixel values, the color, the contrast they are completely different across these images due to varying light conditions. Our model must be robust enough to recognize a cat regardless of whether it’s in bright sunlight, deep snow, or under artifact light.\n\n\n\nChallenges: Background Clutter\n\n\nThen we have Background Clutter. Objects in real world rarely appears against a plain, uniform background. They are embedded in complex often messy environments. The challenge here is for the computer to distinguish the object of interest from everything else in the image. It needs to focus on the relevant features and ignore the distractors, even when the background is cluttered or has similar textures of colors.\n\n\n\nChallenges: Occlusion\n\n\nA very common and difficult challenge is Occlusion. This occurs when parts of the object you’re trying to recognize are hidden from view from other objects. Despite missing large portion of the object, as human, we can still easily identify the cat. For a computer, dealing with these partial view and reasoning about what’s missing is incredibly difficult. It needs to learn to recognize an object even when only some of its characteristic features are present, or when they are distorted by being partially covered.\n\n\n\nChallenges: Deformation\n\n\nAnother significant challenge is deformation. Many objects especially animate ones like animals and people, are not rigid They can change their shape, their pose, their configuration in countless ways. These are all cats, but their body shapes and the relative position of their limbs are drastically different. This isn’t just about camera moving around a rigid object, the object itself is deforming. Our visual system needs to be able to recognize an object class despite these non-rigid transformation. A simple template or a fix set of geometric rule would struggle immensely with these level of variability.\nAnd if deformation wasn’t enough, we also have the huge challenge of Intraclass variation. What this means is that even within a single category, like ‘cat’ there can be an enormous amount of visual diversity. Think about different breed of cats look very different, they come in all sorts of colors and patterns. A good image classifier need to learn a concept of ‘cat’ that is general enough that encompass all these variation. It can’t just memorize one specific type of cat, it has to understand the underlying shared characteristic that define the entire class, despite the wide range of appearances. This is a core reason why simple, rule-based approaches often fail and why we need powerful learning algorithms.\n\n\n\nChallenges: Context\n\n\nAnd that final challenge is Context. Humans are incredibly adept at using context to understand the world. We don’t just see objects in isolation; we see them within a scene, and that surrounding information heavily influences our perception. So what went wrong here? Here our model tend to latch onto local patterns, the stripping alone is enough to push our model prediction toward ‘tiger’. Our model didn’t sufficiently incorporate the fact this is on pavement, in front of a gate, and it turns out that it’s just someone’s small dog, and the tiger-stripes are the shadows cast by the iron bars of the gate across its fur. So, understanding and leveraging context is crucial for robust visual intelligence, but it’s also very difficult for current models to do this effectively. They often rely too heavily on local features and can miss the bigger picture or be fooled by misleading contextual cues.\nGiven all these hurdles, it should be clear that trying to write a program with explicit rules to identify, say, a ‘cat’ in all its possible manifestations and situations is an almost impossible task. We’d be writing if-else statements for years!"
  },
  {
    "objectID": "posts/linear_classification/index.html#the-three-faces-of-a-classifier",
    "href": "posts/linear_classification/index.html#the-three-faces-of-a-classifier",
    "title": "Image classification with linear classifiers",
    "section": "The three faces of a classifier",
    "text": "The three faces of a classifier\nSo now, we’re going to move on to a type of classifier, and arguably one of the most fundamental building blogs in machine learning and deep learning: the Linear Classifier. Linear classifier fall under what’s known as the Parametric Approach. This is a key distinction from K-Nearest Neighbors, which is a non-parametric method. In the parametric approach, we define a score function, let’s call it \\(f(x, W)\\), that map the raw input data(our image x) to class scores. The crucial part here is this \\(W\\). \\(W\\) represents a set of parameters or weights that the model uses. The “learning” process in a parametric model involves finding the optimal values for these parameters \\(W\\) using the training data. Once we’ve learned these parameters, we can effectively discard the training data itself! To make a prediction for a new image, we just need the learned parameters \\(W\\) and the function \\(f\\). This is very different from K-NN where we had to keep all the training data around.\nNow, let’s get specific. What form does this function \\(f(x,W)\\) take for a linear classifier? It’s beautifully simple:\n\\[\nf(x, W) = Wx\n\\]\nThat’s it! It’s a matrix-vector multiplication. The result of \\(Wx\\) will be a vector of class scores. There’s one more small addition we typically make to this linear score function. We often add a bias term, \\(b\\) (though often we just write f(x,W) and assume W implicitly includes b, or b is handled separately).. So, the full form of our linear classifier’s score function becomes:\n\\[\nf(x,W,b) = Wx + b\n\\]\nWhat is this bias \\(b\\)? It allows the score function to have some class-specific preferences that are independent of the input image \\(x\\). Think of it as shifting the baseline score for each class. For example, if in our training data, cats are just generally more common than airplanes, the bias term for “cat” might learn to be slightly higher, giving cats a bit of an advantage even before looking at the image pixels. It’s also common practice to sometimes absorb the bias term into the weight matrix \\(W\\) by appending a constant 1 to the input vector \\(x\\), and adding an extra column to \\(W\\) to hold the bias values. This just makes the notation a bit cleaner, \\(f(x,W) = Wx\\), but conceptually, it’s useful to think of \\(W\\) (the part that multiplies \\(x\\)) and \\(b\\) (the additive part) separately for a moment.\nWhen it comes to interpreting a linear classifier, I think it’s easier for us to look at it from a few different perspectives\n\n\n\nAlgebraic Viewpoint\n\n\nFirst is Algebraic Viewpoint. We need to produce 3 scores (one for cat, one for dog, one for ship). So our output should be a 3x1 vector. This means our weight matrix \\(W\\) must have dimensions [3 x 4] (3 rows for 3 classes, 4 columns to match the 4 pixels in \\(x\\)). And our bias vector \\(b\\) will be [3 x 1]. The first row of \\(W\\) are the weights for the “cat” class. The second row are the weights for the “dog” class. The third row are for the “ship” class. To get the scores, we perform the matrix multiplication \\(Wx\\) and then add \\(b.\\) So, for this input image and these particular weights \\(W\\) and biases \\(b\\), we get scores: Cat: -96.8, Dog: 437.9, Ship: 61.95. Based on these scores, which class would our linear classifier predict? It would predict “Dog,” because 437.9 is the highest score. The “learning” process, which we haven’t discussed yet, would be about finding values for \\(W\\) and \\(b\\) such that for an image that is a cat, the cat score is highest; for an image that is a dog, the dog score is highest, and so on.\nOkay, we’ve seen the algebra. But can we get a more Visual Viewpoint of what these weights W actually represent? For CIFAR-10, each row of \\(W\\) is a vector of 3072 weights. Since our input \\(x\\) is an image, we can reshape each row of \\(W\\) back into a 32x32x3 image. What do these “images” corresponding to the rows of \\(W\\) look like?\n\n\n\nVisual Viewpoint\n\n\nWhat the linear classifier is learning is essentially a single template for each class. When a new image comes in, it’s like the classifier is “matching” that input image against each of these 10 templates using a dot product. If the input image has, say, a lot of blue pixels in the regions where the “plane” template has blue pixels, and not many red pixels where the “plane” template has red pixels, that will contribute to a high score for the “plane” class. These templates are often very blurry and try to capture the “average” appearance of objects in that class. For example, cars can be many colors, but if there are more red cars in the training set, the “car” template might end up looking reddish. If planes are often pictured against a blue sky, the “plane” template might pick up on that blue. This also highlight a limitation: a linear learns only one template per class. But a class like “car” has many variations (red cars, blue cars, trucks, cars viewed from the side, car viewed from the front). A single template will struggle to capture this variability. It might learn a sort “average car”.\n\n\n\nGeometric Viewpoint\n\n\nFinally let’s consider the Geometric Viewpoint. Our score function is \\(s = Wx + b\\). For each class c, the score for that class is \\(s_c = W_c ⋅ x + b_c\\), where \\(W_c\\) is the c-th row of \\(W\\). This is the equation of a line (or a hyperplane in higher dimensions). So, linear classifier is learning a set of hyperplane in the high-dimensional pixel space. Each hyperplane correspond to a class. The decision boundary between any two classes, say class \\(i\\) and class \\(j\\), occurs where their score are equal: \\(W_ix + b_i = W_jx + b_j\\). This equation also defines a hyperplane.\nThe 3D plot on the bottom left shows the actual score surfaces for three classes. Each colored plane represents the scores for one class as a function of a 2D input. The decision boundaries are where these planes intersect. You can see that the regions where one plane is highest correspond to the classification regions for that class. These regions are always convex polygons (or polyhedra in higher dimensions) for a linear classifier. So, geometrically, a linear classifier is carving up the high-dimensional input space using these hyperplanes. It’s trying to find orientations and positions for these hyperplanes such that most of the training examples for a given class fall on the “correct” side of their respective decision boundaries.\nThese three viewpoints – algebraic, visual (template matching), and geometric (hyperplanes) – all describe the same underlying mathematical operation \\(Wx + b\\). Understanding it from these different angles helps build a much richer intuition for what a linear classifier is doing, what it can learn, and also, importantly, what its limitations might be."
  },
  {
    "objectID": "posts/linear_classification/index.html#hard-cases-for-a-linear-classifier",
    "href": "posts/linear_classification/index.html#hard-cases-for-a-linear-classifier",
    "title": "Image classification with linear classifiers",
    "section": "Hard cases for a linear classifier",
    "text": "Hard cases for a linear classifier\nThere are scenarios where, no matter how you orient your lines (or hyperplanes in higher dimensions), you simply cannot perfectly separate the classes.\n\n\n\nCase 1 (left): The XOR problem. Case 2 (middle): The donut problem. Case 3 (right): Multiple modes.\n\n\nIn case 1 we have the XOR problem where class 1 is in the first and third quadrants, class 2 is in the second and forth quadrants. Try to draw a single straight line, that separate the class 1 and class 2. You can do it! You’d need at least two lines, or a non-linear boundary. A linear classifier will fall here, it will make mistakes no matter what i places its decision boundary.\nCase 2 is the donut problem, here, class 1 is an annulus or a ring – points whose L2 norm is between 1 and 2. Class 2 is everything else (inside the inner circle and outside the outer circle). Again, can you separate the blue ring from the pink regions with a single straight line? No. You’d need something like a circular boundary, which is non-linear.\nCase 3 is Multiple modes, Class 1 consists of three distinct, separated circular regions. Class 2 is everything else. A single linear classifier tries to learn one template per class. If a class has multiple, well-separated “prototypes” or modes in the feature space, a linear classifier will struggle. It can’t draw a line to nicely encapsulate all three blue regions while excluding the pink. It might try to find a single hyperplane that does its best, but it will inevitably misclassify many points."
  },
  {
    "objectID": "posts/linear_classification/index.html#choose-a-good-w",
    "href": "posts/linear_classification/index.html#choose-a-good-w",
    "title": "Image classification with linear classifiers",
    "section": "Choose a good W",
    "text": "Choose a good W\nSo, given that we have this linear score function \\(f(x,W) = Wx + b\\), and we understand its capabilities and limitations, the central question becomes: How do we choose a good \\(W\\) (and \\(b\\))? We need a systematic way to 1) define a loss function (or cost function or objective function). This function will take our current \\(W\\) and \\(b\\), run our classifier on the training data, look at the scores it produces, and tell us how “unhappy” we are with those scores. A high loss means our \\(W\\) and \\(b\\) are bad (producing scores that lead to incorrect classifications or low confidence in correct classifications). A low loss means our \\(W\\) and \\(b\\) are good. 2) Once we have this loss function, we need to come up with a way of efficiently finding the parameters (\\(W\\) and \\(b\\)) that minimize this loss function. This is an optimization problem. We want to search through the space of all possible \\(W\\)’s and \\(b\\)’s to find the set that makes our classifier perform best on the training data, according to our loss function.\nMore formally, we are given a dataset of \\(N\\) training examples:\n\\[\n{ (x_i, y_i) }_{i=1}^{N}\n\\]\nWhere \\(x_i\\) is an image (our input vector) and \\(y_i\\) is its true integer label (e.g., 0 for cat, 1 for car, 2 for frog). Typically, the total loss over the entire dataset, \\(L\\), is the average of the losses computed for each individual training example:\n\\[\nL = \\frac{1}{N} \\sum_i L_i(f(x_i, W), y_i)\n\\]\nHere:\n\n\\(f(x_i, W)\\) are the scores our current classifier (with weights \\(W\\)) produces for the i-th training image \\(x_i\\).\n\\(y_i\\) is the true label for that i-th training image.\n\\(L_i\\) is the loss function calculated for that single i-th example. It takes the predicted scores and the true label and tells us how bad the prediction was for that one example.\nWe sum these individual losses \\(L_i\\) over all \\(N\\) training examples and then divide by \\(N\\) to get the average loss\n\nOur goal will be to find the \\(W\\) that minimizes this total loss \\(L\\). Now, the crucial piece we still need to define is: what exactly is this \\(L_i\\) function? How do we take a vector of scores and a true label and turn that into a single number representing the loss for that example? There are several ways to do this, and we’ll look at a very common one for classification next: the Softmax classifier (which uses cross-entropy loss), and also the SVM (hinge) loss."
  },
  {
    "objectID": "posts/linear_classification/index.html#softmax-classifier",
    "href": "posts/linear_classification/index.html#softmax-classifier",
    "title": "Image classification with linear classifiers",
    "section": "Softmax classifier",
    "text": "Softmax classifier\nThe core idea behind the Softmax classifier is that we want to interpret the raw classifier scores as probabilities. Our linear classifier \\(s = f(x_i, W)\\) produces these raw scores. For example our cat image example, we had scores: Cat: 3.2, Car: 5.1, Frog: -1.7. These are just arbitrary real numbers. They could be positive, negative, large, small. They don’t look like probabilities, which should be between 0 and 1 and sum to 1. The Softmax function is what allows us to convert these raw scores into a valid probability distribution over the classes. The formula for the probability of the true class \\(k\\) given the input image \\(x_i\\) (and our weights \\(W\\)) is:\n\\[\nP(Y = k | X = x_i) = \\frac{e^{s_k}}{\\sum_j e^{s_j}}\n\\]\nWhere:\n\n\\(s_k\\) is the raw score for class \\(k\\) (e.g., 3.2 for cat).\n\\(s_j\\) are the raw scores for all classes \\(j\\) (cat, car, frog).\nWe exponentiate each score (\\(e^{s_k}\\)). This makes all the numbers positive, which is good for probabilities.\nThen we normalize by dividing by the sum of all these exponentiated scores. This ensures that the resulting probabilities for all classes sum to 1.\n\nThe raw scores s that go into the Softmax function (our 3.2, 5.1, -1.7) are often called logits or unnormalized log-probabilities. The term “logit” comes from logistic regression, of which Softmax is a generalization to multiple classes\nOkay, so we’ve used the Softmax function to get a probability distribution over the classes for a given input image \\(x_i.\\) Now, how do we define the loss \\(L_i\\) for this single example? If the true class for our example image (the cat) is y_i (let’s say “cat” is class 0), then the loss for this example is defined as the negative log probability of the true class:\n\\[\nL_i = -\\log{P(Y = y_i|X = x_i)}\n\\]\nWhy this particular form for the loss? Probabilities \\(P\\) are between \\(0\\) and \\(1\\). \\(log(P)\\) will be negative (or \\(0\\) if \\(P=1\\)). If the probability of the true class is very high (close to \\(1\\), e.g., \\(P=0.99\\)), then \\(log(P)\\) is close to \\(0\\), and \\(-log(P)\\) is also close to \\(0\\) (a small loss, which is good!). If the probability of the true class is very low (close to \\(0\\), e.g., \\(P=0.01\\)), then \\(log(P)\\) is a large negative number, and \\(-log(P)\\) will be a large positive number (a high loss, which is bad!). So, this \\(-log(P_true_class)\\) loss function does what we want: it penalizes the model heavily if it assigns low probability to the correct answer, and penalizes it very little if it assigns high probability to the correct answer. Minimizing this loss will push the model to make the probability of the true class as close to \\(1\\) as possible. This approach also known as Maximum Likelihood Estimation (MLE). We are choosing the parameters \\(W\\) to maximize the likelihood (or equivalently, the log-likelihood) of observing the true labels \\(y_i\\) given the input data \\(x_i.\\) Taking the negative log-likelihood turns it into a loss minimization problem.\nWe can also think about this loss from an information theory perspective. The “correct probabilities” for our cat image would be: Cat: 1.00, Car: 0.00, Frog: 0.00. This is a probability distribution where all the mass is on the true class. Let’s call this target distribution \\(P\\). Our Softmax classifier produced the distribution Q: Cat: 0.13, Car: 0.87, Frog: 0.00. The loss function we are using, \\(-log P(Y=y_i | X=x_i)\\), is actually equivalent to the Kullback-Leibler (KL) divergence between the true distribution P (which is 1 for the correct class and 0 otherwise) and the predicted distribution Q from our Softmax. The KL divergence \\(D_KL(P || Q)\\) measures how different the distribution \\(Q\\) is from the distribution \\(P\\). It’s defined as \\(\\sum_y P(y)\\log\\frac{P(y)}{Q(y)}\\). When P is a one-hot distribution (1 for the true class \\(y_i\\), \\(0\\) elsewhere), this simplifies to \\(-logQ(y_i)\\), which is exactly our loss function!\nSo, the loss \\(L_i = -log P(Y=y_i | X=x_i)\\) for a Softmax classifier is often called the Cross-Entropy loss between the true distribution (one-hot encoding of the correct label) and the predicted probability distribution from the Softmax.\n\\[\nH(P,Q) = - \\sum_y P(y) log Q(y)\n\\]\nWhen \\(P\\) is one-hot, \\(P(y)\\) is \\(1\\) for the true class \\(y_i\\) and \\(0\\) otherwise. So, the sum collapses to a single term: \\(-1 log Q(y_i) = -log Q(y_i)\\).\nSo, whether you think of it as maximizing the log-likelihood of the correct class, or minimizing the KL divergence, or minimizing the cross-entropy between the true and predicted distributions, it all leads to the same loss function for the Softmax classifier: \\(L_i = -log(\\text{probability\\_of\\_true\\_class})\\). This is a cornerstone loss function for classification problems in deep learning. The overall loss for the dataset, L, would then be the average of these L_i’s over all training examples. Our goal is to find the W that minimizes this total cross-entropy loss.\nSo, just to recap:\n\nWe start with raw scores \\(s = f(x_i, W)\\) from our linear classifier.\nWe want to interpret these as probabilities, so we use the Softmax function: \\(P(Y = k | X = x_i) = \\frac{e^{s_k}}{\\sum_j e^{s_j}}\\). This gives us a probability for each class \\(k\\).\nOur goal is to maximize the probability of the correct class \\(y_i\\).\nThe loss function \\(L_i\\) for a single example is the negative log probability of the true class: \\(L_i = -log P(Y = y_i | X = x_i)\\).\n\nPutting it all together, we can write the loss for a single example \\(x_i\\) with true label \\(y_i\\) and scores \\(s_j\\) (where \\(s_j\\) is the score for class \\(j\\)) directly as:\n\\[ L_i = -log ( \\frac{e^{ s_{y_i} }}{\\sum_j e^{ s_j }} )\\]\nHere, \\(s_yi\\) is the score for the true class \\(y_i\\). This formula combines the Softmax calculation and the negative log operation into one expression for the loss. Minimizing this \\(L_i\\) will push the score of the correct class \\(s_yi\\) to be high relative to the scores of the other classes \\(s_j\\).\nNow, let’s think about some properties of this Softmax loss \\(L_i\\). Two good questions to consider: What is the minimum and maximum possible Softmax loss \\(L_i\\)? And at initialization, our weights W are typically small random numbers. So, the initial scores \\(s_j\\) for all classes will be approximately equal (and close to zero). What will the Softmax loss \\(L_i\\) be in this scenario, assuming there are C classes?\nAnswer for the first question: Minimum possible loss, the loss is \\(L_i = -log(\\text{P\\_correct\\_class})\\). To minimize \\(L_i\\), we need to maximize P_correct_class. The maximum possible probability for the correct class is 1 (i.e., the classifier is 100% certain and correct). If \\(P\\_correct\\_class = 1\\), then \\(log(1) = 0\\), so \\(L_i = -0 = 0\\). Thus, the minimum possible Softmax loss is 0. This occurs when the model perfectly predicts the true class with probability 1. Maximum possible loss, to maximize \\(L_i\\), we need to minimize P_correct_class. The minimum possible probability for the correct class is something very close to 0 (it can’t be exactly 0 because of the exponentiation, but it can be arbitrarily small if the score for the correct class is very, very negative relative to other scores). As P_correct_class approaches 0 from the positive side, log(P_correct_class) approaches negative infinity. Therefore, \\(-log(\\text{ P\\_correct\\_class })\\) approaches positive infinity. So, the Softmax loss can, in theory, go to infinity if the model is extremely confident in a wrong class and assigns vanishingly small probability to the true class.\nNow for question 2: If all \\(s_j\\) are approximately equal (say, \\(s_j \\approx s\\_constant\\)), then \\(e^{s_j}\\) will also be approximately equal for all \\(j\\). The probability for any given class k will be:\n\\[ P(Y=k | X=x_i) = \\frac{e^{s_k}}{\\sum_j e^{s_j}} \\]\nSince all \\(e^{s_j}\\) are roughly the same, let’s say \\(e^{s\\_constant}\\), the sum in the denominator will be \\(C e^{s\\_constant}\\). So, \\(P(Y=k | X=x_i) \\approx \\frac{e^{s\\_constant}}{C e^{ s\\_constant }} = \\frac{1}{C}\\).\nThis makes intuitive sense: if the classifier has no information yet (all scores are equal), it should assign an equal probability of 1/C to each of the C classes. This is a uniform distribution. Now, the loss for any example \\(x_i\\) (regardless of its true class \\(y_i\\), since the probability assigned to every class is 1/C) will be:\n\\[ L_i = -log(P\\_correct\\_class) = -log(\\frac{1}{C}) \\]\nUsing the logarithm property \\(log(\\frac{1}{C}) = log(1) - log(C) = 0 - log(C) = -log(C)\\). So, \\(L_i = -(-log(C)) = log(C)\\). Therefore, at initialization, when the classifier is essentially guessing uniformly, the Softmax loss per example will be approximately \\(log(C)\\), where \\(C\\) is the number of classes. For example, if we have \\(C = 10\\) classes (like in CIFAR-10), then the initial loss we expect to see is \\(L_i = log(10)\\) (natural logarithm of 10), which is approximately 2.3. This is a very useful “sanity check”! When you’re implementing a Softmax classifier and you initialize your weights, if you compute the loss on your first batch of data and it’s wildly different from \\(log(C)\\), you might have a bug somewhere in your loss calculation or your Softmax implementation. For CIFAR-10, you should see an initial loss around 2.3. If you see a loss of, say, 20 or 0.01 at the very start, something is likely wrong."
  },
  {
    "objectID": "posts/linear_classification/index.html#multiclass-svm-loss",
    "href": "posts/linear_classification/index.html#multiclass-svm-loss",
    "title": "Image classification with linear classifiers",
    "section": "Multiclass SVM loss",
    "text": "Multiclass SVM loss\nAlright, so we’ve seen the Softmax classifier and its cross-entropy loss. Now, let’s turn to the other major type of loss function for linear classifiers: the Multiclass SVM loss, also known as the hinge loss. The Multiclass SVM loss has a different philosophy than Softmax. The SVM wants the score of the correct class \\(s_{y_i}\\) to be greater than the score of any incorrect class \\(s_j\\) (where \\(j ≠ y_i\\)) by at least a certain fixed margin, which is commonly denoted by \\(\\Delta\\) and often set to 1. Here’s the form of the loss L_i for a single example:\n\\[\nL_i = \\sum_{j \\ne y_i}\n\\begin{cases}\n0 & \\text{if } s_{y_i} \\geq s_j + 1 \\\\\ns_j - s_{y_i} + 1 & \\text{otherwise}\n\\end{cases}\n= \\sum_{j \\ne y_i} \\max(0, s_j - s{y_i} + 1)\n\\]\nSo, for each training example, we sum up these margin violation penalties over all incorrect classes. If all incorrect classes have scores that are at least \\(\\Delta\\) less than the score of the correct class, then \\(L_i\\) will be 0 for that example.\n\n\n\nSVM loss\n\n\nLet’s visualize what one term \\(\\max(0, s_j - s_{y_i} + 1)\\) of this loss looks like. The x-axis here is representing \\(s_{y_i} - s_j\\), which is the difference between the score of the correct class and the score of one particular incorrect class \\(j\\). The SVM loss wants the score of the correct class \\(s_{y_i}\\) to be greater than the score of an incorrect class \\(s_j\\) by at least the margin \\(\\Delta\\) (which is 1 in our plot). So, if \\(s_{y_i} - s_j \\geq 1\\), meaning the correct score is already beating the incorrect score by the desired margin then the loss for that pair is 0. You can see the loss function is flat at 0 on the right side of the plot, where \\(s_{y_i} - s_j \\geq 1\\). The SVM doesn’t care how much better the correct score is, as long as it’s better by at least the margin. It doesn’t try to push the correct score infinitely higher or the incorrect scores infinitely lower, unlike Softmax which is never fully satisfied. However, if \\(s_{y_i} - s_j \\lt 1\\) (i.e., the margin is violated), then the loss becomes positive. The loss increases linearly as the difference \\(s_{y_i} - s_j\\) gets smaller (or more negative, meaning \\(s_j\\) is much larger than \\(s_{y_i}\\)). This is the hinge shape - zero loss if the margin is met and then a linear penalty for violations. The total \\(L_i\\) for an example is the sum of these hinge losses over all incorrect classes \\(j\\). This encourages the score of the true class \\(y_i\\) to stand out from all other incorrect class scores by at least \\(\\Delta\\). This is a fundamentally different way of thinking about loss compared to Softmax. Softmax wants to correctly estimate the probability distribution; SVM wants to find a decision boundary that separates classes with a good margin.\nSo what is the min/max possible SVM loss L_i? Minimum possible loss, if all margins are satisfied (i.e., for all incorrect classes \\(j\\), \\(s_{y_i} \\geq s_j + \\Delta\\)), then every term in the \\(\\sum \\max(0, s_j - s_{y_i} + \\Delta)\\) will be 0. So, the minimum SVM loss \\(L_i\\) is 0. Maximum possible loss, if the score for the correct class \\(s_{y_i}\\) is extremely negative, and scores for incorrect classes \\(s_j\\) are very positive, then \\(s_j - s_{y_i} + \\Delta\\) can become arbitrarily large and positive for each incorrect class. Since we sum these terms, the total \\(L_i\\) can go to positive infinity.\nAt initialization, \\(W\\) is small, so all scores \\(s_j \\approx 0\\). What is the loss \\(L_i\\), assuming \\(C\\) classes? If all \\(s_j \\approx 0\\) (including \\(s_{y_i} \\approx 0\\)), then for each of the \\(C-1\\) incorrect classes j, the term is:\n\\[s_j - s_{y_i} + \\Delta ≈ 0 - 0 + \\Delta = \\Delta\\]\nSo, \\(\\max(0, \\Delta)\\) is just \\(\\Delta\\) (assuming our margin \\(\\Delta\\) is positive, e.g., \\(\\Delta=1\\)). We sum this \\(\\Delta\\) over all \\(C-1\\) incorrect classes. Therefore, \\(L_i ≈ (C-1) \\Delta\\). If \\(\\Delta=1\\), then the initial loss \\(L\\) is approximately \\(C-1\\). For CIFAR-10 with \\(C=10\\) classes and \\(\\Delta=1\\), the initial SVM loss per example would be around 9. This is another good sanity check for your implementation. If you see an initial SVM loss far from \\(C-1\\), you might have an issue.\nWhat if we used a squared term for the loss: \\(L_i = \\sum_{j \\neq y_i} \\max(0, s_j - s_{y_i} + \\Delta)^2\\)? This is known as the squared hinge loss (or L2-SVM). The standard (L1) hinge loss \\(\\max(0, margin\\_violation)\\) penalizes any violation linearly. Every unit of margin violation contributes equally to the loss. The squared (L2) hinge loss \\(\\max(0, margin\\_violation)^2\\) penalizes larger violations much more heavily than smaller ones due to the squaring. It’s more sensitive to outliers or examples that are very wrong. This can sometimes lead to different solutions. Both have been used, though the L1 hinge loss is perhaps more standard for the classic SVM. The L2 version is smoother (differentiable even when the margin violation is zero, though the max(0,…) still introduces a non-differentiability point when the argument to max is exactly zero), which can sometimes be beneficial for certain optimization algorithms.\nHere’s a python function that calculates the SVM loss for a single example x with true label y, given weight W.\ndef L_i_vecterized(x, y, W):\n1  scores = W.dot(x)\n2  margins = np.maximum(0, scores - score[y] + 1)\n3  margins[y] = 0\n  loss_i = np.sum(margin)\n  return loss_i\n\n1\n\ncalculate scores\n\n2\n\nthen calculate the margins \\(s_j - s_{y_i} + 1\\)\n\n3\n\nonly sum \\(j\\) is not \\(y_i\\) so when \\(j = y_i\\), set to zero\n\n\nThis is a nice, compact vectorized implementation. To get the loss for a whole batch of data, you’d typically loop over your batch, call this function for each example, and then average the results.\nSVM is a margin-based loss. Softmax is a probabilistic loss that cares about the full distribution. In practice, both are widely used, and sometimes one might perform slightly better than the other depending on the dataset and task, but often their performance is quite similar when used in deep networks. The choice can also come down to whether you explicitly need probability outputs from your model."
  },
  {
    "objectID": "posts/paddy/index.html",
    "href": "posts/paddy/index.html",
    "title": "Paddy doctor",
    "section": "",
    "text": "Alright, we’ve been diving deep into tabular data lately, haven’t we? We played with it, had some fun, and now i think it’s time to go deeper to image classification problem. Yeah, we’ve touched on this before with the world simplest model “Is it a Bird?”, or better our simple model for recognizing three types of bears but that was mostly about deployment. This time we’re going all in.\nFrom now on, we’ll be getting into the mechanics of deep learning, and exploring what a solid computer vision model architecture looks like.\nToday we’re dealing with the Paddy Doctor: Paddy Disease Classification competition on Kaggle. The goal? Predict paddy diseases based on images. Through this competitions we will go though, the general architecture, the presizing process, the loss, and improve our model further, alright let’s get right into it shall we?\nif iskaggle:\n  path = setup_comp('paddy-disease-classification', install=\"timm&gt;=0.6.2.dev0\")\nelse:\n  path = Path(\"./data\")\nFirst we need to understand how our data laid out\npath.ls()\n\n(#4) [Path('data/test_images'),Path('data/train.csv'),Path('data/train_images'),Path('data/sample_submission.csv')]\nData is usually provided in one of these two ways:\n(path/\"train_images/\").ls()\n\n(#10) [Path('data/train_images/bacterial_leaf_blight'),Path('data/train_images/bacterial_leaf_streak'),Path('data/train_images/bacterial_panicle_blight'),Path('data/train_images/blast'),Path('data/train_images/brown_spot'),Path('data/train_images/dead_heart'),Path('data/train_images/downy_mildew'),Path('data/train_images/hispa'),Path('data/train_images/normal'),Path('data/train_images/tungro')]\nAs you can see we have 10 folders each represent paddy diseases that we need to predict, in this case each folders will contain images of paddy disease correspond to the parent folder name\ntrain_path = path/'train_images'\nfiles = get_image_files(train_path)\nfiles\n\n(#10407) [Path('data/train_images/bacterial_leaf_blight/100023.jpg'),Path('data/train_images/bacterial_leaf_blight/100049.jpg'),Path('data/train_images/bacterial_leaf_blight/100126.jpg'),Path('data/train_images/bacterial_leaf_blight/100133.jpg'),Path('data/train_images/bacterial_leaf_blight/100148.jpg'),Path('data/train_images/bacterial_leaf_blight/100162.jpg'),Path('data/train_images/bacterial_leaf_blight/100169.jpg'),Path('data/train_images/bacterial_leaf_blight/100234.jpg'),Path('data/train_images/bacterial_leaf_blight/100248.jpg'),Path('data/train_images/bacterial_leaf_blight/100268.jpg'),Path('data/train_images/bacterial_leaf_blight/100289.jpg'),Path('data/train_images/bacterial_leaf_blight/100330.jpg'),Path('data/train_images/bacterial_leaf_blight/100365.jpg'),Path('data/train_images/bacterial_leaf_blight/100382.jpg'),Path('data/train_images/bacterial_leaf_blight/100445.jpg'),Path('data/train_images/bacterial_leaf_blight/100447.jpg'),Path('data/train_images/bacterial_leaf_blight/100513.jpg'),Path('data/train_images/bacterial_leaf_blight/100516.jpg'),Path('data/train_images/bacterial_leaf_blight/100523.jpg'),Path('data/train_images/bacterial_leaf_blight/100541.jpg')...]\nLet’s take a look a one\nimg = PILImage.create(files[0])\nprint(img.size)\nimg.to_thumb(128)\n\n(480, 640)\nThis image has the size of 480x640, let’s check all their sizes. Looping though over 10.000 images is a pain right, so we will do it in parallel\ndef f(o): return PILImage.create(o).size\nsizes = parallel(f, files, n_workers=8)\npd.Series(sizes).value_counts()\n\n(480, 640)    10403\n(640, 480)        4\nName: count, dtype: int64\nWell, there’s almost have the same size except 4 of them with 640x480, we’ll need to resize all of them to the same size, we will talk about it later, but now let’s create a dataloader\ndls = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                get_items=get_image_files,\n                splitter=RandomSplitter(valid_pct=0.2, seed=42),\n                get_y=parent_label,\n                item_tfms=Resize(480, method='squish'),\n                batch_tfms=aug_transforms(size=128, min_scale=0.75)).dataloaders(train_path)\nThis is the principle of our computer vision model mostly, but notice here, we need to focus on these two lines:\nThese lines implement a fastai data augmentation strategy which they often call presizing."
  },
  {
    "objectID": "posts/paddy/index.html#presizing",
    "href": "posts/paddy/index.html#presizing",
    "title": "Paddy doctor",
    "section": "Presizing",
    "text": "Presizing\nWe need our images to have the same dimensions, so that they can collate into tensors to be passed into GPU. We also want to minimize the number of distinct augmentation computations we perform. If possible we should compose our augmentation transforms into fewer transform(to reduce number of computations and lossy operations) and transform images into uniform sizes(for more efficient processing in GPU)\nHowever if we resize images to their final dimensions(the augmented size) and then apply various augmentation transforms it can lead to issues like creating empty zones (e.g., when rotating an image by 45 degrees) which will not teach the computer anything at all. Many rotation and zooming operations will require interpolating1 to create pixel\nTo walk around these challenges, presizing adopts a two-step strategy.\n\nImages are resized to dimensions significantly larger than the target training size as this will create a “buffer zone” around the image allowing for more flexibility in subsequent augmentation.\nAll common augmentation operations, including the final resize to target dimensions, are combined into a single step performed on GPU at the end of the processing, rather than performing the operations individually and interpolating multiple times.\n\n\n\n\nPresizing explained: Showing the two step resize to a large size then apply random crop and augment at the same time\n\n\nAs you can see in the picture it demonstrate what i described earlier\n\nFirst it crop full width or height this time it still do it sequentially before copied to GPU, it make sure that all our images are the same size. On the training set the crop area is chosen randomly2 and on validation set the it always choose the center square of the image. This is in item_tfms\n\n\nThen it uses RandomResizedCrop as a batch transform. It’s applied to a batch all at once on the GPU, making it fast. For the training set, this includes random cropping and other augmentations, and for validation set only resizing to the final size needed for the model is done. This is in batch_tfms\n\n\nResize\nUse Resize as an item transform with a large size you can use pad3 or squish4 instead of crop5(the default) for the initial Resize but what the diff between them? In fact let’s see the different in action shall we? Here’s the original image:\n\ntst_img = PILImage.create('./test_image.jpg').resize((600,400))\ntst_img.to_thumb(224)\n\n\n\n\n\n\n\n\n\n_, axs = plt.subplots(1,3,figsize=(12,4))\nfor ax,method in zip(axs.flatten(), ['squish', 'pad', 'crop']):\n  rsz = Resize(256, method=method)\n  show_image(rsz(tst_img, split_idx=0), ctx=ax, title=method)\n\n\n\n\n\n\n\n\nOn the validation set, the crop is always a center crop (on the dimension that’s cropped).\n\n_, axs = plt.subplots(1,3,figsize=(12,4))\nfor ax,method in zip(axs.flatten(), ['squish', 'pad', 'crop']):\n  rsz = Resize(256, method=method)\n  show_image(rsz(tst_img, split_idx=1), ctx=ax, title=method)\n\n\n\n\n\n\n\n\nMy recommendation:\n\nStart with padding if you’re unsure. It preserve all information and aspect ratios.\nIf padding introduces too much background, try cropping.\nUse squish only if you’re sure it won’t distort important features.\nAlways validate your choice by inspecting resized image and checking model performance\n\nThe best method can vary depending on the dataset the original aspect ratio. Now let’s see what the aug_transforms does under the hood\n\n\nAugmentation\n\n\ntorch.permute??\n\n\nDocstring:\n\npermute(input, dims) -&gt; Tensor\n\n\n\nReturns a view of the original tensor :attr:`input` with its dimensions permuted.\n\n\n\nArgs:\n\n    input (Tensor): the input tensor.\n\n    dims (tuple of int): The desired ordering of dimensions\n\n\n\nExample:\n\n    &gt;&gt;&gt; x = torch.randn(2, 3, 5)\n\n    &gt;&gt;&gt; x.size()\n\n    torch.Size([2, 3, 5])\n\n    &gt;&gt;&gt; torch.permute(x, (2, 0, 1)).size()\n\n    torch.Size([5, 2, 3])\n\nType:      builtin_function_or_method\n\n\n\nMost image processing libraries and formats (like PIL, OpenCV, matplotlib) use the format (Height, Width, Channels) or (H, W, C).\nHowever, PyTorch expects images in the format (Channels, Height, Width) or (C, H, W).\n\n\ntimg = TensorImage(array(tst_img)).permute(2,0,1).float()/255.\ndef _batch_ex(bs): return TensorImage(timg[None].expand(bs, *timg.shape).clone())\ntfms = aug_transforms(size=128, min_scale=0.75)\ny = _batch_ex(9)\nfor t in tfms: y = t(y, split_idx=0)\n_, axs = plt.subplots(1, 3, figsize=(12, 3))\nfor i,ax in enumerate(axs.flatten()): show_image(y[i], ctx=ax)\n\n\n\n\n\n\n\n\nChoosing the correct size for augment transforms is also crucial too as the size parameter in aug_transforms determines the final size of the images that will be fed into the model. Picking the right one depends on the model architecture requirements, each pretrained models requires different input size (e.g., ResNet typically use 224x224), but in fact you can do some experiments here. But aware of this, larger sizes help computer learn more details, but of course require more resources, in other hand smaller sizes are faster to process but may lose some details, it’s a tradeoff\n\n\n\n\n\n\nNote\n\n\n\n\nStart with the standard size for your chosen model architecture(e.g., 224 for many standard models)\nIf computational resources are not a big deal with you and the images have fine details, try increasing the size (e.g., 299, 384, 521)\nIf using transfer learning, stick to the pretrained model’s original input size is the best option i’d say\nFor custom architectures, you got no choice but experiment with different sizes and choose based on performance and resource constraints.\n\n\n\n\n\nChecking DataBlock\nWriting DataBlock is just like writing a blueprint, we will get an error if we have a syntax error some where in the code. So it’s a good practice to always check your data before doing anything further\n\ndls.show_batch(max_n=6)\n\n\n\n\n\n\n\n\nTake a look at the images and and check that each one seems to have correct label or not. In fact, we often have to deal with data with which is not as familiar as domain experts may be. Indeed, if you’re not a paddy doctor it will be hard to look at a random image and speak out the disease right? Since I’m not an expert on paddy diseases, I would use google to search to make sure the images look similar to what i see in the output. Also you can debug the DataBlock by using DataBlock.summary6\nOnce you think your data looks right, it’s a good practice to train a simple model, think about it, when you start trying to improve your model, how can you rate it? Compare it to the previous try you say, I mean what if it’s already worse, so that why we need to know what our baseline result looks like. Maybe you don’t need anything fancy - a basic model might do the job just fine. Or perhaps the data doesn’t seems to train the model at all. These are things that you want to know as soon as possible\n\nlearn = vision_learner(dls, 'resnet26d', metrics=error_rate, path='.')\n\n\nlearn.fine_tune(3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.966901\n1.133069\n0.356079\n01:03\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.311612\n0.885489\n0.277751\n01:03\n\n\n1\n1.032764\n0.691636\n0.218645\n01:04\n\n\n2\n0.871234\n0.645610\n0.203748\n01:03\n\n\n\n\n\nRemember that we’re in a competition right, so it’s nothing better than submit it and see how it will go and again this time we will use an pretty cool tool call fastkaggle in fact we used it earlier. Alright let’s see the submission layout\n\nss = pd.read_csv(path/'sample_submission.csv')\nss\n\n\n\n\n\n\n\n\nimage_id\nlabel\n\n\n\n\n0\n200001.jpg\nNaN\n\n\n1\n200002.jpg\nNaN\n\n\n2\n200003.jpg\nNaN\n\n\n3\n200004.jpg\nNaN\n\n\n4\n200005.jpg\nNaN\n\n\n...\n...\n...\n\n\n3464\n203465.jpg\nNaN\n\n\n3465\n203466.jpg\nNaN\n\n\n3466\n203467.jpg\nNaN\n\n\n3467\n203468.jpg\nNaN\n\n\n3468\n203469.jpg\nNaN\n\n\n\n\n3469 rows × 2 columns\n\n\n\nalright seems like we need to sort the images in order before submitting it\n\ntst_files = get_image_files(path/'test_images').sorted()\ntst_dl = dls.test_dl(tst_files)\n\nLet’s make the prediction on the test set\n\nprobs,_,idxs = learn.get_preds(dl=tst_dl, with_decoded=True)\nidxs\n\n\n\n\n\n\n\n\ntensor([7, 8, 7,  ..., 8, 1, 5])\n\n\nAlright we got the indices of the diseases, we need to map the name to each diseases we can get the label by checking the vocab\n\ndls.vocab\n\n['bacterial_leaf_blight', 'bacterial_leaf_streak', 'bacterial_panicle_blight', 'blast', 'brown_spot', 'dead_heart', 'downy_mildew', 'hispa', 'normal', 'tungro']\n\n\n\nmapping = dict(enumerate(dls.vocab))\nresults = pd.Series(idxs.numpy(), name=\"idxs\").map(mapping)\nresults\n\n0                       hispa\n1                      normal\n2                       hispa\n3                       blast\n4                       blast\n                ...          \n3464               dead_heart\n3465                    hispa\n3466                   normal\n3467    bacterial_leaf_streak\n3468               dead_heart\nName: idxs, Length: 3469, dtype: object\n\n\nBefore submit let’s see if our file looks right\n\nss['label'] = results\nss.to_csv('subm.csv', index=False)\n!head subm.csv\n\nimage_id,label\n200001.jpg,hispa\n200002.jpg,normal\n200003.jpg,hispa\n200004.jpg,blast\n200005.jpg,blast\n200006.jpg,brown_spot\n200007.jpg,dead_heart\n200008.jpg,brown_spot\n200009.jpg,hispa\n\n\n\nif not iskaggle:\n  from kaggle import api\n  api.competition_submit_cli('subm.csv', 'initial rn26d 128px', 'paddy-disease-classification')\n\n100%|██████████| 70.1k/70.1k [00:01&lt;00:00, 48.7kB/s]\n\n\nAlright i got 0.8917 score on the competitions, it’s not that good but it let’s us know what the base line is, then we can improve it later on.\n\n\n\n\n\n\nNote\n\n\n\nRemember that loss is whatever function we’re decided to use to optimize the parameters of our models, here we’re actually not specific what loss to use, fastai will try to find the best loss to use here for us. In this case we’re using cross-entropy loss."
  },
  {
    "objectID": "posts/paddy/index.html#cross-entropy",
    "href": "posts/paddy/index.html#cross-entropy",
    "title": "Paddy doctor",
    "section": "Cross-Entropy",
    "text": "Cross-Entropy\nCross-Entropy loss is of course a loss function - a function that used to optimized the parameter of our model. It work even if our dependent variable has more than two categories, and results in faster and reliable training.\nLet’s look at the activation of our model, in fact let’s just look at one batch of our data\n\nx, y = dls.one_batch()\n\nIt returns the dependent and independent variables as mini-batch\n\ny\n\nTensorCategory([8, 9, 5, 8, 2, 1, 8, 7, 5, 8, 3, 6, 8, 3, 7, 4, 8, 3, 8, 6, 5,\n                6, 0, 5, 8, 8, 7, 5, 8, 9, 8, 8, 7, 7, 9, 4, 7, 3, 9, 7, 7, 5,\n                7, 9, 1, 7, 3, 4, 9, 6, 8, 7, 9, 5, 9, 7, 9, 5, 5, 9, 3, 3, 5,\n                8], device='cuda:0')\n\n\nWe got 64 rows as our batch size is 64, and we get the values ranging from 0 to 9, representing our 10 possible diseases, alright we can even view the predictions in fact it is the activations of the final layer of our neural network by using Learner.get_preds\n\npreds,_ = learn.get_preds(dl=[(x,y)])\npreds[0]\n\n\n\n\n\n\n\n\ntensor([0.2162, 0.0061, 0.0008, 0.0299, 0.0109, 0.0082, 0.0068, 0.0107, 0.5600,\n        0.1503])\n\n\nThe actual prediction are 10 probabilities between 0 and 1, which add up to 1.\n\nlen(preds[0]), preds[0].sum()\n\n(10, tensor(1.))\n\n\n\nSoftmax\nWhen a model runs, it’s last layer produces raw numbers, We call these activations, they are not probabilities yet, we need to change these raw number into probabilities, we want each number show how likely the model thinks each options is. So we use softmax activation in the final layer to ensure that the activations are all between 0 and 1. Softmax is similar to sigmoid function, below is what the sigmoid function look like in case you forget it\n\nplot_function(torch.sigmoid, min=-4,max=4)\n\n\n\n\n\n\n\n\nWhen you apply a sigmoid function to a single column of activation it turns those numbers to be between 0 and 1, it’s pretty useful activations for our final layer. But hold on and think about it, what if we have more activations than just a single column, let’s say we need to create a neural network that predict wether that image is a image of 3 or 7 that returns 2 activations (one for each number). Alright let’s create 6 images and 2 categories(the first columns is 3, and other is 7)\n\ntorch.random.manual_seed(42);\nacts = torch.randn((6,2))*2\nacts\n\ntensor([[ 0.6734,  0.2576],\n        [ 0.4689,  0.4607],\n        [-2.2457, -0.3727],\n        [ 4.4164, -1.2760],\n        [ 0.9233,  0.5347],\n        [ 1.0698,  1.6187]])\n\n\nWe can’t pass this to a sigmoid function directly cuz we can not get rows that add up to 1\n\nacts.sigmoid()\n\ntensor([[0.6623, 0.5641],\n        [0.6151, 0.6132],\n        [0.0957, 0.4079],\n        [0.9881, 0.2182],\n        [0.7157, 0.6306],\n        [0.7446, 0.8346]])\n\n\nWe need to use softmax. Here how we can represent the softmax function\ndef softmax(x): return exp(x) / exp(x).sum(dim=1, keepdim=True)\n\n\nMathematically, here the softmax formula\n\\[\ns(x_{i}) = \\frac {e^{x_i}} {\\sum _{j=1}^N e^{x_j}}\n\\]\n\nsm_acts = torch.softmax(acts, dim=1)\nsm_acts\n\ntensor([[0.6025, 0.3975],\n        [0.5021, 0.4979],\n        [0.1332, 0.8668],\n        [0.9966, 0.0034],\n        [0.5959, 0.4041],\n        [0.3661, 0.6339]])\n\n\nIn fact sofmax is the multi-category version of sigmoid, we need to use it anytime we have more that two categories and the probabilities must add up to 1\nYou might wondering why we have exponential the element here in the sofmax function, first of all, the obvious insight is that it helps to make the number to be positive, it also have a nice property: if one the numbers in our activations x is slightly bigger than the other the exponential will amplify it by make it closer to 1. That means the sofmax function really like to pick one class among others so that make sure that your each picture has definite labels\nSoftmax is just one part of the cross-entropy loss, we need to go through log likelihood\n\n\nLog likelihood\nIn binary case we use torch.where to select between inputs and 1-inputs\ndef mnis_loss(inputs, targets):\n  inputs = inputs.sigmoid()\n  return torch.where(targets==1, 1-inputs, inputs)\nLet’s try to do this using pytorch, first we need to generate our label for 3s and 7s\n\ntarg = tensor([0,1,0,1,1,0])\n\nThen each item of targ we can use that to select the appropriate column of sm_acts using tensor indexing, like this:\n\nidx = range(6)\nsm_acts[idx, targ]\n\ntensor([0.6025, 0.4979, 0.1332, 0.0034, 0.4041, 0.3661])\n\n\nPytorch provide a function which does just that (sm_acts[range(6), targ]) called nll_loss (NLL stands for negative log likelihood)\n\n-sm_acts[idx, targ]\n\ntensor([-0.6025, -0.4979, -0.1332, -0.0034, -0.4041, -0.3661])\n\n\n\nF.nll_loss(sm_acts, targ, reduction=\"none\")\n\ntensor([-0.6025, -0.4979, -0.1332, -0.0034, -0.4041, -0.3661])\n\n\nYou might wondering why we need the negative anyway? Well because we want to minimize the loss, the log likelihood of correct label should be maximized(closer to 0 is better, as \\(\\log(1)=0\\)). However optimization algorithms designed to minimize, not to maximize, by adding a negative sign we convert the maximization problem to minimization problem\nThe math behind it here is that let’s say if p is the probability of the correct class then the negative log likelihood is \\(-\\log(p)\\) as p approaches 1(perfect prediction), the \\(-\\log(p)\\) approaches 0(minimum loss), as p approaches 0(bad predictions) \\(-\\log(p)\\) approaches infinity (maximum loss). Blow are plots demonstrate why we need a negative sign here\n\nfig, (ax1,ax2) = plt.subplots(1, 2, figsize=(12,5))\nplt.sca(ax1)\nplot_function(torch.log, min=0, max=1, ty=\"log(x)\", tx='x')\nax1.set_title('log(x)')\nplt.sca(ax2)\nplot_function(lambda x: -1*torch.log(x), min=0, max=1, ty=\"- log(x)\", tx='x')\nax2.set_title('Negative Log Function')\n\nText(0.5, 1.0, 'Negative Log Function')\n\n\n\n\n\n\n\n\n\n\n\nI love Logs\nWhy is that? what if we have a very very small probabilities or even when working with multi-label classification7 it may involve the multiplication of many small numbers, as you may know it will lead to problems like numerical underflow8 in computers. we want to transform these probabilities to a large values so we can perform mathematical operation on them. And there is a mathematical function that will help us doing that: the logarithm as you can see in the image above\nNot stop there tho, we do want to ensure that our model is able to detect the differences between small numbers as our loss need to be sensitive enough to small changes in probabilities(especially when the model’s predictions are very wrong). Say, probabilities of 0.01 and 0.001 those number are very close together, but in probability, 0.001 is 10 times more confident compare to 0.01. Having said that, by taking the log out of our probabilities, we prevent these important different from being ignored.\nOne more thing that make log being amazing is this relationship:\n\\[\n\\log(a\\times b) = \\log(a) + \\log(b)\n\\]\nWhat this relationship tells us? Well this means the logarithm increases linearly when the argument increases exponentially or multiplicatively. Logarithms are awesome because when we do multiplication which can create really really large or really really small numbers, can be replaced by addition, which produce numbers that our computer can handle\nWe compute the loss on the column that contains the correct label, because there’s only one right answer per example, we don’t need to consider the others, because by the definition of softmax, the remain columns is indeed equal 1 minus the activation correspond to the correct label, then we’ll have a loss function that tells how well we are predicting each image. Therefore, making the activation of the correct label as high as possible will also decreasing the activations of the remaining columns\n\n\nNegative Log Likelihood\nThen what we do next? We will take the mean of negative log of our probabilities in other the word for each sample(image) we take the negative log of the predicted probability for the correct class as above this give us the loss for each individual sample we then calculate the mean of these individual losses across all samples, that give us the negative log likelihood or cross-entropy loss. One thing to note here that the Pytorch nll_loss assume that you already take the log of the softmax, even on it name have the word log but it dose not do it for you unfortunately\nSo that is cross-entropy loss in Pytorch, this is available as F.cross_entropy ofr nn.CrossEntropyLoss the F namespace version seems to be used more often by people\n\nloss_func = nn.CrossEntropyLoss()\n\n\nloss_func(acts, targ)\n\ntensor(1.8045)\n\n\n\nF.cross_entropy(acts, targ)\n\ntensor(1.8045)\n\n\nBy default PyTorch loss functions take the mean of the loss of all items. You can use reduction=‘none’ to disable that:\n\nnn.CrossEntropyLoss(reduction='none')(acts, targ)\n\ntensor([0.5067, 0.6973, 2.0160, 5.6958, 0.9062, 1.0048])\n\n\nLet’s look at the loss above those are numbers that the computer can learn from, but with us human, it is hard to look at those number and and tell how good our model is, so that why we need metrics, Those number are not used in the optimization process but just to help us poor human understand what’s going on. We can also use the confusion matrix to see where our model perform good and not\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix(figsize=(12,12), dpi=60)"
  },
  {
    "objectID": "posts/paddy/index.html#looking-for-a-learning-rate",
    "href": "posts/paddy/index.html#looking-for-a-learning-rate",
    "title": "Paddy doctor",
    "section": "Looking for a learning rate",
    "text": "Looking for a learning rate\nOne of the most important things we can do is to find the just right learning rate, if our learning rate is too high our optimizer will step too far from the minimal loss, and repeating this multiple time just make it jump around each side of the valley. Alright just make the learning rate really small to make sure it will never pass our minimal loss right? well of course it can take many epochs to train our model to go to the point, and it not only a waste of time but potentially causing overfitting because each epoch we go though our entire data one time, if we repeat it too much time we would give the computer a chance to memorize it.\nSo how can we find the perfect learning rate? not too low, not too high but just right? In 2015 the researcher Leslie Smith came up with a brilliant idea, called the learning rate finder. His idea was to start with a very, very small learning rate we use that for one mini-batch(not an epoch) and then look at the losses then increase the learning rate, say, x2, and we do it again and again until the loss get worse instead of better, then we know that we’ve gone too far it’s time to slower down by selecting a learning rate a bit smaller that the previous one.\nHere’s the advice from fastai:\n\nOne order of magnitude less than where the minimum loss was achieved (i.e., the minimum divided by 10)\nThe last point where the loss was clearly decreasing\n\n\nlearn = vision_learner(dls, 'resnet26d', metrics=error_rate, path='.')\nlearn.lr_find(suggest_funcs=(minimum, steep))\n\n\n\n\n\n\n\n\n/home/monarch/miniconda3/lib/python3.12/site-packages/fastai/learner.py:53: FutureWarning:\n\nYou are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n\n\nSuggestedLRs(minimum=0.010000000149011612, steep=0.0006918309954926372)\n\n\n\n\n\n\n\n\n\nWe can see in this plot that the range from 1e-6 to 1e-4 the model doesn’t seem to train at all, from 1e-3 the loss start to decrease until it reaches the minimum at 1e-1 then increasing rapidly, obviously we don’t want the learning rate after 1e-1 as i explained above. So choose 1e-1 then? While the loss minimum around the learning rate of 1e-1 it might give a good results, it’s right at the edge of stable as after this point there’s a sharp increase in the loos, in practice it often safer to choose a learning rate slightly lower than this threshold to ensure stability across different runs or datasets. Alright let’s choose 3e-3 to see\n\nlearn = vision_learner(dls, 'resnet26d', metrics=error_rate, path='.')\nlearn.fine_tune(3, base_lr=3e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.861936\n1.138522\n0.362326\n00:58\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.208978\n0.829224\n0.271985\n01:04\n\n\n1\n0.943550\n0.610545\n0.196060\n01:03\n\n\n2\n0.784365\n0.570067\n0.183085\n01:03\n\n\n\n\n\nAlright, we’ve got a pretty good learning rate, let’s look at how we can fine-tune the weights of a pretrained model."
  },
  {
    "objectID": "posts/paddy/index.html#transfer-learning",
    "href": "posts/paddy/index.html#transfer-learning",
    "title": "Paddy doctor",
    "section": "Transfer Learning",
    "text": "Transfer Learning\nWe’ve used transfer learning a lot in fact but, what it really is, and how it work? Pretrained model is trained on millions of data points(such as ImageNet) then it is fine-tuned for other tasks.\nWe now know that a convolutional neural network consists of many layers with a nonlinear activation between each pairs of layers, followed by one or several final linear layers with an activation function like sofmax at the very end. The final linear layer uses a matrix that has number of columns(which determine the size of the outputs) should match the number of classes in our classification problem. This final layers is useless for us when we fine-tuning it because the number of class is likely different, the specific categories it was trained on to identify are different. So we throw it away and replace it with new linear layer with the correct number of outputs for our specific task.\nWhen we add a new linear layer for our specific task it weights are indeed initialized randomly, despite that the entire pretrained model is not random at all, all the layers before the final layers still retain their pretrained weights, which encoded valuable information, such as finding gradient and edges, and later on layer can identify eyeballs, and fur. We want to train it in a way that make it still remember all of these generally useful ideas that it has trained on, and use that to solve our problem.\nSo our problem is replace the random weights in our added layers with weights that are correctly achieve our desire task without breaking the carefully pretrained weights. So what we can do is to tell the optimizer to only update the weights in those randomly added final layers, don’t change the weights in the rest of the neuron network at all in other word freezing those pretrained layers. When we use fine-tune fastai automatically freezes all the pretrained layers for us, in fact it does these two things:\n\nFirst it trains the randomly added layers for one epoch, with all other layers frozen\nThen it unfreezes all other layers and train them all with the number of epoch we tell it\n\nThat why when fine-tune we always have a table with one column above then the table with the number of epoch blow it.\nIn fact fine_tune first does fit_one_cycle then unfreeze and does fit_one_cycle again. Alright let’s do it manually this time\n\n\nlearn.fine_tune??\n\n\nSignature:\n\nlearn.fine_tune(\n\n    epochs,\n\n    base_lr=0.002,\n\n    freeze_epochs=1,\n\n    lr_mult=100,\n\n    pct_start=0.3,\n\n    div=5.0,\n\n    *,\n\n    lr_max=None,\n\n    div_final=100000.0,\n\n    wd=None,\n\n    moms=None,\n\n    cbs=None,\n\n    reset_opt=False,\n\n    start_epoch=0,\n\n)\n\nSource:   \n\n@patch\n\n@delegates(Learner.fit_one_cycle)\n\ndef fine_tune(self:Learner, epochs, base_lr=2e-3, freeze_epochs=1, lr_mult=100,\n\n              pct_start=0.3, div=5.0, **kwargs):\n\n    \"Fine tune with `Learner.freeze` for `freeze_epochs`, then with `Learner.unfreeze` for `epochs`, using discriminative LR.\"\n\n    self.freeze()\n\n    self.fit_one_cycle(freeze_epochs, slice(base_lr), pct_start=0.99, **kwargs)\n\n    base_lr /= 2\n\n    self.unfreeze()\n\n    self.fit_one_cycle(epochs, slice(base_lr/lr_mult, base_lr), pct_start=pct_start, div=div, **kwargs)\n\nFile:      ~/miniconda3/lib/python3.12/site-packages/fastai/callback/schedule.py\n\nType:      method\n\n\n\nlearn = vision_learner(dls, 'resnet26d', metrics=error_rate, path='.')\nlearn.fit_one_cycle(3, 3e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.797806\n1.022250\n0.333974\n01:01\n\n\n1\n1.111010\n0.680857\n0.217203\n01:02\n\n\n2\n0.841627\n0.611456\n0.190293\n00:59\n\n\n\n\n\nWe unfreeze the model\n\nlearn.unfreeze()\n\nWe also need to fine a new learning rate because we trained it with more layers, and weights that already train for 3 epochs means our previous founded learning rate isn’t appropriate anymore\n\nlearn.lr_find()\n\n\n\n\n\n\n\n\n/home/monarch/miniconda3/lib/python3.12/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state = torch.load(file, map_location=device, **torch_load_kwargs)\n\n\nSuggestedLRs(valley=6.30957365501672e-05)\n\n\n\n\n\n\n\n\n\nalright let’s pick 1e-4 this time\n\nlearn.fit_one_cycle(6, lr_max=1e-4)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.719057\n0.490316\n0.155694\n01:06\n\n\n1\n0.512388\n0.341793\n0.097549\n01:05\n\n\n2\n0.364478\n0.241446\n0.069678\n01:04\n\n\n3\n0.255537\n0.206497\n0.064392\n01:00\n\n\n4\n0.207048\n0.193223\n0.059106\n01:05\n\n\n5\n0.171621\n0.182925\n0.057184\n01:03\n\n\n\n\n\nWell it improve our model a lot from 0.18 to 0.06, in fact when i submit the predictions from the model we’ve built i got a pretty good result, it’s 0.93778, which improves a lot. Alright let’s see how our model train\n\nlearn.recorder.plot_loss()"
  },
  {
    "objectID": "posts/paddy/index.html#how-long-to-train",
    "href": "posts/paddy/index.html#how-long-to-train",
    "title": "Paddy doctor",
    "section": "How Long to Train?",
    "text": "How Long to Train?\nWell the first approach to train should be simply pick a number of epochs that will train in the amount of time that you’re happy to wait for, maybe take a cup of water, scrolling reddit, reading stuff,… then look at the training and validation loss plot like above and if you see it getting better when it comes to your final epochs, then you know that you should train it longer. In other hand you may see that the metrics you chosen really getting worse at the end of the training(remember that it’s not just that we’re looking for the validation loss to get worse, but the actual metrics). While the loss function is essential for optimization, what truly matters are your chosen practical metrics, don’t be overly concerned with validation loss inconstancy if your metric are still improving.\nIf you find you’ve trained for too long (your metrics getting worse, loss getting worse), what you should do is retrain your model from scratch, really, and this time choose the number of epochs based on where your previous best result was found. One more thing if you have extra time available instead of just simply increasing epochs consider using that time to train more parameters or use deeper architecture, this can potentially yield better results than extended training of a simpler model."
  },
  {
    "objectID": "posts/paddy/index.html#mixed-precision-training",
    "href": "posts/paddy/index.html#mixed-precision-training",
    "title": "Paddy doctor",
    "section": "Mixed Precision Training",
    "text": "Mixed Precision Training\nAlright let’s train it for one more time. This time notice i use to_fp16 here, it called mixed-precision training it used for speeding time up especially when we using a big architecture. I highly recommend you to read this post from NVIDIA.\nBut basically you first need to understand what is half-precision? Well it’s a floating-point number format uses 16 bits to represent numbers it called half-precision as the more common are 32 bits (single precision) and 64 bits (double precision).\n\n\n\nhalf float\n\n\nAs you can see we have one bit to represent sign 5 for exponent bits while 10 for fraction bits. For instance, between 1 and 2, it can only represents the number \\(1, 1+2^{-10}, 1+2\\ast2^{-10}\\),… which mean if we plus 1 with a number smaller than \\(2^{-10}\\)(approximately 0.0009765625) we will get 1 instead of a number slightly greater than 1. let’s say 1 + 0.0001 = 1 in half precision, that means it less precise than single or double precision, with only about 3 decimal digits of precision. So it helps reduce the memory usage by half compare to single precision, or we can double our batch, model size. Another very nice feature is that NVIDIA developed its latest GPUs (the Volta generation) to take fully advantage of half-precision tensors.\n\n\nWhen we talk about \\(2^{-10}\\), we’re referring to the smallest positive value that can be represented in the fraction part of the number. We have 10 bits to represent fraction part which mean it can represent \\(2^{10} = 1024\\) different values, these 1024 values are distributed eventually between 0 and 1. In fact, it’s the smallest step between these values so we divide the range(which is 1 here) by the possible value (\\(2^{10}\\)):\n\\[\n\\frac{1}{2^{10}} = 2^{-10}\n\\]\nBut there’re several problem with half precision when using it:\n\nThe weight update is imprecise: What your optimizer does under the hood is basically this equation w = w - lr * w.grad for each weights of your network. So the problem is the w.grad is several order of magnitude below w especially as the network starts to converge, these gradient often become very small as the network is making tiny and tiny adjustment, even smaller than \\(2^{-10}\\) is very common, so when using half precision, obviously the update doesn’t do anything here as FP16 can’t represent the tiny difference between w and (w - lr * w.grad).\nDuring the backpropagation of gradients, the gradients themselves become so small that they are rounded down to 0 in FP16.\nYour activation or loss can be overflow, the opposite problem from the gradients as during forward propagation, activation function like RElU or exponential function like softmax can produce a large values therefore it also make loss result in large numbers (especially in early training), it’s more easier to hit nan(of infinity) in FP16 precision and your training my more likely diverge.\n\nSo the solution for this is mixed precision training, instead of fully train in FP16 precision some of the operations will be done in FP16, others in FP32. The main idea is that we will do the forward pass and the gradient computation in half precision to go fast, but the update in single precision. So our training loop will look like this:\n\ncompute the output with FP16 model, and loss\nback-propagate the gradients in half-precision\nwe copy the gradient in FP32 precision\ndo the update on the master model (in FP32 precision)\ncopy the master model in the FP16 model\n\n\n\n\n\n\n\nNote\n\n\n\nNote that we will lose precision during step 5, and that the 1.0001 in one of the weights will go back to 1. But if the next update corresponds to add 0.0001 again, since the optimizer step is done on the master model, the 1.0001 will become 1.0002 and if we eventually go like this up to 1.0005, the FP16 model will be able to tell the difference.\n\n\nAlright that’s solve the first problem. For the second problem we use something call the gradient scaling. To avoid the gradient getting zeroed by FP16 precision we multiple the loss by the scale factor(often scale=512), by multiplying the loss with a large numbers all the gradients are effectively made larger. Of course we don’t want those 512-scaled gradients to be in the weight updates so that after converting them into FP32 we can divide them by this scale. So it change the loop to:\n\n\nIn fact the scaling factor that we multiply it with the loss can leads our gradients or our loss to be overflow. So there a way around this, which is just simple as this, first we will try with a very high scale factor and see if it cause overflow to our loss or gradients, if it does, we will try with the half of that big value and again until we get the largest loss scale possible that doesn’t make our gradient overflow.\n\ncompute the output with the FP16 model, then the loss\nmultiply the loss by scale and then back-propagate the gradients in half precision\ncopy the gradients in FP32 precision then divide them by the scale\ndo the update on the master model\ncopy the master model in FP16 model\n\nFor the last problem, the tricks offered by NVIDIA are to leave the batchnorm layers in single precision (they don’t have many weights so it’s not a big memory challenge) and compute the loss in single precision (which means converting the last output of the model in single precision before passing it to the loss).\nAlright let’s apply our mixed precision strategy to our model, when we create a learner we call to_fp16().\n\nlearn = vision_learner(dls, 'resnet26d', metrics=error_rate, path='.').to_fp16()\nlearn.lr_find(suggest_funcs=[valley, slide])\n\n\n\n\n\n\n\n\n/usr/local/lib/python3.10/dist-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state = torch.load(file, map_location=device, **torch_load_kwargs)\n\n\nSuggestedLRs(valley=0.0012022644514217973, slide=0.002511886414140463)\n\n\n\n\n\n\n\n\n\n\nlearn.fine_tune(14, 1e-2, freeze_epochs=3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.907815\n1.100638\n0.353676\n00:41\n\n\n1\n1.442608\n1.050639\n0.346468\n00:41\n\n\n2\n1.303783\n0.873419\n0.289284\n00:42\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.892789\n0.587087\n0.187410\n00:42\n\n\n1\n0.721903\n0.487567\n0.159058\n00:41\n\n\n2\n0.625562\n0.414129\n0.140317\n00:42\n\n\n3\n0.559306\n0.321464\n0.098510\n00:41\n\n\n4\n0.437455\n0.248258\n0.069678\n00:41\n\n\n5\n0.332489\n0.218854\n0.059106\n00:42\n\n\n6\n0.265161\n0.221709\n0.059106\n00:41\n\n\n7\n0.205299\n0.193941\n0.052859\n00:41\n\n\n8\n0.189015\n0.177993\n0.044690\n00:42\n\n\n9\n0.141942\n0.173127\n0.044690\n00:42\n\n\n10\n0.137524\n0.150655\n0.041326\n00:41\n\n\n11\n0.111285\n0.146560\n0.034118\n00:41\n\n\n12\n0.106115\n0.145193\n0.034599\n00:42\n\n\n13\n0.105035\n0.143570\n0.033157\n00:41\n\n\n\n\n\nAlright let’s see if it’s better, the best way to check is to submit it to kaggle we just do the same thing as above, just copy and past it here.\n\nss = pd.read_csv(path/'sample_submission.csv')\ntst_files = get_image_files(path/'test_images').sorted()\ntst_dl = dls.test_dl(tst_files)\nprobs,_,idxs = learn.get_preds(dl=tst_dl, with_decoded=True)\nmapping = dict(enumerate(dls.vocab))\nresults = pd.Series(idxs.numpy(), name=\"idxs\").map(mapping)\nss['label'] = results\nss.to_csv('subm.csv', index=False)\n!head subm.csv\n\n/usr/local/lib/python3.10/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.10/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n\n\n\n\n\n\n\n\n\nimage_id,label\n200001.jpg,hispa\n200002.jpg,normal\n200003.jpg,blast\n200004.jpg,blast\n200005.jpg,blast\n200006.jpg,brown_spot\n200007.jpg,dead_heart\n200008.jpg,brown_spot\n200009.jpg,hispa\n\n\nThis time i got a little higher result, i got 0.95967 as score which is pretty understandable as we trained it with 14 epochs, but yeah that was pretty cool."
  },
  {
    "objectID": "posts/paddy/index.html#scaling-up",
    "href": "posts/paddy/index.html#scaling-up",
    "title": "Paddy doctor",
    "section": "Scaling up",
    "text": "Scaling up\nAlright, can we do better, let’s see how far we can go, let’s do some experimenting, we will use different architectures and image processing approaches, for the sake of convenient let’s put our steps together into a little function.\n\n\n\n\n\n\nNote\n\n\n\nNote that we can use ImageDataLoader.from_folder for our dataloader for make it shorter, but in general it the same as DataBlock\n\n\n\ndef train(arch, item, batch, epochs=5):\n  dls = ImageDataLoaders.from_folder(train_path, seed=42, valid_pct=0.2, item_tfms=item, batch_tfms=batch)\n  learn = vision_learner(dls, arch, metrics=error_rate).to_fp16()\n  learn.fine_tune(epochs, 1e-2)\n  return learn\n\nTo have a better result, one way to archive this is to use a better model right, but what to choose, well Jeremy Howard has a really good notebook that helps us choosing a appropriate architecture for our model based on what kind of problem you’re dealing, the GPU memory, error rate, time,… But basically there are two key to choose the right one:\n\nHow similar between our dataset and the pretrained model’s dataset.\nHow large they are.\n\nThen it turned out that when it comes to computer vision model convnext model is one of the best, if not the best till now so let’s give it a try shall we?\n\narch = \"convnext_small_in22k\"\n\nFrom now on, if you not sure what architecture to use, just use this, right. And of course we have different version of convnext we have tinny, small, large… it will take more time to train but of course lower error rate. alright let’s see how it will go\n\nlearn = train(arch, item=Resize(192,method=\"squish\"), batch=aug_transforms(size=128, min_scale=0.75))\n\n/usr/local/lib/python3.10/dist-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name convnext_small_in22k to current convnext_small.fb_in22k.\n  model = create_fn(\n\n\n\n\n\n/usr/local/lib/python3.10/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.10/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.257208\n0.790532\n0.246997\n00:46\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.610890\n0.450510\n0.146564\n00:44\n\n\n1\n0.504428\n0.302570\n0.097069\n00:44\n\n\n2\n0.299630\n0.194396\n0.061989\n00:44\n\n\n3\n0.186308\n0.130406\n0.036521\n00:44\n\n\n4\n0.134839\n0.115092\n0.035079\n00:43\n\n\n\n\n\nWell it did a pretty good job isn’t it? we just do 5 epochs and we archive almost the same as the one we trained with 12 epochs, even our time to go through each epoch is the same(that’s because i reduce the presize to 192X192 for it to run faster but it still produce the same performance as the previous one but with fewer epochs).\nSo one thing we could try is instead of using squish as our pre-processing let’s try using padding, now we will use bigger presize so that when we use padding here we will get entire image but the downside is we also get few extra zero pixels which literally pointless, but whatever let’s see if it work better\n\n\npadding is interesting because it’s the only way of pre-processing images which doesn’t distort them and doesn’t loose anything, if you crop you lose things, if you squish you distort things\n\ndls = ImageDataLoaders.from_folder(train_path, valid_pct=0.2, seed=42,\n    item_tfms=Resize(480, method=ResizeMethod.Pad, pad_mode=PadMode.Zeros))\ndls.show_batch(max_n=3)\n\n\n\n\n\n\n\n\n\nlearn = train(arch, item=Resize((480, 360), method=ResizeMethod.Pad, pad_mode=PadMode.Zeros), batch=aug_transforms(size=(256,192), min_scale=0.75))\n\n/usr/local/lib/python3.10/dist-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name convnext_small_in22k to current convnext_small.fb_in22k.\n  model = create_fn(\n\n\n\n\n\n/usr/local/lib/python3.10/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.10/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.091136\n0.672584\n0.214320\n01:12\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.579350\n0.418952\n0.131187\n01:36\n\n\n1\n0.430034\n0.259760\n0.086497\n01:37\n\n\n2\n0.275291\n0.176225\n0.046612\n01:38\n\n\n3\n0.147642\n0.123821\n0.037963\n01:38\n\n\n4\n0.109251\n0.107270\n0.030274\n01:38\n\n\n\n\n\nAs you can see it indeed did better, and its error_rate is the best we can get so far but not huge different yet!\n\nTest Time Augmentation\nWell first let’s look how can we calculate the error rate manually with our normal prediction\n\nvalid = learn.dls.valid\npreds,targs = learn.get_preds(dl=valid)\nerror_rate(preds, targs)\n\n\n\n\n\n\n\n\nTensorBase(0.0303)\n\n\nWell that actually the previous error-rate we got above, so what i’m doing here? well let’s take a look at the images blow\n\nlearn.dls.train.show_batch(max_n=6, unique=True)\n\n\n\n\n\n\n\n\nNotice that, those are indeed the same picture but it gone through the data augmentation so sometimes it a bit darker, a bit lighter, sometimes it flipped horizontally, some times int zoom into a slightly different section, sometimes it rotate a little bit but those are all the same picture. So the idea of TTA(Test Time Augmentation) is maybe our model would like some of these version better than the others even the original image, so what we can do is we can pass all of these to our model get the prediction of all of them and take the average right, so if you read my previous blog, it’s indeed the mini version of bagging approach. In fastai you can archive this by using the tta in our learn object\n\ntta_preds,_ = learn.tta(dl=valid)\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\n\nerror_rate(tta_preds, targs)\n\nTensorBase(0.0259)\n\n\nSee, we got better result like 10% better our previous. Alright let’s train it with more epochs but this time let’s just make a bigger image and something really interesting is that our images don’t have to be square they just need to be in the same size right, it can be rectangular, having said that all of our original images are nearly 640x480, so we just need to pick one has the same aspect ratio for example 256x192 is good\n\nlearn = train(arch, epochs=12, item=Resize((480, 360), method=ResizeMethod.Pad, pad_mode=PadMode.Zeros), batch=aug_transforms(size=(256,192), min_scale=0.75))\n\n/usr/local/lib/python3.10/dist-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name convnext_small_in22k to current convnext_small.fb_in22k.\n  model = create_fn(\n\n\n\n\n\n/usr/local/lib/python3.10/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.10/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.091136\n0.672584\n0.214320\n01:09\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.534227\n0.295779\n0.100432\n01:25\n\n\n1\n0.406855\n0.259834\n0.077847\n01:26\n\n\n2\n0.345489\n0.217263\n0.065353\n01:26\n\n\n3\n0.272754\n0.186886\n0.056704\n01:26\n\n\n4\n0.220682\n0.205609\n0.054781\n01:27\n\n\n5\n0.182489\n0.122831\n0.037001\n01:27\n\n\n6\n0.118704\n0.119720\n0.035560\n01:26\n\n\n7\n0.099475\n0.117059\n0.034118\n01:26\n\n\n8\n0.071605\n0.094223\n0.025949\n01:26\n\n\n9\n0.055326\n0.096391\n0.025949\n01:26\n\n\n10\n0.037327\n0.096320\n0.024507\n01:26\n\n\n11\n0.035568\n0.094268\n0.023066\n01:27\n\n\n\n\n\nAlright our error_rate down to 2.3% which is pretty good, now, let’s what our error_rate when using tta\n\ntta_preds,targs = learn.tta(dl=learn.dls.valid)\nerror_rate(tta_preds, targs)\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\nTensorBase(0.0235)\n\n\nOops! it’s worse this time, that’s strange, i think it won’t always produce a better error_rate, but maybe it will work well in practice i guess, alright forget about it, let’s submit it\n\nss = pd.read_csv(path/'sample_submission.csv')\ntst_files = get_image_files(path/'test_images').sorted()\ntst_dl = learn.dls.test_dl(tst_files)\npreds,_ = learn.tta(dl=tst_dl)\nidxs = preds.argmax(dim=1)\nvocab = np.array(learn.dls.vocab)\nresults = pd.Series(vocab[idxs], name=\"idxs\")\nss['label'] = results\nss.to_csv('subm.csv', index=False)\n!head subm.csv\n\n/usr/local/lib/python3.10/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.10/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\nimage_id,label\n200001.jpg,hispa\n200002.jpg,normal\n200003.jpg,blast\n200004.jpg,blast\n200005.jpg,blast\n200006.jpg,brown_spot\n200007.jpg,dead_heart\n200008.jpg,brown_spot\n200009.jpg,hispa\n\n\nThis time i got a little bit higher result, around 0.98 which is quite impressive. So we’ve gone through all of the essential concepts that we need to get familiar with as later we will delve deeper and deeper into more amazing thing later on, understand these concepts is like a solid groundwork for us to exploring even more fascinating topics in the future."
  },
  {
    "objectID": "posts/paddy/index.html#footnotes",
    "href": "posts/paddy/index.html#footnotes",
    "title": "Paddy doctor",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nInterpolation in image processing is the technique of estimating new pixel values when resizing or transforming a digital image. It’s like filling in the blanks between known pixels to create a smooth transition when changing an image’s size, rotation or shape↩︎\nWhile the crop is indeed random, but it’s not entirely arbitrary. The goal is to create diversity in the training data, which helps the model learn to recognize objects and features from various perspectives and scales. In practice, it often use multiple crops from a single image during training, this increase the chances of capturing important features at least some of the crops.↩︎\na resize method which resizes the image to fit within the target dimensions and adds padding (usually black) to fill the rest, it keeps the original aspect ratio and all image information, however it can lead to artificial background which might affect model performance. Use it when you have to keep the entire image and the its aspect ratio is important and of course be a wear of extra background↩︎\na resize method which resizes the image to fit the target dimensions, potentially distorting the aspect ratio. it helps preserves all information in the image, but at the same time it can distort the image, potentially altering important features. Use it when the aspect ratio is not crucial for your task, or when your imagees are already mostly square↩︎\nResizes the image and then crops it to fit the target dimensions. Help maintains aspect ratio of the visible part and doesn’t introduce distortion, however it may lose important information at the edges of the image. Use it when the main subject is typically centered in your images, or when edge information is less important.↩︎\nIt will attempt to create a batch from the source you give it, with a lot of details. Also, if it fails, you will see exactly at which point the error happens, and the library will try to give you some help. For instance, one common mistake is to forget to use a Resize transform, so you end up with pictures of different sizes and are not able to batch them.↩︎\nin multi label classification, each instance can belong to multiple classes simultaneously, imagine working on a dog cat classification, where an image could contain both dog and cat at the same time so in this problem it requires us to do multiplications on probabilities which will lead to numerical underflow problem in computer science↩︎\nNumber underflow occurs when a computation results in a number too small for computer to represent accurately, often leading to be rounded to 0↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dai’s blog",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n\n\n\n\n\n\n\n\n\n\nCNN architectures\n\n\n\nDeep Learning\n\nCNN\n\n\n\nHow do we build CNNs? This isn’t just about stacking layers arbitrarily. It involves a nuanced understanding of the various types of layers in CNNs: convolution, pooling, fully connected, and others. Once we’ve designed an architecture, the training process is, of course, where the learning occurs. It’s where the network adapts its parameters to minimize the loss function on the training data.\n\n\n\n\n\nAug 10, 2025\n\n\nBui Huu Dai\n\n57 min\n\n\n\n\n\n\n\n\n\n\n\nNeural network and backpropagation\n\n\n\nDeep Learning\n\nBackpropagation\n\nNeural Networks\n\n\n\n\n\n\n\n\n\nAug 8, 2025\n\n\nBui Huu Dai\n\n49 min\n\n\n\n\n\n\n\n\n\n\n\nRegularization and Optimization\n\n\n\nDeep Learning\n\nOptimization\n\n\n\nRegularization and optimization are the two absolute critical concepts in deep learning that allow our model to learn effectively and generalize well to new data\n\n\n\n\n\nAug 5, 2025\n\n\nBui Huu Dai\n\n41 min\n\n\n\n\n\n\n\n\n\n\n\nImage classification with linear classifiers\n\n\n\nDeep Learning\n\n\n\n\n\n\n\n\n\nAug 2, 2025\n\n\nBui Huu Dai\n\n27 min\n\n\n\n\n\n\n\n\n\n\n\nA brief history of computer vision and deep learning\n\n\n\nComputer Vision\n\nDeep Learning\n\n\n\nFrom early image processing to the rise of neural networks, now we will look back at the evolution of computer vision and deep learning.\n\n\n\n\n\nJul 13, 2025\n\n\nBui Huu Dai\n\n55 min\n\n\n\n\n\n\n\n\n\n\n\nPaddy doctor\n\n\n\nkaggle\n\ncompetition\n\ndeep learning\n\n\n\n\n\n\n\n\n\nDec 30, 2024\n\n\nBui Huu Dai\n\n31 min\n\n\n\n\n\n\n\n\n\n\n\nRandom Forest, Bagging, Boosting\n\n\n\nkaggle\n\ncompetition\n\nrandom forest\n\nbagging\n\nboosting\n\n\n\n\n\n\n\n\n\nNov 10, 2024\n\n\nBui Huu Dai\n\n39 min\n\n\n\n\n\n\n\n\n\n\n\nLooking inside neural networks\n\n\n\nblogging\n\nfastai\n\ntorch\n\n\n\n\n\n\n\n\n\nJul 28, 2024\n\n\nBui Huu Dai\n\n19 min\n\n\n\n\nNo matching items"
  }
]
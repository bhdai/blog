[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to my blog! To learn more about me, check out my website here."
  },
  {
    "objectID": "posts/first-post/index.html",
    "href": "posts/first-post/index.html",
    "title": "My First Blog Post",
    "section": "",
    "text": "image.png"
  },
  {
    "objectID": "posts/first-post/index.html#why-i-started-this-blog",
    "href": "posts/first-post/index.html#why-i-started-this-blog",
    "title": "My First Blog Post",
    "section": "Why I Started This Blog",
    "text": "Why I Started This Blog\nI decided to create this blog for a few reasons:\n\nTo document my learning journey\nTo share interesting insights and ideas\nTo connect with like-minded individuals"
  },
  {
    "objectID": "posts/first-post/index.html#what-to-expect",
    "href": "posts/first-post/index.html#what-to-expect",
    "title": "My First Blog Post",
    "section": "What to Expect",
    "text": "What to Expect\nIn this blog, you can expect to find:\n\nTutorials and guides on various topics\nPersonal reflections and experiences\nInteresting projects and experiments\n\nStay tuned for more content coming soon!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dai’s blog",
    "section": "",
    "text": "From Notebook to Web App: Deploying Your Models with fastai Lesson 2\n\n\n\n\n\n\nblogging\n\n\nfastai\n\n\nhuggingface spaces\n\n\ngradio\n\n\n\n\n\n\n\n\n\nJul 7, 2024\n\n\nBui Huu Dai\n\n\n\n\n\n\n\n\n\n\n\n\nFirst Step in AI: My Experience with fast.ai Lesson 1\n\n\n\n\n\n\nblogging\n\n\nfastai\n\n\n\n\n\n\n\n\n\nJul 4, 2024\n\n\nBui Huu Dai\n\n\n\n\n\n\n\n\n\n\n\n\nMy First Blog Post\n\n\n\n\n\n\ntest\n\n\nquarto\n\n\n\n\n\n\n\n\n\nJun 30, 2024\n\n\nBui Huu Dai\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024-07-07-from-notebook-to-web-app/index.html",
    "href": "posts/2024-07-07-from-notebook-to-web-app/index.html",
    "title": "From Notebook to Web App: Deploying Your Models with fastai Lesson 2",
    "section": "",
    "text": "Welcome back to our deep learning adventure with fastai! In Lesson 2, we dive into the exciting world of putting model into production. Whether you’re a beginner looking to get your feet wet or an experienced practitioner wanting to brush up on your deployment skills, this lesson is packed with practial tips and hands-on technquies to take your models from the notebook to the real world.\nIn this blog post, we’ll cover everything from gathering images to training and deploying models, using tools like Jupyter Notebooks, Gradio, and Hugging Face Spaces. Get ready to explore essential concepts like how to clean your data and see how different deployment platforms stack up against each other.\nBuckle up and let’s get started on this journey and bring your deep learning models to life!"
  },
  {
    "objectID": "posts/2024-07-07-from-notebook-to-web-app/index.html#introduction",
    "href": "posts/2024-07-07-from-notebook-to-web-app/index.html#introduction",
    "title": "From Notebook to Web App: Deploying Your Models with fastai Lesson 2",
    "section": "",
    "text": "Welcome back to our deep learning adventure with fastai! In Lesson 2, we dive into the exciting world of putting model into production. Whether you’re a beginner looking to get your feet wet or an experienced practitioner wanting to brush up on your deployment skills, this lesson is packed with practial tips and hands-on technquies to take your models from the notebook to the real world.\nIn this blog post, we’ll cover everything from gathering images to training and deploying models, using tools like Jupyter Notebooks, Gradio, and Hugging Face Spaces. Get ready to explore essential concepts like how to clean your data and see how different deployment platforms stack up against each other.\nBuckle up and let’s get started on this journey and bring your deep learning models to life!"
  },
  {
    "objectID": "posts/2024-07-07-from-notebook-to-web-app/index.html#gathering-and-cleaning-data",
    "href": "posts/2024-07-07-from-notebook-to-web-app/index.html#gathering-and-cleaning-data",
    "title": "From Notebook to Web App: Deploying Your Models with fastai Lesson 2",
    "section": "Gathering and Cleaning Data",
    "text": "Gathering and Cleaning Data\nIn this section we’ll walk through the process of gathering and cleaning data, leveraging some handy tools and methods introduced in Lesson 2.\n\nImporting and Setting Up\nFirst ensure that you have all necessary libraries and modules in place. If you haven’t already, run the following command to install the fastbook module:\nconda install -y -c fastai fastbook\nNow you can import the required functions from fastbook:\n\nfrom fastbook import *\nfrom fastai.vision.widgets import *\n\n\n\nGathering Images with DuckDuckGo\nUsing DuckDuckGo(ddg) for image searches simplifies the process, as it doesn’t require an API key. Here’s the code to create our dataset of bear images:\n\nbear_types = 'grizzly', 'black', 'teddy'\npath = Path('bear')\nif not path.exists():\n    path.mkdir()\n    for o in bear_types:\n        dest = (path/o)\n        dest.mkdir(exist_ok=True)\n        results = search_images_ddg(f\"{o} bear\")\n        download_images(dest, urls=results)\n\nThis code snippet sets up directories for different bear types and download images into respective folders.\nNext, we verify and clean the downloaded images:\n\nfailed = verify_images(get_image_files(path))\nfailed\n\n(#24) [Path('bear/black/b8d71ddf-a84d-4054-8088-bb08e8cbd814.jpg'),Path('bear/black/bbe19bf5-3d28-4d7e-b8bf-3e7f8afff5af.jpg'),Path('bear/teddy/ee833f9f-ff26-4435-a4cc-89208236c442.jpg'),Path('bear/teddy/f6f4f901-c873-46a6-8ef7-bbd65da2c910.jpg'),Path('bear/teddy/e3c4fc0d-e494-4c8a-9dc5-8510f8407cac.jpg'),Path('bear/teddy/3309df18-2a7f-4d67-b2e8-9f08eea06025.jpg'),Path('bear/teddy/99377b04-a870-4798-9e6a-02543a495395.JPG'),Path('bear/teddy/fbf4430d-0643-444b-a0c5-7d13c43d92b6.jpg'),Path('bear/teddy/4c576f46-fa7e-4800-bb89-9eea7662ab10.jpg'),Path('bear/teddy/50d2e017-49b9-49a1-9538-5e402932e463.jpg')...]\n\n\n\nfailed.map(Path.unlink);\n\nThis step ensures that any currupt images are identified and removed.\n\n\nStructuring Data with DataBlock API\nWe use DataBlock API to structure our data, making it ready for training:\n\nbears = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=Resize(128)\n)\n\ndls = bears.dataloaders(path)\ndls.valid.show_batch(max_n=4, nrows=1)\n\n\n\n\n\n\n\n\nThis show a batch of images using the default resizing method. Different resizing startegies can impact the dataset in various ways."
  },
  {
    "objectID": "posts/2024-07-07-from-notebook-to-web-app/index.html#exploring-resizing-methods",
    "href": "posts/2024-07-07-from-notebook-to-web-app/index.html#exploring-resizing-methods",
    "title": "From Notebook to Web App: Deploying Your Models with fastai Lesson 2",
    "section": "Exploring Resizing Methods",
    "text": "Exploring Resizing Methods\nResizing plays a crucial role in preparing your images for model training. Let’s explore three different resizing methods:\n\nStandard Resize\nThe standard resize method adjust the image size for model while maintaining a specific aspect rato. Here, we pad the images with zeros(black) to ensure the entire image is included:\n\nbears = bears.new(item_tfms=Resize(128, ResizeMethod.Pad, pad_mode='zeros'))\ndls = bears.dataloaders(path)\ndls.valid.show_batch(max_n=4, nrows=1)\n\n\n\n\n\n\n\n\nThis approach, padding with zeros maintains the aspect rato and ensures that the entire image fit within the frame.\n\n\nRandomResizedCrop\nAnother effective method is RandomResizedCrop, which crops different parts of an image each time, providing varied views:\n\nbears = bears.new(item_tfms=RandomResizedCrop(128, min_scale=0.3))\ndls = bears.dataloaders(path)\ndls.train.show_batch(max_n=4, nrows=1, unique=True)\n\n\n\n\n\n\n\n\nRandomResizedCrop is excellent for generating diverse training data. It explores different regions of the same image, enhancing the robustless of your model."
  },
  {
    "objectID": "posts/2024-07-07-from-notebook-to-web-app/index.html#applying-data-augmentation",
    "href": "posts/2024-07-07-from-notebook-to-web-app/index.html#applying-data-augmentation",
    "title": "From Notebook to Web App: Deploying Your Models with fastai Lesson 2",
    "section": "Applying Data Augmentation",
    "text": "Applying Data Augmentation\nData Augmentation increases the diversity of your training data by applying various transformations, such as rotation and flipping:\n\nbears = bears.new(\n    item_tfms=Resize(128),\n    batch_tfms=aug_transforms(mult=2)\n)\ndls = bears.dataloaders(path)\ndls.train.show_batch(max_n=8, nrows=2, unique=True)\n\n\n\n\n\n\n\n\nUsing aug_transforms, we can dynamically modify images during training. This process, called data augmentation, helps the model generalize better by exposing it to various versions of the same iamge. The mult=2 parameter exaggerates the transformations for better visualization."
  },
  {
    "objectID": "posts/2024-07-07-from-notebook-to-web-app/index.html#traning-the-model",
    "href": "posts/2024-07-07-from-notebook-to-web-app/index.html#traning-the-model",
    "title": "From Notebook to Web App: Deploying Your Models with fastai Lesson 2",
    "section": "Traning the Model",
    "text": "Traning the Model\nWith our data ready, we can proceed to train a model using a pre-trained resnet18:\n\nbears = bears.new(\n    item_tfms=RandomResizedCrop(224, min_scale=0.5),\n    batch_tfms=aug_transforms()\n)\ndls = bears.dataloaders(path)\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(4)\n\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00&lt;00:00, 131MB/s] \n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.010127\n0.147814\n0.066038\n00:15\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.187663\n0.105568\n0.037736\n00:13\n\n\n1\n0.165576\n0.146279\n0.037736\n00:13\n\n\n2\n0.140621\n0.171902\n0.047170\n00:14\n\n\n3\n0.117243\n0.166212\n0.047170\n00:13\n\n\n\n\n\nIn this case we use RandomSizedCrop and aug_transforms to create robust data loaders. Traning a model for four epochs results in an error rate of under five percent-quite impressive!"
  },
  {
    "objectID": "posts/2024-07-07-from-notebook-to-web-app/index.html#evaludating-the-model",
    "href": "posts/2024-07-07-from-notebook-to-web-app/index.html#evaludating-the-model",
    "title": "From Notebook to Web App: Deploying Your Models with fastai Lesson 2",
    "section": "Evaludating the Model",
    "text": "Evaludating the Model\nEvaluating the trained model is crucial for understanding its performance and identifying areas for improvement. Lesson 2 introduces several important techniques for model evaluation.\n\nConfusion Matrix Explaination\nThe confusion matrix is powerful tool for examining the perfomance of classification models. It provide insights into which catetgories are commonly confused by the model:\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe confusion matrix shows the model’s predictions against the actual labels. Here’s what it tells us:\n\nThe diagonal represents correct predictions (e.g., 29 black bears correctly predicted as black bears).\nOff-diagonal elements reveal misclassifications (e.g., 2 black bears predicted as grizzly bears)\n\nFor instance, if our bear classifier mislabels a grizzly bear as a black bear, the coressponding cell in the matrix indicates how often this mistaken occurs. It’s a visual representation of “where did we go wrong?” and is essential for refining the model.\nJeremy pointed out that such insight help:\n\nIdentify which categories are inherently difficult to distinguish(e.g., black bears and grizzly bears).\nUnderstand if certain errors systematics and need targeted improvements.\n\nHere’s my model shows:\n\nHigh accuracy in identifying grizzly bears and teddy bears: There are only a couple of misclassification.\nSome confusion between black bears and grizzly bears: This is evident from the few off-diagonal elements\n\nUnderstand these errors hepls us focus on areas that need more training data or better distigushing features.\n\n\nplot_top_losses Explaination\nThe plot_top_losses function highlights the individual images where your model made the worst predictions:\n\ninterp.plot_top_losses(5, nrows=1, figsize=(17,4))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe results show specific cases where the model was either:\n\nHighly confident but wrong: E.g., the model predicted “teddy” with high confidence when it actually “black”.\nCorrect but not confident: E.g., predicting the right class but with low confidence.\n\nBy examining these top losses, you gain insights into why the model might be confused. Here, my model misclassified a black bear as a grizzly bear with high confidence which might indicate that the features used to distigush between these classes are not prominent enough.\nThese insights can help in refining your data and possibly augmenting it to address these specific weaknesses."
  },
  {
    "objectID": "posts/2024-07-07-from-notebook-to-web-app/index.html#clean-the-data-with-imageclassifiercleaner",
    "href": "posts/2024-07-07-from-notebook-to-web-app/index.html#clean-the-data-with-imageclassifiercleaner",
    "title": "From Notebook to Web App: Deploying Your Models with fastai Lesson 2",
    "section": "Clean the Data with ImageClassifierCleaner",
    "text": "Clean the Data with ImageClassifierCleaner\nOnce we’ve evaluated our model, the next important step is data cleaning. Surprisingly, Jeremy suggests cleaning the data after training the initial model. This counterintuitive approach allows the model to highlight problematic data points\n\nImageClassifierCleaner Demonstration\nThe ImageClassifierCleaner widget is a fantastic tool for this purpose. It helps you manually review and clean your dataset based on the model’s predictions:\n\ncleaner = ImageClassifierCleaner(learn)\ncleaner\n\n\nWhen you run this widget, it launches an interactive interface where you can:\n\nSort Image by Loss: Images are ordered by the model’s confidence, making it easy to identify incorrect or ambiguous labels.\nCorrect Labels: Reassign images to the correct categories if they were mislabeled.\nDelete Incorrect Images: Remove images that don’t belong in any category.\n\nJeremy explained how he used it to clean the bear dataset:\n\nBy seleting “teddy bears”, the widget displayed all images classified as teddy bears.\nHe manually review the images, reassigning or deleting those that were incorrecly labeled.\n\nHere’s how you can apply the changes:\n\nfns = get_image_files(path)\nfor idx in cleaner.delete(): cleaner.fns[idx].unlink()\nfor idx, cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat)\n\nThis code snippet updates the dataset based on your interaction with the ImageClassifierCleaner: - Delete: Removes files marked for deletion. - Move: Reassigns files to the correct categories.\n\n\nWhy Clean After Training?\nCleaning the data after training might seems backward, but it has significant advantages:\n\nModel-Assisted Cleaning: The initial model helps identify problematic data points that might be hard to spot manually.\nFocus on Hard Cases: The confusion matrix and top losses highlight the hight areas that need most attention, making your cleaning effort more efficient.\n\nThis process ensures a high quality dataset for subsequent training iterations, leading to better model performance.\nBy incorporating thorough evaluation and cleaning steps, you refine you dataset and improve your model’s accuracy and reliability. These insights are invaluable for building robust deep learning models that perform well on real-world data."
  },
  {
    "objectID": "posts/2024-07-07-from-notebook-to-web-app/index.html#deployment-building-and-deploying-a-model",
    "href": "posts/2024-07-07-from-notebook-to-web-app/index.html#deployment-building-and-deploying-a-model",
    "title": "From Notebook to Web App: Deploying Your Models with fastai Lesson 2",
    "section": "Deployment: Building and Deploying a Model",
    "text": "Deployment: Building and Deploying a Model\nAfter cleaning our data, the next exciting step is to put our model into production. While the book introduces Voilà for creating interactive web application using Jupyter Notebooks, there’s another powerful tool that’s becoming increasingly popular: HuggingFace Spaces. Together with Gradio, they offer an intuitive and powerful way to deploy machine learning models as web applications.\n\nIntroducing HuggingFace Spaces and Gradio.\nHuggingFace Spaces is a platform that allows you to host machine learning model and their interfaces for free. On the other hand, Gradio make it easy to create customizable web interfaces with a few lines of Python code.\n\n\nA shortout of Tanishq Abraham\nBefore diving into techinical details, let’s give a shortout to Tanishq Abraham, one of the most remarkable individuals in the fastai community. Known as a child prodigy, Tanishq has contributed immensely to the community, making complex topics accessable to everyone. I’ve learned a lot from his work and highly recommend checking out his website and his Twitter for more insightful resources.\nTanshiq has also written an exellent blog post the cover everything you need to know about using Gradio and HuggingFace Spaces. You can read his detail guide here\n\n\nSetting Up and Deploying Your Model using Gradio and HuggingFace Spaces\nTo deploy our model, we’ll use HuggingFace Spaces. The set up process is straightforward and free of charge. Follow these step to get started:\nStep 1: Sign Up and Create a New Space\n\nGo to the HuggingFace Spaces page and sign up for an account if you haven’t already.\nClick “Create a new space”.\nGive you space a name and chose a template (you can start with Gradio template).\n\nCongrats! You’ve created a new space. Now, what’s next?\nStep 2: Getting Familiar with Git\nHuggingFace Spaces works through Git, which many developers are already familiar with. Using Git is also a good practice, and Jeremy recommends using Github Desktop and WSL2. Refer to the guide for WSL2 installation to get started.\nCloning the Repository\nTo start working on HuggingFace Spaces you need to clone the repository locally, you have two options for cloning: HTTPs and SSH\n\n\n\n\n\n\n\n\nPros / Cons\nHTTPS\nSSH\n\n\n\n\nPros\nEasier for beginners, no SSH key setup required\nMore secure, no need to enter credentials each time\n\n\nCons\nRequires authentication each time you push\nRequires SSH key setup\n\n\n\nSince I’m using SSH, if you follow along please make sure your SSH key is properly set up in your HuggingFace Spaces user setting\n# Clone the repository using SSH\ngit clone git@huggingface.co:USERNAME/YOUR_REPO_NAME.git\ncd YOUR_REPO_NAME\nStep 3: Prepare Your Model\nMake sure to export your trained model from the notebook:\n\n# in your bear classifier notebook\nlearn.export(\"model.pkl\")\n\nThis saves the trained model as model.pkl, which you’ll need for the deployment.\nStep 4: Building the Gradio Interface\nGradio makes it easy to build an interactive interface. You can use Jupyter Notebook for experimentation and then use nbdev.export.nb_export to convert the notebook into Python script. This tool is very handly for such conversions. Alright, but first make sure you run pip install gradio in your terminal if you haven’t already.\n1. Import Required Libraries:\n\nimport gradio as gr\nfrom fastai.vision.all import *\n\nAnd so we can create a python image library image from that black bear\n\nim = PILImage.create('black.jpg')\nim.thumbnail((192, 192))\nim\n\n\n\n\n\n\n\n\nTurn it into silghtly smaller one so it doesn’t overwhelm my whole screen and there’s is a picture of a black bear so we will use it for experimenting\n2. Load the Model:\n\nlearn = load_learner('model.pkl')\n\nOne of the methods that the learner has is a predict method\n\nlearn.predict(im)\n\n\n\n\n\n\n\n\n('black', tensor(0), tensor([9.9995e-01, 4.9545e-05, 3.8379e-06]))\n\n\nSo if you run it, you can see, even on a laptop, it’s basically instant. It took a really short time to figure out this is a black bear\n3. Define the Prediction Function:\nGradio requires us to give it a function that it’s going to call but first we need to know what labels do we have?\n\nlearn.dls.vocab\n\n['black', 'grizzly', 'teddy']\n\n\n\n#create our categories\ncategories = learn.dls.vocab\n\nSo here’s our function:\n\ndef classify_image(img):\n    img = PILImage.create(img)\n    pred, idx, probs = learn.predict(img)\n    return dict(zip(categories, map(float, probs)))\n\nSo we called predict and that returns three things: the prediction as a string, the index of that, and the probabilities of whether it’s black or grizzly or teddy bear. And what Gradio wants is it wants to get back a dictionary containing each of the possible categories-which is in this case grizzly, black and teddy bear-and the probabilities of each one.\n4. Create the Gradio Interface:\n\nimage = gr.Image(height=512, width=512)\nlabels = gr.Label()\nexamples = [\"grizzly.jpg\", \"black.jpg\", \"teddy.jpg\"]\n\nintf = gr.Interface(fn=classify_image, inputs=image, outputs=labels, examples=examples)\nintf.launch(inline=True)\n\nRunning on local URL:  http://127.0.0.1:7860\nTo create a public link, set `share=True` in `launch()`.\n\nThis code creates a simple Gradio interface where user can upload images, and the model will predict whether it’s a grizzly, black or teddy bear. you can run the interface inline in you Jupyter Notebook for testing.\nStep 5: Export the Notebook to a Python script:\nWe will use nbdev to convert the Jupyter Notebook to a Python script.\n1. Add Metadata and Export Tags:\n- Add `#| default_exp app` to the first cell\n- Add `#| export` to every cell you want to convert\n2. Run the Conversion:\n\nimport nbdev.export\nnbdev.export.nb_export('app.ipynb', '.')\n\nThis command converts the notebook app.ipynb to a python script app.py.\nStep 6: Push Your Changes to HuggingFace Spaces:\nHandle large files like model.pkl uising Git LFS (Large File Storage).\n1. Set up Git LFS:\n\ngit lfs install\ngit lfs track \"*.pkl\"\ngit add .gitattributes\ngit commit -m \"Track .pkl files with Git LFS\" # To be honest, when initializing git lfs, the .gitattributes already supports .pkl files, and in my repo when initialized, it already had .gitattributes file. I don't know why, but I didn't need to commit it anyway, but I still write it here for the sake of completeness. 😉\n\n2. Commit and Push Your Changes:\n\ngit add .\ngit commit -m \"Deploy bear classifier with Gradio interface🐻🎉\"\ngit push"
  },
  {
    "objectID": "posts/2024-07-07-from-notebook-to-web-app/index.html#conclusion",
    "href": "posts/2024-07-07-from-notebook-to-web-app/index.html#conclusion",
    "title": "From Notebook to Web App: Deploying Your Models with fastai Lesson 2",
    "section": "Conclusion",
    "text": "Conclusion\nBy following this guide, you’ve successfully built and deployed your bear classifier using Gradio and HuggingFace Spaces. This powerful combination not only make you model accessible for everyone through the user-friendly web interface but also leverages cutting-edge tool to ensure it easy to maintain and extend.\nDeploying machine learning models in real-world applications is an exciting milestone. It transform your hard work and complex algorithms into actionable insights and tools that can be used by anyone, anywhere. Whether you’re a beginner or an experienced partitioner, the ability to take a model from Jupyter Noteook to a live web app is an invaluable skill in today’s AI driven world.\nIn case you want to explore more about Gradio and HuggingFace, here are some valuable resources:\n\nGradio Documentation: For more on Gradio, refer to the Gradio documentation.\nMy HuggingFace Spaces Bear Classifier: Checkout my deployed bear classifier on HuggingFace Spaces here.\nTanishq Abraham’s Blog: For an in-depth look at deploying model using Gradio and HuggingFace Spaces, make sure to read Tanishq’s excellent blog post.\n\nAlthough I initially planned to look into the HuggingFace Spaces API and deploying your own web app via JavasScript, sometimes technical hitches happend. Whether it’s a client-side issue or just part of the learning curve, don’t let it discourage you. Every challenge is learning opportunity, and with the fast-placed advencements in AI and deployment tools, there’s always something new and exciting around the corner.\nThank you for joining me on this journey to bring deep learning model to life. Embrace the power of open-source tools, keep experimenting, and never stop learing. Happy coding, and may your models alwasy be accurate!"
  },
  {
    "objectID": "posts/2024-06-30-your-deep-learning-journey/index.html",
    "href": "posts/2024-06-30-your-deep-learning-journey/index.html",
    "title": "First Step in AI: My Experience with fast.ai Lesson 1",
    "section": "",
    "text": "Welcome to my deep dive in to the world of deep learning! In this blog post, I’ll be sharing my journey through the first lession of fast.ai course an acclaimed program that makes learning AI accessible and enjoyable.\nFast.ai was created with the goal of making deep learning understandable for everyone, no matter their background, and Lesson 1 accomplishes that by having us build a simple yet fascinating model: a bird classifier. this exciting task not just introduces me to the basics of deep learning but also alow me to experience firsthand the power and simplicity of modern AI tools.\nJoin me as I walk you though key conccept covered in the Lesson 1, from understanding how images are processed by computers to trainning and validating our model. I will also share some personal insights and reflections on the learning process, aiming to make this technicial journey both infomative and relatable.\nWhether you are a bigginer in AI or someone looking for refresh your knowledge, I hope this post inspires and guides you in your own deep learning"
  },
  {
    "objectID": "posts/2024-06-30-your-deep-learning-journey/index.html#introduction",
    "href": "posts/2024-06-30-your-deep-learning-journey/index.html#introduction",
    "title": "First Step in AI: My Experience with fast.ai Lesson 1",
    "section": "",
    "text": "Welcome to my deep dive in to the world of deep learning! In this blog post, I’ll be sharing my journey through the first lession of fast.ai course an acclaimed program that makes learning AI accessible and enjoyable.\nFast.ai was created with the goal of making deep learning understandable for everyone, no matter their background, and Lesson 1 accomplishes that by having us build a simple yet fascinating model: a bird classifier. this exciting task not just introduces me to the basics of deep learning but also alow me to experience firsthand the power and simplicity of modern AI tools.\nJoin me as I walk you though key conccept covered in the Lesson 1, from understanding how images are processed by computers to trainning and validating our model. I will also share some personal insights and reflections on the learning process, aiming to make this technicial journey both infomative and relatable.\nWhether you are a bigginer in AI or someone looking for refresh your knowledge, I hope this post inspires and guides you in your own deep learning"
  },
  {
    "objectID": "posts/2024-06-30-your-deep-learning-journey/index.html#the-xkcd-joke-and-debunking-deep-learning-myths",
    "href": "posts/2024-06-30-your-deep-learning-journey/index.html#the-xkcd-joke-and-debunking-deep-learning-myths",
    "title": "First Step in AI: My Experience with fast.ai Lesson 1",
    "section": "The XKCD Joke and Debunking Deep Learning Myths",
    "text": "The XKCD Joke and Debunking Deep Learning Myths\n\n\n\n\nXKCD Joke\n\n\nJeremy Howard kicked off the lesson with relatable XKCD Joke about how in 2015, detecting a bird in a photo was seen as a challenging task, almost a joke. Fast forward to today, and we can build such as system in mere minutes, showcasing how far deep learning has come.\nMany people believe that diving into deep learning requires extensive mathematical knowledge, huge datasets, and expensive hardware. However, these myths are far from the truth.\n\n\n\n\n\n\n\nMyth(Don’t need)\nTruth\n\n\n\n\nLots of math\nJust high school math is sufficient\n\n\nLots of data\nWe’ve seen record-breaking results with fewer than 50 items of data\n\n\nLots of expensive computer\nYou can perform state-of-the-art work with hardwere available for free of minimal cost"
  },
  {
    "objectID": "posts/2024-06-30-your-deep-learning-journey/index.html#top-down-learning-approach",
    "href": "posts/2024-06-30-your-deep-learning-journey/index.html#top-down-learning-approach",
    "title": "First Step in AI: My Experience with fast.ai Lesson 1",
    "section": "Top-Down Learning Approach",
    "text": "Top-Down Learning Approach\nOne of the most refreshing aspects of fastai course is its top-down teaching approach. Traditional education often starts with the basics and slowly builds up to more complex topics. However, Jeremy Howard and Rachel Thomas believe that learning is more effective when you see the big picture first.\nIn the fastai course, we start by building practicall applications from lesson one, allowing us to see immediate results and understanding the relevance of what we are doing. This approach mirrors how we learn many real-word skills, such as sport or cooking, where we start by trying out the activity and learn the details as needed.\nBy diving straight into creating a deep learning model, we get hands-on experience early on, which helps solidify our understanding and maintain our interest. As we process though the course, we gradually delve deeper into the underlying principles and theories, bulding a robust foundation along the way"
  },
  {
    "objectID": "posts/2024-06-30-your-deep-learning-journey/index.html#understanding-deep-learning",
    "href": "posts/2024-06-30-your-deep-learning-journey/index.html#understanding-deep-learning",
    "title": "First Step in AI: My Experience with fast.ai Lesson 1",
    "section": "Understanding Deep learning",
    "text": "Understanding Deep learning\nDeep learning is a technique for extracting and transforming data, with application ranging from speech recognition to image classification. It uses multiple layer of neural networks, where each layer refines the data received from the previous one. These layers are trained using the algorithms that minimize the errors and improve accuracy, enabling the network to learn specific tasks.\nDeep learning’s power, flexibility, and simplicity make it applicable across various field, including social science, medicine, finance, and more. For instance, despite lacking of medical background, Jeremy Howard founded Enlitic, a company leveraging deep learning to diagnose illnesses. Within months, their algorithm was more effective at identifying malignant tumors than radiologists.\nHere are some areas where deep learing excels:\n\nNatural Language Processing (NLP): Answering question, speech recognition, document summarization, and more.\nComputer Vision: Interpreting satellite images, face recognition, and automous vehicle navigation.\nMedicine: Analyzing radiology images, measuring features and medical scans, and diagnosing diseases.\nBiology: Protein folding, genomics tasks, and cell classification.\nImage Generation: Colorizing images, enhancing resolution, and converting images to artistic style.\nRecommendation System: Web search optimization, product recommendations, and personalized content layout.\nGaming: Mastering games like Chess, Go, and various video games.\nRobotics: Handling challenging objects and complex manipulation tasks.\nOther: Financial forecasting, text-to-speech conversion, and much more.\n\nThe versatility of deep learning lies in its foundation: neuron networks."
  },
  {
    "objectID": "posts/2024-06-30-your-deep-learning-journey/index.html#a-brief-history-of-deep-learning",
    "href": "posts/2024-06-30-your-deep-learning-journey/index.html#a-brief-history-of-deep-learning",
    "title": "First Step in AI: My Experience with fast.ai Lesson 1",
    "section": "A Brief History of Deep Learning",
    "text": "A Brief History of Deep Learning\n\n\n\n\nBiological Neurons vs. Artificial Neural Network\n\n\nDeep learning draws inspiration from human brain’s neural network. The concept of neural network isn’t new; it dates back to 1957 with the creation of the first neural network. The fundamental ideas remain the same today, but advances in hardware and data availability have significantly propelled the field forward."
  },
  {
    "objectID": "posts/2024-06-30-your-deep-learning-journey/index.html#the-sofware-pytorch-fastai-and-jupyter",
    "href": "posts/2024-06-30-your-deep-learning-journey/index.html#the-sofware-pytorch-fastai-and-jupyter",
    "title": "First Step in AI: My Experience with fast.ai Lesson 1",
    "section": "The Sofware: Pytorch, Fastai, and Jupyter",
    "text": "The Sofware: Pytorch, Fastai, and Jupyter\nAt fastai, after extensive testing of various machine learning packages and languages, they decided to adopt Pytorch in 2017 for their course, software development, and research. Pytorch has become the fastest-growing deep learning library and is widely used in academic research and industry. Its flexibiligy and expressiveness make it an excellent foundation for deep learning.\nThe fastai library builds on top of Pytorch, provide high-level functionality for deep learning. This layered architecture allows for a seemless learning experience, make it easier to understand both high-level concepts and low-level operations.\nHowever, the specific software you use a less important than understanding the core principles and techniques of deep learning. Learning to trasition between the libraries is relatively quick, but mastering deep learning foundation is crucial.\nJupyter notebook, a powerful and reflexible tool for data science, will be our primary platform for experimentation. Its interation with fastai and Pytorch makes it ideal for developing and testing deep learning model.\nReady to see it in action? Let’s train our first model!"
  },
  {
    "objectID": "posts/2024-06-30-your-deep-learning-journey/index.html#exploring-the-is-it-a-bird-classifier",
    "href": "posts/2024-06-30-your-deep-learning-journey/index.html#exploring-the-is-it-a-bird-classifier",
    "title": "First Step in AI: My Experience with fast.ai Lesson 1",
    "section": "Exploring the “Is it a Bird?” Classifier",
    "text": "Exploring the “Is it a Bird?” Classifier\nOne of the most exciting part of Lesson 1 was building our own image classifier to determine whether the a given image contains a bird. For this project, we used the fastai libray along with pre-trained model to quickly and efficiently create our classifier. Let’s dive into the code walkthrouh.\nThe basic steps we’ll need to do:\n\nUse DuckDuckGo for search images of “bird photos”\nUse DuckDuckGo to search for images of “forest photos”\nFine-tune a pretrained neural network to recognise these two groups\nTry running this model on a picture of bird and see if it works.\n\n\nSearching for images: DuckDuckGo Search\nInstead of using a big search that reqires an API key, we opted to DuckDuckGo, which doesn’t reqire an API key for image searches. This make the setup simpler and faster.\nBut make sure you run this command in your terminal before run the code to update duckduckgo\npip install -Uqq fastai duckduckgo_search\n\nfrom duckduckgo_search import DDGS\nfrom fastcore.all import *\n\nddgs = DDGS()\n\ndef search_images(term, max_images=30):\n    print(f\"Searching for '{term}'\")\n    return L(ddgs.images(keywords=term, max_results=max_images)).itemgot('image')\n\n\nurls = search_images('bird photos', max_images=1)\nurls[0]\n\nSearching for 'bird photos'\n\n\n'https://images.pexels.com/photos/326900/pexels-photo-326900.jpeg?cs=srgb&dl=wood-flight-bird-326900.jpg&fm=jpg'\n\n\nJeremy Howard mentioned that using import * in Jupyter notebooks is not the big deal because Jupyter only import what we use. This approach simplifies the code and keeps it clean.\nHere’s the quick explaination of the functions and libraries used in this snippet:\nDDGS from duckduckgo_search:\n\nduckduckgo_search: This library allows us to search for iamges using DuckDuckGo without the need for an API key. So no more begging Google for an API key.\nDDGS: The class that does the heavy lifting of searching for images.\n\nfastcore: - fastcore: A foundattional library that make Python feel like a Lamborghini-sleek, powerfull, and fast.\nL:\n\nL: A magical list from fastcore that does way more than the regular Python list. Think of it as a list on steroids.\n\nIn our example, search_images is a function that performs an image search using DuckDuckGo. It’s print out the search term being used and return a list of images URLs retrieved from the search results.\nfor more details on the tools, you can refer to the fastcore documentation and the duckduckgo_search documentation.\n\nfrom fastdownload import download_url\ndest = 'bird.jpg'\ndownload_url(urls[0], dest, show_progress=False)\n\nfrom fastai.vision.all import *\nim = Image.open(dest)\nim.to_thumb(256,256)\n\n\n\n\n\n\n\n\n\ndownload_url(search_images('forest photos', max_images=1)[0], 'forest.jpg', show_progress=False)\nImage.open('forest.jpg').to_thumb(256,256)\n\nSearching for 'forest photos'\n\n\n\n\n\n\n\n\n\nfastdownload and download_url:\n\nfastdownload: Think of this as your friendly neighborhood delivery service, but for files. It’s help with downloading files and datasets easier.\ndownload_url: A function that fetches the file you need from a URL. In our case, it says “Hey URL, gimme that picture!” and save it as bird.png\n\nfastai.vision.all:\n\nTThis module from the fastai library is like a Swiss Army knife for vision tasks, providing all the tools you need, from data loaders to model training utilities.\n\nto_thumb: - A method from the PIL.Image class, which is quite handy it resizes an image to a thumbnail while maintaining the aspect rato. Kind of like shrinking your favourate sweater but in a good way\nThese libraries and function streamline the process of getting and preparing the images for our model. For more detailed documentation, you can refer to the fastdownload, fastai vision, and Pillow documentation.\n\n\nDownloading and Preparing Images\nTo build our dataset, we need to download images for the categories we are interested in (‘forest’ and ‘bird’). Here’s how we did it:\n\nsearches = 'forest','bird'\npath = Path('bird_or_not')\nfrom time import sleep\n\nfor o in searches:\n    dest = (path/o)\n    dest.mkdir(exist_ok=True, parents=True)\n    download_images(dest, urls=search_images(f'{o} photo'))\n    sleep(10)  # Pause between searches to avoid over-loading server\n    download_images(dest, urls=search_images(f'{o} sun photo'))\n    sleep(10)\n    download_images(dest, urls=search_images(f'{o} shade photo'))\n    sleep(10)\n    resize_images(path/o, max_size=400, dest=path/o)\n\nSearching for 'forest photo'\nSearching for 'forest sun photo'\nSearching for 'forest shade photo'\nSearching for 'bird photo'\nSearching for 'bird sun photo'\nSearching for 'bird shade photo'\n\n\nPath:\n\nPath: An object-oriented way to work with filesystem paths. It makes handling files and directories as easy as pie.\n\ndownload_images:\n\ndownload_images: This function fetches a bunch of images from the internet and saves them in a specified directory. Like ordering a pizza, but instead of pizza, you get pictures.\n\nPausing Between Searches:\n\nPausing between searches (sleep(10)) is important to avoid overloading the server. Think of it as giving the server a coffee break between each request.\n\nresize_images:\n\nresize_images: A function from fastai that resizes images to a maximum specified size. This is useful for ensuring all images are of a consistent size before training the model.\n\nFor more details on these tools, you can refer to the pathlib, Vision utils documentation.\n\n\nVerifying and Leaning Images\nAfter download images, it’s enssential to verify them and remove corrupt or invalid images.\n\nfailed = verify_images(get_image_files(path))\nfailed.map(Path.unlink)\nlen(failed)\n\n0\n\n\nverify_images:\n\nverify_images: Think of this as the bouncer for your image dataset, checking IDs to make sure no bad images get through.\n\nget_image_file:\n\nget_image_file: This function grabs all image paths in a directory. It’s like having someone fetch all your misplaced socks in the laundry room.\n\nPath.unlink:\n\nPath.unlink: A method to delete files. This is how we get rid of the bad apples in the bunch.\n\nFortunately, in my case, all downloaded images were valid, so len(failed) return 0–no bad apples in our dataset!\n\n\nThe DataBlock API\nCreating our data loarder is a critical step. The DataBlock API in fastai allows us to define how to transform and manage our data easily.\n\ndls = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=[Resize(192, method='squish')]\n).dataloaders(path, bs=32)\n    \ndls.show_batch(max_n=6)\n\n\n\n\n\n\n\n\nHere’s the breakdown of the arguments in DataBlock:\nblocks:\n\nSpecifies the type of inputs and targets. In our case, we have images (ImageBlock) and categories (CategoryBlock). It’s like saying, “I have pictures of cats and dogs”\n\nget_items:\n\nFunction to get the list of items. Here we’re using get_image_file to retrieve all our iamge files.\n\nsplitter:\n\nDefines how to split the dataset into training and validation sets. RandomSplitter(valid_pct=0.2, seed=42) means 20% of the data will be used for validation. The seed ensures that every time we run the code we get the same split. Think of like setting your DVR to record your favourate show at the same time everyweek.\n\nget_y:\n\nFunction to get the target label from each item. We use parent_label to get the label from parent directory name (e.g., ‘forest’ or ‘bird’)\n\nitem_tfms:\n\nitem transformation to apply. We use Resize(129, method='squish') to resize images to 129x129 pixels by squishing them if necessary.\n\ndataloaders:\n\nCreates the data loaders for our dataset, with a batch size of 32. Data loaders are like conveyor belt that feed the data into your model in manageable chunks.\n\nThe show_batch method is handy way to visialize a batch of data items. It’s like a quick preview to make sure everything looks good.\nFor more details, checkout the fastai DataBlock API documentation.\n\n\nTraining the Model: Welcome to the Learner World\nAfter preparing our dataset, it’s time to train our model. We use the vision_learner function to setup a learner and the powerful fine_tune method to train the model.\n\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(3)\n\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n  0%|          | 0.00/44.7M [00:00&lt;?, ?B/s]100%|██████████| 44.7M/44.7M [00:00&lt;00:00, 145MB/s] \n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.120399\n1.209828\n0.411765\n00:01\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.185352\n0.054729\n0.029412\n00:01\n\n\n1\n0.102830\n0.023147\n0.000000\n00:01\n\n\n2\n0.072183\n0.049310\n0.029412\n00:01\n\n\n\n\n\nvision_learner:\n\nThis create a learner object that combines our data loaders(dls) and a pre-trained model(resnet18). We basically saying, “Hey, take this data and use this model to learn from it.”\n\nresnet18:\n\nA specific architecture of a Convolutional Neuron Network that’s been pre-trained on a large dataset. Think of it as seasoned detective who’s seen it all and just need to be briefed on this specific case.\n\nmetrics=error_rate:\n\nThis specifies that we want to use the error rate as a metric to evaluate our model’s performance. It’s like having a scoreboard to keep track of who’s winning.\n\n\nfine_tune(3):\n\nHere’s where the magic happens. Unlike the traditional fit mothod, fine_tune starts by refining the pre-trained model with our specific data. It’s like taking your detective and train them on a nuances of this particular mystery. The 3 indicates the number of epochs (full cycles throught the training data).\n\nThe fine_tune method is particularly powerful because it starts with a model that already knows a lot (thanks to pre-training) and fine-tune it to specific task. This approach often yields better results, faster and with less data, compared to tranning a model from scratch.\n\n\nMaking Predictions\nFinally, let’s make our bird classifier predict whether or not an image contain a bird.\n\nis_bird,_,probs = learn.predict(PILImage.create('bird.jpg'))\nprint(f\"This is a: {is_bird}.\")\nprint(f\"Probability it's a bird: {probs[0]:.4f}\")\n\n\n\n\n\n\n\n\nThis is a: bird.\nProbability it's a bird: 0.9988\n\n\nPILImage.create:\n\nThis function create a image object from a file. It’s like saying “Hey, look at this picture I just took.”\n\nlearn.predict:\n\nThis method uses our train model to predict what’s in a image. It’s like asking your well-trained detective, “What do you see in this picture?”\nThe method returns three values:\n\nis_bird: The predicted label(whether it’s a bird or not).\nprobs: The probabilites associated with each class.\n\n\nWhen we print out the predicted label and the probability. If the model says it’s a bird with a high probability, you can feel pretty confident your model knows its bird!\nBuilding the “Is it a Bird?” classifier was hands-on way to introduce the principles of deep learning. By leveraging fastai and Pytorch, we could quickly create an effective model with minimal code. This approach of starting with practical, top-down learning ensures that we see immediately results and understand the real world applicability of deep learning from the get-go."
  },
  {
    "objectID": "posts/2024-06-30-your-deep-learning-journey/index.html#what-is-machine-learning",
    "href": "posts/2024-06-30-your-deep-learning-journey/index.html#what-is-machine-learning",
    "title": "First Step in AI: My Experience with fast.ai Lesson 1",
    "section": "What Is Machine Learning",
    "text": "What Is Machine Learning\nAh, the age-old question: What is the machine learning? Well, imagine if your computer was a child, and you were its teacher. Instead of giving it a a strict set of rules to follow(which, let’s be honest, kids hate), you give it examples from which it can learn. In enssence, machine learning is about enabling computer to learn from data rather than being explicitly programmed. It’s like teaching your computer how to ride a bike by letting it practice, fall and get up again, rather than reading it a manual\nLet’s take a closser look at this with a series of visualizations:\n\nTraditional Programming\nIn traditional Programming we write explicit instructions-a program-that processes input to procude results.\n\n\n\n\n\n\n\n\n\nThink of it as following a recipe step-by-step: preheat the oven, mix the ingredients, bake for 30 minutes, and volià, you have a cake.\n\n\nProgram Using Weight And Assignment\nIn machine learning, we use model with weights(parameters) that processes inputs to generates result.\n\n\n\n\n\n\n\n\n\nHere, the model is like a reflexible recipe that can adjust itself. The ingredients(inputs) are mixed differently depending on the weights, and the ouput is a delicious result that varies based on those adjustments.\n\n\nTraining a Machine Learning Model\nTraining a model involves feeding inputs through the model to produce results, measuring performance and updating the weights to improve accuracy.\n\n\n\n\n\n\n\n\n\nThink of it as trial and error. The model tries to bake a cake, and if it’s to salty, it adjusts the recipe (update the weights). Over time, it learns the perfect proportions.\n\n\nUsing a Trained Model\nOnce the model is trained, it can be used just like a tranditional program, taking inputs and producing results predictably.\n\n\n\n\n\n\n\n\n\nNow, you have reliable recipe that consistently makes the perfect cake. The model processes new inputs(ingredients) and produces outputs(cakes) with the learned adjustments."
  },
  {
    "objectID": "posts/2024-06-30-your-deep-learning-journey/index.html#what-our-image-recognizer-learned",
    "href": "posts/2024-06-30-your-deep-learning-journey/index.html#what-our-image-recognizer-learned",
    "title": "First Step in AI: My Experience with fast.ai Lesson 1",
    "section": "What Our Image Recognizer Learned",
    "text": "What Our Image Recognizer Learned\nAt this stage, we have an image recognizer that works very well. But what is it actually doing? Although many people believe that deep learning results in inpenetrable “black box” models (where predictions are given, but no one understand why), this isn’t entirely true. There is a vast body of reseach showing how to inspect deep learning model deeply and gain rich insights for them. However, all kind of machine learning model (including machine learning and traditional statistical models) can be challenging to fully understand, especially when dealing with new data that differs significantly from the training data.\nWhen we fine-tuned our pre-trained model, we adapted the last layers(originally trained on general features like flowers, humans, animals) to specialize in a birds versus non-birds problem. Imagine our model initialy knew how to recognize the entire zoo, but now we’ve trained it to focus solely on recognizing birds. More generally, we could specialize such a pre-trained model on many different tasks."
  },
  {
    "objectID": "posts/2024-06-30-your-deep-learning-journey/index.html#beyond-image-classification-other-application-of-deep-learning",
    "href": "posts/2024-06-30-your-deep-learning-journey/index.html#beyond-image-classification-other-application-of-deep-learning",
    "title": "First Step in AI: My Experience with fast.ai Lesson 1",
    "section": "Beyond Image Classification: Other Application of Deep Learning",
    "text": "Beyond Image Classification: Other Application of Deep Learning\nDeep learning isn’t just about figuring out whether there’s bird in your photo. It’s way more powerful than that! Let’s explore a couple of areas where deep learning make significant strides:\n\nImage Segmentation:\nSegmenation is a process of identifying and labling pixles in an image belonging to the same object. This is critically important for application like autonomous vehicles where the car needs to recognize and localize object such as pedestrians, other vehicles, and road signs. Instead of just saying, “Hey, there’s a cat in a picture”, segmentation says, “Here’s the outline of the cat in this picture”.\nNatural Language Processing (NLP): Deep learning has drammatically improved Natural Language Processing over the last few years. Now computers can:\n\nGenerate text: Write conherent and context-aware essays (but don’t trust them with your love letters just yet).\nTranslate languages: Turn English into Spanish, French, or Klingon (okay, maybe not Klingon…yet)\nAnalize comments: Understand sentiments, detect sarcasm, and probably tell when you’re being a bit snarky.\nLabel words in sentences: Identify parts of speech (nouns, verbs, adjectives, etc.), entities (like names and places), and more.\n\n\nHere’s some cool code to classify the sentiment of a movie review better than anything availible just a few years ago:\n\nfrom fastai.text.all import *\ndls = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid='test')\nlearn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\nlearn.fine_tune(4, 1e-2)\n\n\n\n\n\n\n    \n      \n      100.00% [144441344/144440600 00:03&lt;00:00]\n    \n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [105070592/105067061 00:01&lt;00:00]\n    \n    \n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.462561\n0.395122\n0.822320\n03:08\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.301779\n0.248262\n0.899480\n06:38\n\n\n1\n0.244484\n0.202708\n0.921480\n06:38\n\n\n2\n0.189148\n0.194167\n0.926160\n06:37\n\n\n3\n0.148741\n0.191470\n0.929720\n06:38\n\n\n\n\n\n\nlearn.predict(\"I really liked that movie!\")\n\n\n\n\n\n\n\n\n('pos', tensor(1), tensor([7.8042e-04, 9.9922e-01]))\n\n\nAnd boom! You have a state-of-art sentiment analyzer."
  },
  {
    "objectID": "posts/2024-06-30-your-deep-learning-journey/index.html#the-important-of-validation-and-test-sets",
    "href": "posts/2024-06-30-your-deep-learning-journey/index.html#the-important-of-validation-and-test-sets",
    "title": "First Step in AI: My Experience with fast.ai Lesson 1",
    "section": "The Important of Validation and Test Sets",
    "text": "The Important of Validation and Test Sets\nWe’ve trained our model and it’s looking pretty smart, but know how do we know it’s actually learned something useful? This is where validation and test sets come in.\n\nWhy Do We Need a Validation set?\nThe goal of a model is to make predictions about unseen data. If we trained a model with all our data and evaluated it using the same data, we wouldn’t realy know how well it performs on new, unseen data. It could just memorize the training data(cheating basically). The model could get great results on your training data but bomb when given the data to analyze. To avoid this, we: - We split dataset: We divide our data into traning and validation sets. The trainning set is used to teach the model, and the validation set is used to see how well it’s learning\n\n\nPreventing Overfitting with a Test set\nOverfitting is a common issue where the model preform exceptionally well on the traning set but poorly on the validation set, meaning it has memorized the training data rather than learing the generalizable pattern.\nEven when your model hasn’t fully memorized all your data, it might memorized certain parts of it during earlier traning stages. The longer you train, the better the accuracy on the traning set, but eventually, the validation accuracy will start to decline. This is because your model is begins memorizing the traning data instead of learning the parttern that generalize well. When this happens, we say the model is overfitting.\nHere’s an example to visualize overfitting:\n\n\n\n\nExample of overfitting\n\n\nThe Image shows what happends when you overfit, using a simplified example where we have just one parameter and some randomly generated data. Although the overfitted model’s prediction are accurate for the data near the observed data points, they are way off when outside of that range.\nOverfitting is the single most important and challenging issue when training machine learning models. It’s easy to create a model that does the great job at making predictions on the data it’s been trained on, but making accurate predictions on new data is much harder.\nFor instance, if you writting a handwriteen digit classifier (as we will very soon) and use it to recognize numbers on checks, you won’t see the same numbers the model was trained on–checks will have different variations of handwriting to deal with."
  },
  {
    "objectID": "posts/2024-06-30-your-deep-learning-journey/index.html#wrapping-up",
    "href": "posts/2024-06-30-your-deep-learning-journey/index.html#wrapping-up",
    "title": "First Step in AI: My Experience with fast.ai Lesson 1",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nDeep learning is an exciting field that extends far beyond simple image classification. From understand speech to translate langugaes and detecting malware, it’s applications are vast. Through this blog post, we’ve seen how to build a bird classifier using the fastai library-an accessible, powrful tool that simplifies the complexities of machine learning.\nBy spllitting our data into traning and validation sets, we ensure our model doesn’t cheat and genuinely learns the task at hand. With powerful tools like fastai and the aibility to handle the deverse tasks, deep learning truly has potential to transform numerous industries.\nI hope you enjoyed this journey as much as I did. Remember, the key to mastering deep learing is to keep experimenting and learning. So go ahead, build that next big thing, and maybe teach your computer to recognize your pet fish or translate cat’s meows!"
  },
  {
    "objectID": "posts/2024-06-30-your-deep-learning-journey/index.html#final-thoughts",
    "href": "posts/2024-06-30-your-deep-learning-journey/index.html#final-thoughts",
    "title": "First Step in AI: My Experience with fast.ai Lesson 1",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nThank you for joining me on this deep learning adventure! if you find this blog helpful or inspiring, please share it with others who might also be interested. Deep learning is a continuously envolving field with endless possibilities. Stay curious, keep learning, and don’t hasitate to dive deeper into the world of AI.\nFeel free to leave your comments, questions, or insights below. I’d love to hear your experiences, projects, and what you’re learning. Together, we can continue to explore and push the boundaries of what’s possible with deep learning.\nHappy coding, and may your models always be accurate!"
  }
]
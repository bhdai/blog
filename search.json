[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to my blog! To learn more about me, check out my website here."
  },
  {
    "objectID": "posts/2024-11-12-random-forest/index.html",
    "href": "posts/2024-11-12-random-forest/index.html",
    "title": "Exploring Random Forest, Bagging, Boosting?",
    "section": "",
    "text": "What’s up! It’s been a long time since the last post, i’m quite lazy recently, but from know i will try to write more blog post though. I’ve revisited the Titanic dataset, this time through the lens of ensemble learning techniques. Previously I wrote about this dataset in this blog, but now, let’s dive into why random forests and gradient boosting machine are particularly suitable for tabular data.\nYou might ask, “Why not just use logistic regression?” While it seems simple, logistic regression can be surprisingly difficult to get right especially with transformation, interactions, and outlier handling. Random forests, on the other hand, offers resilience and robustness that are hard to match, which I’ll explain today.\nTo start, building a random forest is insightful help demystify the intricacies of machine learning algorithm. I’ll also touch on bagging and boosting, giving a clear view of their strengths\nOn a practical note, a helpful tip I’ve stumbled upon is using fastai’s import to efficiently bringing in essential libraries like Numpy an pandas. Here’s the snippet to simplify your setup:\nfrom fastai.imports import *\nnp.set_printoptions(linewidth=130)\nThese tools and techniques have enhanced my learning journey, and I’m excited to share these insights with you. Alright without any further ado let’s get right into it."
  },
  {
    "objectID": "posts/2024-11-12-random-forest/index.html#decision-tree",
    "href": "posts/2024-11-12-random-forest/index.html#decision-tree",
    "title": "Exploring Random Forest, Bagging, Boosting?",
    "section": "Decision Tree",
    "text": "Decision Tree\n\nData Processing\nFirst off, ensure that you have the Titanic dataset downloaded, Here’s the quick setup:\n\nimport zipfile, kaggle\n\npath = Path('titanic')\nkaggle.api.competition_download_cli(str(path))\nzipfile.ZipFile(f'{path}.zip').extractall(path)\n\ndf = pd.read_csv(path/'train.csv')\ntst_df = pd.read_csv(path/'test.csv')\nmodes = df.mode().iloc[0]\n\nDownloading titanic.zip to /home/monarch/workplace/random_forest\n\n\n100%|██████████| 34.1k/34.1k [00:00&lt;00:00, 365kB/s]\n\n\n\n\n\n\n\n\nI’ve previously detailed the intricacies of processing the Titanic dataset in a separate blog post which you might find useful. For now, let’s breeze through some basic data processing steps without going into too much detail:\n\ndef proc_data(df):\n    df['Fare'] = df.Fare.fillna(0)\n    df.fillna(modes, inplace=True)\n    df['LogFare'] = np.log1p(df['Fare'])\n    df['Embarked'] = pd.Categorical(df.Embarked)\n    df['Sex'] = pd.Categorical(df.Sex)\n\nproc_data(df)\nproc_data(tst_df)\n\nOur next task involves organizing the data by identifying continuous and categorical variables, along with dependent variable we’re predicting\n\ncats=[\"Sex\",\"Embarked\"]\nconts=['Age', 'SibSp', 'Parch', 'LogFare',\"Pclass\"]\ndep=\"Survived\"\n\nNow, a brief look at how Pandas handles categorical variables. Let’s consider the Sex column:\n\ndf.Sex.head()\n\n0      male\n1    female\n2    female\n3    female\n4      male\nName: Sex, dtype: category\nCategories (2, object): ['female', 'male']\n\n\nIt’s fascinating, although it appears unchanged(still just Male and Female), it’s now a category with a predefine list. Behind the magic, Pandas cleverly assigns numerical codes for these categories for efficient processing:\n\ndf.Sex.cat.codes.head()\n\n0    1\n1    0\n2    0\n3    0\n4    1\ndtype: int8\n\n\nIt’s actually turned them into numbers. This transformation sets the stage for our decision tree modeling\n\n\nBinary Split\nA random forest is essentially an ensemble of decision trees, and each tree is constructed from a series of binary split. But what exactly is a binary split?\nImagine taking all the passengers on the Titanic and dividing them into males and females to examine their survival rates.\n\nimport seaborn as sns\n\nfig,axs = plt.subplots(1,2, figsize=(11,5))\nsns.barplot(data=df, y=dep, x=\"Sex\", ax=axs[0]).set(title=\"Survival rate\")\nsns.countplot(data=df, x=\"Sex\", ax=axs[1]).set(title=\"Histogram\");\n\n\n\n\n\n\n\n\nwe see a stark difference: about a 20% survival rate for males and 75% for females, there are roughly twice as many males as females. If you base a model solely on sex, predicting survival becomes surprisingly effective: men likely didn’t survive, while woman likely did this division by sex exemplifies a binary split - it simple divide the data into two distinct groups.\nTo test the efficacy of this basic model, we first split our data into training and test dataset and encode our categorical variables.\n\nfrom numpy import random\nfrom sklearn.model_selection import train_test_split\n\nrandom.seed(42)\ntrn_df,val_df = train_test_split(df, test_size=0.25)\ntrn_df[cats] = trn_df[cats].apply(lambda x: x.cat.codes)\nval_df[cats] = val_df[cats].apply(lambda x: x.cat.codes)\n\nNext, let’s create function to to extract independent variables (xs) and the dependent variable (y).\n\ndef xs_y(df):\n    xs = df[cats+conts].copy()\n    return xs,df[dep] if dep in df else None\n\ntrn_xs,trn_y = xs_y(trn_df)\nval_xs,val_y = xs_y(val_df)\n\nFrom here we make predictions:\n\nfrom sklearn.metrics import mean_absolute_error\npreds = val_xs.Sex==0\nmean_absolute_error(val_y, preds)\n\n0.21524663677130046\n\n\nA 21.5% error rate isn’t too shabby for such a simple model. Can we do better? Let’s try another variable such as Fare which is continuous.\n\ndf_fare = trn_df[trn_df.LogFare&gt;0]\nfig,axs = plt.subplots(1,2, figsize=(11,5))\nsns.boxenplot(data=df_fare, x=dep, y=\"LogFare\", ax=axs[0])\nsns.kdeplot(data=df_fare, x=\"LogFare\", ax=axs[1]);\n\n\n\n\n\n\n\n\nThe boxenplot shows that those who survived generally paid higher fares.\nSo here’s another model LogFare greater than 2.7:\n\npreds = val_xs.LogFare&gt;2.7\nmean_absolute_error(val_y, preds)\n\n0.336322869955157\n\n\nOh, much worse\nTo evaluate binary split uniformly, regardless of the datatype, We measure how similar the dependent variable values are within each split. We aim for standard deviations within groups, multiplied by group sizes to account for impact differences.\n\ndef _side_score(side, y):\n    tot = side.sum()\n    if tot&lt;=1: return 0\n    return y[side].std()*tot\n\ndef score(col, y, split):\n    lhs = col&lt;=split\n    return (_side_score(lhs,y) + _side_score(~lhs,y))/len(y)\n\nSo for example, if we split by Sex, is greater than or less than 0.5.That’ll create two groups, males and females, and that gives us this score.\n\nscore(trn_xs[\"Sex\"], trn_y, 0.5)\n\n0.4078753098206398\n\n\nAnd if we do LogFare greater than or less than 2.7, it gives us this score.\n\nscore(trn_xs[\"LogFare\"], trn_y, 2.7)\n\n0.4718087395209973\n\n\nLower scores indicates better splits, with Sex outperforming LogFare. But how can we find a best split point i mean we have to try ourself right? In every values and see if the score improve or not right, well that was pretty inefficient. It would be nice if we could find some automatic wway to do al that. Well, of course we can. If we want to find the best split point for Age, and try each one in turn, and see what score we get, if we made a binary split on that level of Age. So here’s a list of all the possible binary split threshold of Age\n\ncol = trn_xs[\"Age\"]\nunq = col.unique()\nunq.sort()\nunq\n\narray([ 0.42,  0.67,  0.75,  0.83,  0.92,  1.  ,  2.  ,  3.  ,  4.  ,  5.  ,  6.  ,  7.  ,  8.  ,  9.  , 10.  , 11.  , 12.  ,\n       13.  , 14.  , 14.5 , 15.  , 16.  , 17.  , 18.  , 19.  , 20.  , 21.  , 22.  , 23.  , 24.  , 24.5 , 25.  , 26.  , 27.  ,\n       28.  , 28.5 , 29.  , 30.  , 31.  , 32.  , 32.5 , 33.  , 34.  , 34.5 , 35.  , 36.  , 36.5 , 37.  , 38.  , 39.  , 40.  ,\n       40.5 , 41.  , 42.  , 43.  , 44.  , 45.  , 45.5 , 46.  , 47.  , 48.  , 49.  , 50.  , 51.  , 52.  , 53.  , 54.  , 55.  ,\n       55.5 , 56.  , 57.  , 58.  , 59.  , 60.  , 61.  , 62.  , 64.  , 65.  , 70.  , 70.5 , 74.  , 80.  ])\n\n\nLet’s go through all of them. For each of them calculate the score and then Numpy and Pytorch have an argmin() function, which tells you what index into that list is the smallest.\n\nscores = np.array([score(col, trn_y, o) for o in unq if not np.isnan(o)])\nunq[scores.argmin()]\n\n6.0\n\n\nHere’s the scores.\n\nscores\n\narray([0.48447755, 0.48351588, 0.48158676, 0.48061929, 0.47964987, 0.480937  , 0.48347294, 0.48171397, 0.47987776, 0.47884826,\n       0.47831672, 0.47949847, 0.47957573, 0.48092137, 0.48130659, 0.48200571, 0.48163287, 0.48124801, 0.48151498, 0.48183316,\n       0.48105614, 0.48202484, 0.48178211, 0.48337829, 0.48439618, 0.48501782, 0.48545475, 0.48556795, 0.48550856, 0.48554074,\n       0.48550094, 0.48504976, 0.48480161, 0.48561331, 0.4852559 , 0.48513473, 0.48529147, 0.48530156, 0.48543741, 0.48569729,\n       0.48571309, 0.48571467, 0.4856701 , 0.48563657, 0.48579877, 0.48579767, 0.4858019 , 0.48580095, 0.48580002, 0.48580178,\n       0.48580211, 0.48579777, 0.4857996 , 0.48580236, 0.48579236, 0.48580043, 0.48580303, 0.4858034 , 0.4857613 , 0.4855666 ,\n       0.48579394, 0.48580506, 0.48580434, 0.48580707, 0.48579364, 0.48580788, 0.48581017, 0.48580597, 0.48581077, 0.48576815,\n       0.48580167, 0.48545792, 0.48567909, 0.48542059, 0.48557468, 0.48492654, 0.4852198 , 0.48548666, 0.48590271, 0.48601112,\n       0.48447755, 0.48543732])\n\n\nCreate a function to calculate this for any column:\n\ndef min_col(df, nm):\n    col,y = df[nm],df[dep]\n    unq = col.dropna().unique()\n    scores = np.array([score(col, y, o) for o in unq if not np.isnan(o)])\n    idx = scores.argmin()\n    return unq[idx],scores[idx]\n\nmin_col(trn_df, \"Age\")\n\n(6.0, 0.47831671750899085)\n\n\nRevealing that is at 6.0 for Age. So now we can just go through and calculates the score for the best split point for each column.\n\ncols = cats+conts\n{o:min_col(trn_df, o) for o in cols}\n\n{'Sex': (0, 0.4078753098206398),\n 'Embarked': (0, 0.478833425731479),\n 'Age': (6.0, 0.47831671750899085),\n 'SibSp': (4, 0.4783740258817423),\n 'Parch': (0, 0.4805296527841601),\n 'LogFare': (2.4390808375825834, 0.4620823937736595),\n 'Pclass': (2, 0.4604826188580666)}\n\n\nAnd if we do that, we find that the lowest score is Sex. So that is how to calculate the best binary split. So we now know that the model we created earlier with Sex is the best single binary split model we can find.\nAnd this simple thing we just did which is finding a single binary split, actually is a type of model, it has a name too, it’s called OneR. And OneR model it turned out in a review of machine learning methods in the 90s is one of the best, if not the best. It’s not a bad idea to always start creating a baseline of OneR, a decision tree with a single binary split.\n\n\nCreating a Tree\n“OneR” is probably not going to cut it for a lot of things, though it’s surprisingly effective, but maybe we could go a step further. And the other step further we could go is by creating a maybe “TwoR”. What if we took each of those groups, males and females in the Titanic dataset, and split each of these into two other groups? So split the males into two groups and split the females into two groups. To do that, we can repeat the exact same piece of code we just did, but let’s remove sex from it:\n\ncols.remove(\"Sex\")\nismale = trn_df.Sex==1\nmales,females = trn_df[ismale],trn_df[~ismale]\n\nThen, run the same piece of code that we just did before, but just for the males:\n\n{o:min_col(males, o) for o in cols}\n\n{'Embarked': (0, 0.387558187041091),\n 'Age': (6.0, 0.37398283710105873),\n 'SibSp': (4, 0.38758642275862637),\n 'Parch': (0, 0.3874704821461953),\n 'LogFare': (2.803360380906535, 0.38048562317581447),\n 'Pclass': (1, 0.3815544200436083)}\n\n\nThis provides a “OneR” rule for how to predict which males survived the Titanic, Interestingly, age turns out to be the biggest predictor for males whether they were greater than or less than 6 determined their survival odds\nSimilarity, for females:\n\n{o:min_col(females, o) for o in cols}\n\n{'Embarked': (0, 0.4295252982857326),\n 'Age': (50.0, 0.4225927658431646),\n 'SibSp': (4, 0.42319212059713585),\n 'Parch': (3, 0.4193314500446157),\n 'LogFare': (4.256321678298823, 0.413505983329114),\n 'Pclass': (2, 0.3335388911567602)}\n\n\nThe passenger class Pclass, or whether they were in first class or not, was the biggest predictor of survival.\nThis process generates a decision tree - a serries of binary splits that gradually categorize our data so that in the leaf nodes, we derive strong predictions about survival\nWe can continue these steps for each of the four groups manually with a couple of extra lines of code, or we can use a decision tree classifier. This class automates the process we just outlined:\n\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\n\nm = DecisionTreeClassifier(max_leaf_nodes=4).fit(trn_xs, trn_y);\n\nAnd one very nice thing it has is it can draw the tree for us. So here’s a tiny little draw_tree function:\n\nimport graphviz\n\ndef draw_tree(t, df, size=10, ratio=0.6, precision=2, **kwargs):\n    s=export_graphviz(t, out_file=None, feature_names=df.columns, filled=True, rounded=True,\n                      special_characters=True, rotate=False, precision=precision, **kwargs)\n    return graphviz.Source(re.sub('Tree {', f'Tree {{ size={size}; ratio={ratio}', s))\n\ndraw_tree(m, trn_xs, size=10)\n\n\n\n\n\n\n\n\nAnd you can see here it’s going to first of all split on sex. Now, it looks a bit weird to say sex is less than or equal to 0.5, but remember our binary characteristics are coded as zero or one. This is just an easy way to denote males versus females.\nFor females, the next split is based on their class. For males, age is the dedicating factor. This creates our four leaf nodes. For instance, of the females in the first class, 116 survived, and only 4 didn’t showing that being a wealthy woman on the Titanic was quite advantageous. On the other hand, among adult males, 68 survived while 350 perished, illustrating the peril they faced.\nThis quick summary showcases why decision trees are favoured in exploratory data analysis; they provide a clear picture of key variables driving the dataset and their predictive power\nOne additional point is the Gini measure, a way of evaluating how good a split is, which i’ve illustrated in the code below:\n\ndef gini(cond):\n    act = df.loc[cond, dep]\n    return 1 - act.mean()**2 - (1-act).mean()**2\n\nTo understand this mathematically: if \\(p\\) is a probability of an instance being classified as a positive class, and \\((1 - p)\\) for the negative class, \\(p^2\\) denotes the chance of both randomly selected instances being positive and \\((1-p)^2\\) being negative. The term \\((1-p^2 - (1-p)^2)\\) gives us the probability of misclassification, subtracting the chances of correctly classifying instances.\nHere’s an example of Gini calculation for gender:\n\ngini(df.Sex=='female'), gini(df.Sex=='male')\n\n(0.3828350034484158, 0.3064437162277842)\n\n\nHere, act.mean()**2 is the probability that two randomly selected individual both survived, and (1 - act.mean())**2 that both did not. Lower Gini impurity suggests a strong skew in survival outcomes, which can be insightful for decision making or predicting survival likelihood based on gender.\nDecision trees thus provide not only visual insights but quantitative ways to discern what’s happening within your dataset.\n\nmean_absolute_error(val_y, m.predict(val_xs))\n\n0.2242152466367713\n\n\nSo that was for the “OneR” version. For the decision tree with four leaf nodes, the mean absolute error was 0.224, which is actually a bit worse. This outcome suggest that due to the small size of the dataset, the “OneR” method was impressively effective, and enhancements weren’t substantial enough to be discerned among the randomness of such a small validation set.\nTo take it further, let’s implement a decision tree with a minimum of 50 samples per leaf node:\n\nm = DecisionTreeClassifier(min_samples_leaf=50)\nm.fit(trn_xs, trn_y)\ndraw_tree(m, trn_xs, size=12)\n\n\n\n\n\n\n\n\nThis indicates that each leaf will contain at least 50 samples, in this context passengers on the Titanic. For example, suppose you’ve identified that 67 people were female, first-class, and under 28. That’s the point where the tree ceases splitting further\nLet’s evaluate this decision tree:\n\nmean_absolute_error(val_y, m.predict(val_xs))\n\n0.18385650224215247\n\n\nWith an absolute error of 0.183, this approach shows a bit of improvement.\nAn interesting aspect of decision trees is the minimal preprocessing required you may have noticed this advantage. There was no need for dummy variables for category features, and although you can create them, it isn’t necessary. Decision trees can manage without these adjustments. We only took the logarithm of the fare to enhance the visual appearance of our graph but the split would operate identically on the original scale, focusing only on data ordering\nMoreover, decision trees are indifferent to outliers, long-tailed distributions, and categorical variables: they handle all these situations effectively.\nThe take away here is that for tabular data, starting with a decision tree-based approach is prudent. It helps create baselines because they are remarkably resilient and offer a robust performance without intricate tuning"
  },
  {
    "objectID": "posts/2024-11-12-random-forest/index.html#random-forest",
    "href": "posts/2024-11-12-random-forest/index.html#random-forest",
    "title": "Exploring Random Forest, Bagging, Boosting?",
    "section": "Random Forest",
    "text": "Random Forest\nNow, what if we wanted to make this more accurate? Could we grow the tree further? We could, but with only 50 samples in these leaves, further splitting would result in the leaf nodes having so little data that their predictions wouldn’t be very meaningful. Naturally, there are limitation to how accurate a decision tree can be. so, what we can do? Enter a fascinating strategy called bagging.\nHere’s the procedure of bagging:\n\nRandomly choose a subset of data rows (a “bootstrap replicate” of the learning set).\nTrain a model using this subset.\nSave that model, then go back to step 1 and repeat several times.\nThis will give you multiple trained models. Predict with all models, and then average their predictions to make the final prediction.\n\nThe core insight of bagging is that although models trained on data subsets will make more errors than a model trained on the full dataset, these errors aren’t correlated across models. Different models will make different errors, and when averaged, those errors offset each other. Thus, average the predictions of all the model sharpens the final prediction with more models providing finer estimations.\nIn essence, a random forest averages the predictions of numerous decision trees, which are generated randomly varying parameters such as training dataset or tree parameters. Bagging is a particular approach to “ensembling” or combining results from multiple models.\nLet’s create one in a few lines. Here’s a function to generate a decision tree:\n\ndef get_tree(prop=0.75):\n    n = len(trn_y)\n    idxs = random.choice(n, int(n*prop))\n    return DecisionTreeClassifier(min_samples_leaf=5).fit(trn_xs.iloc[idxs], trn_y.iloc[idxs])\n\nHere, prop denotes the data proportion used, say 75% each time with n as the sample size. Random samples idxs are selected based on the specified proportion, and a decision tree is built from this subset.\nLet’s get 100 trees and compile them into a list:\n\ntrees = [get_tree() for t in range(100)]\nall_probs = [t.predict(val_xs) for t in trees]\navg_probs = np.stack(all_probs).mean(0)\n\nmean_absolute_error(val_y, avg_probs)\n\n0.2272645739910314\n\n\nBy collecting predictions from these trees, stacking them, and averaging their predictions, we have our random forest.\nRandom forests are remarkably simple yet powerful. A key feature is that they also randomly select subset of columns to build decision trees, changing the column subset with each node split. The idea is to maintain randomness, yet retain usefulness. For more efficient implementation, we use RandomForestClassifier:\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(100, min_samples_leaf=5)\nrf.fit(trn_xs, trn_y);\nmean_absolute_error(val_y, rf.predict(val_xs))\n\n0.18834080717488788\n\n\nHere, we specify the number of trees and samples per leaf, then fit the classifier. While our mean absolute error might not surpass a single decision tree due to dataset constraints, it remains robust\nOne can inspect the built decision trees to identify split columns. Monitoring column improvements in Gini across decision trees yields a feature importance plot:\n\npd.DataFrame(dict(cols=trn_xs.columns, imp=m.feature_importances_)).plot('cols', 'imp', 'barh');\n\n\n\n\n\n\n\n\nFeature importance plots demonstrate a feature’s significance by indicating how frequently and effectively it was used for splits. The Sex variable emerges as most significant, follow by Pclass, with other variables less crucial. And this is another reason, by the way, why the random forest isn’t really particularly helpful, because it’s just a easy split to do, basically all the matter is what class you are in and whether you’re male of female.\nRandom Forests, due to their versatility with data distribution and categorical variable handling, allow immediate and insightful datasets analyses. For large datasets, they quickly reveal key features, facilitating further focused analysis."
  },
  {
    "objectID": "posts/2024-11-12-random-forest/index.html#what-else-can-we-do-with-random-forest",
    "href": "posts/2024-11-12-random-forest/index.html#what-else-can-we-do-with-random-forest",
    "title": "Exploring Random Forest, Bagging, Boosting?",
    "section": "What else can we do with Random Forest",
    "text": "What else can we do with Random Forest\nThere are other things that you can do with Random Forests and the Titanic dataset is a small one, so it doesn’t highlight the full power of Random Forests. For a bigger and more numerically interesting dataset, let’s consider the auction price of heavy industrial equipment. This dataset is from The Blue Book for Bulldozers Kaggle competition. I highly recommended taking a peek at the overview and the dataset on the competition page before we start.\n\nPreparing Stuff\n\nDownloading the Dataset\n\n\nImport stuff click to show the code\nfrom fastbook import *\nfrom pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype\nfrom fastai.tabular.all import *\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom dtreeviz.trees import *\nfrom IPython.display import Image, display_svg, SVG\n\nimport warnings\nwarnings.simplefilter('ignore', FutureWarning)\n\npd.options.display.max_rows = 20\npd.options.display.max_columns = 8\n\n\nPick a path to download the dataset:\n\ncomp = 'bluebook-for-bulldozers'\npath = URLs.path(comp)\npath\n\nPath('/home/monarch/.fastai/archive/bluebook-for-bulldozers')\n\n\nUse the Kaggle API to download the data to the specified path and extract it:\n\nfrom kaggle import api\n\nif not path.exists():\n    path.mkdir(parents=true)\n    api.competition_download_cli(comp, path=path)\n    shutil.unpack_archive(str(path/f'{comp}.zip'), str(path))\n\npath.ls(file_type='text')\n\nDownloading bluebook-for-bulldozers.zip to /home/monarch/.fastai/archive/bluebook-for-bulldozers\n\n\n100%|██████████| 48.4M/48.4M [00:07&lt;00:00, 6.52MB/s]\n\n\n\n\n\n(#7) [Path('/home/monarch/.fastai/archive/bluebook-for-bulldozers/Machine_Appendix.csv'),Path('/home/monarch/.fastai/archive/bluebook-for-bulldozers/Test.csv'),Path('/home/monarch/.fastai/archive/bluebook-for-bulldozers/TrainAndValid.csv'),Path('/home/monarch/.fastai/archive/bluebook-for-bulldozers/Valid.csv'),Path('/home/monarch/.fastai/archive/bluebook-for-bulldozers/ValidSolution.csv'),Path('/home/monarch/.fastai/archive/bluebook-for-bulldozers/median_benchmark.csv'),Path('/home/monarch/.fastai/archive/bluebook-for-bulldozers/random_forest_benchmark_test.csv')]\n\n\nI’ll now walk you through the dataset. If you examine the Data tab on the competition page, here are the key fields found in train.csv:\n\nSalesID: The unique identifier of the sale.\nMachineID: the unique identifier of the machine. A machine can be sold multiple times.\nsaleprice: The auction sale price of the machine (only provided in train.csv)\nsaledate: The date the sale occurred.\n\nWe begin by reading the training set into Pandas DataFrame. It’s generally advisable to specify low_memory=False unless Pandas runs out of memory and throws an error. By default, low_memory is True, instructing Pandas to process data in chucks, which may lead to inconsistent column data types and subsequent data processing or modeling errors.\n\ndf = pd.read_csv(path/'TrainAndValid.csv', low_memory=False)\ndf.columns\n\nIndex(['SalesID', 'SalePrice', 'MachineID', 'ModelID', 'datasource',\n       'auctioneerID', 'YearMade', 'MachineHoursCurrentMeter', 'UsageBand',\n       'saledate', 'fiModelDesc', 'fiBaseModel', 'fiSecondaryDesc',\n       'fiModelSeries', 'fiModelDescriptor', 'ProductSize',\n       'fiProductClassDesc', 'state', 'ProductGroup', 'ProductGroupDesc',\n       'Drive_System', 'Enclosure', 'Forks', 'Pad_Type', 'Ride_Control',\n       'Stick', 'Transmission', 'Turbocharged', 'Blade_Extension',\n       'Blade_Width', 'Enclosure_Type', 'Engine_Horsepower', 'Hydraulics',\n       'Pushblock', 'Ripper', 'Scarifier', 'Tip_Control', 'Tire_Size',\n       'Coupler', 'Coupler_System', 'Grouser_Tracks', 'Hydraulics_Flow',\n       'Track_Type', 'Undercarriage_Pad_Width', 'Stick_Length', 'Thumb',\n       'Pattern_Changer', 'Grouser_Type', 'Backhoe_Mounting', 'Blade_Type',\n       'Travel_Controls', 'Differential_Type', 'Steering_Controls'],\n      dtype='object')\n\n\nThat’s many columns to scour! Start by exploring the dataset to familiarize yourself with the data content in each column. Soon we’ll focus on the most compelling bits.\nWith ordinal columns, it’s beneficial to specify meaningful order. These columns contain strings with an inherent sequence. For example, check out the ProducSize levels:\n\ndf['ProductSize'].unique()\n\narray([nan, 'Medium', 'Small', 'Large / Medium', 'Mini', 'Large', 'Compact'], dtype=object)\n\n\nInstruct Pandas about the relevant order of these levels:\n\nsizes = 'Large','Large / Medium','Medium','Small','Mini','Compact'\n\ndf['ProductSize'] = df['ProductSize'].astype('category')\ndf['ProductSize'].cat.set_categories(sizes, ordered=True)\n\n0            NaN\n1         Medium\n2            NaN\n3          Small\n4            NaN\n           ...  \n412693      Mini\n412694      Mini\n412695      Mini\n412696      Mini\n412697      Mini\nName: ProductSize, Length: 412698, dtype: category\nCategories (6, object): ['Large' &lt; 'Large / Medium' &lt; 'Medium' &lt; 'Small' &lt; 'Mini' &lt; 'Compact']\n\n\nIn this dataset, Kaggle suggests using Root Mean Square Log Error (RMSLE) as the metric for comparing actual versus predicted auction prices.\n\ndep_var = 'SalePrice'\ndf[dep_var] = np.log(df[dep_var])\n\nThis transformation ensures that the target variables is in format suitable for modeling.\n\n\nData Preparation\nThe first piece of data preparation we need to to do is enrich our representation of dates. The fundamental basis of the decision tree that we just discussed is bisection (dividing a group into two). We look at the ordinal variables and divide the dataset based on whether the variable’s value is greater (ow lower) than a threshold, and we look at the categorical variables and divide the dataset based on whether the variable’s level is a particular level. This algorithm divides the dataset based on both original and categorical data\nBut how does this apply to a common data type, the date? You might want to tree at date as an ordinal value because it is meaningful to say that one date is greater than other. However, dates are a bit different from most ordinal values in that some dates are qualitatively different from others, which is often relevant to the systems we are modeling.\nTo help our algorithm handle dates intelligently, we’d like our model to know ore than whether a date is more recent or less recent than other. We might want our model to make decisions based on that date’s day of the week, on whether a day is holiday, on what month it is in, and so forth. To accomplish this, we replace every date column with a set of date metadata columns, such as holiday, day of the week, and month. These columns provide categorical data that we suspect will be useful.\nFastai comes with a function to do this for us that mean we only need to pass in a column name that contains dates:\n\ndf = add_datepart(df, 'saledate')\n\n# do the same for the test set\ndf_test = pd.read_csv(path/'Test.csv', low_memory=False)\ndf_test = add_datepart(df_test, 'saledate')\n\nWe can see that there are now many new columns in our DataFrame:\n\n' '.join(o for o in df.columns if o.startswith('sale'))\n\n'saleYear saleMonth saleWeek saleDay saleDayofweek saleDayofyear saleIs_month_end saleIs_month_start saleIs_quarter_end saleIs_quarter_start saleIs_year_end saleIs_year_start saleElapsed'\n\n\nThis a solid first step, but we need further data cleaning. For this, we will use fastai objects called TabularPandas and TabularProc.\nAnother aspect of preparatory processing is ensuring we can handle strings and missing data. We will use fastai’s class TabularPandas, which wraps a Pandas DataFrame and offers some conveniences. when we say it “wraps” a DataFrame, it means taking a Pandas DataFrame as input and adding additional specifically useful for machine-learning tasks with tabular data. To populate a TabularPandas, we will utilize two TabularProcs: Categorify and FillMissing.\nTabularProcs are unique data transformation process used in fastai designed to prepare you data to ML models. We introduce two specific TabularProcs here:\n\nCategorify: convert categorical columns text or non numeric data into numeric categories. For instance, a column Color with values like “Red”, “Blue”, “Green” could be encoded as 1, 2, 3.\nFillMissing: Manages missing data in your dataset. it replaces missing values with the column’s median value and creates a new boolean column to flag rows that originally had missing values.\n\nHow TabularProc differs from regular transforms:\n\nReturns the exact same object that’s passed to it, after modifying the object in place, which optimizes memory efficiency especially with large datasets.\nExecutes the transformation immediately when the data is first passed in rather than delaying until the data is accessed.\n\nIn practical terms, when using TabularPandas with TabularProcs:\n\nStart with your raw data in a Pandas DataFrame.\nWrap this DataFrame with TabularPandas.\nApply TabularProcs (Categorify and FillMissing)\nThese procs instantly process all your data, converting categories to numbers and filling in missing values.\nThe outcome is a dataset ready for machine learning models, with all categorical data converted and missing values addressed.\n\nThis methodology streamlines the data preparation process, ensure consistent data processing ready for model training or inference.\n\nprocs = [Categorify, FillMissing]\n\nTabularPandas will also manage the dataset split into training and validation sets for us.\n\n\nHandling a Time Series\nWhen dealing with time series data, randomly selecting a subset of data points for training and validation is not sufficient, as sequence of data is vital. The test set represents a future six-month period starting from May 2012, thus not overlapping with the training set. This setup is intentional because the competition sponsor aims to evaluate the model’s predictive capability selected from a later time than your training dataset.\nThe provided Kaggle training data concludes in April 2012. Therefore, we’ll construct to focused training dataset comprising data from before November 2011 and establish a validation set with data from after November 2011.\nThis is achieved using np.where, which helps in obtaining indices for specific conditions:\n\ncond = (df.saleYear&lt;2011) | (df.saleMonth&lt;10)\ntrain_idx = np.where( cond)[0]\nvalid_idx = np.where(~cond)[0]\n\nsplits = (list(train_idx),list(valid_idx))\n\nTabularPandas requires knowledge of which columns are continuous and which are categorical. We can simplify this with the cont_cat_split helper function:\n\ncont,cat = cont_cat_split(df, 1, dep_var=dep_var)\nto = TabularPandas(df, procs, cat, cont, y_names=dep_var, splits=splits)\n\nThis setup turns TabularPandasinto something akin to a fastai Dataset object, with accessible train and valid attributes:\n\nlen(to.train),len(to.valid)\n\n(404710, 7988)\n\n\nIt’s possible to view the dataset’s categorical variables still represented as strings:\n\nto.show(3)\n\n\n\n\n\nUsageBand\nfiModelDesc\nfiBaseModel\nfiSecondaryDesc\nfiModelSeries\nfiModelDescriptor\nProductSize\nfiProductClassDesc\nstate\nProductGroup\nProductGroupDesc\nDrive_System\nEnclosure\nForks\nPad_Type\nRide_Control\nStick\nTransmission\nTurbocharged\nBlade_Extension\nBlade_Width\nEnclosure_Type\nEngine_Horsepower\nHydraulics\nPushblock\nRipper\nScarifier\nTip_Control\nTire_Size\nCoupler\nCoupler_System\nGrouser_Tracks\nHydraulics_Flow\nTrack_Type\nUndercarriage_Pad_Width\nStick_Length\nThumb\nPattern_Changer\nGrouser_Type\nBackhoe_Mounting\nBlade_Type\nTravel_Controls\nDifferential_Type\nSteering_Controls\nsaleIs_month_end\nsaleIs_month_start\nsaleIs_quarter_end\nsaleIs_quarter_start\nsaleIs_year_end\nsaleIs_year_start\nauctioneerID_na\nMachineHoursCurrentMeter_na\nSalesID\nMachineID\nModelID\ndatasource\nauctioneerID\nYearMade\nMachineHoursCurrentMeter\nsaleYear\nsaleMonth\nsaleWeek\nsaleDay\nsaleDayofweek\nsaleDayofyear\nsaleElapsed\nSalePrice\n\n\n\n\n0\nLow\n521D\n521\nD\n#na#\n#na#\n#na#\nWheel Loader - 110.0 to 120.0 Horsepower\nAlabama\nWL\nWheel Loader\n#na#\nEROPS w AC\nNone or Unspecified\n#na#\nNone or Unspecified\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n2 Valve\n#na#\n#na#\n#na#\n#na#\nNone or Unspecified\nNone or Unspecified\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\nStandard\nConventional\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n1139246\n999089\n3157\n121\n3.0\n2004\n68.0\n2006\n11\n46\n16\n3\n320\n1.163635e+09\n11.097410\n\n\n1\nLow\n950FII\n950\nF\nII\n#na#\nMedium\nWheel Loader - 150.0 to 175.0 Horsepower\nNorth Carolina\nWL\nWheel Loader\n#na#\nEROPS w AC\nNone or Unspecified\n#na#\nNone or Unspecified\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n2 Valve\n#na#\n#na#\n#na#\n#na#\n23.5\nNone or Unspecified\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\nStandard\nConventional\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n1139248\n117657\n77\n121\n3.0\n1996\n4640.0\n2004\n3\n13\n26\n4\n86\n1.080259e+09\n10.950807\n\n\n2\nHigh\n226\n226\n#na#\n#na#\n#na#\n#na#\nSkid Steer Loader - 1351.0 to 1601.0 Lb Operating Capacity\nNew York\nSSL\nSkid Steer Loaders\n#na#\nOROPS\nNone or Unspecified\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\nAuxiliary\n#na#\n#na#\n#na#\n#na#\n#na#\nNone or Unspecified\nNone or Unspecified\nNone or Unspecified\nStandard\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n1139249\n434808\n7009\n121\n3.0\n2001\n2838.0\n2004\n2\n9\n26\n3\n57\n1.077754e+09\n9.210340\n\n\n\n\n\nHowever, all underlying data has been converted to numeric form:\n\nto.items.head(3)\n\n\n\n\n\n\n\n\nSalesID\nSalePrice\nMachineID\nModelID\n...\nsaleIs_year_start\nsaleElapsed\nauctioneerID_na\nMachineHoursCurrentMeter_na\n\n\n\n\n0\n1139246\n11.097410\n999089\n3157\n...\n1\n1.163635e+09\n1\n1\n\n\n1\n1139248\n10.950807\n117657\n77\n...\n1\n1.080259e+09\n1\n1\n\n\n2\n1139249\n9.210340\n434808\n7009\n...\n1\n1.077754e+09\n1\n1\n\n\n\n\n3 rows × 67 columns\n\n\n\nCategorical columns undergo transformation by substituting each unique category with a number. These numbers are assigned consecutively as they first appear, implying no intrinsic value to these numbers, unless ordered categories (like ProductSize) pre-specify the sequence. You can check the mapping through the classes attribute:\n\nto.classes['ProductSize']\n\n['#na#', 'Compact', 'Large', 'Large / Medium', 'Medium', 'Mini', 'Small']\n\n\nA neat feature in fastai is the ability to save processed data, which can be time-consuming. Saving the data allows you to resume further work without repeating the preprocessing steps. Fastai utilizes Python’s pickle system for this purpose:\n\nsave_pickle(path/'to.pkl',to)\n\nto retrieve it later you’ll simply do:\n\nto = load_pickle(path/'to.pkl')\n\nWith preprocessing complete, we’re set to create a decision tree.\n\n\n\nDecision Tree Ensembles\nLet’s consider how we find the right questions to ask when creating decision trees. Fortunately we don’t have to do this manually computer are designed for this purpose! Here’s a simple overview of training a decision tree:\n\nLoop through each column of the dataset in turn.\nFor each column, loop through each possible level of that column in turn.\nTry splitting the data into two groups, based on whether they are greater than or less than that value (or if it is a categorical variable, based on whether they are equal to or not equal to that level of that categorical variable).\nFind the average sale price for each of those two groups, and see how close that is to the actual sale price of each of the items of equipment in that group. That is, treat this as a very simple “model” where our predictions are simply the average sale price of the item’s group.\nAfter looping through all of the columns and all the possible levels for each, pick the split point that gave the best predictions using that simple model.\nWe now have two different groups for our data, based on this selected split. Treat each of these as separate datasets, and find the best split for each by going back to step 1 for each group.\nContinue this process recursively, until you have reached some stopping criterion for each group—for instance, stop splitting a group further when it has only 20 items in it.\n\nTo implement this, start by defining your independent and dependent variables:\n\nxs,y = to.train.xs,to.train.y\nvalid_xs,valid_y = to.valid.xs,to.valid.y\n\n\nxs.head()\n\n\n\n\n\n\n\n\nUsageBand\nfiModelDesc\nfiBaseModel\nfiSecondaryDesc\n...\nsaleDay\nsaleDayofweek\nsaleDayofyear\nsaleElapsed\n\n\n\n\n0\n2\n963\n298\n43\n...\n16\n3\n320\n1.163635e+09\n\n\n1\n2\n1745\n529\n57\n...\n26\n4\n86\n1.080259e+09\n\n\n2\n1\n336\n111\n0\n...\n26\n3\n57\n1.077754e+09\n\n\n3\n1\n3716\n1381\n0\n...\n19\n3\n139\n1.305763e+09\n\n\n4\n3\n4261\n1538\n0\n...\n23\n3\n204\n1.248307e+09\n\n\n\n\n5 rows × 66 columns\n\n\n\nOnce your data is numeric and lacks missing values, you can create a decision tree:\n\nm = DecisionTreeRegressor(max_leaf_nodes=4)\nm.fit(xs, y);\n\nHere, we’ve instructed sklearn to create four leaf nodes. To visualize what the model has learned, we can display the tree:\n\ndraw_tree(m, xs, size=10, leaves_parallel=True, precision=2)\n\n\n\n\n\n\n\n\nUnderstanding this visualization helps in graphing decision tree:\n\nTop node: Represents the entire dataset before any splits. Average sale price (log) is 10.10, with a mean squared error of 0.48.\nFirst split: Based on coupler_system.\n\n\nLeft branch: coupler_system &lt; 0.5 (360,847 records, avg. 10.21)\nRight branch: coupler_system &gt; 0.5 (43,863 records, avg. 9.21)\n\n\nSecond split (on left branch): Based on YearMade.\n\n\nLeft sub-branch: YearMade &lt;= 1991.5 (155,724 records, avg. 9.97)\nRight sub-branch: YearMade &gt; 1991.5 (205,123 records, avg. 10.4)\n\n\nLeaf nodes: The bottom row, where no more splits occur.\n\nWe can display this information using Terence Parr’s dtreeviz library to enhance visualization:\n\nsamp_idx = np.random.permutation(len(y))[:500]\ndtreeviz(m, xs.iloc[samp_idx], y.iloc[samp_idx], xs.columns, dep_var,\n        fontname='DejaVu Sans', scale=1.6, label_fontsize=10,\n        orientation='LR')\n\n\n\n\n\n\n\n\nThis visualization illuminates data distribution, showcasing issues like bulldozers dated to the year 1000, likely placeholders for missing data. For modeling precision, these can be substituted with 1950 to improve visualization clarity without significantly influencing model results:\n\nxs.loc[xs['YearMade']&lt;1900, 'YearMade'] = 1950\nvalid_xs.loc[valid_xs['YearMade']&lt;1900, 'YearMade'] = 1950\n\nThis update clarifies the tree visualization while maintaining the models integrity. After making this change, re-evaluate the decision tree:\n\nm = DecisionTreeRegressor(max_leaf_nodes=4).fit(xs, y)\n\ndtreeviz(m, xs.iloc[samp_idx], y.iloc[samp_idx], xs.columns, dep_var,\n        fontname='DejaVu Sans', scale=1.6, label_fontsize=10,\n        orientation='LR')\n\n\n\n\n\n\n\n\nNow, let’s leverage the decision tree algorithm to generate a more complex model. This time, we’ll refrain from specifying any stopping criteria, such as max_leaf_nodes:\n\nm = DecisionTreeRegressor()\nm.fit(xs, y);\n\nTo evaluate our model’s performance, we’ll define a function to compute the root mean squared error(RMSE) which was the scoring criterion in this competition:\n\ndef r_mse(pred,y): return round(math.sqrt(((pred-y)**2).mean()), 6)\ndef m_rmse(m, xs, y): return r_mse(m.predict(xs), y)\nm_rmse(m, xs, y)\n\n0.0\n\n\nThe output is 0.0. At the first glance, it appears that our model is flawless. But hold on, we need to evalueate the validation set to check for overfitting:\n\nm_rmse(m, valid_xs, valid_y)\n\n0.332239\n\n\nThe validation set RMSE is 0.332239, indicating potential overfitting. Let’s investigating further\n\nm.get_n_leaves(), len(xs)\n\n(324338, 404710)\n\n\nIt turns out our model has nearly as many leaves as data point! This occurs because sklearn’s default setting allow continual splitting until there’s just one item per leaf node. We can address this by adjusting the stopping rule to require each leaf node to have at least 25 auction records:\n\nm = DecisionTreeRegressor(min_samples_leaf=25)\nm.fit(to.train.xs, to.train.y)\nm_rmse(m, xs, y), m_rmse(m, valid_xs, valid_y)\n\n(0.243049, 0.308857)\n\n\nThis results in a more balanced model. Let’s verify the new number of leaves:\n\nm.get_n_leaves()\n\n12432\n\n\nDecision trees are adept at modelling data due to their adaptability to nonlinear relationships and variable interactions. Nonetheless, a compromise exist between generallizability (achieved with smaller trees) and training accuracy (achieved with larger trees)\nHow do wee balance these strengths? We’ll explore further after covering essential aspect handling categorical variables.\nIn deep learning, categorical variables are often one-hot encoded and fed into embedding layers. However, decision trees lack embedding layers so how can we leverage untreated categorical variables efficiently? let’s consider a use-case with product codes.\nSuppose we have an auction dataset with product codes (categorical variables) and sale prices. “Product X” for instance, consistently sells at a premium. Decision trees split data based on features optimally partition the target variable. A split distinguishing “Product X” from others creates:\n\nGroup A: containing product X\nGroup B: containing all other products\n\nThis chose arises because “Product X” is notably pricier, leading Group A to have a higher average price than Group B. This split provides valuable insights for price prediction, prompting the algorithm to prefer it. The decision tree isolates “Product X” quickly, allowing precise price predictions while evaluating other products’ prices.\nOne-hot encoding is another option; it transforms a single categorical column into multiple binary columns, each representing a category level. Pandas offers the get_dummies method which does just that.\nHowever, there’s little evidence that one-hot encoding enhances results. Thus, we tend to avoid it when unnecessary, as it complicates data handling.\n\n\nCreating a Random Forest\nCreating a random forest involves a process similar to crafting a decision tree, but with added flexibility through parameters that determine the number of trees, data point subset size(rows), and field subset size(columns):\n\ndef rf(xs, y, n_estimators=40, max_samples=200_000,\n       max_features=0.5, min_samples_leaf=5, **kwargs):\n    return RandomForestRegressor(n_jobs=-1, n_estimators=n_estimators,\n        max_samples=max_samples, max_features=max_features,\n        min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y)\n\nHere’s an explanation of the parameters used in the function:\n\nn_estimators: specifies the number of tree in the forest.\nmax_samples: indicates how many rows to sample when training each tree.\nmax_features: sets the number of columns to sample at each split (e.g., 0.5 means using half of the columns).\nmin_samples_leaf: determines the minimum number of samples required in the leaf node, controlling the tree depth.\n\nAdditionally, n_jobs=-1 ensures that all available CPUs are utilized for parallel tree building. This function allows quick experimentation with different configurations.\nInitiating the random forest model is straightforward:\n\nm = rf(xs, y);\n\nBy using multiple trees rather than a single DecisionTreeRegressor, the validation RMSE significantly improves:\n\nm_rmse(m, xs, y), m_rmse(m, valid_xs, valid_y)\n\n(0.171371, 0.233223)\n\n\nA distinctive feature of random forests is the resilience hyperparameter configurations, particularly max_features.\n\n\n\n\n\n\nNote\n\n\n\nWhen we say random forests show resilience to hyperparameter configurations, it means that the algorithm performs well across a range of different hyperparameter settings. It doesn’t require very precise tuning to achieve good results, making it a flexible option in many applications.\n\n\nThe N_estimators parameter can be set to as high as value as feasible, the more trees, the greater the accuracy potential\nFor visualizing effects of varying max_features with increasing tree counts, refer to sklearn’s documentation which provides insightful plots.\n\nThe image demonstrates:\n\nBlue line: represents minimal features usage.\nGreen line: represents maximal feature usage (full feature set). Subsets of features combined with numerous trees usualy yield the lowest error.\n\nTo explore the impact of n_estimators analyze predictions from each individual tree within the forest (accessible via the estimators_ attribute):\n\npreds = np.stack([t.predict(valid_xs) for t in m.estimators_])\n\n\nr_mse(preds.mean(0), valid_y)\n\n0.233223\n\n\nThis calculation, preds.mean(0), parallels the overall random forest prediction. Observe RMSE progression as trees are added:\n\nplt.plot([r_mse(preds[:i+1].mean(0), valid_y) for i in range(40)]);\n\n\n\n\n\n\n\n\nDespite improved RMSE in training, the validation set’s performance may deteriorate due to potential overfitting or time discrepancies. This challenge is addressable by leveraging the out-of-bag (OOB) error methodology in random forests, offering valuable insights.\nIn the next section, we’ll delve deeper into creating a random forest and optimizing it’s performance.\n\n\nOut of Bag Error\nIn a random forest, each tree is trained on different subset of data. Consequently, there’s a unique opportunity: each tree has an implicit validation set composed of the data rows not selected for its training, know as out-of-bag (OOB) data.\nOOB error is particularly useful when dealing with a limited dataset, as it offers a measure of model generalization without needing to withhold data for a separate validation set. These OOB predictions are stored in the oob_prediction_ attribute. Remember, these are compared with training labels, as the OOB calculation involves the training set:\n\nr_mse(m.oob_prediction_, y)\n\n0.211234\n\n\nThe OOB error frequently appears lower than the validation set error, hinting that other factors might contribute to the validation error, hinting that other factors might contribute to the validation error outside mere generalization discrepancies. We’ll delve into these causes soon.\n\n\nModel Interpretation\nInterpreting models trained on tabular data presents valuable insights. Higher understanding can be sought in ares like:\n\nHow confident are we in our predictions using a particular row of data?\nFor predicting with a particular row of data, what were the most important factors, and how did they influence that prediction?\nWhich columns are the strongest predictors, which can we ignore?\nWhich columns are effectively redundant with each other, for purposes of prediction?\nHow do predictions vary, as we vary these columns?\n\nRandom forests are adept at addressing these questions. Let’s start with evaluating confidence in predictions!\nModel predictions are an average of individual tree predictions, providing an estimated value. But how can we gauge the confidence of this estimate? One simplistic approach is using the standard deviations of tree predictions - higher deviations imply less confidence, suggesting that caution is needed, especially in scenarios where tree predictions are inconsistent.\nIn creating the random forest, predictions over the validations set were obtained using Python’s list comprehension:\n\npreds = np.stack([t.predict(valid_xs) for t in m.estimators_])\n\n\npreds.shape\n\n(40, 7988)\n\n\nThis results in a prediction for each tree across all validation set auctions (40 trees, 7,988 auctions). With this data, compute the standard deviation of predictions for each auction:\n\npreds_std = preds.std(0)\npreds_std[:5]\n\narray([0.2000169 , 0.08355874, 0.113672  , 0.2747    , 0.12065141])\n\n\nThe standard deviations highlight varying levels of confidence across auctions. A lower deviation signals stronger agreement among trees, leading to higher confidence. Conversely, higher deviations indicate disagreement, pointing towards lower confidence. In practical applications like auction bidding, this information is useful; you might reconsider bidding when predictions show low certainty.\n\nFeature Importance\nKnowing a model’s predictive accuracy is critical, but equally important is understanding how those predictions are made. Feature importance offers valuable insight into this process. Sklearn’s random forest model provides feature importance scores via the feature_importance_ attributes. Here’s a simple function load these scores into a DataFrame and sort them\n\ndef rf_feat_importance(m, df):\n    return pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}\n                       ).sort_values('imp', ascending=False)\nfi = rf_feat_importance(m, xs)\nfi[:10]\n\n\n\n\n\n\n\n\ncols\nimp\n\n\n\n\n57\nYearMade\n0.166375\n\n\n30\nCoupler_System\n0.113599\n\n\n6\nProductSize\n0.103802\n\n\n7\nfiProductClassDesc\n0.078686\n\n\n3\nfiSecondaryDesc\n0.054542\n\n\n54\nModelID\n0.052919\n\n\n65\nsaleElapsed\n0.050521\n\n\n31\nGrouser_Tracks\n0.041514\n\n\n12\nEnclosure\n0.039451\n\n\n32\nHydraulics_Flow\n0.035355\n\n\n\n\n\n\n\nEvaluating the features importances reveals that a few columns significantly contribute to the model’s predictions, most notably, YearMade and ProductSize.\nTo visualize these importance, plotting them can clarify their relative value:\n\ndef plot_fi(fi):\n    return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)\n\nplot_fi(fi[:30]);\n\n\n\n\n\n\n\n\n\n\nRemoving Low-Importance Variables\nA subset of columns might suffice to maintain accuracy while enhancing simplicity by discarding low-importance variables. Let’s retain only those with an importance score above 0.005:\n\nto_keep = fi[fi.imp&gt;0.005].cols\nlen(to_keep)\n\n22\n\n\nRetrain the model using this refined feature set:\n\nxs_imp = xs[to_keep]\nvalid_xs_imp = valid_xs[to_keep]\n\nm = rf(xs_imp, y)\nm_rmse(m, xs_imp, y), m_rmse(m, valid_xs_imp, valid_y)\n\n(0.180965, 0.231633)\n\n\nThe models accuracy remain consistent, yet fewer columns necessitate examination:\n\nlen(xs.columns), len(xs_imp.columns)\n\n(66, 22)\n\n\nSimplifying a model is often the initial step in enhancing it having 78 columns can be overwhelming for deep analysis. Particularly, a learner, more interpretable model is simpler to deploy and manage.\nRevisiting the feature importance plot provides clearer insights:\n\nplot_fi(rf_feat_importance(m, xs_imp));\n\n\n\n\n\n\n\n\nWhile interpreting, redundancy may arise as seen with ProductGroup and ProductGroupDesc. Attempting to remove such redundant features can further streamline interpretation.\n\n\nRemoving Redundant Variables\nWe’ll begin by clustering columns to identify pairs that are closely aligned often suggesting redundancy:\n\ncluster_columns(xs_imp)\n\n\n\n\n\n\n\n\nThe chart generated from clustering will reveal which columns were merged early on. Notably, pairs like ProductGroup with ProductGroupDesc, saleYear with saleElapsed, and fiModelDesc with fiBaseModel are likely correlated to the point of redundancy.\nNext, we will attempt to simplify the model by removing these related features. We begin by defining a function to quickly train a random forest and capture the out-of-bag(OOB) score. This score, ranging from 1.0 for perfection to near-zero, provides a relative comparison metric as we remove redundant columns:\n\ndef get_oob(df):\n    m = RandomForestRegressor(n_estimators=40, min_samples_leaf=15,\n        max_samples=50000, max_features=0.5, n_jobs=-1, oob_score=True)\n    m.fit(df, y)\n    return m.oob_score_\n\nFirst, we’ll confirm our baseline score with all columns:\n\nget_oob(xs_imp)\n\n0.8760739540611289\n\n\nNext, test the impact of removing each potentially redundant variable individually:\n\n{c:get_oob(xs_imp.drop(c, axis=1)) for c in (\n    'saleYear', 'saleElapsed', 'ProductGroupDesc','ProductGroup',\n    'fiModelDesc', 'fiBaseModel',\n    'Hydraulics_Flow','Grouser_Tracks', 'Coupler_System')}\n\n{'saleYear': 0.8742959821922331,\n 'saleElapsed': 0.8698149904307536,\n 'ProductGroupDesc': 0.8755334280543031,\n 'ProductGroup': 0.8745495772129529,\n 'fiModelDesc': 0.8743458666758965,\n 'fiBaseModel': 0.8748827464781819,\n 'Hydraulics_Flow': 0.8762012623754625,\n 'Grouser_Tracks': 0.8755826405754699,\n 'Coupler_System': 0.8758570604637711}\n\n\nWe’ll also explore the effect of dropping one columns from each identified pair:\n\nto_drop = ['saleYear', 'ProductGroupDesc', 'fiBaseModel', 'Grouser_Tracks']\nget_oob(xs_imp.drop(to_drop, axis=1))\n\n0.8743053306321846\n\n\nEncouragingly, the model’s performance remains largely unchanged. We will now finalize this reduce dataset:\n\nxs_final = xs_imp.drop(to_drop, axis=1)\nvalid_xs_final = valid_xs_imp.drop(to_drop, axis=1)\n\nsave_pickle(path/'xs_final.pkl', xs_final)\nsave_pickle(path/'valid_xs_final.pkl', valid_xs_final)\n\nFor later retrieval, you can load these condensed datasets with:\n\nxs_final = load_pickle(path/'xs_final.pkl')\nvalid_xs_final = load_pickle(path/'valid_xs_final.pkl')\n\nLet’s verify that the RMSE remains consistent after this reduction:\n\nm = rf(xs_final, y)\nm_rmse(m, xs_final, y), m_rmse(m, valid_xs_final, valid_y)\n\n(0.182663, 0.231313)\n\n\nBy concentrating on key variables and eliminating redundancies, we’ve streamlined our model significantly. Now, let’s further explore how these influential variables affect predictions using partial dependence plots.\n\n\nPartial Dependence\nAlright, let’s get a feel for these predictions. Imagine checking out the menu at a restaurant. Before ordering, you’d want to know what’s popular, right? We do the same thing with our data. For ProductSize, we count how many times each size appears using something like Pandas’ value_counts method and then plot this on a bar chart. Here’s our code in action:\n\np = valid_xs_final['ProductSize'].value_counts(sort=False).plot.barh()\nc = to.classes['ProductSize']\nplt.yticks(range(len(c)), c);\n\n\n\n\n\n\n\n\nTurns out, the biggest “dish” on our menu is labeled Compact but look at #na#, fastai’s way of showing missing values. No big surprise there!\nWhat about YearMade? This time, instead of a bar chart, we whip out a histogram.\n\nax = valid_xs_final['YearMade'].hist()\n\n\n\n\n\n\n\n\nApart from 1950, which we used as placeholder for unknown years, most machines were crafted post-1990. Vintage anyone?\nPartial dependence plots help us see what would happen to the sale price if one feature changed while everything else stayed the same.\nFor YearMade, we can’t just average sale prices by year because many things change over time. Instead, we replace every year value with a single year, like 1950, and calculate the average predicted sale price. We repeat this for each year, up to 2011, to see how YearMade alone affects price.\nThen, we plot the results:\n\nfrom sklearn.inspection import PartialDependenceDisplay\nfig,ax = plt.subplots(figsize=(12, 4))\nPartialDependenceDisplay.from_estimator(m, valid_xs_final, ['YearMade', 'ProductSize'],\n                                        grid_resolution=20, ax=ax)\n\n\n\n\n\n\n\n\nFor YearMade, after 1990, there’s a clear pattern: prices rise as the year increase. This make sense because older items depreciate.\nThe plot for ProductSize show that the group with missing values has the lowest prices. Understanding why these values are missing is crucial, as sometimes they can be good predictors, or they could indicate an issue like data leakage\n\n\nData Leakage\nIn the world of data mining, there’s a tricky issue known as data leakage, described in detail by Shachar Kaufman, Saharon Rosset, and Claudia Perlich in their paper, Leakage in Data Mining: Formulation, Detection, and Avoidance. They define it as the unintentional introduction of information about the target of a data mining problem that shouldn’t be available to mine from. To put it simply, it’s like saying ‘it rains on rainy days,’ where the model mistakenly uses the target itself as an input.\nData leakage can be subtle, appearing in various forms, and one such form is through missing values. Here are the straightforward steps to spot data leakage:\n\nAssess whether your model’s accuracy seems too perfect. If it feels too good to be true, leakage might be playing a part.\nEvaluate the significant predictors. If they don’t add up in a practical sense, then something might be off.\nAnalyze the partial dependence plots. If they yield nonsensical results, you could be facing a leakage issue.\n\nAdditionally, tools like tree interpreters can aid in understanding which factors are influencing specific predictions.\nAvoiding data leakage demands meticulous attention through all phases of data handling—from collection to preparation. The key is adopting a “learn-now, predict-later” approach, ensuring that models are built without any preview of the answers.\n\n\nTree Interpreter\nBefore we go in please make sure you’re already have treeinterpreter and waterfallcharts installed if not run this in your terminal\npip install treeinterpreter\npip install waterfallcharts\nAt the start of this section, we said that we wanted to be able to answer five questions:\n\n\n\n\n\n\nNote\n\n\n\n\nHow confident are we in our predictions using particular row of data?\nFor predicting with a particular row of data, what were the most important factors, and how did they influence that predictions?\nWhich columns are the strongest predictors, which can we ignore?\nWhich columns are effectively redundant with each other, for purpose of prediction?\nHow do predictions vary, as we vary these columns?\n\n\n\nWe’ve addressed four of these, leaving only the second question. To tackle this, we’ll use the treeinterpreter library, along with the waterfallcharts library for visualization.\n\nfrom treeinterpreter import treeinterpreter\nfrom waterfall_chart import plot as waterfall\n\nWhile we’ve computed feature importances across entire random forest, we can apply a similar concept to a single row of data. This approach examines the contribution of each variable to improving the model at each branch of every tree, then sums these contributions per variables for a specific data point.\nFor example, if we’re analyzing a particular auction item predicted to be expensive, we can understand why by examining that single row of data. We’ll process it through each decision tree, observing the split used at each point and calculating the increase or decrease in addition compared to the parent node. This process is repeated for every tree, summing up the total change in importance by split variable.\n\n\n\n\n\n\nNote\n\n\n\nFor example, if you’re predicting house prices:\n\nThe bias might be the average house price in your dataset.\nA positive contribution from the “number of bedrooms” feature would indicate that having more bedrooms increased the predicted price.\nA negative contribution from the “distance from city center” feature might indicate that being further from the city center decreased the predicted price.\n\n\n\nLet’s select the first few rows of our validation set:\n\nrow = valid_xs_final.iloc[:5]\n\nWe can then use treeinterpreter:\n\nprediction,bias,contributions = treeinterpreter.predict(m, row.values)\n\nHere, prediction is the random forest’s prediction, bias is the prediction based on the mean of the dependent variable, and contributions shows how each feature (independent variable) in your input data contributed to moving the prediction away from the bias. The sum of contributions plus bias equals the prediction for each row\n\nprediction[0], bias[0], contributions[0].sum()\n\n(array([10.06313964]), 10.104746057831763, -0.04160642242374439)\n\n\nTo visualize the contributions clearly, we can use waterfall plot:\n\nwaterfall(valid_xs_final.columns, contributions[0], threshold=0.08,\n          rotation_value=45,formatting='{:,.3f}');\n\n\n\n\n\n\n\n\nThis plot demonstrates how positive and negative contributes from all independent variables sum up to create the final prediction, show in the rightmost column labeled net.\nThis type of information is particularly valuable in production environments, rather than during model development. It can provide users of your data product with insightful information about the underlying reasoning behind the predictions.\nHaving explored these classic machine learning techniques, we’re now ready to see how deep learning can contribute to solving this problem"
  },
  {
    "objectID": "posts/2024-11-12-random-forest/index.html#extrapolation-and-neuron-networks",
    "href": "posts/2024-11-12-random-forest/index.html#extrapolation-and-neuron-networks",
    "title": "Exploring Random Forest, Bagging, Boosting?",
    "section": "Extrapolation and Neuron Networks",
    "text": "Extrapolation and Neuron Networks\nRandom forests, like all machine learning or deep learning algorithms, don’t always generalize well to new data. Lets explore this issue, particularly focusing on the extrapolation problem that random forests face.\n\nThe Extrapolation Problem\nConsider a simple task: making prediction from 40 data points showing a slightly noisy linear relationship. We’ll create this data and visualize it:\n\nnp.random.seed(42)\nx_lin = torch.linspace(0,20, steps=40)\ny_lin = x_lin + torch.randn_like(x_lin)\nplt.scatter(x_lin, y_lin);\n\n\n\n\n\n\n\n\nWe need to reshape our data for sklearn, which expect a matrix of independent variables:\n\nxs_lin = x_lin.unsqueeze(1)\nx_lin.shape,xs_lin.shape\n\n(torch.Size([40]), torch.Size([40, 1]))\n\n\n\nx_lin[:,None].shape\n\ntorch.Size([40, 1])\n\n\nNow, let’s create a random forest using the first 30 rows for training:\n\nm_lin = RandomForestRegressor().fit(xs_lin[:30],y_lin[:30])\n\nWe’ll test the model on the full dataset and visualize the results:\n\nplt.scatter(x_lin, y_lin, 20)\nplt.scatter(x_lin, m_lin.predict(xs_lin), color='red', alpha=0.5);\n\n\n\n\n\n\n\n\nHere’s where we encounter a significant issue: our predictions outside the training data domain are consistently too low. This happens because a random forest average value of the rows in a leaf. Consequently, a random forest can’t predict values outside the rage of its training data.\nThis limitation is particularly problematic for data with time-based trends, like inflation, where future predictions are needed. Your predictions will systematically be too low.\nThe problem isn’t limited to time variables, though. Random forest struggle to extrapolate beyond the types of data they’ve seen in a more general sense. That’s wy it’s crucial to ensure our validation set doesn’t contain out-of-domain data\n\n\nFinding Out-of-Domain Data\nIdentifying whether your test set is distributed differently from your training data can be challenging. Interestingly, we can use a random forest to help us with this task. Here’s how:\nInstead of predicting our actual dependent variable, we’ll try to predict whether a row belongs to the validation set or the training set. Let’s combine our training and validation sets, create a new dependent variable representing the dataset origin, and build a random forest:\n\ndf_dom = pd.concat([xs_final, valid_xs_final])\nis_valid = np.array([0]*len(xs_final) + [1]*len(valid_xs_final))\n\nm = rf(df_dom, is_valid)\nrf_feat_importance(m, df_dom)[:6]\n\n\n\n\n\n\n\n\ncols\nimp\n\n\n\n\n6\nsaleElapsed\n0.910266\n\n\n11\nSalesID\n0.073707\n\n\n14\nMachineID\n0.012246\n\n\n0\nYearMade\n0.000813\n\n\n9\nfiModelDesc\n0.000535\n\n\n5\nModelID\n0.000471\n\n\n\n\n\n\n\nThis reveals three columns that differ significantly between the sets: saleElapsed, SalesID and MachineID. saleElapsed directly encoded the date, while SalesID and MachineID likely represent incrementing identifiers over time.\nLet’s compare the RMSE of our original model with versions that exclude these columns:\n\nm = rf(xs_final, y)\nprint('orig', m_rmse(m, valid_xs_final, valid_y))\n\nfor c in ('SalesID','saleElapsed','MachineID'):\n    m = rf(xs_final.drop(c,axis=1), y)\n    print(c, m_rmse(m, valid_xs_final.drop(c,axis=1), valid_y))\n\norig 0.231001\nSalesID 0.230214\nsaleElapsed 0.235865\nMachineID 0.231447\n\n\nIt appears that we can remove SalesID and MachineID without losing accuracy:\n\ntime_vars = ['SalesID','MachineID']\nxs_final_time = xs_final.drop(time_vars, axis=1)\nvalid_xs_time = valid_xs_final.drop(time_vars, axis=1)\n\nm = rf(xs_final_time, y)\nm_rmse(m, valid_xs_time, valid_y)\n\n0.228264\n\n\nRemoving these variables slightly improves the model’s accuracy and should make it more resilient over time, easier to maintain, and understand.\nSometimes, using only recent data can help. Let’s try using data from the most recent years:\n\nxs['saleYear'].hist();\n\n\n\n\n\n\n\n\n\nfilt = xs['saleYear']&gt;2004\nxs_filt = xs_final_time[filt]\ny_filt = y[filt]\n\n\nm = rf(xs_filt, y_filt)\nm_rmse(m, xs_filt, y_filt), m_rmse(m, valid_xs_time, valid_y)\n\n(0.176448, 0.228537)\n\n\nThis yields a slightly improvement, demonstrating that using your entire dataset isn’t always the best approach; sometimes subset can perform better.\nI recommend building a model with is_valid as the dependent variable for all datasets. This can uncover subtle domain shift issues that might otherwise go unnoticed.\nNext, we’ll explore whether using a neural network can further improve our results\n\n\nUsing Neural Networks\nTo build a neural network model, we’ll follow a similar approach to our random forest setup. First, let’s replicate the steps for creating the TabularPandas object:\n\ndf_nn = pd.read_csv(path/'TrainAndValid.csv', low_memory=False)\ndf_nn['ProductSize'] = df_nn['ProductSize'].astype('category')\ndf_nn['ProductSize'].cat.set_categories(sizes, ordered=True)\ndf_nn[dep_var] = np.log(df_nn[dep_var])\ndf_nn = add_datepart(df_nn, 'saledate')\n\nWe can utilize the column selection from our random forest model for the neural network:\n\ndf_nn_final = df_nn[list(xs_final_time.columns) + [dep_var]]\n\nNeural networks handle categorical columns differently than decision trees. Embedding are an effective method for categorical variables in neural nets. Fastai determines which columns should be treated as categorical by comparing the number of distinct levels to the max_card parameter. We’ll use 9,000 as our max_card to avoid unnecessarily large embeddings:\n\ncont_nn,cat_nn = cont_cat_split(df_nn_final, max_card=9000, dep_var=dep_var)\n\nIt’s crucial to ensure that saleElapsed isn’t treated as a categorical variable as we need to predict auction sale prices in the feature. Let’s verify the continuous variable\n\n\n\n\n\n\nNote\n\n\n\nAs a continuous variable, saleElapsed can capture trends over time. If it were treated as a categorical variable, you’d lose the ability to interpolate or extrapolate between known values, which is crucial for prediction.\nWhen you’re predicting auction sale prices for future dates, you’ll be dealing with ‘saleElapsed’ values that weren’t in your training data. If ‘saleElapsed’ were categorical, your model wouldn’t know how to handle these new values.\n\n\n\ncont_nn\n\n['saleElapsed']\n\n\nNow, let’s examine the cardinality of our chosen categorical variables:\n\ndf_nn_final[cat_nn].nunique()\n\nYearMade                73\nCoupler_System           2\nProductSize              6\nfiProductClassDesc      74\nfiSecondaryDesc        177\nModelID               5281\nEnclosure                6\nHydraulics_Flow          3\nfiModelDesc           5059\nfiModelDescriptor      140\nHydraulics              12\nProductGroup             6\nDrive_System             4\nTire_Size               17\nTrack_Type               2\ndtype: int64\n\n\nWe notice two “model” variables with similar high cardinalities, suggesting potential redundancy. To reduce the embedding matrix size. Let’s assess the impact of removing one of these model columns on our random forest:\n\nxs_filt2 = xs_filt.drop('fiModelDescriptor', axis=1)\nvalid_xs_time2 = valid_xs_time.drop('fiModelDescriptor', axis=1)\nm2 = rf(xs_filt2, y_filt)\nm_rmse(m2, xs_filt2, y_filt), m_rmse(m2, valid_xs_time2, valid_y)\n\n(0.178386, 0.229505)\n\n\ngiven the minimal impact, We’ll remove fiModelDescriptor from our neural network predictors:\n\ncat_nn.remove('fiModelDescriptor')\n\nWhen creating our TabularPandas object for the neural network, we need to add normalization, which is crucial for neural networks but unnecessary for random forests:\n\nprocs_nn = [Categorify, FillMissing, Normalize]\nto_nn = TabularPandas(df_nn_final, procs_nn, cat_nn, cont_nn,\n                      splits=splits, y_names=dep_var)\n\nSince tabular models and data generally don’t require much GPU RAM, we can use larger batch sizes:\n\ndls = to_nn.dataloaders(1024)\n\nFor regression models, it’s advisable to set y_range. Let’s find the min and max of our dependent variable:\n\ny = to_nn.train.y\ny.min(),y.max()\n\n(8.465899467468262, 11.863582611083984)\n\n\nNow we can create the Learner for our tabular model. We’ll use MSE as the loss function and increase the default layer sizes to 500 and 250 for our large dataset:\n\nlearn = tabular_learner(dls, y_range=(8,12), layers=[500,250],\n                        n_out=1, loss_func=F.mse_loss)\nlearn.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.00013182566908653826)\n\n\n\n\n\n\n\n\n\nWe’ll train with fit_one_cycle for a few epochs:\n\nlearn.fit_one_cycle(5, 1e-2)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.061921\n0.067224\n00:05\n\n\n1\n0.051130\n0.056330\n00:04\n\n\n2\n0.046388\n0.054012\n00:03\n\n\n3\n0.041853\n0.054157\n00:03\n\n\n4\n0.040173\n0.052207\n00:03\n\n\n\n\n\nLet’s compare the result to our earlier random forest using the r_mse function:\n\npreds,targs = learn.get_preds()\nr_mse(preds,targs)\n\n\n\n\n\n\n\n\n0.228488\n\n\nThe neural network performs better than the random forest, although it take longer to train and requires more careful hyprerparameter tuning\nWe’ll save our model for future use:\n\nlearn.save('nn')\n\nPath('models/nn.pth')\n\n\nTo further improve generalization, we can use ensemble learning, which evolves averaging predictions from several models."
  },
  {
    "objectID": "posts/2024-11-12-random-forest/index.html#ensembling",
    "href": "posts/2024-11-12-random-forest/index.html#ensembling",
    "title": "Exploring Random Forest, Bagging, Boosting?",
    "section": "Ensembling",
    "text": "Ensembling\nThe success of random forests is rooted in the principle that while individual trees have errors, these errors are not correlated. With enough trees, the average of these errors should approach zero. We can apply similar reasoning to combine predictions from different algorithms.\nIn our case, we have two distinct models: a random forest and a neural network. Their different approaches likely result in different types of errors. Therefore, averaging their predictions could potentially outperform either model individually.\nIt’s worth nothing that a random forest is itself an ensemble, By combining it with a neural network, we’re creating an ensemble of ensembles! While ensembling may not revolutionize your modeling process, it can provide a welcome boost to your exiting model.\nOne small challenge we face is the different output types from our Pytorch and sklearn models. Pytorch gives a rank-2 tensor (a column matrix), while sklearn produces a rank-1 array (a vector). We can address this using squeeze to remove unit axes and to_np to convert to Numpy array\n\nrf_preds = m.predict(valid_xs_time)\nens_preds = (to_np(preds.squeeze()) + rf_preds) /2\n\nThis ensemble approach yield better result than either model individually:\n\nr_mse(ens_preds,valid_y)\n\n0.222895"
  },
  {
    "objectID": "posts/2024-11-12-random-forest/index.html#boosting",
    "href": "posts/2024-11-12-random-forest/index.html#boosting",
    "title": "Exploring Random Forest, Bagging, Boosting?",
    "section": "Boosting",
    "text": "Boosting\nWhile our previous ensembling approach used bagging (combination many models trained on different data subsets by averaging), another important technique is boosting, where models are added instead of averaged.\nBoosting works as follow:\n\nTrain a small, underfitting model on you dataset.\nCalculate this model predictions for the training set.\nSubtract these predictions from the actual targets to get the “residuals”(the error for each training point).\nReturn to step 1, but use the residuals as the new training targets.\nRepeat this process until reaching a stopping criterion(e.g., maximum number of trees or worsening validation set error).\n\nIn this approach, each new tree attempts to fit the combined error of all previous trees. As we continually create new residuals by subtracting each new tree’s predictions from the previous residuals, these residuals progressively decrease.\nTo make predictions with a boosted tree ensemble, we calculate predictions from each tree and sum them. This approach has many variations and names, including Gradient Boosting Machines (GBMs) and Gradient Boosted Decision Trees (GBDTs). XGBoost is currently the most popular implementation.\nUnlike random forests, boosting can lead to overfitting. In random forests, adding more trees doesn’t cause overfitting because each tree is independent. However, in a boosted ensemble, more trees continuously improve the training error, potentially leading to overfitting on the validation set."
  },
  {
    "objectID": "posts/2024-11-12-random-forest/index.html#key-takeaway",
    "href": "posts/2024-11-12-random-forest/index.html#key-takeaway",
    "title": "Exploring Random Forest, Bagging, Boosting?",
    "section": "Key takeaway",
    "text": "Key takeaway\nWe have discussed two approaches to tabular modeling: decision tree ensembles and neural networks. We’ve also mentioned two different decision tree ensembles: random forests, and gradient boosting machines. Each is very effective, but each also has compromises:\n\nRandom forests are the easiest to train, because they are extremely resilient to hyperparameter choices and require very little preprocessing. They are very fast to train, and should not overfit if you have enough trees. But they can be a little less accurate, especially if extrapolation is required, such as predicting future time periods.\nGradient boosting machines in theory are just as fast to train as random forests, but in practice you will have to try lots of different hyperparameters. They can overfit, but they are often a little more accurate than random forests.\nNeural networks take the longest time to train, and require extra preprocessing, such as normalization; this normalization needs to be used at inference time as well. They can provide great results and extrapolate well, but only if you are careful with your hyperparameters and take care to avoid overfitting.\n\nWe suggest starting your analysis with a random forest. This will give you a strong baseline, and you can be confident that it’s a reasonable starting point. You can then use that model for feature selection and partial dependence analysis, to get a better understanding of your data.\nFrom that foundation, you can try neural nets and GBMs, and if they give you significantly better results on your validation set in a reasonable amount of time, you can use them. If decision tree ensembles are working well for you, try adding the embeddings for the categorical variables to the data, and see if that helps your decision trees learn better.\nAlright guys, it’s been a long post huh? Thanks for reading all of those, catch you on the flip side, and I’ll see you… next time!"
  },
  {
    "objectID": "posts/2024-06-30-your-deep-learning-journey/index.html",
    "href": "posts/2024-06-30-your-deep-learning-journey/index.html",
    "title": "First Step in AI",
    "section": "",
    "text": "Welcome to my deep dive in to the world of deep learning! In this blog post, I’ll be sharing my journey through the first lesson of fast.ai course an acclaimed program that makes learning AI accessible and enjoyable.\nFast.ai was created with the goal of making deep learning understandable for everyone, no matter their background, and Lesson 1 accomplishes that by having us build a simple yet fascinating model: a bird classifier. This exciting task not just introduces me to the basics of deep learning but also allow me to experience firsthand the power and simplicity of modern AI tools.\nJoin me as I walk you though key concept covered in the Lesson 1, from understanding how images are processed by computers to training and validating our model. I will also share some personal insights and reflections on the learning process, aiming to make this technical journey both informative and relatable.\nWhether you are a beginner in AI or someone looking for refresh your knowledge, I hope this post inspires and guides you in your own deep learning"
  },
  {
    "objectID": "posts/2024-06-30-your-deep-learning-journey/index.html#introduction",
    "href": "posts/2024-06-30-your-deep-learning-journey/index.html#introduction",
    "title": "First Step in AI",
    "section": "",
    "text": "Welcome to my deep dive in to the world of deep learning! In this blog post, I’ll be sharing my journey through the first lesson of fast.ai course an acclaimed program that makes learning AI accessible and enjoyable.\nFast.ai was created with the goal of making deep learning understandable for everyone, no matter their background, and Lesson 1 accomplishes that by having us build a simple yet fascinating model: a bird classifier. This exciting task not just introduces me to the basics of deep learning but also allow me to experience firsthand the power and simplicity of modern AI tools.\nJoin me as I walk you though key concept covered in the Lesson 1, from understanding how images are processed by computers to training and validating our model. I will also share some personal insights and reflections on the learning process, aiming to make this technical journey both informative and relatable.\nWhether you are a beginner in AI or someone looking for refresh your knowledge, I hope this post inspires and guides you in your own deep learning"
  },
  {
    "objectID": "posts/2024-06-30-your-deep-learning-journey/index.html#the-xkcd-joke-and-debunking-deep-learning-myths",
    "href": "posts/2024-06-30-your-deep-learning-journey/index.html#the-xkcd-joke-and-debunking-deep-learning-myths",
    "title": "First Step in AI",
    "section": "The XKCD Joke and Debunking Deep Learning Myths",
    "text": "The XKCD Joke and Debunking Deep Learning Myths\n\n\n\n\nXKCD Joke\n\n\nJeremy Howard kicked off the lesson with relatable XKCD Joke about how in 2015, detecting a bird in a photo was seen as a challenging task, almost a joke. Fast forward to today, and we can build such as system in mere minutes, showcasing how far deep learning has come.\nMany people believe that diving into deep learning requires extensive mathematical knowledge, huge datasets, and expensive hardware. However, these myths are far from the truth.\n\n\n\n\n\n\n\nMyth(Don’t need)\nTruth\n\n\n\n\nLots of math\nJust high school math is sufficient\n\n\nLots of data\nWe’ve seen record-breaking results with fewer than 50 items of data\n\n\nLots of expensive computer\nYou can perform state-of-the-art work with hardware available for free of minimal cost"
  },
  {
    "objectID": "posts/2024-06-30-your-deep-learning-journey/index.html#top-down-learning-approach",
    "href": "posts/2024-06-30-your-deep-learning-journey/index.html#top-down-learning-approach",
    "title": "First Step in AI",
    "section": "Top-Down Learning Approach",
    "text": "Top-Down Learning Approach\nOne of the most refreshing aspects of fastai course is its top-down teaching approach. Traditional education often starts with the basics and slowly builds up to more complex topics. However, Jeremy Howard and Rachel Thomas believe that learning is more effective when you see the big picture first.\nIn the fastai course, we start by building practically applications from lesson one, allowing us to see immediate results and understanding the relevance of what we are doing. This approach mirrors how we learn many real-word skills, such as sport or cooking, where we start by trying out the activity and learn the details as needed.\nBy diving straight into creating a deep learning model, we get hands-on experience early on, which helps solidify our understanding and maintain our interest. As we process though the course, we gradually delve deeper into the underlying principles and theories, building a robust foundation along the way"
  },
  {
    "objectID": "posts/2024-06-30-your-deep-learning-journey/index.html#understanding-deep-learning",
    "href": "posts/2024-06-30-your-deep-learning-journey/index.html#understanding-deep-learning",
    "title": "First Step in AI",
    "section": "Understanding Deep learning",
    "text": "Understanding Deep learning\nDeep learning is a technique for extracting and transforming data, with application ranging from speech recognition to image classification. It uses multiple layer of neural networks, where each layer refines the data received from the previous one. These layers are trained using the algorithms that minimize the errors and improve accuracy, enabling the network to learn specific tasks.\nDeep learning’s power, flexibility, and simplicity make it applicable across various field, including social science, medicine, finance, and more. For instance, despite lacking of medical background, Jeremy Howard founded Enlitic, a company leveraging deep learning to diagnose illnesses. Within months, their algorithm was more effective at identifying malignant tumors than radiologists.\nHere are some areas where deep learning excels:\n\nNatural Language Processing (NLP): Answering question, speech recognition, document summarization, and more.\nComputer Vision: Interpreting satellite images, face recognition, and autonomous vehicle navigation.\nMedicine: Analyzing radiology images, measuring features and medical scans, and diagnosing diseases.\nBiology: Protein folding, genomics tasks, and cell classification.\nImage Generation: Colorizing images, enhancing resolution, and converting images to artistic style.\nRecommendation System: Web search optimization, product recommendations, and personalized content layout.\nGaming: Mastering games like Chess, Go, and various video games.\nRobotics: Handling challenging objects and complex manipulation tasks.\nOther: Financial forecasting, text-to-speech conversion, and much more.\n\nThe versatility of deep learning lies in its foundation: neuron networks."
  },
  {
    "objectID": "posts/2024-06-30-your-deep-learning-journey/index.html#a-brief-history-of-deep-learning",
    "href": "posts/2024-06-30-your-deep-learning-journey/index.html#a-brief-history-of-deep-learning",
    "title": "First Step in AI",
    "section": "A Brief History of Deep Learning",
    "text": "A Brief History of Deep Learning\n\n\n\n\nBiological Neurons vs. Artificial Neural Network\n\n\nDeep learning draws inspiration from human brain’s neural network. The concept of neural network isn’t new; it dates back to 1957 with the creation of the first neural network. The fundamental ideas remain the same today, but advances in hardware and data availability have significantly propelled the field forward."
  },
  {
    "objectID": "posts/2024-06-30-your-deep-learning-journey/index.html#the-sofware-pytorch-fastai-and-jupyter",
    "href": "posts/2024-06-30-your-deep-learning-journey/index.html#the-sofware-pytorch-fastai-and-jupyter",
    "title": "First Step in AI",
    "section": "The Sofware: Pytorch, Fastai, and Jupyter",
    "text": "The Sofware: Pytorch, Fastai, and Jupyter\nAt fastai, after extensive testing of various machine learning packages and languages, they decided to adopt Pytorch in 2017 for their course, software development, and research. Pytorch has become the fastest-growing deep learning library and is widely used in academic research and industry. Its flexibility and expressiveness make it an excellent foundation for deep learning.\nThe fastai library builds on top of Pytorch, provide high-level functionality for deep learning. This layered architecture allows for a seamless learning experience, make it easier to understand both high-level concepts and low-level operations.\nHowever, the specific software you use a less important than understanding the core principles and techniques of deep learning. Learning to transition between the libraries is relatively quick, but mastering deep learning foundation is crucial.\nJupyter notebook, a powerful and reflexible tool for data science, will be our primary platform for experimentation. Its interaction with fastai and Pytorch makes it ideal for developing and testing deep learning model.\nReady to see it in action? Let’s train our first model!"
  },
  {
    "objectID": "posts/2024-06-30-your-deep-learning-journey/index.html#exploring-the-is-it-a-bird-classifier",
    "href": "posts/2024-06-30-your-deep-learning-journey/index.html#exploring-the-is-it-a-bird-classifier",
    "title": "First Step in AI",
    "section": "Exploring the “Is it a Bird?” Classifier",
    "text": "Exploring the “Is it a Bird?” Classifier\nOne of the most exciting part of Lesson 1 was building our own image classifier to determine whether the given image contains a bird. For this project, we used the fastai library along with pre-trained model to quickly and efficiently create our classifier. Let’s dive into the code walkthrough.\nThe basic steps we’ll need to do:\n\nUse DuckDuckGo for search images of “bird photos”\nUse DuckDuckGo to search for images of “forest photos”\nFine-tune a pre-trained neural network to recognize these two groups\nTry running this model on a picture of bird and see if it works.\n\n\nSearching for images: DuckDuckGo Search\nInstead of using a big search that requires an API key, we opted to DuckDuckGo, which doesn’t require an API key for image searches. This make the setup simpler and faster.\nBut make sure you run this command in your terminal before run the code to update DuckDuckGo\npip install -Uqq fastai duckduckgo_search\n\nfrom duckduckgo_search import DDGS\nfrom fastcore.all import *\n\nddgs = DDGS()\n\ndef search_images(term, max_images=30):\n    print(f\"Searching for '{term}'\")\n    return L(ddgs.images(keywords=term, max_results=max_images)).itemgot('image')\n\n\nurls = search_images('bird photos', max_images=1)\nurls[0]\n\nSearching for 'bird photos'\n\n\n'https://images.pexels.com/photos/326900/pexels-photo-326900.jpeg?cs=srgb&dl=wood-flight-bird-326900.jpg&fm=jpg'\n\n\nJeremy Howard mentioned that using import * in Jupyter notebooks is not the big deal because Jupyter only import what we use. This approach simplifies the code and keeps it clean.\nHere’s the quick explanation of the functions and libraries used in this snippet:\nDDGS from duckduckgo_search:\n\nduckduckgo_search: This library allows us to search for images using DuckDuckGo without the need for an API key. So no more begging Google for an API key.\nDDGS: The class that does the heavy lifting of searching for images.\n\nfastcore: - fastcore: A foundational library that make Python feel like a Lamborghini-sleek, powerful, and fast.\nL:\n\nL: A magical list from fastcore that does way more than the regular Python list. Think of it as a list on steroids.\n\nIn our example, search_images is a function that performs an image search using DuckDuckGo. It’s print out the search term being used and return a list of images URLs retrieved from the search results.\nfor more details on the tools, you can refer to the fastcore documentation and the duckduckgo_search documentation.\n\nfrom fastdownload import download_url\ndest = 'bird.jpg'\ndownload_url(urls[0], dest, show_progress=False)\n\nfrom fastai.vision.all import *\nim = Image.open(dest)\nim.to_thumb(256,256)\n\n\n\n\n\n\n\n\n\ndownload_url(search_images('forest photos', max_images=1)[0], 'forest.jpg', show_progress=False)\nImage.open('forest.jpg').to_thumb(256,256)\n\nSearching for 'forest photos'\n\n\n\n\n\n\n\n\n\nfastdownload and download_url:\n\nfastdownload: Think of this as your friendly neighborhood delivery service, but for files. It’s help with downloading files and datasets easier.\ndownload_url: A function that fetches the file you need from a URL. In our case, it says “Hey URL, gimme that picture!” and save it as bird.png\n\nfastai.vision.all:\n\nThis module from the fastai library is like a Swiss Army knife for vision tasks, providing all the tools you need, from data loaders to model training utilities.\n\nto_thumb: - A method from the PIL.Image class, which is quite handy it resizes an image to a thumbnail while maintaining the aspect ratio. Kind of like shrinking your favorite sweater but in a good way\nThese libraries and function streamline the process of getting and preparing the images for our model. For more detailed documentation, you can refer to the fastdownload, fastai vision, and Pillow documentation.\n\n\nDownloading and Preparing Images\nTo build our dataset, we need to download images for the categories we are interested in (‘forest’ and ‘bird’). Here’s how we did it:\n\nsearches = 'forest','bird'\npath = Path('bird_or_not')\nfrom time import sleep\n\nfor o in searches:\n    dest = (path/o)\n    dest.mkdir(exist_ok=True, parents=True)\n    download_images(dest, urls=search_images(f'{o} photo'))\n    sleep(10)  # Pause between searches to avoid over-loading server\n    download_images(dest, urls=search_images(f'{o} sun photo'))\n    sleep(10)\n    download_images(dest, urls=search_images(f'{o} shade photo'))\n    sleep(10)\n    resize_images(path/o, max_size=400, dest=path/o)\n\nSearching for 'forest photo'\nSearching for 'forest sun photo'\nSearching for 'forest shade photo'\nSearching for 'bird photo'\nSearching for 'bird sun photo'\nSearching for 'bird shade photo'\n\n\nPath:\n\nPath: An object-oriented way to work with filesystem paths. It makes handling files and directories as easy as pie.\n\ndownload_images:\n\ndownload_images: This function fetches a bunch of images from the internet and saves them in a specified directory. Like ordering a pizza, but instead of pizza, you get pictures.\n\nPausing Between Searches:\n\nPausing between searches (sleep(10)) is important to avoid overloading the server. Think of it as giving the server a coffee break between each request.\n\nresize_images:\n\nresize_images: A function from fastai that resizes images to a maximum specified size. This is useful for ensuring all images are of a consistent size before training the model.\n\nFor more details on these tools, you can refer to the pathlib, Vision utils documentation.\n\n\nVerifying and Leaning Images\nAfter download images, it’s essential to verify them and remove corrupt or invalid images.\n\nfailed = verify_images(get_image_files(path))\nfailed.map(Path.unlink)\nlen(failed)\n\n0\n\n\nverify_images:\n\nverify_images: Think of this as the bouncer for your image dataset, checking IDs to make sure no bad images get through.\n\nget_image_file:\n\nget_image_file: This function grabs all image paths in a directory. It’s like having someone fetch all your misplaced socks in the laundry room.\n\nPath.unlink:\n\nPath.unlink: A method to delete files. This is how we get rid of the bad apples in the bunch.\n\nFortunately, in my case, all downloaded images were valid, so len(failed) return 0–no bad apples in our dataset!\n\n\nThe DataBlock API\nCreating our data loader is a critical step. The DataBlock API in fastai allows us to define how to transform and manage our data easily.\n\ndls = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=[Resize(192, method='squish')]\n).dataloaders(path, bs=32)\n    \ndls.show_batch(max_n=6)\n\n\n\n\n\n\n\n\nHere’s the breakdown of the arguments in DataBlock:\nblocks:\n\nSpecifies the type of inputs and targets. In our case, we have images (ImageBlock) and categories (CategoryBlock). It’s like saying, “I have pictures of cats and dogs”\n\nget_items:\n\nFunction to get the list of items. Here we’re using get_image_file to retrieve all our image files.\n\nsplitter:\n\nDefines how to split the dataset into training and validation sets. RandomSplitter(valid_pct=0.2, seed=42) means 20% of the data will be used for validation. The seed ensures that every time we run the code we get the same split. Think of like setting your DVR to record your favorite show at the same time every week.\n\nget_y:\n\nFunction to get the target label from each item. We use parent_label to get the label from parent directory name (e.g., ‘forest’ or ‘bird’)\n\nitem_tfms:\n\nitem transformation to apply. We use Resize(129, method='squish') to resize images to 129x129 pixels by squishing them if necessary.\n\ndataloaders:\n\nCreates the data loaders for our dataset, with a batch size of 32. Data loaders are like conveyor belt that feed the data into your model in manageable chunks.\n\nThe show_batch method is handy way to visualize a batch of data items. It’s like a quick preview to make sure everything looks good.\nFor more details, checkout the fastai DataBlock API documentation.\n\n\nTraining the Model: Welcome to the Learner World\nAfter preparing our dataset, it’s time to train our model. We use the vision_learner function to setup a learner and the powerful fine_tune method to train the model.\n\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(3)\n\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n  0%|          | 0.00/44.7M [00:00&lt;?, ?B/s]100%|██████████| 44.7M/44.7M [00:00&lt;00:00, 145MB/s] \n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.120399\n1.209828\n0.411765\n00:01\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.185352\n0.054729\n0.029412\n00:01\n\n\n1\n0.102830\n0.023147\n0.000000\n00:01\n\n\n2\n0.072183\n0.049310\n0.029412\n00:01\n\n\n\n\n\nvision_learner:\n\nThis create a learner object that combines our data loaders(dls) and a pre-trained model(resnet18). We basically saying, “Hey, take this data and use this model to learn from it.”\n\nresnet18:\n\nA specific architecture of a Convolutional Neuron Network that’s been pre-trained on a large dataset. Think of it as seasoned detective who’s seen it all and just need to be briefed on this specific case.\n\nmetrics=error_rate:\n\nThis specifies that we want to use the error rate as a metric to evaluate our model’s performance. It’s like having a scoreboard to keep track of who’s winning.\n\n\nfine_tune(3):\n\nHere’s where the magic happens. Unlike the traditional fit method, fine_tune starts by refining the pre-trained model with our specific data. It’s like taking your detective and train them on a nuances of this particular mystery. The 3 indicates the number of epochs (full cycles through the training data).\n\nThe fine_tune method is particularly powerful because it starts with a model that already knows a lot (thanks to pre-training) and fine-tune it to specific task. This approach often yields better results, faster and with less data, compared to training a model from scratch.\n\n\nMaking Predictions\nFinally, let’s make our bird classifier predict whether or not an image contain a bird.\n\nis_bird,_,probs = learn.predict(PILImage.create('bird.jpg'))\nprint(f\"This is a: {is_bird}.\")\nprint(f\"Probability it's a bird: {probs[0]:.4f}\")\n\n\n\n\n\n\n\n\nThis is a: bird.\nProbability it's a bird: 0.9988\n\n\nPILImage.create:\n\nThis function create a image object from a file. It’s like saying “Hey, look at this picture I just took.”\n\nlearn.predict:\n\nThis method uses our train model to predict what’s in a image. It’s like asking your well-trained detective, “What do you see in this picture?”\nThe method returns three values:\n\nis_bird: The predicted label(whether it’s a bird or not).\nprobs: The probabilities associated with each class.\n\n\nWhen we print out the predicted label and the probability. If the model says it’s a bird with a high probability, you can feel pretty confident your model knows its bird!\nBuilding the “Is it a Bird?” classifier was hands-on way to introduce the principles of deep learning. By leveraging fastai and Pytorch, we could quickly create an effective model with minimal code. This approach of starting with practical, top-down learning ensures that we see immediately results and understand the real world applicability of deep learning from the get-go."
  },
  {
    "objectID": "posts/2024-06-30-your-deep-learning-journey/index.html#what-is-machine-learning",
    "href": "posts/2024-06-30-your-deep-learning-journey/index.html#what-is-machine-learning",
    "title": "First Step in AI",
    "section": "What Is Machine Learning",
    "text": "What Is Machine Learning\nAh, the age-old question: What is the machine learning? Well, imagine if your computer was a child, and you were its teacher. Instead of giving it a strict set of rules to follow(which, let’s be honest, kids hate), you give it examples from which it can learn. In essence, machine learning is about enabling computer to learn from data rather than being explicitly programmed. It’s like teaching your computer how to ride a bike by letting it practice, fall and get up again, rather than reading it a manual\nLet’s take a closer look at this with a series of visualizations:\n\nTraditional Programming\nIn traditional Programming we write explicit instructions-a program-that processes input to produce results.\n\n\n\n\n\n\n\nG\n\n\n\nprogram\n\n\n\n\nprogram\n\n\n\nresults\n\nresults\n\n\n\nprogram-&gt;results\n\n\n\n\n\ninputs\n\ninputs\n\n\n\ninputs-&gt;program\n\n\n\n\n\n\n\n\n\n\nThink of it as following a recipe step-by-step: preheat the oven, mix the ingredients, bake for 30 minutes, and voilà, you have a cake.\n\n\nProgram Using Weight And Assignment\nIn machine learning, we use model with weights(parameters) that processes inputs to generates result.\n\n\n\n\n\n\n\nG\n\n\n\nmodel\n\n\n\n\nmodel\n\n\n\nresults\n\nresults\n\n\n\nmodel-&gt;results\n\n\n\n\n\ninputs\n\ninputs\n\n\n\ninputs-&gt;model\n\n\n\n\n\nweights\n\nweights\n\n\n\nweights-&gt;model\n\n\n\n\n\n\n\n\n\n\nHere, the model is like a reflexible recipe that can adjust itself. The ingredients(inputs) are mixed differently depending on the weights, and the output is a delicious result that varies based on those adjustments.\n\n\nTraining a Machine Learning Model\nTraining a model involves feeding inputs through the model to produce results, measuring performance and updating the weights to improve accuracy.\n\n\n\n\n\n\n\nG\n\n\n\nmodel\n\n\n\n\nmodel\n\n\n\nresults\n\nresults\n\n\n\nmodel-&gt;results\n\n\n\n\n\ninputs\n\ninputs\n\n\n\ninputs-&gt;model\n\n\n\n\n\nperformance\n\nperformance\n\n\n\nresults-&gt;performance\n\n\n\n\n\nweights\n\nweights\n\n\n\nweights-&gt;model\n\n\n\n\n\nperformance-&gt;weights\n\n\nupdate\n\n\n\n\n\n\n\n\nThink of it as trial and error. The model tries to bake a cake, and if it’s to salty, it adjusts the recipe (update the weights). Over time, it learns the perfect proportions.\n\n\nUsing a Trained Model\nOnce the model is trained, it can be used just like a traditional program, taking inputs and producing results predictably.\n\n\n\n\n\n\n\nG\n\n\n\nmodel\n\n\n\n\nmodel\n\n\n\nresults\n\nresults\n\n\n\nmodel-&gt;results\n\n\n\n\n\ninputs\n\ninputs\n\n\n\ninputs-&gt;model\n\n\n\n\n\n\n\n\n\n\nNow, you have reliable recipe that consistently makes the perfect cake. The model processes new inputs(ingredients) and produces outputs(cakes) with the learned adjustments."
  },
  {
    "objectID": "posts/2024-06-30-your-deep-learning-journey/index.html#what-our-image-recognizer-learned",
    "href": "posts/2024-06-30-your-deep-learning-journey/index.html#what-our-image-recognizer-learned",
    "title": "First Step in AI",
    "section": "What Our Image Recognizer Learned",
    "text": "What Our Image Recognizer Learned\nAt this stage, we have an image recognizer that works very well. But what is it actually doing? Although many people believe that deep learning results in impenetrable “black box” models (where predictions are given, but no one understand why), this isn’t entirely true. There is a vast body of research showing how to inspect deep learning model deeply and gain rich insights for them. However, all kind of machine learning model (including machine learning and traditional statistical models) can be challenging to fully understand, especially when dealing with new data that differs significantly from the training data.\nWhen we fine-tuned our pre-trained model, we adapted the last layers(originally trained on general features like flowers, humans, animals) to specialize in a birds versus non-birds problem. Imagine our model initially knew how to recognize the entire zoo, but now we’ve trained it to focus solely on recognizing birds. More generally, we could specialize such a pre-trained model on many different tasks."
  },
  {
    "objectID": "posts/2024-06-30-your-deep-learning-journey/index.html#beyond-image-classification-other-application-of-deep-learning",
    "href": "posts/2024-06-30-your-deep-learning-journey/index.html#beyond-image-classification-other-application-of-deep-learning",
    "title": "First Step in AI",
    "section": "Beyond Image Classification: Other Application of Deep Learning",
    "text": "Beyond Image Classification: Other Application of Deep Learning\nDeep learning isn’t just about figuring out whether there’s bird in your photo. It’s way more powerful than that! Let’s explore a couple of areas where deep learning make significant strides:\n\nImage Segmentation:\nSegmentation is a process of identifying and labeling pixels in an image belonging to the same object. This is critically important for application like autonomous vehicles where the car needs to recognize and localize object such as pedestrians, other vehicles, and road signs. Instead of just saying, “Hey, there’s a cat in a picture”, segmentation says, “Here’s the outline of the cat in this picture”.\nNatural Language Processing (NLP): Deep learning has dramatically improved Natural Language Processing over the last few years. Now computers can:\n\nGenerate text: Write coherent and context-aware essays (but don’t trust them with your love letters just yet).\nTranslate languages: Turn English into Spanish, French, or Klingon (okay, maybe not Klingon…yet)\nAnalyze comments: Understand sentiments, detect sarcasm, and probably tell when you’re being a bit snarky.\nLabel words in sentences: Identify parts of speech (nouns, verbs, adjectives, etc.), entities (like names and places), and more.\n\n\nHere’s some cool code to classify the sentiment of a movie review better than anything available just a few years ago:\n\nfrom fastai.text.all import *\ndls = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid='test')\nlearn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\nlearn.fine_tune(4, 1e-2)\n\n\n\n\n\n\n    \n      \n      100.00% [144441344/144440600 00:03&lt;00:00]\n    \n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [105070592/105067061 00:01&lt;00:00]\n    \n    \n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.462561\n0.395122\n0.822320\n03:08\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.301779\n0.248262\n0.899480\n06:38\n\n\n1\n0.244484\n0.202708\n0.921480\n06:38\n\n\n2\n0.189148\n0.194167\n0.926160\n06:37\n\n\n3\n0.148741\n0.191470\n0.929720\n06:38\n\n\n\n\n\n\nlearn.predict(\"I really liked that movie!\")\n\n\n\n\n\n\n\n\n('pos', tensor(1), tensor([7.8042e-04, 9.9922e-01]))\n\n\nAnd boom! You have a state-of-art sentiment analyzer."
  },
  {
    "objectID": "posts/2024-06-30-your-deep-learning-journey/index.html#the-important-of-validation-and-test-sets",
    "href": "posts/2024-06-30-your-deep-learning-journey/index.html#the-important-of-validation-and-test-sets",
    "title": "First Step in AI",
    "section": "The Important of Validation and Test Sets",
    "text": "The Important of Validation and Test Sets\nWe’ve trained our model and it’s looking pretty smart, but know how do we know it’s actually learned something useful? This is where validation and test sets come in.\n\nWhy Do We Need a Validation set?\nThe goal of a model is to make predictions about unseen data. If we trained a model with all our data and evaluated it using the same data, we wouldn’t really know how well it performs on new, unseen data. It could just memorize the training data(cheating basically). The model could get great results on your training data but bomb when given the data to analyze. To avoid this, we: - We split dataset: We divide our data into training and validation sets. The training set is used to teach the model, and the validation set is used to see how well it’s learning\n\n\nPreventing Overfitting with a Test set\nOverfitting is a common issue where the model preform exceptionally well on the training set but poorly on the validation set, meaning it has memorized the training data rather than learning the generalizable pattern.\nEven when your model hasn’t fully memorized all your data, it might memorized certain parts of it during earlier training stages. The longer you train, the better the accuracy on the training set, but eventually, the validation accuracy will start to decline. This is because your model is begins memorizing the training data instead of learning the pattern that generalize well. When this happens, we say the model is overfitting.\nHere’s an example to visualize overfitting:\n\n\n\n\nExample of overfitting\n\n\nThe Image shows what happens when you overfit, using a simplified example where we have just one parameter and some randomly generated data. Although the overfitted model’s prediction are accurate for the data near the observed data points, they are way off when outside of that range.\nOverfitting is the single most important and challenging issue when training machine learning models. It’s easy to create a model that does the great job at making predictions on the data it’s been trained on, but making accurate predictions on new data is much harder.\nFor instance, if you writing a handwritten digit classifier (as we will very soon) and use it to recognize numbers on checks, you won’t see the same numbers the model was trained on–checks will have different variations of handwriting to deal with."
  },
  {
    "objectID": "posts/2024-06-30-your-deep-learning-journey/index.html#wrapping-up",
    "href": "posts/2024-06-30-your-deep-learning-journey/index.html#wrapping-up",
    "title": "First Step in AI",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nDeep learning is an exciting field that extends far beyond simple image classification. From understand speech to translate languages and detecting malware, it’s applications are vast. Through this blog post, we’ve seen how to build a bird classifier using the fastai library-an accessible, powerful tool that simplifies the complexities of machine learning.\nBy splitting our data into training and validation sets, we ensure our model doesn’t cheat and genuinely learns the task at hand. With powerful tools like fastai and the ability to handle the diverse tasks, deep learning truly has potential to transform numerous industries.\nI hope you enjoyed this journey as much as I did. Remember, the key to mastering deep learning is to keep experimenting and learning. So go ahead, build that next big thing, and maybe teach your computer to recognize your pet fish or translate cat’s meows!"
  },
  {
    "objectID": "posts/2024-06-30-your-deep-learning-journey/index.html#final-thoughts",
    "href": "posts/2024-06-30-your-deep-learning-journey/index.html#final-thoughts",
    "title": "First Step in AI",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nThank you for joining me on this deep learning adventure! If you find this blog helpful or inspiring, please share it with others who might also be interested. Deep learning is a continuously evolving field with endless possibilities. Stay curious, keep learning, and don’t hesitate to dive deeper into the world of AI.\nFeel free to leave your comments, questions, or insights below. I’d love to hear your experiences, projects, and what you’re learning. Together, we can continue to explore and push the boundaries of what’s possible with deep learning.\nHappy coding, and may your models always be accurate!"
  },
  {
    "objectID": "posts/2024-07-28-from-neuron-to-gradient/index.html",
    "href": "posts/2024-07-28-from-neuron-to-gradient/index.html",
    "title": "Looking Inside Neural Networks, How It Really Work?",
    "section": "",
    "text": "Hey there. It’s been a couple of week since my last post - blame exams and obsessive quest to tweak every configuration setting for my workflow (which is turned into a week-long habit hole - i regret nothing). But today, I’m excited to dive back into the world of AI and share my latest escapades from Lesson 3 of the FastAI course taught by the indomitable Jeremy Horawd. Spoiler alert: it’s packed with enough neural wonders to make your brain do a happy dance.\nIn the coming post, I’ll guide you through:\n\nPicking of right AI model that’s just right for you\nDissecting the anatomy of these models (paramedics not required)\nThe inner workings of neuron networks\nThe Titanic competition\n\nSo, hold onto your neural nets and let’s jump right into it, shall we?"
  },
  {
    "objectID": "posts/2024-07-28-from-neuron-to-gradient/index.html#introduction",
    "href": "posts/2024-07-28-from-neuron-to-gradient/index.html#introduction",
    "title": "Looking Inside Neural Networks, How It Really Work?",
    "section": "",
    "text": "Hey there. It’s been a couple of week since my last post - blame exams and obsessive quest to tweak every configuration setting for my workflow (which is turned into a week-long habit hole - i regret nothing). But today, I’m excited to dive back into the world of AI and share my latest escapades from Lesson 3 of the FastAI course taught by the indomitable Jeremy Horawd. Spoiler alert: it’s packed with enough neural wonders to make your brain do a happy dance.\nIn the coming post, I’ll guide you through:\n\nPicking of right AI model that’s just right for you\nDissecting the anatomy of these models (paramedics not required)\nThe inner workings of neuron networks\nThe Titanic competition\n\nSo, hold onto your neural nets and let’s jump right into it, shall we?"
  },
  {
    "objectID": "posts/2024-07-28-from-neuron-to-gradient/index.html#choosing-the-right-model",
    "href": "posts/2024-07-28-from-neuron-to-gradient/index.html#choosing-the-right-model",
    "title": "Looking Inside Neural Networks, How It Really Work?",
    "section": "Choosing the Right Model",
    "text": "Choosing the Right Model\nWe’ll explore how to choose an image model that’s efficient, reliable, and cost-effective—much like selecting the perfect gadget. I’ll walk you through a practical example comparing two popular image models by training a pet detector model.\nLet’s start by setting up our environment.\n\nfrom fastai.vision.all import *\nimport timm\n\n\npath = untar_data(URLs.PETS)/'images'\n\n\ndls = ImageDataLoaders.from_name_func(\n    \".\",\n    get_image_files(path),\n    valid_pct=0.2,\n    seed=42,\n    label_func=RegexLabeller(pat=r'^([^/]+)_\\d+'),\n    item_tfms=Resize(224)\n)\n\n\n\n\n\n\n    \n      \n      100.00% [811712512/811706944 01:13&lt;00:00]\n    \n    \n\n\nLet’s break down what’s happening here. We’re using The Oxford-IIIT Pet dataset, fetched with a nifty little URL constant provide by FastAI. If you’re staring at the pattern pat=r'^([^/]+)\\_\\d+' like it’s some alien script, fear not! It’s just a regular expression used to extract label from filenames using fastai RegexLabeller\nHere’s the cheat sheet for the pattern:\n\n^ asserts the start of a string.\n([^/]+) matches one or more characters that are not forward slash and captures them as a group.\n_ matches an underscore.\n\\d+ matches one ore more digits.\n\nNow, let’s visualize our data:\n\ndls.show_batch(max_n=4)\n\n\n\n\n\n\n\n\nAnd, it’s training time! We start with a ResNet34 architecture:\n\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(3)\n\nDownloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n\n100%|██████████| 83.3M/83.3M [00:00&lt;00:00, 147MB/s] \n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.491942\n0.334319\n0.105548\n00:26\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.454661\n0.367568\n0.112991\n00:32\n\n\n1\n0.272869\n0.274704\n0.081867\n00:33\n\n\n2\n0.144361\n0.246424\n0.073072\n00:33\n\n\n\n\n\nAfter about two minutes, we reached a 7% error rate—not too shabby! However, there’s one catch: while ResNet34 is dependable like a classic family car, it isn’t the fastest option out there. To really amp things up, we need to find a more advanced, high-performance model.\n\nExploring the Model Landscape\nThe PyTorch image model library offers a wide range of architectures—not quite a zillion, but enough to give you plenty of options. Many of these models are built on mathematical functions like ReLUs (Rectified Linear Units), which we’ll discuss in more detail later. Ultimately, choosing the right model comes down to three key factors:\n\nSpeed\n\nMemory Usage\n\nAccuracy\n\n\n\nThe “Which Image Model is Best?” Notebook\nI highly recommend taking a look at Jeremy Howard’s excellent notebook, “Which image models are best?”. It’s a valuable resource for finding the best architecture for your needs. If you find it helpful, do check it out and consider giving it an upvote—Jeremy’s insights are solid.\nI’ve also included a copy of the plot below for quick reference. Enjoy exploring the model landscape!\n\nPlotly = require('https://cdn.plot.ly/plotly-latest.min.js');\ndf_results = FileAttachment(\"results-imagenet.csv\").csv()\ndf = FileAttachment(\"benchmark-infer-amp-nhwc-pt111-cu113-rtx3090.csv\").csv()\n\ndf_merged = {\n  let df_results_processed = df_results.map(r =&gt; ({ ...r, model_org: r.model, model: r.model.split('.')[0] }));\n\n  const dfColumns = Object.keys(df[0]);\n  const dfResultsColumns = Object.keys(df_results_processed[0]);\n\n  return df.flatMap(d =&gt; {\n    const matches = df_results_processed.filter(r =&gt; r.model === d.model);\n    return matches.map(match =&gt; {\n      let mergedRow = {};\n      dfColumns.forEach(col =&gt; {\n        if (dfResultsColumns.includes(col) && col !== 'model') { mergedRow[`${col}_x`] = d[col]; } else { mergedRow[col] = d[col]; }\n      });\n      dfResultsColumns.forEach(col =&gt; {\n        if (dfColumns.includes(col) && col !== 'model') { mergedRow[`${col}_y`] = match[col]; } else { mergedRow[col] = match[col]; }\n      });\n      return mergedRow;\n    });\n  });\n}\n\ndf_final = df_merged\n  .map(d =&gt; ({...d, secs: 1 / Number(d.infer_samples_per_sec)}))\n  .map(d =&gt; {\n    const familyMatch = d.model.match(/^([a-z]+?(?:v2)?)(?:\\d|_|$)/);\n    let family = familyMatch ? familyMatch[1] : '';\n    if (d.model.includes('in22')) family += '_in22';\n    if (d.model.match(/resnet.*d/)) family += 'd';\n    return {...d, family: family};\n  })\n  .filter(d =&gt; !d.model.endsWith('gn'))\n  .filter(d =&gt; /^re[sg]netd?|beit|convnext|levit|efficient|vit|vgg|swin/.test(d.family));\n{\n  const uniqueFamilies = [...new Set(df_final.map(d =&gt; d.family))];\n  const colorScale = uniqueFamilies.map((family, index) =&gt; { return `hsl(${index * 360 / uniqueFamilies.length}, 70%, 50%)`; });\n  const traces = uniqueFamilies.map((family, index) =&gt; {\n    const familyData = df_final.filter(d =&gt; d.family === family);\n    return {\n      name: family,\n      x: familyData.map(d =&gt; d.secs),\n      y: familyData.map(d =&gt; Number(d.top1)),\n      mode: 'markers',\n      type: 'scatter',\n      marker: { size: familyData.map(d =&gt; Math.pow(Number(d.infer_img_size), 2) / 5700), color: colorScale[index], },\n      text: familyData.map(d =&gt; `${d.model}&lt;br&gt; family=${d.family}&lt;br&gt; secs=${d.secs.toFixed(8)}&lt;br&gt; top1=${Number(d.top1).toFixed(3)}&lt;br&gt; size=${d.param_count_x}&lt;br&gt; infer_img_size=${d.infer_img_size}`),\n      hoverinfo: 'text',\n      hoverlabel: { bgcolor: colorScale[index] }\n    };\n  });\n  const layout = {\n    title: 'Inference',\n    width: 795,\n    height: 750,\n    autosize: true,\n    xaxis: { title: 'secs', type: 'log', autorange: true, gridcolor: 'rgb(233,233,233)', },\n    yaxis: { title: 'top1', range: [65, 90], gridcolor: 'rgb(233,233,233)', },\n    plot_bgcolor: 'rgb(240,240,255)',\n    showlegend: true,\n    legend: { title: {text: 'family'}, itemclick: 'toggle', itemdoubleclick: 'toggleothers' },\n    hovermode: 'closest'\n  };\n  const config = {responsive: true};\n  const plot = DOM.element('div');\n  Plotly.newPlot(plot, traces, layout, config);\n  return plot;\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere’s a breakdown of the plot from the notebook:\n\nThe X-axis represents seconds per sample (the lower, the better performance).\nThe Y-axis reflects the accuracy (higher is preferable).\n\nIn an ideal scenario, you would choose models that are located in the upper left corner of the plot. Although ResNet34 is a reliable choice—like a pair of trusty jeans—it’s no longer considered state-of-the-art. It’s time to explore the ConvNeXT models!\nBefore you get started, ensure that you have the timm package installed. You can install it using pip or conda:\npip install timm\nor\nconda install timm\nAfter that, let’s search for all available ConvNeXT models.\n\ntimm.list_models(\"convnext*\")\n\n\n\n['convnext_atto',\n 'convnext_atto_ols',\n 'convnext_base',\n 'convnext_femto',\n 'convnext_femto_ols',\n 'convnext_large',\n 'convnext_large_mlp',\n 'convnext_nano',\n 'convnext_nano_ols',\n 'convnext_pico',\n 'convnext_pico_ols',\n 'convnext_small',\n 'convnext_tiny',\n 'convnext_tiny_hnf',\n 'convnext_xlarge',\n 'convnext_xxlarge',\n 'convnextv2_atto',\n 'convnextv2_base',\n 'convnextv2_femto',\n 'convnextv2_huge',\n 'convnextv2_large',\n 'convnextv2_nano',\n 'convnextv2_pico',\n 'convnextv2_small',\n 'convnextv2_tiny']\n\n\nFound one? Awesome! Now, let’s put it to the test. We’ll specify the architecture as a string when we call vision_learner, Why previous time when we use ResNet34 we don’t need to pass it as string? you say! That’s because ResNet34 was built in fastai library so you just need to call it but with ConvNext you have to pass the arch as a string for it to work, alright let’s see what it look like:\n\narch = 'convnext_tiny.fb_in22k'\nlearn = vision_learner(dls, arch, metrics=error_rate).to_fp16()\nlearn.fine_tune(3)\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.123377\n0.240116\n0.081191\n00:27\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.260218\n0.225793\n0.071719\n00:34\n\n\n1\n0.199426\n0.169573\n0.059540\n00:33\n\n\n2\n0.132157\n0.166686\n0.056834\n00:33\n\n\n\n\n\n\n\nResults Are In!\nThe training time increased slightly—by about 3 to 4 seconds—but here’s the exciting part: the error rate dropped from 7.3% to 5.6%!\nNow, those model names might seem a bit cryptic at first glance. Here’s a quick guide to help you decode them:\n\nNames like Tiny, Small, Large, etc.: These indicate the model’s size and resource requirements.\nfb_in22k: This means the model was trained on the ImageNet dataset with 22,000 image categories by Facebook AI Research (FAIR).\n\nIn general, ConvNeXT models tend to outperform others in accuracy for standard photographs of natural objects. In summary, we’ve seen how choosing the right architecture can make a significant difference by balancing speed, memory usage, and accuracy. Stay tuned as we dive even deeper into the intricacies of neural networks next!"
  },
  {
    "objectID": "posts/2024-07-28-from-neuron-to-gradient/index.html#whats-in-the-model",
    "href": "posts/2024-07-28-from-neuron-to-gradient/index.html#whats-in-the-model",
    "title": "Looking Inside Neural Networks, How It Really Work?",
    "section": "What’s in the Model?",
    "text": "What’s in the Model?\nAlright, you see? Our model did better, right? Now, you’ve probably wondering, how do we turn this awesome piece of neural magic into an actual application? They key is to save the trained model so that users won’t have to wait for the training time.\nTo do that, we export our learner with the following command, creating a magical file called model.pkl:\n\nlearn.export('model.pkl')\n\nFor those of you who’ve followed my previous blog posts, you’ll recall that when I deploy an application on HuggingFace Spaces, I simply load the model.pkl file. This way, the learner functions almost identically to the trained learn object—and the best part is, you no longer have to wait forever!\nNow, you might be wondering, “What exactly did we do here? What’s inside this model.pkl file?”\n\nDissecting the model.pkl File\nLet’s take a closer look. The model.pkl file is essentially a saved learner, and it contains two main components:\n\nPre-processing Steps: These include all the procedures needed to transform your raw images into a format that the model can understand. In other words, it stores the information from your DataLoaders (dls), DataBlock, or any other pre-processing pipeline you’ve set up.\nThe Trained Model: This is the core component—a trained model that’s ready to make predictions.\n\nTo inspect its contents, we can load the model back up and examine it.\n\nm = learn.model\nm\n\n\n\nSequential(\n  (0): TimmBody(\n    (model): ConvNeXt(\n      (stem): Sequential(\n        (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n        (1): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n      )\n      (stages): Sequential(\n        (0): ConvNeXtStage(\n          (downsample): Identity()\n          (blocks): Sequential(\n            (0): ConvNeXtBlock(\n              (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n              (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=96, out_features=384, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (norm): Identity()\n                (fc2): Linear(in_features=384, out_features=96, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n            (1): ConvNeXtBlock(\n              (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n              (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=96, out_features=384, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (norm): Identity()\n                (fc2): Linear(in_features=384, out_features=96, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n            (2): ConvNeXtBlock(\n              (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n              (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=96, out_features=384, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (norm): Identity()\n                (fc2): Linear(in_features=384, out_features=96, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n          )\n        )\n        (1): ConvNeXtStage(\n          (downsample): Sequential(\n            (0): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n            (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n          )\n          (blocks): Sequential(\n            (0): ConvNeXtBlock(\n              (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n              (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=192, out_features=768, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (norm): Identity()\n                (fc2): Linear(in_features=768, out_features=192, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n            (1): ConvNeXtBlock(\n              (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n              (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=192, out_features=768, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (norm): Identity()\n                (fc2): Linear(in_features=768, out_features=192, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n            (2): ConvNeXtBlock(\n              (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n              (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=192, out_features=768, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (norm): Identity()\n                (fc2): Linear(in_features=768, out_features=192, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n          )\n        )\n        (2): ConvNeXtStage(\n          (downsample): Sequential(\n            (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n            (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n          )\n          (blocks): Sequential(\n            (0): ConvNeXtBlock(\n              (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n              (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (norm): Identity()\n                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n            (1): ConvNeXtBlock(\n              (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n              (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (norm): Identity()\n                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n            (2): ConvNeXtBlock(\n              (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n              (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (norm): Identity()\n                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n            (3): ConvNeXtBlock(\n              (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n              (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (norm): Identity()\n                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n            (4): ConvNeXtBlock(\n              (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n              (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (norm): Identity()\n                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n            (5): ConvNeXtBlock(\n              (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n              (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (norm): Identity()\n                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n            (6): ConvNeXtBlock(\n              (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n              (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (norm): Identity()\n                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n            (7): ConvNeXtBlock(\n              (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n              (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (norm): Identity()\n                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n            (8): ConvNeXtBlock(\n              (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n              (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (norm): Identity()\n                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n          )\n        )\n        (3): ConvNeXtStage(\n          (downsample): Sequential(\n            (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n            (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n          )\n          (blocks): Sequential(\n            (0): ConvNeXtBlock(\n              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (norm): Identity()\n                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n            (1): ConvNeXtBlock(\n              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (norm): Identity()\n                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n            (2): ConvNeXtBlock(\n              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (norm): Identity()\n                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n          )\n        )\n      )\n      (norm_pre): Identity()\n      (head): NormMlpClassifierHead(\n        (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n        (norm): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n        (flatten): Flatten(start_dim=1, end_dim=-1)\n        (pre_logits): Identity()\n        (drop): Dropout(p=0.0, inplace=False)\n        (fc): Identity()\n      )\n    )\n  )\n  (1): Sequential(\n    (0): AdaptiveConcatPool2d(\n      (ap): AdaptiveAvgPool2d(output_size=1)\n      (mp): AdaptiveMaxPool2d(output_size=1)\n    )\n    (1): fastai.layers.Flatten(full=False)\n    (2): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): Dropout(p=0.25, inplace=False)\n    (4): Linear(in_features=1536, out_features=512, bias=False)\n    (5): ReLU(inplace=True)\n    (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (7): Dropout(p=0.5, inplace=False)\n    (8): Linear(in_features=512, out_features=37, bias=False)\n  )\n)\n\n\n\n\nWhat’s All This Stuff?\nAlright, there’s a lot to digest here. Basically, the model is structured in layers upon layers. Here’s the breakdown:\nTimmBody: this contains most of the model architecture. Inside the TimmBody. You’ll find:\n\nModel: The main model components.\nStem: The initial layers that process the raw input.\nStages: There are further broken down into multiple blocks, each packed with convolutional layers. normalization layers, and more.\n\n\n\nLet’s Peek Inside a Layer\nTo dig deeper into what these layers contain, you can use a really convenient Pytorch method called get_submodule:\n\nl = m.get_submodule('0.model.stem.1')\nl\n\nLayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n\n\nAs you can see it return a LayerNorm2d layer. Wondering what this LayerNorm2d thing is all about? It comprises a mathematical function for normalization and bunch of parameters:\n\nprint(list(l.parameters()))\n\n\n\n[Parameter containing:\ntensor([ 1.2546e+00,  1.9191e+00,  1.2191e+00,  1.0385e+00, -3.7148e-04,\n         7.6571e-01,  8.8668e-01,  1.6324e+00,  7.0477e-01,  3.2892e+00,\n         7.8641e-01, -1.7453e-03,  1.0006e+00, -2.0514e-03,  3.2976e+00,\n        -1.2112e-03,  1.9842e+00,  1.0206e+00,  4.4522e+00,  2.5476e-01,\n         2.7248e+00,  9.2616e-01,  1.2374e+00,  4.3668e-03,  1.7875e+00,\n         5.4292e-01,  4.6268e+00,  1.1599e-02, -5.4437e-04,  3.4510e+00,\n         1.3520e+00,  4.1267e+00,  2.6876e+00,  4.1197e+00,  3.4007e+00,\n         8.5053e-01,  7.3569e-01,  3.9801e+00,  1.2851e+00,  6.3985e-01,\n         2.6897e+00,  1.1181e+00,  1.1699e+00,  5.5318e-01,  2.3341e+00,\n        -3.0504e-04,  9.7000e-01,  2.3409e-03,  1.1984e+00,  1.7897e+00,\n         4.0138e-01,  4.5116e-01,  9.7186e-01,  3.9881e+00,  6.5935e-01,\n         6.8778e-01,  9.8614e-01,  2.7053e+00,  1.2169e+00,  7.6268e-01,\n         3.3019e+00,  1.6200e+00,  9.5547e-01,  2.1216e+00,  6.2951e-01,\n         4.0349e+00,  8.9246e-01, -2.9147e-03,  4.0874e+00,  1.0639e+00,\n         1.3963e+00,  1.6683e+00,  4.6571e-04,  7.6833e-01,  8.8542e-01,\n         6.4305e-01,  1.3443e+00,  7.1566e-01,  5.4763e-01,  2.0902e+00,\n         1.1952e+00,  3.0668e-01,  2.9682e-01,  1.4709e+00,  4.0830e+00,\n        -7.8233e-04,  1.1455e+00,  3.8835e+00,  3.5997e+00,  4.8206e-01,\n         2.1703e-01, -1.6550e-04,  6.4791e-01,  3.0069e+00,  3.0463e+00,\n         4.6374e-03], device='cuda:0', requires_grad=True), Parameter containing:\ntensor([-9.8183e-02, -4.0191e-02,  4.1647e+00, -8.9313e-03,  3.7929e-03,\n        -2.7139e-02, -3.1174e-02, -7.9865e-02, -1.4053e-01, -6.3492e-02,\n         3.2160e-01, -3.3837e-01, -5.6851e-02, -4.0384e-03, -4.7630e-02,\n        -2.6376e-02, -4.0858e-02, -4.0886e-02,  8.7548e-03, -2.4149e-02,\n         8.5088e-03, -1.6333e-01, -4.0154e+00,  5.2989e-01, -5.3410e-01,\n         2.8046e+00,  3.5663e-02, -1.0321e-02, -1.1255e-03, -1.1721e-01,\n        -1.3768e-01,  1.8840e-02, -9.5614e-02, -1.3149e-01, -1.9291e-01,\n        -6.8939e-02, -3.6672e-02, -1.2902e-01,  1.5387e-01,  3.6398e-03,\n        -6.6185e-02,  5.8841e-02, -9.1987e-02, -1.1453e+00, -5.4502e-02,\n        -5.3649e-03, -1.8238e-01,  2.3167e-02,  3.8862e-02, -5.9394e-02,\n        -4.1380e-02, -5.6917e-02, -4.3903e-02, -1.2954e-02, -1.1092e-01,\n         7.0337e-03, -3.9300e-02, -1.5816e-01, -9.8132e-02, -1.8553e-01,\n        -1.1112e-01, -1.8186e-01, -3.4278e-02, -2.6474e-02,  1.4192e+00,\n        -3.1935e-02, -4.3245e-02, -2.7030e-01, -4.6695e-02, -6.4756e-04,\n         2.6561e-01,  1.8779e-01,  6.9716e-01, -3.0647e-01,  8.1973e-02,\n        -1.0845e+00,  1.4999e-02, -4.4244e-02, -8.0861e-02, -6.8972e-02,\n        -1.3070e-01, -1.7093e-02, -1.9623e-02, -3.9345e-02, -6.9878e-02,\n         1.2335e-02, -5.9947e-02, -3.5691e-02, -7.9831e-02, -7.4387e-02,\n        -9.5232e-03, -3.7763e-01, -1.1987e-02, -2.5113e-02, -6.2690e-02,\n        -3.0666e-04], device='cuda:0', requires_grad=True)]\n\n\nAnother example: Let’s inspect a layer deeper inside:\n\nl = m.get_submodule('0.model.stages.0.blocks.1.mlp.fc1')\nprint(l)\nprint(list(l.parameters()))\n\n\n\nLinear(in_features=96, out_features=384, bias=True)\n[Parameter containing:\ntensor([[ 0.0227, -0.0014,  0.0404,  ...,  0.0016, -0.0453,  0.0083],\n        [-0.1439,  0.0169,  0.0261,  ...,  0.0126, -0.1044,  0.0565],\n        [-0.0655, -0.0327,  0.0056,  ..., -0.0414,  0.0659, -0.0401],\n        ...,\n        [-0.0089,  0.0699,  0.0003,  ...,  0.0040,  0.0415, -0.0191],\n        [ 0.0019,  0.0321,  0.0297,  ..., -0.0299, -0.0304,  0.0555],\n        [ 0.1211, -0.0355, -0.0045,  ..., -0.0062,  0.0240, -0.0114]],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([-0.4049, -0.7419, -0.4234, -0.1651, -0.3027, -0.1899, -0.5534, -0.6270,\n        -0.3008, -0.4253, -0.5996, -0.4107, -0.2173, -1.7935, -0.3170, -0.1163,\n        -0.4483, -0.2847, -0.4343, -0.4945, -0.4064, -1.1403, -0.6754, -1.7236,\n        -0.2954, -0.2655, -0.2188, -0.3913, -0.4148, -0.4771,  0.2366, -0.7542,\n        -0.5851, -0.1821, -1.5273, -0.3625, -2.4688, -2.3461, -0.6110, -0.4114,\n        -0.6963, -0.5764, -0.5878, -0.0318, -2.0354, -0.2859, -0.3954, -0.8404,\n        -2.2399, -1.0874, -0.2296, -0.9002, -0.7585, -0.8834, -0.3753, -0.4548,\n        -0.3836, -0.4048, -2.0231, -1.0264, -0.4106, -1.1566, -0.2225, -0.4251,\n        -0.2496, -0.4224, -0.0975, -1.4017, -0.6887, -0.4370, -0.2931, -0.4643,\n        -0.4959, -1.2535, -1.0720, -1.2966, -0.6276, -1.4162, -2.3081, -2.4540,\n        -0.4258, -0.9987, -0.4638, -0.3147, -0.2417, -0.8744, -0.2828, -1.4208,\n        -0.3257, -0.3202, -0.0603, -0.1894, -0.2496, -0.6130, -0.2975, -2.1466,\n        -0.4129, -0.3677, -1.9813, -0.3814, -0.3785, -0.2294, -0.3698, -0.3256,\n        -0.5585, -2.4192, -0.4589, -1.7748, -0.3995, -0.4092, -0.3517, -0.5331,\n        -1.6535, -1.8190,  0.6264, -0.4059,  0.5873, -2.2074, -0.2438, -2.4539,\n        -0.2283, -0.6865,  0.6988,  0.6476, -0.6445, -0.3452, -0.3276, -0.5700,\n        -0.5173, -0.2775, -0.4089, -0.3020, -0.4872, -0.4952, -0.4072, -0.4356,\n        -0.5102, -0.4128, -2.0918, -0.2826, -0.5830, -1.5835,  0.6139, -0.8504,\n        -0.4669, -2.1358, -0.3418, -0.3767, -0.3345, -0.3960, -0.3886, -0.5667,\n        -0.2225, -1.3059, -0.4600, -0.3927, -0.4667, -0.4214, -0.4755, -0.2866,\n        -1.5805, -0.1787, -0.4367, -0.3172,  1.5731, -0.4046, -0.4838, -0.2576,\n        -0.5612, -0.4264, -0.2578, -0.3175, -0.4620, -1.9552, -1.9145, -0.3960,\n         0.3988, -2.3519, -0.9688, -0.2831, -1.9001, -0.4180,  0.0159, -1.1109,\n        -0.4921, -0.3177, -1.8909, -0.3101, -0.8136, -2.3345, -0.3845, -0.3847,\n        -0.1974, -0.4445, -1.6233, -2.5485, -0.3176, -1.2715, -1.1479,  0.6149,\n        -0.3748, -0.3949, -2.0747, -0.4657, -0.3780, -0.4957, -0.3282, -1.9219,\n        -2.0019, -0.5307, -0.2554, -1.1160, -0.3517, -2.2185, -1.1393,  0.5364,\n        -0.3217, -2.0389, -0.4655,  0.1850, -0.5830, -0.3128,  0.6180, -0.2125,\n        -2.3538, -0.9699, -0.9785, -0.3667, -0.4502, -1.9564, -0.2662, -1.1755,\n        -0.4198, -0.9024, -0.3605, -0.5172, -1.1879, -0.4190, -0.4770, -1.5560,\n        -0.4011, -0.6518, -0.4818, -0.2423,  0.6909, -0.5081, -0.4304, -0.6068,\n        -0.4000, -0.3329, -0.3596, -1.6108, -0.2371, -0.2467, -0.4545,  0.1807,\n        -0.3227, -0.3918, -0.3515, -0.3755, -1.2178, -0.3999, -0.3578, -0.2882,\n        -1.7483, -0.2363, -0.1599, -0.2640, -0.9769, -1.3065, -0.4148, -0.2663,\n        -0.3933, -0.4627, -0.2174,  0.2140, -0.5733, -0.2766, -0.3659, -0.5172,\n        -0.3484, -0.3362, -0.6445,  0.6866, -0.3738, -0.2902, -2.0863, -0.4882,\n        -0.2597, -1.0496, -1.6616, -0.3398, -0.5111, -0.5659, -0.3027, -0.5048,\n        -0.2877, -0.2841, -0.1982, -0.6910, -0.2873, -2.1121, -0.8927, -0.2301,\n        -1.5013, -0.4734, -2.2292, -0.4022, -0.2926, -0.4199,  0.6646, -0.3047,\n        -0.1688, -0.3749, -0.6433, -2.3348, -0.3101, -1.2730, -0.8193, -1.0593,\n        -0.0934, -1.6387,  0.3426, -0.8484, -0.4910, -0.5001, -1.0631, -0.3534,\n        -1.1564, -0.3842, -0.3172, -0.6432, -0.9083, -0.6567, -0.6490,  0.6337,\n        -0.2662, -1.3202, -1.1623, -1.2032, -2.0577, -0.3001, -1.3596, -0.4612,\n        -0.5024, -0.4950, -0.3156, -0.3272, -0.2669, -0.4279, -0.3296, -0.3011,\n        -1.6635,  0.6434, -0.9455,  0.6099, -0.4234,  0.3917, -0.4944, -0.4284,\n        -0.2587, -0.4952, -2.1991, -0.2601, -0.3934, -0.4565, -0.5816, -0.3487,\n        -0.7372, -0.3589, -0.4894, -2.0105,  0.4557, -0.8055, -1.7748, -0.3512,\n        -0.5359, -0.2101, -0.3955, -0.4782, -1.1457, -0.3974, -2.2115, -0.2838],\n       device='cuda:0', requires_grad=True)]\n\n\nWhat do these numbers mean, you ask? Essentially, they represent the learned parameters of the model— the weights that have been fine-tuned during training. These weights form the “secret sauce” that enables the model to distinguish between, say, a basset hound and a tabby cat.\nNext, we’ll dive into how neural networks function behind the scenes, exploring the mechanisms that transform these parameters into powerful predictions."
  },
  {
    "objectID": "posts/2024-07-28-from-neuron-to-gradient/index.html#how-neural-networks-really-work",
    "href": "posts/2024-07-28-from-neuron-to-gradient/index.html#how-neural-networks-really-work",
    "title": "Looking Inside Neural Networks, How It Really Work?",
    "section": "How Neural Networks Really Work",
    "text": "How Neural Networks Really Work\nTo answer the burning question from before, let’s dive into the marvels of neural networks. Yes, Jeremy Howard has an amazing notebook called “How does a neural net really work?” that’s perfect for beginners. But, I’m here to give you a walkthrough with a dash of humor!\nMachine learning models are like very smart shape-fitting artists. They find pattern in data and learn to recognize them. We’ll start simple - with a quadratic function. Let’s see how it all works:\n\n\nCode\nimport plotly\nimport plotly.express as px\nimport torch\nimport numpy as np\nfrom IPython.display import display, HTML\n\n # Tomas Mazak's workaround for MathJax in VSCode\nplotly.offline.init_notebook_mode()\ndisplay(HTML(\n    '&lt;script type=\"text/javascript\" async src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_SVG\"&gt;&lt;/script&gt;'\n)) \ndef plot_function(f, title=None, min=-2.1, max=2.1):\n    x = torch.linspace(min, max, steps=100)\n    y = f(x)\n    return px.line(x=x, y=y, title=title)\n\n\n\ndef f(x): return 3 * x**2 + 2 * x + 1\nplot_function(f, title=r\"$3x^2 + 2x + 1$\")\n\n\nfunction f(x, a, b, c) { return a*x**2 + b*x + c }\nx = { return Array.from({ length: 40 }, (_, i) =&gt; -2 + (i * (2 - (-2)) / (40 - 1))); }\ny = x.map(element =&gt; f(element, a, b, c))\n{\n  var trace1 = { x: x, y: y, mode: 'lines', name: 'quadratic'};\n  var data = [trace1];\n  var layout = { title: \"3x²+ 2x + 1\", xaxis: { title: 'x', zeroline: false, }, yaxis: { title: \"y\", } };\n  const div = DOM.element('div');\n  Plotly.newPlot(div, data, layout);\n  return div;\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat we want to do here is straightforward: suppose we don’t know the exact mathematical function, and we’re trying to reconstruct it from some data. Here’s the actual function, and our goal is to approximate it using a variety of quadratic equations.\nCreating Quadratics on Demand\nIn Python, the partial function lets us fix certain parameters of a function to generate different variations. It’s like having a playlist of your favorite songs with the flexibility to change the lyrics whenever you want!\n\nfrom functools import partial\n\ndef quad(a, b, c, x): return a * x**2 + b * x + c\ndef mkquad(a, b, c): return partial(quad, a, b, c)\n\n\nIntroducing Noise\nIn real life, data never fits perfectly to a function. There’s always some noise, it’s often as messy and unpredictable as a doctor’s illegible handwriting. Let’s add some noise to our data:\n\ndef noise(x, scale): return np.random.normal(scale=scale, size=x.shape)\ndef add_noise(x, mult, add): return x * (1 + noise(x, mult)) + noise(x, add)\n\n\nnp.random.seed(42)\nx = torch.linspace(-2, 2, steps=40)\ny = add_noise(f(x), 0.15, 1.5)\npx.scatter(x=x, y=y)\n\n\ndata = FileAttachment(\"dataponts.csv\").csv()\nx_data = data.map(item =&gt; item.x);\ny_data = data.map(item =&gt; item.y);\n{\n  var trace1 = { x:  x_data , y:  y_data , mode: 'markers', name: 'data ponts'};\n  var data = [trace1];\n  var layout = { title:\"\", xaxis: { title: 'x', }, yaxis: { title:\"y\", } };\n  const div = DOM.element('div');\n  Plotly.newPlot(div, data, layout);\n  return div;\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis noisy data is inspired by the quadratic function but comes with a sprinkle of randomness.\nPlot Quadratics with Sliders: Interactive Fun\nEver played with sliders to adjust stuff? Here’s your chance to do the same with quadratics. You can tweak the coefficients a, b, and c to fit the noisy data manually.\n\nviewof a = Inputs.range([-1, 4], {label: \"a\", step: 0.1})\nviewof b = Inputs.range([-1, 4], {label: \"b\", step: 0.1})\nviewof c = Inputs.range([-1, 4], {label: \"c\", step: 0.1})\n{\n  var trace1 = { x: x, y: y, mode: 'lines', name: 'quadratic'};\n  var trace2 = { x:  x_data , y:  y_data , mode: 'markers', name: 'data ponts'};\n  var data = [trace1, trace2];\n  var layout = { title: `Interactive Quadratics`, xaxis: { title: 'x', zeroline: false, }, yaxis: { title: `${a}x² + ${b}x + ${c}`, } };\n  const div = DOM.element('div');\n  Plotly.newPlot(div, data, layout);\n  return div;\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBut who wants to be a human slider forever? We need a more scientific approach to measure how well our function fits the data. Enter loss functions - the unsung heroes of machine learning.\n\n\nMeet the Mean Squared Error (MSE)\nMSE stands for Mean Squared Error. It’s a way to measure how far off our predictions are from the actual values. Here’s how you define it:\n\ndef mse(preds, acts): return ((preds - acts)**2).mean()\n\nNow, let’s use MSE to evaluate our quadratics. This function will calculate the loss (how bad our predictions are) and give us a number we can use to improve our model.\n\nfunction mse(preds, acts) {\n  const squared_error = [];\n  for (let i=0; i &lt; preds.length; i++) {\n    const error = preds[i] - acts[i];\n    squared_error.push(error**2);\n  }\n  const mse = squared_error.reduce((acc, curr) =&gt; acc+curr, 0) / preds.length;\n  return mse;\n}\n_y = x.map(element =&gt; f(element, _a, _b, _c))\nviewof _a = Inputs.range([-1, 4], {label: \"a\", step: 0.1})\nviewof _b = Inputs.range([-1, 4], {label: \"b\", step: 0.1})\nviewof _c = Inputs.range([-1, 4], {label: \"c\", step: 0.1})\n{\n  const loss = mse(_y, y_data)\n  var trace1 = { x: x, y: _y, mode: 'lines', name: 'quadratic'};\n  var trace2 = { x:  x_data , y:  y_data , mode: 'markers', name: 'data ponts'};\n  var data = [trace1, trace2];\n  var layout = { title: `Loss: ${loss.toFixed(4)}`, xaxis: { title: 'x', zeroline: false, }, yaxis: { title: `${_a}x² + ${_b}x + ${_c}`, } };\n  const div = DOM.element('div');\n  Plotly.newPlot(div, data, layout);\n  return div;\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWith Mean Squared Error (MSE), you can objectively assess whether a model’s fit is improving without relying solely on visual inspection. Instead of manually adjusting parameters—which can be tedious and inefficient—we can automate the process using calculus.\n\n\nThe Power of Derivatives\nOne straightforward approach might be to manually tweak each parameter and observe how the loss, which quantifies the model’s prediction error, changes. However, there’s a far more efficient method: by computing the derivative of the loss function with respect to the parameters. These derivatives, also known as gradients, indicate the direction and rate at which the loss changes. This information is crucial for guiding the optimization process.\n\n\nLeveraging PyTorch\nFortunately, PyTorch automates the calculation of these derivatives, greatly simplifying the optimization process. For example, consider a function called quad_mse, which computes the Mean Squared Error between our observed noisy data and a quadratic model defined by parameters [a, b, c]. This function serves as a foundation for adjusting the model parameters in an informed and efficient way.\n\ndef quad_mse(params):\n    f = mkquad(*params)\n    return mse(f(x), y)\n\nThis function takes the coefficients (a, b, c), creates a quadratic function, and then returns the MSE of the predicted values against the actual noisy data.\n\nquad_mse([1.5, 1.5, 1.5])\n\ntensor(6.7798, dtype=torch.float64)\n\n\nWe get a MSE of 6.78, and yes, it’s a tenser (just a fancy array with some extra Pytorch powers). Let’s make it easier to hand:\n\nabc = torch.tensor([1.5, 1.5, 1.5])\nabc.requires_grad_()\n\ntensor([1.5000, 1.5000, 1.5000], requires_grad=True)\n\n\nNow, our tensor is ready to calculate gradients for these coefficients whenever used in computations. Pass this to quad_mse to verify:\n\nloss = quad_mse(abc)\nprint(loss)\n\ntensor(6.7798, dtype=torch.float64, grad_fn=&lt;MeanBackward0&gt;)\n\n\nAs expected, we get that magical tensor value 6.78. Nothing fancy yet? Hold on. We now tell Pytorch to store the gradients:\n\nloss.backward()\n\nNo fireworks, but something profound just happened. Run this:\n\nprint(abc.grad)\n\ntensor([-7.6934, -0.4701, -2.8031])\n\n\nVoila! You’ve got the gradients or slopes. They tell us how much the loss changes if you tweak each parameter-perfect for finding the optimal values.\n\n\nUpdating Parameters Using Gradients\nTo bring our loss down, we adjust the parameters in the direction that reduces the loss. Essentially, we descend down the gradient:\n\nwith torch.no_grad():\n    abc -= abc.grad * 0.01\n    loss = quad_mse(abc)\nprint(loss)\n\ntensor(6.1349, dtype=torch.float64)\n\n\nThis operation subtracts a small proportion of the gradient from each parameter, resulting in an updated set of parameters and a reduction of the loss from 6.78 to 6.13.\nNote that using the context manager with torch.no_grad() disables gradient computation for the weight and bias update step, as this update does not require gradient tracking.\n\n\nAutomating Gradient Descent\nInstead of performing updates manually, you can automate the process using a loop to handle multiple iterations of gradient descent.\n\nfor i in range(5):\n    loss = quad_mse(abc)\n    loss.backward()\n    with torch.no_grad():\n        abc -= abc.grad * 0.01\n        print(f\"Step {i}; loss: {loss:.2f} \")\n        abc.grad.zero_()  # Clear the gradient after each step\n\nStep 0; loss: 6.13 \nStep 1; loss: 5.05 \nStep 2; loss: 4.68 \nStep 3; loss: 4.37 \nStep 4; loss: 4.10 \n\n\n\nabc\n\ntensor([1.9329, 1.5305, 1.6502], requires_grad=True)\n\n\nAfter about five gradient descent iterations, the parameters have adjusted incrementally toward their optimal values. These parameters continuously update to minimize the loss and capture the underlying patterns in your data.\n\n\nWelcome to Optimization: The Role of Gradient Descent\nThe process of fine-tuning parameters to reduce prediction error is known as optimization, with gradient descent being one of the most widely used methods. Nearly all machine learning models—including complex neural networks—rely on some variant of this technique.\n\n\nThe Importance of ReLUs\nSimple quadratic functions are often insufficient for modeling real-world data, which tends to exhibit far greater complexity. When distinguishing subtle visual features in images, for example, a more sophisticated approach is required.\nThis is where the Rectified Linear Unit (ReLU) comes in. As an activation function, ReLU serves as a fundamental building block for constructing highly flexible models capable of capturing intricate patterns.\n\ndef rectified_linear(m, b, x):\n    y = m * x + b\n    return torch.clip(y, min=0.0)\n\nThis function is a simple line y = mx + b. The torch.clip() function takes anything blow zero and flatlines it at zero. Essentially, this turns any negative output into zero, while keeping positive values unchanged.\nHere’s what the ReLU looks like:\n\nplot_function(partial(rectified_linear, 1, 1))\n\n\nfunction rectified_linear(m, b, x) {\n  const y = m*x + b;\n  return Math.max(y, 0);\n}\nviewof m = Inputs.range([-1, 4], {label: \"m\", step: 0.1})\nviewof b_ = Inputs.range([-1, 4], {label: \"b\", step: 0.1})\n{\n  const _y = x.map(element =&gt; rectified_linear(m, b_, element));\n  var trace1 = {\n    x: x,\n    y: _y,\n    mode: 'lines',\n    name: 'ReLU',\n  };\n  var data = [trace1];\n  var layout = {\n    title: \"Rectified Linear Unit\",\n    xaxis: { title: \"x\", },\n    yaxis: { title: \"y\", range: [-1, 4] }\n  };\n  const div = DOM.element('div');\n  Plotly.newPlot(div, data, layout);\n  return div;\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImagine a line rising up at a 45-degree angle until it hits zero-at which point it surrenders to the great oblivion blow it. Now, you can adjust the coefficients m (slope) and b (intercept) and watch the magic happen.\n\n\nThe Power of Double ReLU: Fun With Functions\nWhy stop at one ReLU when you can have double the fun with two?\n\ndef double_relu(m1, b1, m2, b2, x):\n    return rectified_linear(m1, b1, x) + rectified_linear(m2, b2, x)\n\nThis function combines two ReLUs. Let’s plot this end see what unfolds:\n\nplot_function(partial(double_relu, 1, 1, -1, 1))\n\n\nfunction double_relu(m1,b1,m2,b2,x) {\n  return rectified_linear(m1,b1,x) + rectified_linear(m2,b2,x)\n}\nviewof m1 = Inputs.range([-2, 4], {label: \"m1\", step: 0.1})\nviewof b1 = Inputs.range([-2, 4], {label: \"b1\", step: 0.1})\nviewof m2 = Inputs.range([-2, 4], {label: \"m2\", step: 0.1})\nviewof b2 = Inputs.range([-2, 4], {label: \"b2\", step: 0.1})\n{\n  const _y = x.map(element =&gt; double_relu(m1,b1,m2,b2, element));\n  var trace1 = {\n    x: x,\n    y: _y,\n    mode: 'lines',\n    name: 'Double ReLU',\n  };\n  var data = [trace1];\n  var layout = {\n    title: \"Double Rectified Linear Unit\",\n    xaxis: { title: \"x\", },\n    yaxis: { title: \"y\", range: [-1, 4] }\n  };\n  const div = DOM.element('div');\n  Plotly.newPlot(div, data, layout);\n  return div;\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou’ll notice a downward slope that hooks upward into another slope. Tweak the coefficients m1, b1, m2, and b2, and watch the slopes and hooks dance around!\n\n\nInfinity Flexible ReLUs\nThink this is fun? Imagine adding a million ReLUs together. In face, you can add as many as you want to create function as wiggly and complex as you desire.\nBehold the power of ReLUs! With enough ReLUs, you can match any data pattern with incredible precision. you want a function that isn’t just 2D but spreads across multiply dimensions? You got it! ReLUs can do 3D, 4D, 5D…, nD.\n\n\nNeed Parameters? We’ve got Gradient Descent\nBut we need parameters to make magic happen, right? Here’s where gradient descent swoops in to save the day. By continuously tweaking these coefficients based on our loss calculations, we gradually descend towards the perfect parameter set.\n\n\nThe Big Picture: Adding ReLus and Gradient Descent === Deep Learning\nBelieve it or not, this is the essence of deep learning. Everything else-every other tweak is just about making this process faster and more efficient, sparking those “a-ha!” moments.\nQuoting Jeremy Howard:\n\n\n“Now I remember a few years ago when I said something like this in a class, somebody on the forum was like, this reminds me of that thing about how to draw an owl. Okay, step one: draw two circles; step two: daw the rest of the owl”\n\nThis explanation highlights the fundamental components of deep learning. At its core, deep learning involves leveraging activation functions like ReLUs, optimizing parameters through gradient descent, and processing data samples to generate predictions. Essentially, when you stack layers of ReLUs and systematically adjust parameters using gradient descent, the network learns to map inputs to outputs—much like an image of an owl being recognized by the computer.\nWhenever the concepts feel overwhelming, remember that the process boils down to these basics: using gradient descent to fine-tune parameters and combining numerous activation functions to capture complex patterns in your data.\nAnd that’s the essence of deep learning—simple building blocks working together to create sophisticated models. Stay curious and explore how each component contributes to the overall process."
  },
  {
    "objectID": "posts/nlp-beginner/index.html",
    "href": "posts/nlp-beginner/index.html",
    "title": "Fundamentals of Text Classification and Correlation in Natural Language Processing",
    "section": "",
    "text": "Hello and welcome, aspiring data scientist and NLP enthusiasts!\nToday, we’re going to explore the fascinating of Natural Language Processing (NLP) and its application in text classification.\nIn this blog post, we’ll walk through the process of tackling a text similarity problem using modern NLP techniques. We’ll cover everything from data preparation to model training and evaluation. Along a way, we’ll also delve into correlation analysis using a classic dataset, helping us understand the importance of metrics in machine learning.\nWe’ll be using popular libraries like Transformers, pandas, matplotlib to bring our data to life and gain meaningful insights.\nAlright enough chit chat, let’s roll up our sleeves and dive into the world of NLP."
  },
  {
    "objectID": "posts/nlp-beginner/index.html#data-preparation",
    "href": "posts/nlp-beginner/index.html#data-preparation",
    "title": "Fundamentals of Text Classification and Correlation in Natural Language Processing",
    "section": "Data Preparation",
    "text": "Data Preparation\nAlright, let’s drive into the data preparation for the U.S. Patent Phrase to Phrase Matching challenge. To start, you need to download the dataset using the Kaggle APi. Follow the installation instruction in their Github page, and then you can grab the dataset with the following command:\nkaggle competitions download -c us-patent-phrase-to-phrase-matching\nWith the data in hand, let’s firer up python environment and take a closer look:\n\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Define the path to the data\npath = Path(\"./data\")\ndf = pd.read_csv(path / \"train.csv\")\n\n# Display the dataframe\ndf.head()\n\n\n\n\n\n\n\n\nid\nanchor\ntarget\ncontext\nscore\n\n\n\n\n0\n37d61fd2272659b1\nabatement\nabatement of pollution\nA47\n0.50\n\n\n1\n7b9652b17b68b7a4\nabatement\nact of abating\nA47\n0.75\n\n\n2\n36d72442aefd8232\nabatement\nactive catalyst\nA47\n0.25\n\n\n3\n5296b0c19e1ce60e\nabatement\neliminating process\nA47\n0.50\n\n\n4\n54c1e3b9184cb5b6\nabatement\nforest region\nA47\n0.00\n\n\n\n\n\n\n\nExamining the data is always the first step. Let’s peek at some summery statics to get an initial sense of what we’re working with:\n\ndf.describe(include=\"object\")\n\n\n\n\n\n\n\n\nid\nanchor\ntarget\ncontext\n\n\n\n\ncount\n36473\n36473\n36473\n36473\n\n\nunique\n36473\n733\n29340\n106\n\n\ntop\n8d135da0b55b8c88\ncomponent composite coating\ncomposition\nH01\n\n\nfreq\n1\n152\n24\n2186\n\n\n\n\n\n\n\nNotice something interesting? The anchor field has only 733 unique values in a dataset of 36,000 entries. That’s a clear sign of repetition, suggesting some phrases appear frequently.\nNow, onto preparing data for our model. In this task, our goal is to determine if two phrases have similar meanings. To give our model more context. Let’s add structure labels like “TEXT1:” and “TEXt2:”. This helps in making data more informative, alright because we have to do the same thing to our test set so i will write a function for this:\n\ndef preprocess_function(examples):\n    examples[\"input\"] = 'TEXT1: ' + examples[\"context\"] + \"; TEXT2: \" + examples[\"target\"] + \"; ANC1: \" + examples[\"anchor\"]\n    return examples\n\nAdding context can be incredibly valuable providing the model with the background it need to understand nuanced of differences or similarities, especially in specialized field like patents.\nNext we convert our Pandas DataFrame into a Dataset Object and apply the process function:\n\nfrom datasets import Dataset, DatasetDict\n\n# Convert the DataFrame to a Dataset\nds = Dataset.from_pandas(df)\n\n# Apply the preprocessing function\nds = ds.map(preprocess_function)\n\n# Show the first 5 preprocessed inputs\nprint(ds[\"input\"][:5])\n\n\n\n\n['TEXT1: A47; TEXT2: abatement of pollution; ANC1: abatement', 'TEXT1: A47; TEXT2: act of abating; ANC1: abatement', 'TEXT1: A47; TEXT2: active catalyst; ANC1: abatement', 'TEXT1: A47; TEXT2: eliminating process; ANC1: abatement', 'TEXT1: A47; TEXT2: forest region; ANC1: abatement']\n\n\nBy structuring the data in this manner, we offer the model clear, consistent inputs, improving its ability to analyze and match the patent accurately"
  },
  {
    "objectID": "posts/nlp-beginner/index.html#tokenization",
    "href": "posts/nlp-beginner/index.html#tokenization",
    "title": "Fundamentals of Text Classification and Correlation in Natural Language Processing",
    "section": "Tokenization",
    "text": "Tokenization\nIn the previous section, we got the data ready, Now, let’s dive into tokenization. a crucial step before feeding the data into a neuron network.\nAs i already talk about it in the previous blog post, Neuron network work with numbers not text so how do we bridge this gab, there are to main steps: Tokenization and Numericalization.\n\nStep 1: Tokenization\nTokenizations involves splitting the text into smaller units called tokens. Tokens can be words, subwords or even character depending on the tokenization technique. For simplicity let’s think tokens are words\nOnce we split the text into tokens, we compile a list of all unique tokens, this list is known as the vocabulary. Every token in this vocabulary gets assigned a unique number. This mapping from tokens to numbers allows a neural network to process the text data.\nHowever, there’s a catch! The larger the vocabulary, the more memory and data you need for training. To keep thing manageable, modern tokenization techniques often break words into subwords. Which help to minimize the vocabulary size. This process of breaking text into smaller units is called tokenization, and the smaller units are referred as tokens.\n\n\nStep 2: Numericalization\nIn this step each token is converted into its unique ID based on its position in the vocabulary this process is called numericalization\n\n\nChoosing a Tokenizer\nChoosing a tokenizer involves several little decisions. The good news? you don’t have to these decisions yourself. Pre-trained models come with their own tokenization methods, and to use these model effectively, you need to use same tokenization approach they were trained with.\nEnter HuggingFace’s transformers library. It provides a convenient way to load pre-trained models and their corresponding tokenizers. One highly versatile model is deberta-v3\nHere’s how you can set it up:\n\nmodel_nm = \"microsoft/deberta-v3-base\"\n\nWe choose this model to ensure our tokenization matches the pre-trained model’s tokenization process. This consistency is crucial.\nTo load the tokenizer that correspond to our chosen model, we you AutoTokenizer from Hugging Face’s transformer library. This utility automatically fetches the correct tokenizer for our model:\n\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\n# Load the tokenizer for our model\ntokz = AutoTokenizer.from_pretrained(model_nm)\n\n\n\n\n\n\n\n\n\n\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n\n\nWith the tokenizer loaded, you can now tokenize your text data in the same way the model was trained to do.\nAlright, at this point we can use our tokenization to process strings. Let’s start with a simple example. If we pass this string “Hi folks, I’m Dai from my mother!” to the tokenizer, we can see how it breaks the text in to tokens\n\ntokz.tokenize(\"Hi folks, I'm Dai from my mother!\")\n\n['▁Hi', '▁folks', ',', '▁I', \"'\", 'm', '▁Dai', '▁from', '▁my', '▁mother', '!']\n\n\nYou’ll notice doesn’t just split the text into words. If you’ve wondered “I’m” is one word or two, you’ll find that it’s actually three tokens according to this tokenizer. These tokens include partial words and punctuations. The character looks like underscore here signify the start of the word\nHere’s a less common sentence: “A platypus is an ornithorhynchus anatinus”.\n\ntokz.tokenize(\"A platypus is an ornithorhynchus anatinus.\")\n\n['▁A',\n '▁platypus',\n '▁is',\n '▁an',\n '▁or',\n 'ni',\n 'tho',\n 'rhynch',\n 'us',\n '▁an',\n 'at',\n 'inus',\n '.']\n\n\nIn this vocabulary, “platypus” gets its own token, but “ornithorhynchus” is split into smaller parts. Each of these tokens corresponds to an entry in a predefined vocabulary list created during the model’s pre-training. Somewhere in that list, we’ll find “A”, and it will have a specific number. This process of converting tokens to numbers is called numericalization. To accomplish this for our dataset, we create a function that tokenizes the “input” field:\n\ndef tok_func(x): return tokz(x[\"input\"])\n\nSince tokenization can be time-consuming, especially for large dataset. Leveraging parallel processing can save time. we’ll utilize batching to process multiple entries simultaneously\n\ntok_ds = ds.map(tok_func, batched=True)\n\n\n\n\nEnsure that batched=True is passed to enable batch processing. This approach uses the tokenizer library optimized for performance wit Rust. Batch processing can significantly speed up the tokenization process\nNow let’s examine a row from our tokenized dataset:\n\ntok_ds[0][\"input\"], tok_ds[0][\"input_ids\"]\n\n('TEXT1: A47; TEXT2: abatement of pollution; ANC1: abatement',\n [1,\n  54453,\n  435,\n  294,\n  336,\n  5753,\n  346,\n  54453,\n  445,\n  294,\n  47284,\n  265,\n  6435,\n  346,\n  23702,\n  435,\n  294,\n  47284,\n  2])\n\n\nThe output show that the dataset retains the original string in the “input” field and includes a new “input_ids” field with the numerical representation of each token. The numbers here are the numerical positions of tokens in the vocabulary. This step efficiently convert our text into number, readying it for model input.\nAlright, we now need to prepare our labels. Transformer always assumes that your labels has column name labels, but it our data set there’s no labels but score, we need to rename it.\n\ntok_ds = tok_ds.rename_column(\"score\", \"labels\")\n\nNow let’s split our dataset into train and validation set\n\ndds = tok_ds.train_test_split(0.2, seed=42)\ndds\n\nDatasetDict({\n    train: Dataset({\n        features: ['id', 'anchor', 'target', 'context', 'labels', 'input', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 29178\n    })\n    test: Dataset({\n        features: ['id', 'anchor', 'target', 'context', 'labels', 'input', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 7295\n    })\n})"
  },
  {
    "objectID": "posts/nlp-beginner/index.html#metric-and-loss",
    "href": "posts/nlp-beginner/index.html#metric-and-loss",
    "title": "Fundamentals of Text Classification and Correlation in Natural Language Processing",
    "section": "Metric and Loss",
    "text": "Metric and Loss\nWhen working with a validation set, one of the key steps is to measure some metrics. A metric, such as “accuracy” gives us a quantifiable measure of how good our model is.\nFor example, if you look at the competition’s overview page, you’ll see that they use the Pearson correlation coefficient to evaluate submissions. Consequently, we will also use this metric to measure our model performance.\nA common question arises here: is the metric the same as the loss function? The answer is nuanced. The metric used to evaluate model performance isn’t necessarily the one to be used for optimization during the training. Taking the derivative of accuracy to find the gradient for parameter updates, for example, isn’t very effective.\nWhy? You ask! Because accuracy doesn’t change significantly unless predictions switch from incorrect to correct, resulting in gradients being nearly zero everywhere. Instead, we prefer smooth functions like MSE or MAE for the loss function. These provide better gradients for optimization, helping improve the model more effectively."
  },
  {
    "objectID": "posts/nlp-beginner/index.html#pearson-correlation-coefficient",
    "href": "posts/nlp-beginner/index.html#pearson-correlation-coefficient",
    "title": "Fundamentals of Text Classification and Correlation in Natural Language Processing",
    "section": "Pearson Correlation Coefficient",
    "text": "Pearson Correlation Coefficient\nLet’s focus on our metric of interest today, the Pearson Correlation Coefficient often abbreviated as r. This coefficient is a widely used measure that evaluates the similarity between two variable, if our predictions closely match the actual values, r will be high, ranging between -1 and 1. An r of -1 means predictions are exactly the wrong answers (which could still be useful, as reversing the answer would yield perfect results), while r of 1 signifies perfect predictions.\nYou can read this for more info about the formula and example but i think the best way to understand how data behaves is to look at real-life data. Scikit-learn offers numerous datasets, one of which is the “California housing” dataset. Here’s how you can explore this dataset:\n\nfrom sklearn.datasets import fetch_california_housing\nhousing = fetch_california_housing(as_frame=True)\nhousing = housing['data'].join(housing['target']).sample(1000, random_state=52)\nhousing.head()\n\n\n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\nMedHouseVal\n\n\n\n\n7506\n3.0550\n37.0\n5.152778\n1.048611\n729.0\n5.062500\n33.92\n-118.28\n1.054\n\n\n4720\n3.0862\n35.0\n4.697897\n1.055449\n1159.0\n2.216061\n34.05\n-118.37\n3.453\n\n\n12888\n2.5556\n24.0\n4.864905\n1.129222\n1631.0\n2.395007\n38.66\n-121.35\n1.057\n\n\n13344\n3.0057\n32.0\n4.212687\n0.936567\n1378.0\n5.141791\n34.05\n-117.64\n0.969\n\n\n7173\n1.9083\n42.0\n3.888554\n1.039157\n1535.0\n4.623494\n34.05\n-118.19\n1.192\n\n\n\n\n\n\n\nTo compute the Pearson correlation coefficient, Numpy provides the corrcoef() function, which return a correlation matrix:\n\nnp.set_printoptions(precision=2, suppress=True)\nnp.corrcoef(housing, rowvar=False)\n\narray([[ 1.  , -0.12,  0.43, -0.08,  0.01, -0.07, -0.12,  0.04,  0.68],\n       [-0.12,  1.  , -0.17, -0.06, -0.31,  0.  ,  0.03, -0.13,  0.12],\n       [ 0.43, -0.17,  1.  ,  0.76, -0.09, -0.07,  0.12, -0.03,  0.21],\n       [-0.08, -0.06,  0.76,  1.  , -0.08, -0.07,  0.09,  0.  , -0.04],\n       [ 0.01, -0.31, -0.09, -0.08,  1.  ,  0.16, -0.15,  0.13,  0.  ],\n       [-0.07,  0.  , -0.07, -0.07,  0.16,  1.  , -0.16,  0.17, -0.27],\n       [-0.12,  0.03,  0.12,  0.09, -0.15, -0.16,  1.  , -0.93, -0.16],\n       [ 0.04, -0.13, -0.03,  0.  ,  0.13,  0.17, -0.93,  1.  , -0.03],\n       [ 0.68,  0.12,  0.21, -0.04,  0.  , -0.27, -0.16, -0.03,  1.  ]])\n\n\nHowever, we need a single correlation number rather than a matrix. Fortunately, if we pass in a pair of variable, we could still get a matrix, from which we can extract the desired coefficient.\n\nnp.corrcoef(housing.MedInc, housing.MedHouseVal)\n\narray([[1.  , 0.68],\n       [0.68, 1.  ]])\n\n\nTo get a specific correlation coefficient, simply select the zeroth row and the first column:\n\ndef corr(x, y):  return np.corrcoef(x, y)[0][1]\n\ncorr(housing.MedInc, housing.MedHouseVal)\n\n0.6760250732906\n\n\nTo visualize the correlation, we can plot the data and display the correlation coefficient r:\n\nimport matplotlib.pyplot as plt\n\ndef show_corr(df, a, b):\n    x, y = df[a], df[b]\n    plt.scatter(x, y, alpha=0.5, s=4)\n    plt.title(f'{a} vs {b}; r: {corr(x, y):.2f}')\n\nFor instance, the plot of “median income” vs. “median house value” shows an r of 0.68.\n\nshow_corr(housing, 'MedInc', 'MedHouseVal')\n\n\n\n\n\n\n\n\nExploring other pairs, like “median income” vs. “number of rooms per house,” gives an r of 0.43.\n\nshow_corr(housing, 'MedInc', 'AveRooms')\n\n\n\n\n\n\n\n\nThis plot reveals interesting insights and anomalies. Certain houses have many rooms but lower incomes, potentially indicate shared accommodations. Pearson’s r can be sensitive to outliers, and removing them can significantly change the correlation.\n\nsubset = housing[housing.AveRooms &lt; 15]\nshow_corr(subset, 'MedInc', 'AveRooms')\n\n\n\n\n\n\n\n\nBy removing outliers, the correlation increase fromm 0.43 to 0.68, emphasizing the importance of carefully considering outliers\nHere’s another example correlating “median hours value” with “average number of room”:\n\nshow_corr(subset, 'MedHouseVal', 'AveRooms')\n\n\n\n\n\n\n\n\nAnd another one correlating “house age” with “average number of rooms”:\n\nshow_corr(subset, 'MedHouseVal', 'AveRooms')\n\n\n\n\n\n\n\n\nTo keep track of our training progress, it’s important to report the Pearson correlation coefficient after each epoch. Hugging Face’s Trainer API expects a dictionary as output, with keys labeling each metrics. Here’s how you can define a function to calculate and return the Pearson correlation coefficient:\n\ndef corr_d(eval_pred): return {'pearson': corr(*eval_pred)}"
  },
  {
    "objectID": "posts/nlp-beginner/index.html#training-with-hugging-face-transformers",
    "href": "posts/nlp-beginner/index.html#training-with-hugging-face-transformers",
    "title": "Fundamentals of Text Classification and Correlation in Natural Language Processing",
    "section": "Training with Hugging Face Transformers",
    "text": "Training with Hugging Face Transformers\nIn the world of Hugging Face, the concept of a “learner” in fast.ai is paralleled by the trainer. Let’s start by importing necessary components:\n\nfrom transformers import TrainingArguments, Trainer\n\nWhen training a model, we don’t process the entire dataset at once; instead we use a “batch” or “mini-batch” of data for each training step. This approach leverages the parallel processing capabilities of GPUs.\nIn our case, let’s set the batch size to 128. Large batch size speed up training by maximizing GPU utilization but beware of potential “out of memory” errors if set too high.\nNext, configure the training arguments:\n\nbs = 80\nepochs = 4\nlr = 8e-5\n\nargs = TrainingArguments(\n    'outputs',\n    learning_rate=lr,\n    warmup_ratio=0.1,\n    lr_scheduler_type='cosine',\n    fp16=True,  # Enable mixed precision training\n    eval_strategy=\"epoch\",\n    per_device_train_batch_size=bs,\n    per_device_eval_batch_size=bs*2,\n    num_train_epochs=epochs,\n    weight_decay=0.01,\n    report_to='none',\n)\n\nMost of these arguments can be left as defaults, but the key ones to focus on are the batch size (bs), number of epoches (epoches), and learning rate (lr).\nTo begin model training for sequence classification, we use AutoModelForSequenceClassification:\n\nmodel = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=1)\ntrainer = Trainer(\n    model, args, \n    train_dataset=dds['train'], eval_dataset=dds['test'],\n    tokenizer=tokz, compute_metrics=corr_d\n)\n\n\n\n\nSome weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n\n\nThis function sets up our model, instantiates the trainer with the relevant datasets and tokenizer, and specifies the metric function.\nFinally, we sart the trainign process:\n\ntrainer.train();\n\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n\n\n\n      \n      \n      [732/732 09:27, Epoch 4/4]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nPearson\n\n\n\n\n1\nNo log\n0.027016\n0.784697\n\n\n2\nNo log\n0.022490\n0.818065\n\n\n3\n0.027900\n0.021511\n0.834274\n\n\n4\n0.027900\n0.021852\n0.836955\n\n\n\n\n\n\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n\n\nUpon starting the training, we achieve an inspiring correlation of 0.836. This impressive result was obtain in about ten minutes, demonstrating the power of leveraging pre-trained models.\nAchieving the high correlation score underscore the value of using a pre-trained model. The deberta-v3 model we used already possesses an immense amount of linguistic knowledge, allowing it to assess the similarity between phrases effectively right out of the box. By fine-tuning it on our specific task, we capitalized on this foundation, leading to rapid and accurate results\nAs we continue training, monitoring the Pearson correlations coefficient helps ensure our model is moving in the right direction, improving its ability to match phrases accurately."
  },
  {
    "objectID": "posts/nlp-beginner/index.html#evaluating-model-predictions",
    "href": "posts/nlp-beginner/index.html#evaluating-model-predictions",
    "title": "Fundamentals of Text Classification and Correlation in Natural Language Processing",
    "section": "Evaluating Model Predictions",
    "text": "Evaluating Model Predictions\nWith our model trained and achieving a promising Pearson correlation coefficient, it’s time to evaluate its performance on the test set.\nJust as we did with the training data we need to load and preprocess our test dataset.\n\neval_df = pd.read_csv(path/\"test.csv\")\neval_ds = Dataset.from_pandas(eval_df)\neval_ds = eval_ds.map(preprocess_function).map(tok_func, batched=True)\n\n\n\n\n\n\n\nNext we use trainer to make predictions on the test dataset, and at this point it crucial to inspect the predictions.\n\npreds = trainer.predict(eval_ds).predictions.astype(float)\npreds\n\n\n\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n\n\n\n\n\narray([[ 0.7 ],\n       [ 0.73],\n       [ 0.58],\n       [ 0.37],\n       [-0.02],\n       [ 0.53],\n       [ 0.48],\n       [-0.01],\n       [ 0.25],\n       [ 1.06],\n       [ 0.26],\n       [ 0.24],\n       [ 0.8 ],\n       [ 0.8 ],\n       [ 0.81],\n       [ 0.41],\n       [ 0.27],\n       [-0.  ],\n       [ 0.66],\n       [ 0.41],\n       [ 0.55],\n       [ 0.23],\n       [ 0.09],\n       [ 0.23],\n       [ 0.53],\n       [ 0.  ],\n       [ 0.01],\n       [-0.01],\n       [ 0.  ],\n       [ 0.69],\n       [ 0.34],\n       [-0.  ],\n       [ 0.7 ],\n       [ 0.58],\n       [ 0.38],\n       [ 0.21]])\n\n\nObserving the prediction might reveal that some value fall outside the [0, 1] range. This highlights the important of examining your data at every stage. To fix these out-of-bound predictions, we employ clamping:\n\npreds = np.clip(preds, 0, 1)\npreds\n\n\n\narray([[0.7 ],\n       [0.73],\n       [0.58],\n       [0.37],\n       [0.  ],\n       [0.53],\n       [0.48],\n       [0.  ],\n       [0.25],\n       [1.  ],\n       [0.26],\n       [0.24],\n       [0.8 ],\n       [0.8 ],\n       [0.81],\n       [0.41],\n       [0.27],\n       [0.  ],\n       [0.66],\n       [0.41],\n       [0.55],\n       [0.23],\n       [0.09],\n       [0.23],\n       [0.53],\n       [0.  ],\n       [0.01],\n       [0.  ],\n       [0.  ],\n       [0.69],\n       [0.34],\n       [0.  ],\n       [0.7 ],\n       [0.58],\n       [0.38],\n       [0.21]])\n\n\nSo, that’s a wrap! We’ve navigated through the essentials of training and evaluating a machine learning model using Hugging Face Transformer. From understanding metrics and loss functions to fine-tune a pre-trained model and keeping a close eye on our Pearson Correlation Coefficient we’ve covered a lot of ground.\nThe key take a way? Always keep an eye on your data and metrics. Those little tweaks can make a big difference. Leveraging pre-trained models can save you tons of time and give you a major head start.\nKeep experimenting and stay curious, and don’t forget to have fun with your models. Machine learning is all about trying a new things and learn from each step.\nAnd hey, thanks nerds!"
  },
  {
    "objectID": "posts/2024-12-30-deep-learning-concepts/index.html",
    "href": "posts/2024-12-30-deep-learning-concepts/index.html",
    "title": "Some Deep Learning Concepts You Need To Understand",
    "section": "",
    "text": "Alright, we’ve been diving deep into tabular data lately, haven’t we? We played with it, had some fun, and now i think it’s time to go deeper to image classification problem. Yeah, we’ve touched on this before with the world simplest model “Is it a Bird?”, or better our simple model for recognizing three types of bears but that was mostly about deployment. This time we’re going all in.\nFrom now on, we’ll be getting into the mechanics of deep learning, and exploring what a solid computer vision model architecture looks like."
  },
  {
    "objectID": "posts/2024-12-30-deep-learning-concepts/index.html#paddy-doctor",
    "href": "posts/2024-12-30-deep-learning-concepts/index.html#paddy-doctor",
    "title": "Some Deep Learning Concepts You Need To Understand",
    "section": "Paddy Doctor",
    "text": "Paddy Doctor\nToday we’re dealing with the Paddy Doctor: Paddy Disease Classification competition on Kaggle. The goal? Predict paddy diseases based on images. Through this competitions we will go though, the general architecture, the presizing process, the loss, and improve our model further, alright let’s get right into it shall we?\n\nif iskaggle:\n  path = setup_comp('paddy-disease-classification', install=\"timm&gt;=0.6.2.dev0\")\nelse:\n  path = Path(\"./data\")\n\nFirst we need to understand how our data laid out\n\npath.ls()\n\n(#4) [Path('data/test_images'),Path('data/train.csv'),Path('data/train_images'),Path('data/sample_submission.csv')]\n\n\nData is usually provided in one of these two ways:\n\nIndividual files representing items of data, like images, text, can be organized in folders or with file name representing information about the images\nA table of data as we dealt with them before, where each row is an item which may include filenames providing a connection between the table and data in other format, such as text documents and images.\n\n\n(path/\"train_images/\").ls()\n\n(#10) [Path('data/train_images/bacterial_leaf_blight'),Path('data/train_images/bacterial_leaf_streak'),Path('data/train_images/bacterial_panicle_blight'),Path('data/train_images/blast'),Path('data/train_images/brown_spot'),Path('data/train_images/dead_heart'),Path('data/train_images/downy_mildew'),Path('data/train_images/hispa'),Path('data/train_images/normal'),Path('data/train_images/tungro')]\n\n\nAs you can see we have 10 folders each represent paddy diseases that we need to predict, in this case each folders will contain images of paddy disease correspond to the parent folder name\n\ntrain_path = path/'train_images'\nfiles = get_image_files(train_path)\nfiles\n\n(#10407) [Path('data/train_images/bacterial_leaf_blight/100023.jpg'),Path('data/train_images/bacterial_leaf_blight/100049.jpg'),Path('data/train_images/bacterial_leaf_blight/100126.jpg'),Path('data/train_images/bacterial_leaf_blight/100133.jpg'),Path('data/train_images/bacterial_leaf_blight/100148.jpg'),Path('data/train_images/bacterial_leaf_blight/100162.jpg'),Path('data/train_images/bacterial_leaf_blight/100169.jpg'),Path('data/train_images/bacterial_leaf_blight/100234.jpg'),Path('data/train_images/bacterial_leaf_blight/100248.jpg'),Path('data/train_images/bacterial_leaf_blight/100268.jpg'),Path('data/train_images/bacterial_leaf_blight/100289.jpg'),Path('data/train_images/bacterial_leaf_blight/100330.jpg'),Path('data/train_images/bacterial_leaf_blight/100365.jpg'),Path('data/train_images/bacterial_leaf_blight/100382.jpg'),Path('data/train_images/bacterial_leaf_blight/100445.jpg'),Path('data/train_images/bacterial_leaf_blight/100447.jpg'),Path('data/train_images/bacterial_leaf_blight/100513.jpg'),Path('data/train_images/bacterial_leaf_blight/100516.jpg'),Path('data/train_images/bacterial_leaf_blight/100523.jpg'),Path('data/train_images/bacterial_leaf_blight/100541.jpg')...]\n\n\nLet’s take a look a one\n\nimg = PILImage.create(files[0])\nprint(img.size)\nimg.to_thumb(128)\n\n(480, 640)\n\n\n\n\n\n\n\n\n\nThis image has the size of 480x640, let’s check all their sizes. Looping though over 10.000 images is a pain right, so we will do it in parallel\n\ndef f(o): return PILImage.create(o).size\nsizes = parallel(f, files, n_workers=8)\npd.Series(sizes).value_counts()\n\n(480, 640)    10403\n(640, 480)        4\nName: count, dtype: int64\n\n\nWell, there’s almost have the same size except 4 of them with 640x480, we’ll need to resize all of them to the same size, we will talk about it later, but now let’s create a dataloader\n\ndls = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                get_items=get_image_files,\n                splitter=RandomSplitter(valid_pct=0.2, seed=42),\n                get_y=parent_label,\n                item_tfms=Resize(480, method='squish'),\n                batch_tfms=aug_transforms(size=128, min_scale=0.75)).dataloaders(train_path)\n\nThis is the principle of our computer vision model mostly, but notice here, we need to focus on these two lines:\nitem_tfms=Resize(480, method='squish'),\nbatch_tfms=aug_transforms(size=128, min_scale=0.75)).dataloaders(train_path)\nThese lines implement a fastai data augmentation strategy which they often call presizing."
  },
  {
    "objectID": "posts/2024-12-30-deep-learning-concepts/index.html#presizing",
    "href": "posts/2024-12-30-deep-learning-concepts/index.html#presizing",
    "title": "Some Deep Learning Concepts You Need To Understand",
    "section": "Presizing",
    "text": "Presizing\nWe need our images to have the same dimensions, so that they can collate into tensors to be passed into GPU. We also want to minimize the number of distinct augmentation computations we perform. If possible we should compose our augmentation transforms into fewer transform(to reduce number of computations and lossy operations) and transform images into uniform sizes(for more efficient processing in GPU)\nHowever if we resize images to their final dimensions(the augmented size) and then apply various augmentation transforms it can lead to issues like creating empty zones (e.g., when rotating an image by 45 degrees) which will not teach the computer anything at all. Many rotation and zooming operations will require interpolating1 to create pixel\nTo walk around these challenges, presizing adopts a two-step strategy.\n\nImages are resized to dimensions significantly larger than the target training size as this will create a “buffer zone” around the image allowing for more flexibility in subsequent augmentation.\nAll common augmentation operations, including the final resize to target dimensions, are combined into a single step performed on GPU at the end of the processing, rather than performing the operations individually and interpolating multiple times.\n\n\n\n\nPresizing explained: Showing the two step resize to a large size then apply random crop and augment at the same time\n\n\nAs you can see in the picture it demonstrate what i described earlier\n\nFirst it crop full width or height this time it still do it sequentially before copied to GPU, it make sure that all our images are the same size. On the training set the crop area is chosen randomly2 and on validation set the it always choose the center square of the image. This is in item_tfms\n\n\nThen it uses RandomResizedCrop as a batch transform. It’s applied to a batch all at once on the GPU, making it fast. For the training set, this includes random cropping and other augmentations, and for validation set only resizing to the final size needed for the model is done. This is in batch_tfms\n\n\nResize\nUse Resize as an item transform with a large size you can use pad3 or squish4 instead of crop5(the default) for the initial Resize but what the diff between them? In fact let’s see the different in action shall we? Here’s the original image:\n\ntst_img = PILImage.create('./test_image.jpg').resize((600,400))\ntst_img.to_thumb(224)\n\n\n\n\n\n\n\n\n\n_, axs = plt.subplots(1,3,figsize=(12,4))\nfor ax,method in zip(axs.flatten(), ['squish', 'pad', 'crop']):\n  rsz = Resize(256, method=method)\n  show_image(rsz(tst_img, split_idx=0), ctx=ax, title=method)\n\n\n\n\n\n\n\n\nOn the validation set, the crop is always a center crop (on the dimension that’s cropped).\n\n_, axs = plt.subplots(1,3,figsize=(12,4))\nfor ax,method in zip(axs.flatten(), ['squish', 'pad', 'crop']):\n  rsz = Resize(256, method=method)\n  show_image(rsz(tst_img, split_idx=1), ctx=ax, title=method)\n\n\n\n\n\n\n\n\nMy recommendation:\n\nStart with padding if you’re unsure. It preserve all information and aspect ratios.\nIf padding introduces too much background, try cropping.\nUse squish only if you’re sure it won’t distort important features.\nAlways validate your choice by inspecting resized image and checking model performance\n\nThe best method can vary depending on the dataset the original aspect ratio. Now let’s see what the aug_transforms does under the hood\n\n\nAugmentation\n\n\ntorch.permute??\n\n\nDocstring:\n\npermute(input, dims) -&gt; Tensor\n\n\n\nReturns a view of the original tensor :attr:`input` with its dimensions permuted.\n\n\n\nArgs:\n\n    input (Tensor): the input tensor.\n\n    dims (tuple of int): The desired ordering of dimensions\n\n\n\nExample:\n\n    &gt;&gt;&gt; x = torch.randn(2, 3, 5)\n\n    &gt;&gt;&gt; x.size()\n\n    torch.Size([2, 3, 5])\n\n    &gt;&gt;&gt; torch.permute(x, (2, 0, 1)).size()\n\n    torch.Size([5, 2, 3])\n\nType:      builtin_function_or_method\n\n\n\nMost image processing libraries and formats (like PIL, OpenCV, matplotlib) use the format (Height, Width, Channels) or (H, W, C).\nHowever, PyTorch expects images in the format (Channels, Height, Width) or (C, H, W).\n\n\ntimg = TensorImage(array(tst_img)).permute(2,0,1).float()/255.\ndef _batch_ex(bs): return TensorImage(timg[None].expand(bs, *timg.shape).clone())\ntfms = aug_transforms(size=128, min_scale=0.75)\ny = _batch_ex(9)\nfor t in tfms: y = t(y, split_idx=0)\n_, axs = plt.subplots(1, 3, figsize=(12, 3))\nfor i,ax in enumerate(axs.flatten()): show_image(y[i], ctx=ax)\n\n\n\n\n\n\n\n\nChoosing the correct size for augment transforms is also crucial too as the size parameter in aug_transforms determines the final size of the images that will be fed into the model. Picking the right one depends on the model architecture requirements, each pretrained models requires different input size (e.g., ResNet typically use 224x224), but in fact you can do some experiments here. But aware of this, larger sizes help computer learn more details, but of course require more resources, in other hand smaller sizes are faster to process but may lose some details, it’s a tradeoff\n\n\n\n\n\n\nNote\n\n\n\n\nStart with the standard size for your chosen model architecture(e.g., 224 for many standard models)\nIf computational resources are not a big deal with you and the images have fine details, try increasing the size (e.g., 299, 384, 521)\nIf using transfer learning, stick to the pretrained model’s original input size is the best option i’d say\nFor custom architectures, you got no choice but experiment with different sizes and choose based on performance and resource constraints.\n\n\n\n\n\nChecking DataBlock\nWriting DataBlock is just like writing a blueprint, we will get an error if we have a syntax error some where in the code. So it’s a good practice to always check your data before doing anything further\n\ndls.show_batch(max_n=6)\n\n\n\n\n\n\n\n\nTake a look at the images and and check that each one seems to have correct label or not. In fact, we often have to deal with data with which is not as familiar as domain experts may be. Indeed, if you’re not a paddy doctor it will be hard to look at a random image and speak out the disease right? Since I’m not an expert on paddy diseases, I would use google to search to make sure the images look similar to what i see in the output. Also you can debug the DataBlock by using DataBlock.summary6\nOnce you think your data looks right, it’s a good practice to train a simple model, think about it, when you start trying to improve your model, how can you rate it? Compare it to the previous try you say, I mean what if it’s already worse, so that why we need to know what our baseline result looks like. Maybe you don’t need anything fancy - a basic model might do the job just fine. Or perhaps the data doesn’t seems to train the model at all. These are things that you want to know as soon as possible\n\nlearn = vision_learner(dls, 'resnet26d', metrics=error_rate, path='.')\n\n\nlearn.fine_tune(3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.966901\n1.133069\n0.356079\n01:03\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.311612\n0.885489\n0.277751\n01:03\n\n\n1\n1.032764\n0.691636\n0.218645\n01:04\n\n\n2\n0.871234\n0.645610\n0.203748\n01:03\n\n\n\n\n\nRemember that we’re in a competition right, so it’s nothing better than submit it and see how it will go and again this time we will use an pretty cool tool call fastkaggle in fact we used it earlier. Alright let’s see the submission layout\n\nss = pd.read_csv(path/'sample_submission.csv')\nss\n\n\n\n\n\n\n\n\nimage_id\nlabel\n\n\n\n\n0\n200001.jpg\nNaN\n\n\n1\n200002.jpg\nNaN\n\n\n2\n200003.jpg\nNaN\n\n\n3\n200004.jpg\nNaN\n\n\n4\n200005.jpg\nNaN\n\n\n...\n...\n...\n\n\n3464\n203465.jpg\nNaN\n\n\n3465\n203466.jpg\nNaN\n\n\n3466\n203467.jpg\nNaN\n\n\n3467\n203468.jpg\nNaN\n\n\n3468\n203469.jpg\nNaN\n\n\n\n\n3469 rows × 2 columns\n\n\n\nalright seems like we need to sort the images in order before submitting it\n\ntst_files = get_image_files(path/'test_images').sorted()\ntst_dl = dls.test_dl(tst_files)\n\nLet’s make the prediction on the test set\n\nprobs,_,idxs = learn.get_preds(dl=tst_dl, with_decoded=True)\nidxs\n\n\n\n\n\n\n\n\ntensor([7, 8, 7,  ..., 8, 1, 5])\n\n\nAlright we got the indices of the diseases, we need to map the name to each diseases we can get the label by checking the vocab\n\ndls.vocab\n\n['bacterial_leaf_blight', 'bacterial_leaf_streak', 'bacterial_panicle_blight', 'blast', 'brown_spot', 'dead_heart', 'downy_mildew', 'hispa', 'normal', 'tungro']\n\n\n\nmapping = dict(enumerate(dls.vocab))\nresults = pd.Series(idxs.numpy(), name=\"idxs\").map(mapping)\nresults\n\n0                       hispa\n1                      normal\n2                       hispa\n3                       blast\n4                       blast\n                ...          \n3464               dead_heart\n3465                    hispa\n3466                   normal\n3467    bacterial_leaf_streak\n3468               dead_heart\nName: idxs, Length: 3469, dtype: object\n\n\nBefore submit let’s see if our file looks right\n\nss['label'] = results\nss.to_csv('subm.csv', index=False)\n!head subm.csv\n\nimage_id,label\n200001.jpg,hispa\n200002.jpg,normal\n200003.jpg,hispa\n200004.jpg,blast\n200005.jpg,blast\n200006.jpg,brown_spot\n200007.jpg,dead_heart\n200008.jpg,brown_spot\n200009.jpg,hispa\n\n\n\nif not iskaggle:\n  from kaggle import api\n  api.competition_submit_cli('subm.csv', 'initial rn26d 128px', 'paddy-disease-classification')\n\n100%|██████████| 70.1k/70.1k [00:01&lt;00:00, 48.7kB/s]\n\n\nAlright i got 0.8917 score on the competitions, it’s not that good but it let’s us know what the base line is, then we can improve it later on.\n\n\n\n\n\n\nNote\n\n\n\nRemember that loss is whatever function we’re decided to use to optimize the parameters of our models, here we’re actually not specific what loss to use, fastai will try to find the best loss to use here for us. In this case we’re using cross-entropy loss."
  },
  {
    "objectID": "posts/2024-12-30-deep-learning-concepts/index.html#cross-entropy",
    "href": "posts/2024-12-30-deep-learning-concepts/index.html#cross-entropy",
    "title": "Some Deep Learning Concepts You Need To Understand",
    "section": "Cross-Entropy",
    "text": "Cross-Entropy\nCross-Entropy loss is of course a loss function - a function that used to optimized the parameter of our model. It work even if our dependent variable has more than two categories, and results in faster and reliable training.\nLet’s look at the activation of our model, in fact let’s just look at one batch of our data\n\nx, y = dls.one_batch()\n\nIt returns the dependent and independent variables as mini-batch\n\ny\n\nTensorCategory([8, 9, 5, 8, 2, 1, 8, 7, 5, 8, 3, 6, 8, 3, 7, 4, 8, 3, 8, 6, 5,\n                6, 0, 5, 8, 8, 7, 5, 8, 9, 8, 8, 7, 7, 9, 4, 7, 3, 9, 7, 7, 5,\n                7, 9, 1, 7, 3, 4, 9, 6, 8, 7, 9, 5, 9, 7, 9, 5, 5, 9, 3, 3, 5,\n                8], device='cuda:0')\n\n\nWe got 64 rows as our batch size is 64, and we get the values ranging from 0 to 9, representing our 10 possible diseases, alright we can even view the predictions in fact it is the activations of the final layer of our neural network by using Learner.get_preds\n\npreds,_ = learn.get_preds(dl=[(x,y)])\npreds[0]\n\n\n\n\n\n\n\n\ntensor([0.2162, 0.0061, 0.0008, 0.0299, 0.0109, 0.0082, 0.0068, 0.0107, 0.5600,\n        0.1503])\n\n\nThe actual prediction are 10 probabilities between 0 and 1, which add up to 1.\n\nlen(preds[0]), preds[0].sum()\n\n(10, tensor(1.))\n\n\n\nSoftmax\nWhen a model runs, it’s last layer produces raw numbers, We call these activations, they are not probabilities yet, we need to change these raw number into probabilities, we want each number show how likely the model thinks each options is. So we use softmax activation in the final layer to ensure that the activations are all between 0 and 1. Softmax is similar to sigmoid function, below is what the sigmoid function look like in case you forget it\n\nplot_function(torch.sigmoid, min=-4,max=4)\n\n\n\n\n\n\n\n\nWhen you apply a sigmoid function to a single column of activation it turns those numbers to be between 0 and 1, it’s pretty useful activations for our final layer. But hold on and think about it, what if we have more activations than just a single column, let’s say we need to create a neural network that predict wether that image is a image of 3 or 7 that returns 2 activations (one for each number). Alright let’s create 6 images and 2 categories(the first columns is 3, and other is 7)\n\ntorch.random.manual_seed(42);\nacts = torch.randn((6,2))*2\nacts\n\ntensor([[ 0.6734,  0.2576],\n        [ 0.4689,  0.4607],\n        [-2.2457, -0.3727],\n        [ 4.4164, -1.2760],\n        [ 0.9233,  0.5347],\n        [ 1.0698,  1.6187]])\n\n\nWe can’t pass this to a sigmoid function directly cuz we can not get rows that add up to 1\n\nacts.sigmoid()\n\ntensor([[0.6623, 0.5641],\n        [0.6151, 0.6132],\n        [0.0957, 0.4079],\n        [0.9881, 0.2182],\n        [0.7157, 0.6306],\n        [0.7446, 0.8346]])\n\n\nWe need to use softmax. Here how we can represent the softmax function\ndef softmax(x): return exp(x) / exp(x).sum(dim=1, keepdim=True)\n\n\nMathematically, here the softmax formula\n\\[\ns(x_{i}) = \\frac {e^{x_i}} {\\sum _{j=1}^N e^{x_j}}\n\\]\n\nsm_acts = torch.softmax(acts, dim=1)\nsm_acts\n\ntensor([[0.6025, 0.3975],\n        [0.5021, 0.4979],\n        [0.1332, 0.8668],\n        [0.9966, 0.0034],\n        [0.5959, 0.4041],\n        [0.3661, 0.6339]])\n\n\nIn fact sofmax is the multi-category version of sigmoid, we need to use it anytime we have more that two categories and the probabilities must add up to 1\nYou might wondering why we have exponential the element here in the sofmax function, first of all, the obvious insight is that it helps to make the number to be positive, it also have a nice property: if one the numbers in our activations x is slightly bigger than the other the exponential will amplify it by make it closer to 1. That means the sofmax function really like to pick one class among others so that make sure that your each picture has definite labels\nSoftmax is just one part of the cross-entropy loss, we need to go through log likelihood\n\n\nLog likelihood\nIn binary case we use torch.where to select between inputs and 1-inputs\ndef mnis_loss(inputs, targets):\n  inputs = inputs.sigmoid()\n  return torch.where(targets==1, 1-inputs, inputs)\nLet’s try to do this using pytorch, first we need to generate our label for 3s and 7s\n\ntarg = tensor([0,1,0,1,1,0])\n\nThen each item of targ we can use that to select the appropriate column of sm_acts using tensor indexing, like this:\n\nidx = range(6)\nsm_acts[idx, targ]\n\ntensor([0.6025, 0.4979, 0.1332, 0.0034, 0.4041, 0.3661])\n\n\nPytorch provide a function which does just that (sm_acts[range(6), targ]) called nll_loss (NLL stands for negative log likelihood)\n\n-sm_acts[idx, targ]\n\ntensor([-0.6025, -0.4979, -0.1332, -0.0034, -0.4041, -0.3661])\n\n\n\nF.nll_loss(sm_acts, targ, reduction=\"none\")\n\ntensor([-0.6025, -0.4979, -0.1332, -0.0034, -0.4041, -0.3661])\n\n\nYou might wondering why we need the negative anyway? Well because we want to minimize the loss, the log likelihood of correct label should be maximized(closer to 0 is better, as \\(\\log(1)=0\\)). However optimization algorithms designed to minimize, not to maximize, by adding a negative sign we convert the maximization problem to minimization problem\nThe math behind it here is that let’s say if p is the probability of the correct class then the negative log likelihood is \\(-\\log(p)\\) as p approaches 1(perfect prediction), the \\(-\\log(p)\\) approaches 0(minimum loss), as p approaches 0(bad predictions) \\(-\\log(p)\\) approaches infinity (maximum loss). Blow are plots demonstrate why we need a negative sign here\n\nfig, (ax1,ax2) = plt.subplots(1, 2, figsize=(12,5))\nplt.sca(ax1)\nplot_function(torch.log, min=0, max=1, ty=\"log(x)\", tx='x')\nax1.set_title('log(x)')\nplt.sca(ax2)\nplot_function(lambda x: -1*torch.log(x), min=0, max=1, ty=\"- log(x)\", tx='x')\nax2.set_title('Negative Log Function')\n\nText(0.5, 1.0, 'Negative Log Function')\n\n\n\n\n\n\n\n\n\n\n\nI love Logs\nWhy is that? what if we have a very very small probabilities or even when working with multi-label classification7 it may involve the multiplication of many small numbers, as you may know it will lead to problems like numerical underflow8 in computers. we want to transform these probabilities to a large values so we can perform mathematical operation on them. And there is a mathematical function that will help us doing that: the logarithm as you can see in the image above\nNot stop there tho, we do want to ensure that our model is able to detect the differences between small numbers as our loss need to be sensitive enough to small changes in probabilities(especially when the model’s predictions are very wrong). Say, probabilities of 0.01 and 0.001 those number are very close together, but in probability, 0.001 is 10 times more confident compare to 0.01. Having said that, by taking the log out of our probabilities, we prevent these important different from being ignored.\nOne more thing that make log being amazing is this relationship:\n\\[\n\\log(a\\times b) = \\log(a) + \\log(b)\n\\]\nWhat this relationship tells us? Well this means the logarithm increases linearly when the argument increases exponentially or multiplicatively. Logarithms are awesome because when we do multiplication which can create really really large or really really small numbers, can be replaced by addition, which produce numbers that our computer can handle\nWe compute the loss on the column that contains the correct label, because there’s only one right answer per example, we don’t need to consider the others, because by the definition of softmax, the remain columns is indeed equal 1 minus the activation correspond to the correct label, then we’ll have a loss function that tells how well we are predicting each image. Therefore, making the activation of the correct label as high as possible will also decreasing the activations of the remaining columns\n\n\nNegative Log Likelihood\nThen what we do next? We will take the mean of negative log of our probabilities in other the word for each sample(image) we take the negative log of the predicted probability for the correct class as above this give us the loss for each individual sample we then calculate the mean of these individual losses across all samples, that give us the negative log likelihood or cross-entropy loss. One thing to note here that the Pytorch nll_loss assume that you already take the log of the softmax, even on it name have the word log but it dose not do it for you unfortunately\nSo that is cross-entropy loss in Pytorch, this is available as F.cross_entropy ofr nn.CrossEntropyLoss the F namespace version seems to be used more often by people\n\nloss_func = nn.CrossEntropyLoss()\n\n\nloss_func(acts, targ)\n\ntensor(1.8045)\n\n\n\nF.cross_entropy(acts, targ)\n\ntensor(1.8045)\n\n\nBy default PyTorch loss functions take the mean of the loss of all items. You can use reduction=‘none’ to disable that:\n\nnn.CrossEntropyLoss(reduction='none')(acts, targ)\n\ntensor([0.5067, 0.6973, 2.0160, 5.6958, 0.9062, 1.0048])\n\n\nLet’s look at the loss above those are numbers that the computer can learn from, but with us human, it is hard to look at those number and and tell how good our model is, so that why we need metrics, Those number are not used in the optimization process but just to help us poor human understand what’s going on. We can also use the confusion matrix to see where our model perform good and not\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix(figsize=(12,12), dpi=60)"
  },
  {
    "objectID": "posts/2024-12-30-deep-learning-concepts/index.html#looking-for-a-learning-rate",
    "href": "posts/2024-12-30-deep-learning-concepts/index.html#looking-for-a-learning-rate",
    "title": "Some Deep Learning Concepts You Need To Understand",
    "section": "Looking for a learning rate",
    "text": "Looking for a learning rate\nOne of the most important things we can do is to find the just right learning rate, if our learning rate is too high our optimizer will step too far from the minimal loss, and repeating this multiple time just make it jump around each side of the valley. Alright just make the learning rate really small to make sure it will never pass our minimal loss right? well of course it can take many epochs to train our model to go to the point, and it not only a waste of time but potentially causing overfitting because each epoch we go though our entire data one time, if we repeat it too much time we would give the computer a chance to memorize it.\nSo how can we find the perfect learning rate? not too low, not too high but just right? In 2015 the researcher Leslie Smith came up with a brilliant idea, called the learning rate finder. His idea was to start with a very, very small learning rate we use that for one mini-batch(not an epoch) and then look at the losses then increase the learning rate, say, x2, and we do it again and again until the loss get worse instead of better, then we know that we’ve gone too far it’s time to slower down by selecting a learning rate a bit smaller that the previous one.\nHere’s the advice from fastai:\n\nOne order of magnitude less than where the minimum loss was achieved (i.e., the minimum divided by 10)\nThe last point where the loss was clearly decreasing\n\n\nlearn = vision_learner(dls, 'resnet26d', metrics=error_rate, path='.')\nlearn.lr_find(suggest_funcs=(minimum, steep))\n\n\n\n\n\n\n\n\n/home/monarch/miniconda3/lib/python3.12/site-packages/fastai/learner.py:53: FutureWarning:\n\nYou are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n\n\nSuggestedLRs(minimum=0.010000000149011612, steep=0.0006918309954926372)\n\n\n\n\n\n\n\n\n\nWe can see in this plot that the range from 1e-6 to 1e-4 the model doesn’t seem to train at all, from 1e-3 the loss start to decrease until it reaches the minimum at 1e-1 then increasing rapidly, obviously we don’t want the learning rate after 1e-1 as i explained above. So choose 1e-1 then? While the loss minimum around the learning rate of 1e-1 it might give a good results, it’s right at the edge of stable as after this point there’s a sharp increase in the loos, in practice it often safer to choose a learning rate slightly lower than this threshold to ensure stability across different runs or datasets. Alright let’s choose 3e-3 to see\n\nlearn = vision_learner(dls, 'resnet26d', metrics=error_rate, path='.')\nlearn.fine_tune(3, base_lr=3e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.861936\n1.138522\n0.362326\n00:58\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.208978\n0.829224\n0.271985\n01:04\n\n\n1\n0.943550\n0.610545\n0.196060\n01:03\n\n\n2\n0.784365\n0.570067\n0.183085\n01:03\n\n\n\n\n\nAlright, we’ve got a pretty good learning rate, let’s look at how we can fine-tune the weights of a pretrained model."
  },
  {
    "objectID": "posts/2024-12-30-deep-learning-concepts/index.html#transfer-learning",
    "href": "posts/2024-12-30-deep-learning-concepts/index.html#transfer-learning",
    "title": "Some Deep Learning Concepts You Need To Understand",
    "section": "Transfer Learning",
    "text": "Transfer Learning\nWe’ve used transfer learning a lot in fact but, what it really is, and how it work? Pretrained model is trained on millions of data points(such as ImageNet) then it is fine-tuned for other tasks.\nWe now know that a convolutional neural network consists of many layers with a nonlinear activation between each pairs of layers, followed by one or several final linear layers with an activation function like sofmax at the very end. The final linear layer uses a matrix that has number of columns(which determine the size of the outputs) should match the number of classes in our classification problem. This final layers is useless for us when we fine-tuning it because the number of class is likely different, the specific categories it was trained on to identify are different. So we throw it away and replace it with new linear layer with the correct number of outputs for our specific task.\nWhen we add a new linear layer for our specific task it weights are indeed initialized randomly, despite that the entire pretrained model is not random at all, all the layers before the final layers still retain their pretrained weights, which encoded valuable information, such as finding gradient and edges, and later on layer can identify eyeballs, and fur. We want to train it in a way that make it still remember all of these generally useful ideas that it has trained on, and use that to solve our problem.\nSo our problem is replace the random weights in our added layers with weights that are correctly achieve our desire task without breaking the carefully pretrained weights. So what we can do is to tell the optimizer to only update the weights in those randomly added final layers, don’t change the weights in the rest of the neuron network at all in other word freezing those pretrained layers. When we use fine-tune fastai automatically freezes all the pretrained layers for us, in fact it does these two things:\n\nFirst it trains the randomly added layers for one epoch, with all other layers frozen\nThen it unfreezes all other layers and train them all with the number of epoch we tell it\n\nThat why when fine-tune we always have a table with one column above then the table with the number of epoch blow it.\nIn fact fine_tune first does fit_one_cycle then unfreeze and does fit_one_cycle again. Alright let’s do it manually this time\n\n\nlearn.fine_tune??\n\n\nSignature:\n\nlearn.fine_tune(\n\n    epochs,\n\n    base_lr=0.002,\n\n    freeze_epochs=1,\n\n    lr_mult=100,\n\n    pct_start=0.3,\n\n    div=5.0,\n\n    *,\n\n    lr_max=None,\n\n    div_final=100000.0,\n\n    wd=None,\n\n    moms=None,\n\n    cbs=None,\n\n    reset_opt=False,\n\n    start_epoch=0,\n\n)\n\nSource:   \n\n@patch\n\n@delegates(Learner.fit_one_cycle)\n\ndef fine_tune(self:Learner, epochs, base_lr=2e-3, freeze_epochs=1, lr_mult=100,\n\n              pct_start=0.3, div=5.0, **kwargs):\n\n    \"Fine tune with `Learner.freeze` for `freeze_epochs`, then with `Learner.unfreeze` for `epochs`, using discriminative LR.\"\n\n    self.freeze()\n\n    self.fit_one_cycle(freeze_epochs, slice(base_lr), pct_start=0.99, **kwargs)\n\n    base_lr /= 2\n\n    self.unfreeze()\n\n    self.fit_one_cycle(epochs, slice(base_lr/lr_mult, base_lr), pct_start=pct_start, div=div, **kwargs)\n\nFile:      ~/miniconda3/lib/python3.12/site-packages/fastai/callback/schedule.py\n\nType:      method\n\n\n\nlearn = vision_learner(dls, 'resnet26d', metrics=error_rate, path='.')\nlearn.fit_one_cycle(3, 3e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.797806\n1.022250\n0.333974\n01:01\n\n\n1\n1.111010\n0.680857\n0.217203\n01:02\n\n\n2\n0.841627\n0.611456\n0.190293\n00:59\n\n\n\n\n\nWe unfreeze the model\n\nlearn.unfreeze()\n\nWe also need to fine a new learning rate because we trained it with more layers, and weights that already train for 3 epochs means our previous founded learning rate isn’t appropriate anymore\n\nlearn.lr_find()\n\n\n\n\n\n\n\n\n/home/monarch/miniconda3/lib/python3.12/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state = torch.load(file, map_location=device, **torch_load_kwargs)\n\n\nSuggestedLRs(valley=6.30957365501672e-05)\n\n\n\n\n\n\n\n\n\nalright let’s pick 1e-4 this time\n\nlearn.fit_one_cycle(6, lr_max=1e-4)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.719057\n0.490316\n0.155694\n01:06\n\n\n1\n0.512388\n0.341793\n0.097549\n01:05\n\n\n2\n0.364478\n0.241446\n0.069678\n01:04\n\n\n3\n0.255537\n0.206497\n0.064392\n01:00\n\n\n4\n0.207048\n0.193223\n0.059106\n01:05\n\n\n5\n0.171621\n0.182925\n0.057184\n01:03\n\n\n\n\n\nWell it improve our model a lot from 0.18 to 0.06, in fact when i submit the predictions from the model we’ve built i got a pretty good result, it’s 0.93778, which improves a lot. Alright let’s see how our model train\n\nlearn.recorder.plot_loss()"
  },
  {
    "objectID": "posts/2024-12-30-deep-learning-concepts/index.html#how-long-to-train",
    "href": "posts/2024-12-30-deep-learning-concepts/index.html#how-long-to-train",
    "title": "Some Deep Learning Concepts You Need To Understand",
    "section": "How Long to Train?",
    "text": "How Long to Train?\nWell the first approach to train should be simply pick a number of epochs that will train in the amount of time that you’re happy to wait for, maybe take a cup of water, scrolling reddit, reading stuff,… then look at the training and validation loss plot like above and if you see it getting better when it comes to your final epochs, then you know that you should train it longer. In other hand you may see that the metrics you chosen really getting worse at the end of the training(remember that it’s not just that we’re looking for the validation loss to get worse, but the actual metrics). While the loss function is essential for optimization, what truly matters are your chosen practical metrics, don’t be overly concerned with validation loss inconstancy if your metric are still improving.\nIf you find you’ve trained for too long (your metrics getting worse, loss getting worse), what you should do is retrain your model from scratch, really, and this time choose the number of epochs based on where your previous best result was found. One more thing if you have extra time available instead of just simply increasing epochs consider using that time to train more parameters or use deeper architecture, this can potentially yield better results than extended training of a simpler model."
  },
  {
    "objectID": "posts/2024-12-30-deep-learning-concepts/index.html#mixed-precision-training",
    "href": "posts/2024-12-30-deep-learning-concepts/index.html#mixed-precision-training",
    "title": "Some Deep Learning Concepts You Need To Understand",
    "section": "Mixed Precision Training",
    "text": "Mixed Precision Training\nAlright let’s train it for one more time. This time notice i use to_fp16 here, it called mixed-precision training it used for speeding time up especially when we using a big architecture. I highly recommend you to read this post from NVIDIA.\nBut basically you first need to understand what is half-precision? Well it’s a floating-point number format uses 16 bits to represent numbers it called half-precision as the more common are 32 bits (single precision) and 64 bits (double precision).\n\n\n\nhalf float\n\n\nAs you can see we have one bit to represent sign 5 for exponent bits while 10 for fraction bits. For instance, between 1 and 2, it can only represents the number \\(1, 1+2^{-10}, 1+2\\ast2^{-10}\\),… which mean if we plus 1 with a number smaller than \\(2^{-10}\\)(approximately 0.0009765625) we will get 1 instead of a number slightly greater than 1. let’s say 1 + 0.0001 = 1 in half precision, that means it less precise than single or double precision, with only about 3 decimal digits of precision. So it helps reduce the memory usage by half compare to single precision, or we can double our batch, model size. Another very nice feature is that NVIDIA developed its latest GPUs (the Volta generation) to take fully advantage of half-precision tensors.\n\n\nWhen we talk about \\(2^{-10}\\), we’re referring to the smallest positive value that can be represented in the fraction part of the number. We have 10 bits to represent fraction part which mean it can represent \\(2^{10} = 1024\\) different values, these 1024 values are distributed eventually between 0 and 1. In fact, it’s the smallest step between these values so we divide the range(which is 1 here) by the possible value (\\(2^{10}\\)):\n\\[\n\\frac{1}{2^{10}} = 2^{-10}\n\\]\nBut there’re several problem with half precision when using it:\n\nThe weight update is imprecise: What your optimizer does under the hood is basically this equation w = w - lr * w.grad for each weights of your network. So the problem is the w.grad is several order of magnitude below w especially as the network starts to converge, these gradient often become very small as the network is making tiny and tiny adjustment, even smaller than \\(2^{-10}\\) is very common, so when using half precision, obviously the update doesn’t do anything here as FP16 can’t represent the tiny difference between w and (w - lr * w.grad).\nDuring the backpropagation of gradients, the gradients themselves become so small that they are rounded down to 0 in FP16.\nYour activation or loss can be overflow, the opposite problem from the gradients as during forward propagation, activation function like RElU or exponential function like softmax can produce a large values therefore it also make loss result in large numbers (especially in early training), it’s more easier to hit nan(of infinity) in FP16 precision and your training my more likely diverge.\n\nSo the solution for this is mixed precision training, instead of fully train in FP16 precision some of the operations will be done in FP16, others in FP32. The main idea is that we will do the forward pass and the gradient computation in half precision to go fast, but the update in single precision. So our training loop will look like this:\n\ncompute the output with FP16 model, and loss\nback-propagate the gradients in half-precision\nwe copy the gradient in FP32 precision\ndo the update on the master model (in FP32 precision)\ncopy the master model in the FP16 model\n\n\n\n\n\n\n\nNote\n\n\n\nNote that we will lose precision during step 5, and that the 1.0001 in one of the weights will go back to 1. But if the next update corresponds to add 0.0001 again, since the optimizer step is done on the master model, the 1.0001 will become 1.0002 and if we eventually go like this up to 1.0005, the FP16 model will be able to tell the difference.\n\n\nAlright that’s solve the first problem. For the second problem we use something call the gradient scaling. To avoid the gradient getting zeroed by FP16 precision we multiple the loss by the scale factor(often scale=512), by multiplying the loss with a large numbers all the gradients are effectively made larger. Of course we don’t want those 512-scaled gradients to be in the weight updates so that after converting them into FP32 we can divide them by this scale. So it change the loop to:\n\n\nIn fact the scaling factor that we multiply it with the loss can leads our gradients or our loss to be overflow. So there a way around this, which is just simple as this, first we will try with a very high scale factor and see if it cause overflow to our loss or gradients, if it does, we will try with the half of that big value and again until we get the largest loss scale possible that doesn’t make our gradient overflow.\n\ncompute the output with the FP16 model, then the loss\nmultiply the loss by scale and then back-propagate the gradients in half precision\ncopy the gradients in FP32 precision then divide them by the scale\ndo the update on the master model\ncopy the master model in FP16 model\n\nFor the last problem, the tricks offered by NVIDIA are to leave the batchnorm layers in single precision (they don’t have many weights so it’s not a big memory challenge) and compute the loss in single precision (which means converting the last output of the model in single precision before passing it to the loss).\nAlright let’s apply our mixed precision strategy to our model, when we create a learner we call to_fp16().\n\nlearn = vision_learner(dls, 'resnet26d', metrics=error_rate, path='.').to_fp16()\nlearn.lr_find(suggest_funcs=[valley, slide])\n\n\n\n\n\n\n\n\n/usr/local/lib/python3.10/dist-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state = torch.load(file, map_location=device, **torch_load_kwargs)\n\n\nSuggestedLRs(valley=0.0012022644514217973, slide=0.002511886414140463)\n\n\n\n\n\n\n\n\n\n\nlearn.fine_tune(14, 1e-2, freeze_epochs=3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.907815\n1.100638\n0.353676\n00:41\n\n\n1\n1.442608\n1.050639\n0.346468\n00:41\n\n\n2\n1.303783\n0.873419\n0.289284\n00:42\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.892789\n0.587087\n0.187410\n00:42\n\n\n1\n0.721903\n0.487567\n0.159058\n00:41\n\n\n2\n0.625562\n0.414129\n0.140317\n00:42\n\n\n3\n0.559306\n0.321464\n0.098510\n00:41\n\n\n4\n0.437455\n0.248258\n0.069678\n00:41\n\n\n5\n0.332489\n0.218854\n0.059106\n00:42\n\n\n6\n0.265161\n0.221709\n0.059106\n00:41\n\n\n7\n0.205299\n0.193941\n0.052859\n00:41\n\n\n8\n0.189015\n0.177993\n0.044690\n00:42\n\n\n9\n0.141942\n0.173127\n0.044690\n00:42\n\n\n10\n0.137524\n0.150655\n0.041326\n00:41\n\n\n11\n0.111285\n0.146560\n0.034118\n00:41\n\n\n12\n0.106115\n0.145193\n0.034599\n00:42\n\n\n13\n0.105035\n0.143570\n0.033157\n00:41\n\n\n\n\n\nAlright let’s see if it’s better, the best way to check is to submit it to kaggle we just do the same thing as above, just copy and past it here.\n\nss = pd.read_csv(path/'sample_submission.csv')\ntst_files = get_image_files(path/'test_images').sorted()\ntst_dl = dls.test_dl(tst_files)\nprobs,_,idxs = learn.get_preds(dl=tst_dl, with_decoded=True)\nmapping = dict(enumerate(dls.vocab))\nresults = pd.Series(idxs.numpy(), name=\"idxs\").map(mapping)\nss['label'] = results\nss.to_csv('subm.csv', index=False)\n!head subm.csv\n\n/usr/local/lib/python3.10/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.10/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n\n\n\n\n\n\n\n\n\nimage_id,label\n200001.jpg,hispa\n200002.jpg,normal\n200003.jpg,blast\n200004.jpg,blast\n200005.jpg,blast\n200006.jpg,brown_spot\n200007.jpg,dead_heart\n200008.jpg,brown_spot\n200009.jpg,hispa\n\n\nThis time i got a little higher result, i got 0.95967 as score which is pretty understandable as we trained it with 14 epochs, but yeah that was pretty cool."
  },
  {
    "objectID": "posts/2024-12-30-deep-learning-concepts/index.html#scaling-up",
    "href": "posts/2024-12-30-deep-learning-concepts/index.html#scaling-up",
    "title": "Some Deep Learning Concepts You Need To Understand",
    "section": "Scaling up",
    "text": "Scaling up\nAlright, can we do better, let’s see how far we can go, let’s do some experimenting, we will use different architectures and image processing approaches, for the sake of convenient let’s put our steps together into a little function.\n\n\n\n\n\n\nNote\n\n\n\nNote that we can use ImageDataLoader.from_folder for our dataloader for make it shorter, but in general it the same as DataBlock\n\n\n\ndef train(arch, item, batch, epochs=5):\n  dls = ImageDataLoaders.from_folder(train_path, seed=42, valid_pct=0.2, item_tfms=item, batch_tfms=batch)\n  learn = vision_learner(dls, arch, metrics=error_rate).to_fp16()\n  learn.fine_tune(epochs, 1e-2)\n  return learn\n\nTo have a better result, one way to archive this is to use a better model right, but what to choose, well Jeremy Howard has a really good notebook that helps us choosing a appropriate architecture for our model based on what kind of problem you’re dealing, the GPU memory, error rate, time,… But basically there are two key to choose the right one:\n\nHow similar between our dataset and the pretrained model’s dataset.\nHow large they are.\n\nThen it turned out that when it comes to computer vision model convnext model is one of the best, if not the best till now so let’s give it a try shall we?\n\narch = \"convnext_small_in22k\"\n\nFrom now on, if you not sure what architecture to use, just use this, right. And of course we have different version of convnext we have tinny, small, large… it will take more time to train but of course lower error rate. alright let’s see how it will go\n\nlearn = train(arch, item=Resize(192,method=\"squish\"), batch=aug_transforms(size=128, min_scale=0.75))\n\n/usr/local/lib/python3.10/dist-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name convnext_small_in22k to current convnext_small.fb_in22k.\n  model = create_fn(\n\n\n\n\n\n/usr/local/lib/python3.10/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.10/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.257208\n0.790532\n0.246997\n00:46\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.610890\n0.450510\n0.146564\n00:44\n\n\n1\n0.504428\n0.302570\n0.097069\n00:44\n\n\n2\n0.299630\n0.194396\n0.061989\n00:44\n\n\n3\n0.186308\n0.130406\n0.036521\n00:44\n\n\n4\n0.134839\n0.115092\n0.035079\n00:43\n\n\n\n\n\nWell it did a pretty good job isn’t it? we just do 5 epochs and we archive almost the same as the one we trained with 12 epochs, even our time to go through each epoch is the same(that’s because i reduce the presize to 192X192 for it to run faster but it still produce the same performance as the previous one but with fewer epochs).\nSo one thing we could try is instead of using squish as our pre-processing let’s try using padding, now we will use bigger presize so that when we use padding here we will get entire image but the downside is we also get few extra zero pixels which literally pointless, but whatever let’s see if it work better\n\n\npadding is interesting because it’s the only way of pre-processing images which doesn’t distort them and doesn’t loose anything, if you crop you lose things, if you squish you distort things\n\ndls = ImageDataLoaders.from_folder(train_path, valid_pct=0.2, seed=42,\n    item_tfms=Resize(480, method=ResizeMethod.Pad, pad_mode=PadMode.Zeros))\ndls.show_batch(max_n=3)\n\n\n\n\n\n\n\n\n\nlearn = train(arch, item=Resize((480, 360), method=ResizeMethod.Pad, pad_mode=PadMode.Zeros), batch=aug_transforms(size=(256,192), min_scale=0.75))\n\n/usr/local/lib/python3.10/dist-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name convnext_small_in22k to current convnext_small.fb_in22k.\n  model = create_fn(\n\n\n\n\n\n/usr/local/lib/python3.10/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.10/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.091136\n0.672584\n0.214320\n01:12\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.579350\n0.418952\n0.131187\n01:36\n\n\n1\n0.430034\n0.259760\n0.086497\n01:37\n\n\n2\n0.275291\n0.176225\n0.046612\n01:38\n\n\n3\n0.147642\n0.123821\n0.037963\n01:38\n\n\n4\n0.109251\n0.107270\n0.030274\n01:38\n\n\n\n\n\nAs you can see it indeed did better, and its error_rate is the best we can get so far but not huge different yet!\n\nTest Time Augmentation\nWell first let’s look how can we calculate the error rate manually with our normal prediction\n\nvalid = learn.dls.valid\npreds,targs = learn.get_preds(dl=valid)\nerror_rate(preds, targs)\n\n\n\n\n\n\n\n\nTensorBase(0.0303)\n\n\nWell that actually the previous error-rate we got above, so what i’m doing here? well let’s take a look at the images blow\n\nlearn.dls.train.show_batch(max_n=6, unique=True)\n\n\n\n\n\n\n\n\nNotice that, those are indeed the same picture but it gone through the data augmentation so sometimes it a bit darker, a bit lighter, sometimes it flipped horizontally, some times int zoom into a slightly different section, sometimes it rotate a little bit but those are all the same picture. So the idea of TTA(Test Time Augmentation) is maybe our model would like some of these version better than the others even the original image, so what we can do is we can pass all of these to our model get the prediction of all of them and take the average right, so if you read my previous blog, it’s indeed the mini version of bagging approach. In fastai you can archive this by using the tta in our learn object\n\ntta_preds,_ = learn.tta(dl=valid)\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\n\nerror_rate(tta_preds, targs)\n\nTensorBase(0.0259)\n\n\nSee, we got better result like 10% better our previous. Alright let’s train it with more epochs but this time let’s just make a bigger image and something really interesting is that our images don’t have to be square they just need to be in the same size right, it can be rectangular, having said that all of our original images are nearly 640x480, so we just need to pick one has the same aspect ratio for example 256x192 is good\n\nlearn = train(arch, epochs=12, item=Resize((480, 360), method=ResizeMethod.Pad, pad_mode=PadMode.Zeros), batch=aug_transforms(size=(256,192), min_scale=0.75))\n\n/usr/local/lib/python3.10/dist-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name convnext_small_in22k to current convnext_small.fb_in22k.\n  model = create_fn(\n\n\n\n\n\n/usr/local/lib/python3.10/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.10/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.091136\n0.672584\n0.214320\n01:09\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.534227\n0.295779\n0.100432\n01:25\n\n\n1\n0.406855\n0.259834\n0.077847\n01:26\n\n\n2\n0.345489\n0.217263\n0.065353\n01:26\n\n\n3\n0.272754\n0.186886\n0.056704\n01:26\n\n\n4\n0.220682\n0.205609\n0.054781\n01:27\n\n\n5\n0.182489\n0.122831\n0.037001\n01:27\n\n\n6\n0.118704\n0.119720\n0.035560\n01:26\n\n\n7\n0.099475\n0.117059\n0.034118\n01:26\n\n\n8\n0.071605\n0.094223\n0.025949\n01:26\n\n\n9\n0.055326\n0.096391\n0.025949\n01:26\n\n\n10\n0.037327\n0.096320\n0.024507\n01:26\n\n\n11\n0.035568\n0.094268\n0.023066\n01:27\n\n\n\n\n\nAlright our error_rate down to 2.3% which is pretty good, now, let’s what our error_rate when using tta\n\ntta_preds,targs = learn.tta(dl=learn.dls.valid)\nerror_rate(tta_preds, targs)\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\nTensorBase(0.0235)\n\n\nOops! it’s worse this time, that’s strange, i think it won’t always produce a better error_rate, but maybe it will work well in practice i guess, alright forget about it, let’s submit it\n\nss = pd.read_csv(path/'sample_submission.csv')\ntst_files = get_image_files(path/'test_images').sorted()\ntst_dl = learn.dls.test_dl(tst_files)\npreds,_ = learn.tta(dl=tst_dl)\nidxs = preds.argmax(dim=1)\nvocab = np.array(learn.dls.vocab)\nresults = pd.Series(vocab[idxs], name=\"idxs\")\nss['label'] = results\nss.to_csv('subm.csv', index=False)\n!head subm.csv\n\n/usr/local/lib/python3.10/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.10/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\nimage_id,label\n200001.jpg,hispa\n200002.jpg,normal\n200003.jpg,blast\n200004.jpg,blast\n200005.jpg,blast\n200006.jpg,brown_spot\n200007.jpg,dead_heart\n200008.jpg,brown_spot\n200009.jpg,hispa\n\n\nThis time i got a little bit higher result, around 0.98 which is quite impressive. So we’ve gone through all of the essential concepts that we need to get familiar with as later we will delve deeper and deeper into more amazing thing later on, understand these concepts is like a solid groundwork for us to exploring even more fascinating topics in the future."
  },
  {
    "objectID": "posts/2024-12-30-deep-learning-concepts/index.html#footnotes",
    "href": "posts/2024-12-30-deep-learning-concepts/index.html#footnotes",
    "title": "Some Deep Learning Concepts You Need To Understand",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nInterpolation in image processing is the technique of estimating new pixel values when resizing or transforming a digital image. It’s like filling in the blanks between known pixels to create a smooth transition when changing an image’s size, rotation or shape↩︎\nWhile the crop is indeed random, but it’s not entirely arbitrary. The goal is to create diversity in the training data, which helps the model learn to recognize objects and features from various perspectives and scales. In practice, it often use multiple crops from a single image during training, this increase the chances of capturing important features at least some of the crops.↩︎\na resize method which resizes the image to fit within the target dimensions and adds padding (usually black) to fill the rest, it keeps the original aspect ratio and all image information, however it can lead to artificial background which might affect model performance. Use it when you have to keep the entire image and the its aspect ratio is important and of course be a wear of extra background↩︎\na resize method which resizes the image to fit the target dimensions, potentially distorting the aspect ratio. it helps preserves all information in the image, but at the same time it can distort the image, potentially altering important features. Use it when the aspect ratio is not crucial for your task, or when your imagees are already mostly square↩︎\nResizes the image and then crops it to fit the target dimensions. Help maintains aspect ratio of the visible part and doesn’t introduce distortion, however it may lose important information at the edges of the image. Use it when the main subject is typically centered in your images, or when edge information is less important.↩︎\nIt will attempt to create a batch from the source you give it, with a lot of details. Also, if it fails, you will see exactly at which point the error happens, and the library will try to give you some help. For instance, one common mistake is to forget to use a Resize transform, so you end up with pictures of different sizes and are not able to batch them.↩︎\nin multi label classification, each instance can belong to multiple classes simultaneously, imagine working on a dog cat classification, where an image could contain both dog and cat at the same time so in this problem it requires us to do multiplications on probabilities which will lead to numerical underflow problem in computer science↩︎\nNumber underflow occurs when a computation results in a number too small for computer to represent accurately, often leading to be rounded to 0↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dai’s blog",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n\n\n\n\n\n\n\n\n\n\nSome Deep Learning Concepts You Need To Understand\n\n\n\nkaggle\n\ncompetition\n\ndeep learning\n\n\n\n\n\n\n\n\n\nDec 30, 2024\n\n\nBui Huu Dai\n\n31 min\n\n\n\n\n\n\n\n\n\n\n\nExploring Random Forest, Bagging, Boosting?\n\n\n\nkaggle\n\ncompetition\n\nrandom forest\n\nbagging\n\nboosting\n\n\n\n\n\n\n\n\n\nNov 10, 2024\n\n\nBui Huu Dai\n\n39 min\n\n\n\n\n\n\n\n\n\n\n\nFundamentals of Text Classification and Correlation in Natural Language Processing\n\n\n\nkaggle\n\nhuggingface spaces\n\nnlp\n\n\n\n\n\n\n\n\n\nSep 13, 2024\n\n\nBui Huu Dai\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\nLooking Inside Neural Networks, How It Really Work?\n\n\n\nblogging\n\nfastai\n\ntorch\n\n\n\n\n\n\n\n\n\nJul 28, 2024\n\n\nBui Huu Dai\n\n19 min\n\n\n\n\n\n\n\n\n\n\n\nFirst Step in AI\n\n\n\nblogging\n\nfastai\n\n\n\n\n\n\n\n\n\nJul 4, 2024\n\n\nBui Huu Dai\n\n20 min\n\n\n\n\nNo matching items"
  }
]
[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to my blog! To learn more about me, check out my website here."
  },
  {
    "objectID": "posts/titanic_competition/index.html",
    "href": "posts/titanic_competition/index.html",
    "title": "Sink or Swim: Navigating Deep Learning with the Titanic Competition",
    "section": "",
    "text": "Hey there, data enthusiasts! Today, I’m excited to share my experience with the Titanic competition on Kaggle. This challenge has been a fantastic learning opportunity, pushing me to build a deep learning model from scratch.\nIn this blog post, I’ll walk you through my journey, sharing the ups and downs, the lessons learned, and maybe even a few tips for those of you looking to dive into deep learning yourselves"
  },
  {
    "objectID": "posts/titanic_competition/index.html#the-mission-predict-and-survive",
    "href": "posts/titanic_competition/index.html#the-mission-predict-and-survive",
    "title": "Sink or Swim: Navigating Deep Learning with the Titanic Competition",
    "section": "The Mission: Predict and Survive!",
    "text": "The Mission: Predict and Survive!\nAlright, here’s the deal: The Titanic competition is like the ultimate “What if?” game. Our mission, should we choose accept it(and we totally should), is to build a machine learning model that can predict whether a passenger survived the Titanic disaster.\nThink of it as creating a time-traveling survival predictor:\n\nInput: Passenger information\nOutput: “Survived” or “Didn’t survive”\n\nIn data science lingo, we’re talking about a binary classification problem. It’s like teaching a computer to sort passenger into two group: the lucky ones who made it, and those who, unfortunately, didn’t.\nNow, while many folks tackle this with traditional machine learning methods, we’re going to kick it up a notch. We’ll be diving into the deep en(pun intended) by creating a deep learning model with a neuron network. It’s like giving our computer a super-power brain to solve this historical puzzle!\nWhy go for a neuron network, you ask? Well, why climb a hill when you can scale a mountain? It’s more complex, sure, but it’s also way more exciting and potentially more powerful. Plus, it’ll give us a taste of the cutting-edge techniques used in modern data science.\nSo, buckle up (or should i say, put on your life jackets?)! We’re about to embark on a journey that combines historical tragedy, predictive analytics, and the power of deep learning, it’s going to be challenging, it’s going to be insightful, and most importantly, it’s going to be a ton of fun!\nRemember, in the world of computer science as well as data science, we’re not just crunching numbers - we’re uncovering stories, solving mysteries, any maybe, just maybe, learning something that could help in the future crises. So let’s dive in and see what secrets the Titanic data set holds for us!"
  },
  {
    "objectID": "posts/titanic_competition/index.html#why-the-titanic-its-more-than-just-blockbuster",
    "href": "posts/titanic_competition/index.html#why-the-titanic-its-more-than-just-blockbuster",
    "title": "Sink or Swim: Navigating Deep Learning with the Titanic Competition",
    "section": "Why the Titanic? It’s More Than Just Blockbuster!",
    "text": "Why the Titanic? It’s More Than Just Blockbuster!\nYou might be wondering, “Why are ew obsessing over a century-old shipwreck?” Well, in the data science world, the Titanic dataset is like that classic book everyone’s read - it’s a rite of passage! Here’s why it’s so awesome for beginners:\n\nData Buffet: The dataset is a smorgasbord of passenger info. It’s like having a well-stocked pantry - you’ve got everything you need to whip up some tasty insight!\nMissing Pieces: Just like a real-world dataset, it’s got some holes. Time to channle your inner detective and fill into those blanks!\nfeature Crafting: Thinking of as data origami - you get to fold and shape new features from the existing ones. It’s where creative meets numbers!\nReal Stakes: This isn’t just some made-up scenario. These were real people on real ship. It adds a whole new levels of meaning to your analysis."
  },
  {
    "objectID": "posts/titanic_competition/index.html#understand-the-data",
    "href": "posts/titanic_competition/index.html#understand-the-data",
    "title": "Sink or Swim: Navigating Deep Learning with the Titanic Competition",
    "section": "Understand the Data",
    "text": "Understand the Data\nthe data is splited into two part: the train.csv which is used to train out model, and the test.csv which is for test our model and push the submition into the competition. you can download the dataset manually here or set up the kaggle api, you can follow this to setup kaggle api then use this command to download the dataset\nkaggle competitions download -c titanic\nhere’s the data dictionary\n\n\n\n\n\n\n\n\nVariable\nDefinition\nKey\n\n\n\n\nsurvival\nSurvival\n0 = No, 1 = Yes\n\n\npclass\nTicket class\n1 = 1st, 2 = 2nd, 3 = 3rd\n\n\nsex\nSex\n\n\n\nAge\nAge in years\n\n\n\nsibsp\n# of siblings / spouses aboard the Titanic\n\n\n\nparch\n# of parents / children aboard the Titanic\n\n\n\nticket\nTicket number\n\n\n\nfare\nPassenger fare\n\n\n\ncabin\nCabin number\n\n\n\nembarked\nPort of Embarkation\nC = Cherbourg, Q = Queenstown, S = Southampton\n\n\n\n\n\nCode\n# Import and read dataset\nimport torch, numpy as np, pandas as pd\nfrom torch import tensor\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch.nn.functional as F\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import KNNImputer\n\ntorch.manual_seed(442)\nsns.set_style('whitegrid')\n\n# setup layout option\nnp.printoptions(linewidth=140)\ntorch.set_printoptions(linewidth=140, sci_mode=False, edgeitems=7)\npd.set_option(\"display.width\", 140)\n\npath = Path(\"./input\")\ntrain_path = path/\"train.csv\"\ntrn_df = pd.read_csv(train_path)\ntrn_df.head()\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS"
  },
  {
    "objectID": "posts/titanic_competition/index.html#pre-process-data",
    "href": "posts/titanic_competition/index.html#pre-process-data",
    "title": "Sink or Swim: Navigating Deep Learning with the Titanic Competition",
    "section": "Pre-process Data",
    "text": "Pre-process Data\nAlright, folks, let’s dive into the first step of our data adventure: cleaning up our dataset! As we all know, a clean dataset is like a smooth road for our deep learning model to cruise on.\n\nHandling Missing Values\nLet’s start by taking a look at our data to see what we’re dealing with:\n\ntrn_df.isna().sum()\n\nPassengerId      0\nSurvived         0\nPclass           0\nName             0\nSex              0\nAge            177\nSibSp            0\nParch            0\nTicket           0\nFare             0\nCabin          687\nEmbarked         2\ndtype: int64\n\n\nFrom this, we can see that the Age, and Cabin columns have a lot of missing values. Let’s break down how we’ll handle these\n\nDealing with Missing Age Values\nFirst, let’s visualize the distribution of Age to understand it’s importance:\n\nsns.displot(trn_df['Age'], kde=False, bins=30)\n\n\n\n\n\n\n\n\nAs you can see from the plot Age is one of the most crucial features. Younger passengers had a higher chance to survival, while older passenger had a low chance. So, we need to handle Age with extra care.\nTo fill in the missing Age values, we’ll use the technique called K-Nearest Neighbors(KNN) imputation. Here’s how it works:\n\n# Age imputation\nage_imputer = KNNImputer(n_neighbors=5)\ntrn_df[\"Age\"] = age_imputer.fit_transform(trn_df[[\"Age\", \"Pclass\", \"SibSp\", \"Parch\"]])[:, 0]\n\nHere’s the breakdown:\n\nWe select relevant columns that are likely correlated with Age (Pclass, SibSp, and Parch).\nThe fit_transform method is called on the age_imputer instance, passing these columns.\n\nFit: the imputer calculate the distances between rows based on the selected columns and indentifies the 5 nearest neighbors for each instance with with missing Age values.\nTransform: For each missing Age value, the imputer calculate the mean age of the 5 nearest neighbors and fills in the missing value with this mean.\n\n[:, 0] selects the first columns of the resulting array (the imputed Age, values) and assigns it back to the Age columns trn_df.\n\nAnd that’s our K-Nearest Neighbors(KNN) imputation in action!\n\n\nHandling Missing Cabin and Embarked Values\nNow, let’s talk about the Cabin column. This column has lot of missing values. and it doesn’t have a significant impact on survival rates. So, we’ll use a simple approach to fill these missing values:\n\nmodes = trn_df.mode().iloc[0]\ntrn_df.fillna(modes, inplace=True)\n\nHere, we’re fillng the missing Cabin and two missing Embarded value with the most frequently occurring value (mode). The mode method in Pandas finds the most common value(s) in DataFrame or Series. If multiple modes exist, it return all of them\n\n\n\nOutlier Detection and Handling\nNext, let’s inspect the Fare column for any outlier:\n\ntrn_df.describe(include=np.number)\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\n\n\n\n\ncount\n891.000000\n891.000000\n891.000000\n891.000000\n891.000000\n891.000000\n891.000000\n\n\nmean\n446.000000\n0.383838\n2.308642\n29.828249\n0.523008\n0.381594\n32.204208\n\n\nstd\n257.353842\n0.486592\n0.836071\n13.293378\n1.102743\n0.806057\n49.693429\n\n\nmin\n1.000000\n0.000000\n1.000000\n0.420000\n0.000000\n0.000000\n0.000000\n\n\n25%\n223.500000\n0.000000\n2.000000\n22.000000\n0.000000\n0.000000\n7.910400\n\n\n50%\n446.000000\n0.000000\n3.000000\n30.000000\n0.000000\n0.000000\n14.454200\n\n\n75%\n668.500000\n1.000000\n3.000000\n35.800000\n1.000000\n0.000000\n31.000000\n\n\nmax\n891.000000\n1.000000\n3.000000\n80.000000\n8.000000\n6.000000\n512.329200\n\n\n\n\n\n\n\nYou’ll notice that the mean fare is around 32, but he maximum values is a Whopping 512.3292. Such extreme values can cause issues for our model because they can dominate the results. Let’s visualize this with a histogram:\n\ntrn_df['Fare'].hist();\n\n\n\n\n\n\n\n\nTo fix this, we can take the logarithm of the Fare values, which heps to compress the range and make the distribution more reasonable. Since the Fare column contains zeros and log(0) is undefined, we’ll add 1 to all values before applying the logarithm\n\ntrn_df[\"LogFare\"] = np.log1p(trn_df['Fare'])\n\n\ntrn_df['LogFare'].hist();\n\n\n\n\n\n\n\n\nAlright it’s look better now!\nAnd that’s it for data cleaning! We’ve handled missing values and outliers, setting the stage for effective model training, Next up, we’ll dive into feature engineering, whish is where the fun really begins!\n\n\nFeature Engineering\nAlright, buckle up ’cause we’re about to embark on a feature engineering adventure!\nFirst up, we’re gonna create a super coll FamilySize feature. Why? because family matter, especially when you’re trying to survive a shipwreck!\n\ntrn_df[\"FamilySize\"] = trn_df[\"SibSp\"] + trn_df[\"Parch\"] + 1\ntrn_df[\"IsAlone\"] = (trn_df[\"FamilySize\"] == 1).astype(int)\n\nNow, let’s visualize this bad boy:\n\nsns.countplot(x='Survived', data=trn_df, hue='FamilySize', palette='RdBu_r')\n\n\n\n\n\n\n\n\nWhoa, check out that plot! It’s like a family reunion, but with survival rates. Looks like having a big family might’ve been a bit of bummer for survival chances. Maybe it was harder to round up the whole crew when things got dicey? On the flip side, small families (up to 4 members) seemed to have better luck. Family-sized life rafts, perhaps?\nNext up, we’re gonna play “Name That Title”!\n\n# Extract title from name\ntrn_df['Title'] = trn_df['Name'].str.extract(r' ([A-Za-z]+)\\.', expand=False)\n# Group uncommon titles\nrare_titles = ['Lady', 'Countess','Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona']\ntrn_df['Title'] = trn_df['Title'].replace(rare_titles, 'Rare')\n# Replace some variations\ntrn_df['Title'] = trn_df['Title'].replace(['Mlle', 'Ms'], 'Miss')\ntrn_df['Title'] = trn_df['Title'].replace('Mme', 'Mrs')\n# Mapping\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\ntrn_df['Title'] = trn_df['Title'].map(title_mapping)\n\nWe’re extracting titles faster than you can say “I’m the king of the world!” We’ve got our common title, we’re grouping the rare ones(because being a Countess doesn’t help much when the ship’s going down), and we’re doing a little title cleanup.\nNow, let’s see what these titles tell us about survival:\n\n# Calculate proportions\nproportions = trn_df.groupby('Title')['Survived'].value_counts(normalize=True).rename('proportion').reset_index()\n\nsns.barplot(x='Title', y='proportion', hue='Survived', data=proportions)\nplt.title('Survival Rate by Title')\nplt.xlabel('Title')\nplt.ylabel('Proportion')\nplt.show()\n\n\n\n\n\n\n\n\nHoly shipwreck, Batman! look at those survival rates! Poor Mr.1 is going down with the ship, while Mss 2 and Mrs. 3 are living the best lifeboat life.\nIt captures important historical context about the Titanic disaster, particularly the “women and children first” policy during evacuation.\nWhen we use this feature in our model, it should be able to learn these different survival probabilities associated with each title, potentially improving its predictive accuracy.\nNext, we’re gonna group these passengers by age\n\ntrn_df['AgeBin'] = pd.cut(trn_df['Age'], bins=[0, 12, 20, 40, 60, np.inf], labels=[1, 2, 3, 4, 5])\n\nand for our grand finale, we’re gonna one-hot encode these categorical columns:\n\n# one hot encode columns\ntrn_df = pd.get_dummies(trn_df, columns=[\"Sex\", \"Pclass\", \"Embarked\", \"AgeBin\"], drop_first=True, dtype=float)\n\nAlright that’s it, let’s write a function to do all of this for the life easier, because we also need to apply it to our test set so that we can make a proper prediction\n\ndef preprocess_data(df):\n    age_imputer = KNNImputer(n_neighbors=5)\n    df[\"Age\"] = age_imputer.fit_transform(df[[\"Age\", \"Pclass\", \"SibSp\", \"Parch\"]])[:, 0]\n    modes = trn_df.mode().iloc[0]\n    trn_df.fillna(modes, inplace=True)\n    df[\"LogFare\"] = np.log1p(df[\"Fare\"])\n    df[\"FamilySize\"] = df[\"SibSp\"] + df[\"Parch\"] + 1\n    df['IsAlone'] = (df['FamilySize'] == 1).astype(int)\n    df['Title'] = df['Name'].str.extract(r' ([A-Za-z]+)\\.', expand=False)\n    rare_titles = ['Lady', 'Countess','Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona']\n    df['Title'] = df['Title'].replace(rare_titles, 'Rare')\n    df['Title'] = df['Title'].replace(['Mlle', 'Ms'], 'Miss')\n    df['Title'] = df['Title'].replace('Mme', 'Mrs')\n    title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n    df['Title'] = df['Title'].map(title_mapping)\n    df['AgeBin'] = pd.cut(df['Age'], bins=[0, 12, 20, 40, 60, np.inf], labels=[1, 2, 3, 4, 5])\n    \n    # One-hot encoding\n    df = pd.get_dummies(df, columns=['Sex', 'Pclass', 'Embarked', 'AgeBin'], drop_first=True, dtype=float)\n    return df\n\nFinally, let’s round up our feature posse:\n\ndef get_columns(name):\n    return [col for col in trn_df.columns if col.startswith(name)]\nadded_cols = get_columns((\"Sex_\", \"Pclass\", \"Embarked_\", \"AgeBin_\"))\n\nindep_cols = ['Title', 'Age', 'SibSp', 'Parch', 'LogFare', 'FamilySize', 'IsAlone'] + added_cols\nt_dep = tensor(trn_df[\"Survived\"])\n\nAnd there you have it, folks! We’ve engineeried these features faster than the Titanic sank(too soon?). Your models gonna love these new features more than Rose loved Jack. Now go forth and predict those survival rates!"
  },
  {
    "objectID": "posts/titanic_competition/index.html#build-a-deep-learning-model-from-scratch-lets-get-our-hands-dirty",
    "href": "posts/titanic_competition/index.html#build-a-deep-learning-model-from-scratch-lets-get-our-hands-dirty",
    "title": "Sink or Swim: Navigating Deep Learning with the Titanic Competition",
    "section": "Build a Deep Learning Model from Scratch: Let’s Get Our Hands Dirty!",
    "text": "Build a Deep Learning Model from Scratch: Let’s Get Our Hands Dirty!\n\nPreparing the Canvas\nAlright, folks! Before we dive into the exciting world of deep learning, we need to prepare our data. It’s like setting up our art studio before creating a masterpiece. Let’s take a look at what we’re working with:\n\nt_indep = trn_df[indep_cols].values\nt_indep\n\narray([[ 1. , 22. ,  1. , ...,  1. ,  0. ,  0. ],\n       [ 3. , 38. ,  1. , ...,  1. ,  0. ,  0. ],\n       [ 2. , 26. ,  0. , ...,  1. ,  0. ,  0. ],\n       ...,\n       [ 2. , 28.8,  1. , ...,  1. ,  0. ,  0. ],\n       [ 1. , 26. ,  0. , ...,  1. ,  0. ,  0. ],\n       [ 1. , 32. ,  0. , ...,  1. ,  0. ,  0. ]])\n\n\nWhoa there! If you peek at t_indep, you’ll notice something funky. The age columns is partying way to hard compared to it’s friends. This could through our model for a loop, so let’s calm it down a bit\nNow, we could go old school and divide each column by its maximum value. It’s like telling your loudest friend to use their indoor voice. The formula would look something like this:\n\\(Normalized\\ value = \\frac{Feature\\ value} {Maximum\\ value\\ of\\ that\\ feature}\\)\nBut hey, we’re not here to play it safe! We’re going to use a cool trick called StandardScaler. It’s like giving each feature i’s own personal stylist. Here’s the magic behind it:\n\\(z = \\frac{x - \\mu}{\\sigma}\\)\nWhere:\n\n\\(x\\) is the original feature value.\n\\(\\mu\\) is the mean of the feature values.\n\\(\\sigma\\) is the standard deviation of the feature values.\n\\(z\\) is the standardized value.\n\nWhy StandardScaler, you ask? Well, it’s got some neat perks:\n\nIt handles outliers like a boss. Max scaling can sometimes squish all your other values when one outliers decides to go crazy.\nit’s a gradient descent’s best friend. when you’re dealing with neural networks, having features one a similar scale is like having a smooth road for your optimization on cruise on.\n\nLet’s wave our magic wand and see what happens:\n\nscaler = StandardScaler()\nt_indep = tensor(scaler.fit_transform(t_indep), dtype=torch.float)\n#dependent variable\nt_dep = tensor(trn_df['Survived'].values)\nt_indep\n\ntensor([[-0.7076, -0.5892,  0.4328, -0.4737, -0.8797,  0.0592, -1.2316,  ...,  0.9026, -0.3076,  0.6158, -0.3753,  0.7874, -0.4115,\n         -0.1591],\n        [ 1.2352,  0.6151,  0.4328, -0.4737,  1.3612,  0.0592, -1.2316,  ..., -1.1079, -0.3076, -1.6238, -0.3753,  0.7874, -0.4115,\n         -0.1591],\n        [ 0.2638, -0.2881, -0.4745, -0.4737, -0.7985, -0.5610,  0.8119,  ...,  0.9026, -0.3076,  0.6158, -0.3753,  0.7874, -0.4115,\n         -0.1591],\n        [ 1.2352,  0.3893,  0.4328, -0.4737,  1.0620,  0.0592, -1.2316,  ..., -1.1079, -0.3076,  0.6158, -0.3753,  0.7874, -0.4115,\n         -0.1591],\n        [-0.7076,  0.3893, -0.4745, -0.4737, -0.7842, -0.5610,  0.8119,  ...,  0.9026, -0.3076,  0.6158, -0.3753,  0.7874, -0.4115,\n         -0.1591],\n        [-0.7076,  0.2312, -0.4745, -0.4737, -0.7386, -0.5610,  0.8119,  ...,  0.9026,  3.2514, -1.6238, -0.3753,  0.7874, -0.4115,\n         -0.1591],\n        [-0.7076,  1.8194, -0.4745, -0.4737,  1.0381, -0.5610,  0.8119,  ..., -1.1079, -0.3076,  0.6158, -0.3753, -1.2700,  2.4304,\n         -0.1591],\n        ...,\n        [-0.7076, -0.3634, -0.4745, -0.4737, -0.9051, -0.5610,  0.8119,  ...,  0.9026, -0.3076,  0.6158, -0.3753,  0.7874, -0.4115,\n         -0.1591],\n        [ 1.2352,  0.6903, -0.4745,  5.7328,  0.4575,  2.5397, -1.2316,  ...,  0.9026,  3.2514, -1.6238, -0.3753,  0.7874, -0.4115,\n         -0.1591],\n        [ 3.1780, -0.2129, -0.4745, -0.4737, -0.3337, -0.5610,  0.8119,  ..., -1.1079, -0.3076,  0.6158, -0.3753,  0.7874, -0.4115,\n         -0.1591],\n        [ 0.2638, -0.8150, -0.4745, -0.4737,  0.4871, -0.5610,  0.8119,  ..., -1.1079, -0.3076,  0.6158,  2.6646, -1.2700, -0.4115,\n         -0.1591],\n        [ 0.2638, -0.0774,  0.4328,  2.0089,  0.2420,  1.2994, -1.2316,  ...,  0.9026, -0.3076,  0.6158, -0.3753,  0.7874, -0.4115,\n         -0.1591],\n        [-0.7076, -0.2881, -0.4745, -0.4737,  0.4871, -0.5610,  0.8119,  ..., -1.1079, -0.3076, -1.6238, -0.3753,  0.7874, -0.4115,\n         -0.1591],\n        [-0.7076,  0.1635, -0.4745, -0.4737, -0.8190, -0.5610,  0.8119,  ...,  0.9026,  3.2514, -1.6238, -0.3753,  0.7874, -0.4115,\n         -0.1591]])\n\n\nLook at that! Our t_indep is now looking sharp and ready for action.\nBut wait, there’s more! We’re going to split our data into a training set and a validation set. Sure, our dataset might be on the smaller side, but having a validation set is like having a trusty sidekick. It’s helps us keep an eye on our model’s performance and prevents it from getting too cocky (aka overfitting). We’ll use the RandomSplitter in the fastai library to split our dataset\n\ntorch.manual_seed(442)\nfrom fastai.data.transforms import RandomSplitter\ntrn_split, val_split = RandomSplitter(seed=42)(trn_df)\n\ntrn_indep, val_indep = t_indep[trn_split], t_indep[val_split]\ntrn_dep, val_dep = t_dep[trn_split], t_dep[val_split]\n\nAnd there you have it, folks! We’ve got everything we need to start our training montage. Get ready, because we’re about to embark on a wild ride through the world of deep learning. It’s gooing to be a fun one!"
  },
  {
    "objectID": "posts/titanic_competition/index.html#the-model-architecture-our-neural-blueprint",
    "href": "posts/titanic_competition/index.html#the-model-architecture-our-neural-blueprint",
    "title": "Sink or Swim: Navigating Deep Learning with the Titanic Competition",
    "section": "The Model Architecture: Our Neural Blueprint",
    "text": "The Model Architecture: Our Neural Blueprint\nFirst things first, let’s create a a function to initialize our coefficients. Think of this as laying the foundation for our neural masterpiece\n\ndef init_coeffs(n_input, n_hidden=10):\n    torch.manual_seed(442)\n    layer_1 = torch.randn(n_input, n_hidden) / np.sqrt(n_input)\n    layer_2 = torch.randn(n_hidden, 1) / np.sqrt(n_hidden)\n    const = torch.rand(1)[0]\n    return (layer_1.requires_grad_(), layer_2.requires_grad_(), const.requires_grad_())\n\nNow, you might wondering, “Why only one hidden layer with 10 units?” Well, my friends, sometimes less is more? With a small dataset like Titanic, a simple architecture often works best - compact but effective!\nBut wait, what’s with tat square root division? Great question! It’s all about keeping our neural network balanced. By dividing by the square root of inputs, we’re making sure our data doesn’t go haywire as it flows through the network. It’s like adding just the right amount of spice to your cooking - not too much - not too little!\nNext up, let’s write a function to calculate our predictions:\n\ndef calc_preds(coeffs, indep):\n    l1, l2, const = coeffs\n    res = F.relu(indep @ l1) \n    res = res@l2 + const\n    return torch.sigmoid(res)\n\nThis is where magic happens! We’re using matrix multiplication (that’s what the @ symbol does) and applying our good friend RelLU. Remember our chat about ReLU earlier? This is where it comes into play! If you want to dive deeper into the power of ReLU, check out my previous blog post here.\nNow, let’s talk about loss. No, not the kind you feel when MU loses, but the kind that tells use how well our model is doing:\n\ndef calc_loss(coeffs, indeps, deps):\n    return torch.abs(calc_preds(coeffs, indeps)-deps.float().unsqueeze(1)).mean()\n\nWe’re using mean absolute error here. It’s like measuring how far off our guess are from the real answers and taking the average. Simple, but effective!\nAlright, now for the main event - What happens in one epoch of training:\n\ndef one_epoch(coeffs, lr, batch_size=64):\n    n = len(trn_indep)\n    for i in range(0, n, batch_size):\n        batch_indep = trn_indep[i:i+batch_size]\n        batch_dep = trn_dep[i:i+batch_size]\n        \n        loss = calc_loss(coeffs, batch_indep, batch_dep)\n        loss.backward()\n\n        with torch.no_grad():\n            for layer in coeffs:\n                layer.sub_(layer.grad * lr)\n                layer.grad.zero_()\n\nThis is where we use the mini-batch technique. It’s like learning from a small group of examples at a time instead of trying to memorize the whole text book at one. It’s more efficient and helps our model learn better!\nFinally, let’s put it all together in our training function:\n\ndef train_model(epochs=300, lr=0.01):\n    torch.manual_seed(442)\n    coeffs = init_coeffs(trn_indep.shape[1])\n    best_val_loss = float(\"inf\")\n    patience = 20\n    counter = 0\n\n    for epoch in range(epochs):\n        one_epoch(coeffs, lr)\n        with torch.no_grad():\n            train_loss = calc_loss(coeffs, trn_indep, trn_dep)\n            val_loss = calc_loss(coeffs, val_indep, val_dep)\n        \n        if val_loss &lt; best_val_loss:\n            best_val_loss = val_loss\n            best_coeffs = [c.clone() for c in coeffs]\n            counter = 0\n        else:\n            counter += 1\n        \n        if counter &gt; patience:\n            print(f\"Early stopping at epoch {epoch}\")\n            break\n\n        if epoch % 5 == 0:\n            print(f\"Epoch {epoch}: Train Loss {train_loss:.4f}, Val Loss {val_loss:.4f}\")\n        lr *= 0.97  # learning rate schedule\n    return best_coeffs\n\nThis function is like the conductor of our neural orchestra. It initializes our model, trains it for a number of epochs, and learning rate scheduling (to help our model converge more precisely).\nAnd there you have it, folks! We’ve built a neural network from scratch. It might not look like much, but this little guy is ready to tackle the Titanic competition. In our next post, we’ll put it to the test and see how it performs."
  },
  {
    "objectID": "posts/titanic_competition/index.html#evaluation-and-submission-the-moment-of-truth",
    "href": "posts/titanic_competition/index.html#evaluation-and-submission-the-moment-of-truth",
    "title": "Sink or Swim: Navigating Deep Learning with the Titanic Competition",
    "section": "Evaluation and Submission: the Moment of Truth!",
    "text": "Evaluation and Submission: the Moment of Truth!\nAlright, folks, it’s time to put our homemade neural network to the test! Let’s see how well it can predict who survived the Titanic disaster.\nFirst, we need a way to measure our model’s performance. Here’s our accuracy function:\n\n# Evaluate the model\ndef acc(coeffs): \n    return (val_dep.bool() == (calc_preds(coeffs, val_indep) &gt; 0.5).squeeze()).float().mean()\n\nThis function compare our model’s predictions with the actual survival outcomes and calculates the percentage of correct guesses. Simple, but effective!\nNow, let’s train our model and see how it performs:\n\ncoeffs = train_model(epochs=50, lr=1) \nprint(f\"Validation Accuracy: {acc(coeffs):.4f}\")\n\nEpoch 0: Train Loss 0.3485, Val Loss 0.3620\nEpoch 5: Train Loss 0.1921, Val Loss 0.1934\nEpoch 10: Train Loss 0.1819, Val Loss 0.1850\nEpoch 15: Train Loss 0.1775, Val Loss 0.1805\nEpoch 20: Train Loss 0.1739, Val Loss 0.1783\nEpoch 25: Train Loss 0.1715, Val Loss 0.1771\nEpoch 30: Train Loss 0.1702, Val Loss 0.1759\nEpoch 35: Train Loss 0.1693, Val Loss 0.1750\nEpoch 40: Train Loss 0.1686, Val Loss 0.1744\nEpoch 45: Train Loss 0.1680, Val Loss 0.1741\nValidation Accuracy: 0.8315\n\n\nAnd the results are in!\nWell, well, well! look at that! our little neural network is showing some serious potential. We’re seeing the losses decrease over time for both our training and validation sets, which is exactly what we want. It means our model is learning!\nAnd that validation accuracy? 83.15%! Not too shabby for a model we built from scratch, right? In the world of the Titanic competition, that’s a pretty solid score.(not count those who use extra data to train their model, we all know that).\nBut the real test is yet to come. How will our model perform on the unseen test data? Let’s find out!\nAfter preprocessing our test data(just like we did with our trainding data), we make our predictions:\n\n## Load and preprocess test data\ntest_df = pd.read_csv(path / \"test.csv\")\ntest_df['Fare'] = test_df.Fare.fillna(0)\ntest_df = preprocess_data(test_df)\n\ntst_indep = torch.tensor(test_df[indep_cols].values, dtype=torch.float)\n# Scale test data using the same scaler\ntst_indep = torch.tensor(scaler.transform(tst_indep), dtype=torch.float)\n\n# Make predictions\ntest_df['Survived'] = (calc_preds(coeffs, tst_indep)&gt;0.5).int()\n\nLet’s take a quick look at our predictions:\n\nsub_df = test_df[['PassengerId', 'Survived']]\nprint(sub_df[\"Survived\"].sum())\nprint(sub_df[\"Survived\"].value_counts())\n\n149\nSurvived\n0    269\n1    149\nName: count, dtype: int64\n\n\nInteresting! Our model predicts that 149 passengers survived the Titanic disaster. This about 35.6% of the test set, which is pretty close to the actual survival rate of the Titanic (about 32%). It’s a good sign that our model isn’t wildly off in its predictions.\nNow for the moment of truth - submitting to Kaggle!\nif you have kaggle api you can use this command\nkaggle competitions submit -c titanic -f sub.csv -m \"submit to competition\"\nor you can upload the csv file manual in the competition submit page\nAFter submitting, we got anaccuracy of 0.77751% on the test set. That’s 77.75% accuracy on unseen data! For a first attempt with a model we built from scratch, that’s pretty impressive. We’re definitely in the right track!\nWhile 77.75% accuracy is great start, there’s always rooms for improvement. Let’s see if we can push our accuracy even higher."
  },
  {
    "objectID": "posts/titanic_competition/index.html#leveling-up-our-neural-network-the-pytorch-edition",
    "href": "posts/titanic_competition/index.html#leveling-up-our-neural-network-the-pytorch-edition",
    "title": "Sink or Swim: Navigating Deep Learning with the Titanic Competition",
    "section": "Leveling Up Our Neural Network: The Pytorch Edition!",
    "text": "Leveling Up Our Neural Network: The Pytorch Edition!\nAlright, Neural network enthusiasts! We’ve had some success with our homemade model, but now it’s time to kick things up a notch. We’re going to harness the power or Pytorch to create a slightly more sophisticated neural network. Buckle up, because this is where things get exciting!\nalready let’s write our new architecture.\n\nclass SimpleNN(torch.nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super(SimpleNN, self).__init__()\n        self.fc1 = torch.nn.Linear(input_size, hidden_size)\n        self.bn1 = torch.nn.BatchNorm1d(hidden_size)\n        self.fc2 = torch.nn.Linear(hidden_size, 1)\n    \n    def forward(self, x):\n        x = F.relu(self.bn1(self.fc1(x)))\n        x = torch.sigmoid(self.fc2(x))\n        return x\n\nThis little beauty is like our previous model’s cooler, more sophisticated cousin. It’s still simple, but it’s packing some extra punch:\n\nWe’ve got two fully connected layers (fc1 and fc2), just like before.\nBut wait, what’s that bn1? That’s batch normalization layer! it’s like traffic cop for our data, making sure everything flows smoothly between layers.\nWe’re still using ReLU and sigmoid activations, because hey, if it ain’t broke, don’t fix it!\n\n\nTraining Our New Model: The Pytorch Way\nNow, let’s look at our sniny new training function:\n\ndef train_model(model, X_train, y_train, X_val, y_val, epochs=300, lr=0.01, batch_size=32):\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=0.01)  # L2 regularization\n    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)  # Step LR schedule\n    \n    best_val_loss = float('inf')\n    patience = 20\n    counter = 0\n    \n    for epoch in range(epochs):\n        model.train()\n        for i in range(0, len(X_train), batch_size):\n            batch_X = X_train[i:i+batch_size]\n            batch_y = y_train[i:i+batch_size]\n            \n            optimizer.zero_grad()\n            outputs = model(batch_X)\n            loss = F.binary_cross_entropy(outputs, batch_y.unsqueeze(1))\n            loss.backward()\n            optimizer.step()\n        \n        scheduler.step()\n        \n        model.eval()\n        with torch.no_grad():\n            val_outputs = model(X_val)\n            val_loss = F.binary_cross_entropy(val_outputs, y_val.unsqueeze(1))\n            \n        if val_loss &lt; best_val_loss:\n            best_val_loss = val_loss\n            best_model = model.state_dict()\n            counter = 0\n        else:\n            counter += 1\n        \n        if counter &gt; patience:\n            print(f\"Early stopping at epoch {epoch}\")\n            break\n        \n        if epoch % 10 == 0:\n            print(f\"Epoch {epoch}: Train Loss {loss.item():.4f}, Val Loss {val_loss.item():.4f}\")\n    \n    model.load_state_dict(best_model)\n    return model\n\nThis function is like a personal trainer for our model. Here’s what’s new:\n\nWe’re using Adam optimizer - it’s like giving our model a smart personal coach that adapts the trainign intensity for each parameter and ’cause this post is a bit too long so i think i will explain Adam optimizer in the different blog post.\nWe’ve added a learning rate scheduler. It’s like adjusting the difficulty of our model’s workout every 50 epochs. Maybe i’ll explain this in another blog post too, it worth an blog post to talk about this.\nWe’re using binary cross-entropy loss now, which is perfect for our binary classification problem.\n\nNow, we have the forward pass:\noptimizer.zero_grad()\noutputs = model(batch_X)\nloss = F.binary_cross_entropy(outputs, batch_y.unsqueeze(1))\nloss.backward()\noptimizer.step()\nThis is where the real learning happens:\n\nWe clear out any leftover gradients with optimizer.zero_grad(). It’s like wiping the whiteboard clean before a new lesson.\noutputs = model(batch_X) it our model making it’s best guess based on what it know so far.\nWe calculate how wrong we were with loss = F.binary_cross_entropy(...). this is like getting our test results back\nFinally, optimizer.step() applies these adjustments. it’s like making notes on how to improve for the next test\n\nAfter each epoch, we adjust our learning rate:\nscheduler.step()\nthis is like adjusting the difficulty as we get better. We don’t want things to be too easy or too hard!\nLastly, we check how we’re doing on the validation set:\nmodel.eval()\nwith torch.no_grad():\n    val_outputs = model(X_val)\n    val_loss = F.binary_cross_entropy(val_outputs, y_val.unsqueeze(1))\nthis is just as before so nothing to explain here alright\nalright let’s put our new model through it’s paces:\n\n# Prepare the data\nscaler = StandardScaler()\nX = scaler.fit_transform(trn_df[indep_cols].values)\ny = trn_df['Survived'].values\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\nX_train = torch.FloatTensor(X_train)\ny_train = torch.FloatTensor(y_train)\nX_val = torch.FloatTensor(X_val)\ny_val = torch.FloatTensor(y_val)\n\n# Initialize and train the model\ninput_size = X_train.shape[1]\nhidden_size = 10\nmodel = SimpleNN(input_size, hidden_size)\n\ntrained_model = train_model(model, X_train, y_train, X_val, y_val, epochs=50, lr=0.02)\n\n# Evaluate the model\nmodel.eval()\nwith torch.no_grad():\n    val_outputs = model(X_val)\n    val_preds = (val_outputs &gt; 0.5).float()\n    accuracy = (val_preds.squeeze() == y_val).float().mean()\n    print(f\"Validation Accuracy: {accuracy.item():.4f}\")\n\nEpoch 0: Train Loss 0.3662, Val Loss 0.4901\nEpoch 10: Train Loss 0.2244, Val Loss 0.4254\nEpoch 20: Train Loss 0.2155, Val Loss 0.4271\nEarly stopping at epoch 29\nValidation Accuracy: 0.8268\n\n\nOur model decided to call it quits after just 29 epochs. Our validation accuracy is …82.68%? That’s actually a bit lower that our previous model. But don’t panic! Sometimes, a slightly lower validation accuracy can lead to better performance on unseen data. It’s like how sometimes taking it easyy in practice can lead to better performance in the big game.\nalright let submit to kaggle to see how our model performs on test set, shall we!\n\n## Load and preprocess test data\ntest_df = pd.read_csv(path / \"test.csv\")\ntest_df = preprocess_data(test_df)\n\n# Ensure test_df has all the necessary columns\nfor col in indep_cols:\n    if col not in test_df.columns:\n        test_df[col] = 0  # or some appropriate default value\n\nX_test = test_df[indep_cols].values\nX_test = scaler.transform(X_test)\nX_test = torch.FloatTensor(X_test)\n\ni just did as before\n\n# Make predictions on test set\nmodel.eval()\nwith torch.no_grad():\n    test_outputs = model(X_test)\n    test_preds = (test_outputs &gt; 0.5).int()\ntest_df[\"Survived\"] = test_preds\n\nsame thing here\n\nsub_df = test_df[['PassengerId', 'Survived']]\nprint(sub_df[\"Survived\"].sum())\nprint(sub_df[\"Survived\"].value_counts())\n\n149\nSurvived\n0    269\n1    149\nName: count, dtype: int64\n\n\nalright everythig seems good till now, let’s make our submit shall we?\n\nsub_df.to_csv(\"sub_ver3.csv\", index=False)\n\n\n!head sub_ver3.csv\n\nPassengerId,Survived\n892,0\n893,0\n894,0\n895,0\n896,1\n897,0\n898,1\n899,0\n900,1\n\n\nalright in the time i write this blog post i got this result \nwe got an accuracy of 0.78708! That’s 78.71%, which is a solid improvement over our previous score of 77.51%. We’ve climbed another rung on the Kaggle leaderboard ladder!\n\n\nWhat Have We Learned?\n\nSometimes, a more complex model (like our PyTorch version) can lead to better generalization, even if the validation accuracy is slightly lower.\nEarly stopping can prevent overfitting - our model knew when to quit while it was ahead.\nConsistency in predictions (149 survivors in both models) suggests we’re on the right track.\n\nRemember, in the world of machine learning as well as deep learning, there’s always room for improvement. So keep experimenting, keep learning, and who knows? Maybe you’ll be the one to finally crack the Titanic code and reach that coveted top spot on the Kaggle leaderboard!\nUntil next time, happy modeling, and may the gradients be ever in your favor!"
  },
  {
    "objectID": "posts/2024-07-28-from-neuron-to-gradient/index.html",
    "href": "posts/2024-07-28-from-neuron-to-gradient/index.html",
    "title": "From Neurons to Gradients: Unpacking FastAI Lesson 3",
    "section": "",
    "text": "Hey there, fellow data adventurers! 👋 It’s been a couple of week since my last post - blame exams and and obsessive quest to tweak every configuration setting for my workflow (which is turned into a week-long habit hole - i regret nothing). But today, I’m excited to dive back into the world of AI and share my latest escapades from Lesson 3 of the FastAI course taught by the indomitable Jeremy Horawd. Spoiler alert: it’s packed with enough neural wonders to make your brain do a happy dance.🕺\nIn the coming post, I’ll guide you through:\n\nPicking of right AI model that’s just right for you\nDissecting the anatomy of these models (paramedics not required)🧬\nThe inner workings of neuron networks 🧠\nThe Titanic competition\n\nSo, hold onto your neural nets and let’s jump right into it, shall we?"
  },
  {
    "objectID": "posts/2024-07-28-from-neuron-to-gradient/index.html#introduction",
    "href": "posts/2024-07-28-from-neuron-to-gradient/index.html#introduction",
    "title": "From Neurons to Gradients: Unpacking FastAI Lesson 3",
    "section": "",
    "text": "Hey there, fellow data adventurers! 👋 It’s been a couple of week since my last post - blame exams and and obsessive quest to tweak every configuration setting for my workflow (which is turned into a week-long habit hole - i regret nothing). But today, I’m excited to dive back into the world of AI and share my latest escapades from Lesson 3 of the FastAI course taught by the indomitable Jeremy Horawd. Spoiler alert: it’s packed with enough neural wonders to make your brain do a happy dance.🕺\nIn the coming post, I’ll guide you through:\n\nPicking of right AI model that’s just right for you\nDissecting the anatomy of these models (paramedics not required)🧬\nThe inner workings of neuron networks 🧠\nThe Titanic competition\n\nSo, hold onto your neural nets and let’s jump right into it, shall we?"
  },
  {
    "objectID": "posts/2024-07-28-from-neuron-to-gradient/index.html#choosing-the-right-model-a-guide-to-navigating-the-neural-jungle",
    "href": "posts/2024-07-28-from-neuron-to-gradient/index.html#choosing-the-right-model-a-guide-to-navigating-the-neural-jungle",
    "title": "From Neurons to Gradients: Unpacking FastAI Lesson 3",
    "section": "Choosing the Right Model: A Guide to Navigating the Neural Jungle",
    "text": "Choosing the Right Model: A Guide to Navigating the Neural Jungle\nAlright folks, buckle up! We’re diving into the exhilarating world of choosing the perfect image model. It’s like shopping for a new gadget: you want something sleek, efficient, and most importantly - something that get the job done without draining your power (or breaking the bank),\nI’m going to guide you through the hands-on example to illustrate the difference between two popular image models. So, let’s play around with training a pet detector model, shall we?\nFirst things first, let’s get our setup ready:\n\nfrom fastai.vision.all import *\nimport timm\n\n\npath = untar_data(URLs.PETS)/'images'\n\n\ndls = ImageDataLoaders.from_name_func(\n    \".\",\n    get_image_files(path),\n    valid_pct=0.2,\n    seed=42,\n    label_func=RegexLabeller(pat=r'^([^/]+)_\\d+'),\n    item_tfms=Resize(224)\n)\n\n\n\n\n\n\n    \n      \n      100.00% [811712512/811706944 01:13&lt;00:00]\n    \n    \n\n\nLemme break down what’s happening here. We’re using The Oxford-IIIT Pet dataset, fetched with a nifty little URL constant provide by FastAI. If you’re staring at the pattern pat=r'^([^/]+)\\_\\d+' like it’s some alien script, fear not! It’s just a regular expression used to extract label from filenames using fastai RegexLabeller\nHere’s the cheat sheet for the pattern:\n\n^ asserts the start of a string.\n([^/]+) matches one or more characters that are not forward slash and captures them as a group.\n_ matches an underscore.\n\\d+ matches one ore more digits.\n\nNow, let’s visualize our data:\n\ndls.show_batch(max_n=4)\n\n\n\n\n\n\n\n\nAnd, it’s training time! We start with a ResNet34 architecture:\n\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(3)\n\nDownloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n\n100%|██████████| 83.3M/83.3M [00:00&lt;00:00, 147MB/s] \n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.491942\n0.334319\n0.105548\n00:26\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.454661\n0.367568\n0.112991\n00:32\n\n\n1\n0.272869\n0.274704\n0.081867\n00:33\n\n\n2\n0.144361\n0.246424\n0.073072\n00:33\n\n\n\n\n\nAlright after about 2 minutes, we hit a 7% error rate. Not too shabby! But here’s the catch: ResNet34 is like the reliable old family car of neuron networks - good but not the fastest on the block. to spice thing up, we need to find a better, more turbo-changed model🏎️\n\nTime to Upgrade: Diving into the Model Jungle\nThere are zillion architectures in the Pytorch image model library - ok, maybe not a zillion, but a lot! Most are mathematical functions like RELUs(Rectified Linear Units), which we’ll get into shortly. So, which model should we choose? It boil down to three things:\n\nSpeed\nMemory Usage\nAccuracy\n\n\n\nThe “Which Image Model is Best?” Notebook\nCheck out this gem by Jeremy Howard: Which image models are best. It’s a treasure trove for finding the perfect architecture, and i highly recommend you go to his notebook read it and you should totally upvote it because Jeremy rocks.\ni just copy the plot into here for so you can look at that quickly (but remember to give him a upvote).\n\nPlotly = require('https://cdn.plot.ly/plotly-latest.min.js');\ndf_results = FileAttachment(\"results-imagenet.csv\").csv()\ndf = FileAttachment(\"benchmark-infer-amp-nhwc-pt111-cu113-rtx3090.csv\").csv()\n\ndf_merged = {\n  let df_results_processed = df_results.map(r =&gt; ({ ...r, model_org: r.model, model: r.model.split('.')[0] }));\n\n  const dfColumns = Object.keys(df[0]);\n  const dfResultsColumns = Object.keys(df_results_processed[0]);\n\n  return df.flatMap(d =&gt; {\n    const matches = df_results_processed.filter(r =&gt; r.model === d.model);\n    return matches.map(match =&gt; {\n      let mergedRow = {};\n      dfColumns.forEach(col =&gt; {\n        if (dfResultsColumns.includes(col) && col !== 'model') { mergedRow[`${col}_x`] = d[col]; } else { mergedRow[col] = d[col]; }\n      });\n      dfResultsColumns.forEach(col =&gt; {\n        if (dfColumns.includes(col) && col !== 'model') { mergedRow[`${col}_y`] = match[col]; } else { mergedRow[col] = match[col]; }\n      });\n      return mergedRow;\n    });\n  });\n}\n\ndf_final = df_merged\n  .map(d =&gt; ({...d, secs: 1 / Number(d.infer_samples_per_sec)}))\n  .map(d =&gt; {\n    const familyMatch = d.model.match(/^([a-z]+?(?:v2)?)(?:\\d|_|$)/);\n    let family = familyMatch ? familyMatch[1] : '';\n    if (d.model.includes('in22')) family += '_in22';\n    if (d.model.match(/resnet.*d/)) family += 'd';\n    return {...d, family: family};\n  })\n  .filter(d =&gt; !d.model.endsWith('gn'))\n  .filter(d =&gt; /^re[sg]netd?|beit|convnext|levit|efficient|vit|vgg|swin/.test(d.family));\n{\n  const uniqueFamilies = [...new Set(df_final.map(d =&gt; d.family))];\n  const colorScale = uniqueFamilies.map((family, index) =&gt; { return `hsl(${index * 360 / uniqueFamilies.length}, 70%, 50%)`; });\n  const traces = uniqueFamilies.map((family, index) =&gt; {\n    const familyData = df_final.filter(d =&gt; d.family === family);\n    return {\n      name: family,\n      x: familyData.map(d =&gt; d.secs),\n      y: familyData.map(d =&gt; Number(d.top1)),\n      mode: 'markers',\n      type: 'scatter',\n      marker: { size: familyData.map(d =&gt; Math.pow(Number(d.infer_img_size), 2) / 5700), color: colorScale[index], },\n      text: familyData.map(d =&gt; `${d.model}&lt;br&gt; family=${d.family}&lt;br&gt; secs=${d.secs.toFixed(8)}&lt;br&gt; top1=${Number(d.top1).toFixed(3)}&lt;br&gt; size=${d.param_count_x}&lt;br&gt; infer_img_size=${d.infer_img_size}`),\n      hoverinfo: 'text',\n      hoverlabel: { bgcolor: colorScale[index] }\n    };\n  });\n  const layout = {\n    title: 'Inference',\n    width: 1000,\n    height: 800,\n    xaxis: { title: 'secs', type: 'log', autorange: true, gridcolor: 'rgb(233,233,233)', },\n    yaxis: { title: 'top1', range: [65, 90], gridcolor: 'rgb(233,233,233)', },\n    plot_bgcolor: 'rgb(240,240,255)',\n    showlegend: true,\n    legend: { title: {text: 'family'}, itemclick: 'toggle', itemdoubleclick: 'toggleothers' },\n    hovermode: 'closest'\n  };\n  const config = {responsive: true};\n  const plot = DOM.element('div');\n  Plotly.newPlot(plot, traces, layout, config);\n  return plot;\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere’s the plot breakdown from the notebook:\n\nThe X-axis shows seconds per sample(how fast it is) - to the left is better.\nThe Y-axis shows accuracy - higher is better.\n\nIdeally, you want models that hover around the top left corner. We often use ResNet34 because it’s like the comfortable pair of jeans everyone swears by. But it’s not the cutting-edge model anymore. Let’s explore something better: ConvNeXT models! 🎉\nFirst, make sure you install the timm via pip or conda:\npip install timm\nor\nconda install timm\nThen, let’s search for all the ConvNext models:\n\ntimm.list_models(\"convnext*\")\n\n\n\n['convnext_atto',\n 'convnext_atto_ols',\n 'convnext_base',\n 'convnext_femto',\n 'convnext_femto_ols',\n 'convnext_large',\n 'convnext_large_mlp',\n 'convnext_nano',\n 'convnext_nano_ols',\n 'convnext_pico',\n 'convnext_pico_ols',\n 'convnext_small',\n 'convnext_tiny',\n 'convnext_tiny_hnf',\n 'convnext_xlarge',\n 'convnext_xxlarge',\n 'convnextv2_atto',\n 'convnextv2_base',\n 'convnextv2_femto',\n 'convnextv2_huge',\n 'convnextv2_large',\n 'convnextv2_nano',\n 'convnextv2_pico',\n 'convnextv2_small',\n 'convnextv2_tiny']\n\n\nFound one? Awesome! Now, let’s put it to the test. We’ll specify the architecture as a string when we call vision_learner, Why previous time when we use ResNet34 we don’t need to pass it as string? you say! That’s because ResNet34 was built in fastai library so you just need to call it but with ConvNext you have to pass the arch as a string for it to work, alright let’s see what it look like:\n\narch = 'convnext_tiny.fb_in22k'\nlearn = vision_learner(dls, arch, metrics=error_rate).to_fp16()\nlearn.fine_tune(3)\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.123377\n0.240116\n0.081191\n00:27\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.260218\n0.225793\n0.071719\n00:34\n\n\n1\n0.199426\n0.169573\n0.059540\n00:33\n\n\n2\n0.132157\n0.166686\n0.056834\n00:33\n\n\n\n\n\n\n\nResults Are In!\nTraining time goes up a little bit like 3, 4 seconds. But, and here’s the kicker - the accuracy jumps from 7.3% error down to 5.6%!🚀\nThe model names might looks cryptic. Here’s the decoder ring:\n\nTiny, small, large. etc.: Size and resource demands.\nfb_in22k: Trained on ImageNet dataset with 22,000 image categories by Facebook AI Research(FAIR)\n\nThese ConvNeXT models generally outperform others in terms of accuracy on standard photos of natural objects. So, there you have it! We’ve seen how to choose and implement a better architecture for your image models. Remember, it’s all about finding the right balance between speed, memory, and accuracy. Stay tuned, as we’ll tackle deeper intricacies of neural networks next 🎢"
  },
  {
    "objectID": "posts/2024-07-28-from-neuron-to-gradient/index.html#whats-in-the-model",
    "href": "posts/2024-07-28-from-neuron-to-gradient/index.html#whats-in-the-model",
    "title": "From Neurons to Gradients: Unpacking FastAI Lesson 3",
    "section": "What’s in the Model?",
    "text": "What’s in the Model?\nAlright, you see? Our model did better, right? Now, you’ve probably wondering, how do we turn this awesome piece of neural magic into an actual application? They key is to save the trained model so that users won’t have to wait for the training time.\nTo do that, we export our learner with the following command, creating a magical file called model.pkl:\n\nlearn.export('model.pkl')\n\nFor those of you who’ve stuck around through my previous blog posts, you’ll remember that when i deploy an application on HuggingFace Spaces, I simply load this model.pkl file. This way, the learner operates almost exactly like the trained learn object, but it’s really instantly means no more waiting for eons!\nNow, you might be scratching your head, wondering. “what did we just do exactly? What inside this model.pkl file?”\n\nDissecting the model.pkl File\nAlright, grab your virtual scalpel, because we’re doing some model surgery! The model.pkl file is essentially a saved learner, and it houses two main things:\n\nPre-processing Steps: This includes all the steps needed to turn your raw images into something model can understand. Essentially, this is the information your DataLoaders(dls), DataBlock, or any other pre-processing pipeline you’ve set up.\nThe Trained Model: This is the most crucial part - a model that has been trained and is ready to make predictions.\n\nTo peek inside, we can load the model back up and check it out:\n\nm = learn.model\nm\n\n\n\nSequential(\n  (0): TimmBody(\n    (model): ConvNeXt(\n      (stem): Sequential(\n        (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n        (1): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n      )\n      (stages): Sequential(\n        (0): ConvNeXtStage(\n          (downsample): Identity()\n          (blocks): Sequential(\n            (0): ConvNeXtBlock(\n              (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n              (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=96, out_features=384, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (norm): Identity()\n                (fc2): Linear(in_features=384, out_features=96, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n            (1): ConvNeXtBlock(\n              (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n              (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=96, out_features=384, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (norm): Identity()\n                (fc2): Linear(in_features=384, out_features=96, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n            (2): ConvNeXtBlock(\n              (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n              (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=96, out_features=384, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (norm): Identity()\n                (fc2): Linear(in_features=384, out_features=96, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n          )\n        )\n        (1): ConvNeXtStage(\n          (downsample): Sequential(\n            (0): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n            (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n          )\n          (blocks): Sequential(\n            (0): ConvNeXtBlock(\n              (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n              (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=192, out_features=768, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (norm): Identity()\n                (fc2): Linear(in_features=768, out_features=192, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n            (1): ConvNeXtBlock(\n              (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n              (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=192, out_features=768, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (norm): Identity()\n                (fc2): Linear(in_features=768, out_features=192, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n            (2): ConvNeXtBlock(\n              (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n              (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=192, out_features=768, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (norm): Identity()\n                (fc2): Linear(in_features=768, out_features=192, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n          )\n        )\n        (2): ConvNeXtStage(\n          (downsample): Sequential(\n            (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n            (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n          )\n          (blocks): Sequential(\n            (0): ConvNeXtBlock(\n              (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n              (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (norm): Identity()\n                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n            (1): ConvNeXtBlock(\n              (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n              (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (norm): Identity()\n                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n            (2): ConvNeXtBlock(\n              (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n              (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (norm): Identity()\n                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n            (3): ConvNeXtBlock(\n              (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n              (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (norm): Identity()\n                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n            (4): ConvNeXtBlock(\n              (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n              (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (norm): Identity()\n                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n            (5): ConvNeXtBlock(\n              (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n              (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (norm): Identity()\n                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n            (6): ConvNeXtBlock(\n              (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n              (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (norm): Identity()\n                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n            (7): ConvNeXtBlock(\n              (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n              (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (norm): Identity()\n                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n            (8): ConvNeXtBlock(\n              (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n              (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (norm): Identity()\n                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n          )\n        )\n        (3): ConvNeXtStage(\n          (downsample): Sequential(\n            (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n            (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n          )\n          (blocks): Sequential(\n            (0): ConvNeXtBlock(\n              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (norm): Identity()\n                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n            (1): ConvNeXtBlock(\n              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (norm): Identity()\n                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n            (2): ConvNeXtBlock(\n              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n              (mlp): Mlp(\n                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n                (act): GELU()\n                (drop1): Dropout(p=0.0, inplace=False)\n                (norm): Identity()\n                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n                (drop2): Dropout(p=0.0, inplace=False)\n              )\n              (shortcut): Identity()\n              (drop_path): Identity()\n            )\n          )\n        )\n      )\n      (norm_pre): Identity()\n      (head): NormMlpClassifierHead(\n        (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n        (norm): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n        (flatten): Flatten(start_dim=1, end_dim=-1)\n        (pre_logits): Identity()\n        (drop): Dropout(p=0.0, inplace=False)\n        (fc): Identity()\n      )\n    )\n  )\n  (1): Sequential(\n    (0): AdaptiveConcatPool2d(\n      (ap): AdaptiveAvgPool2d(output_size=1)\n      (mp): AdaptiveMaxPool2d(output_size=1)\n    )\n    (1): fastai.layers.Flatten(full=False)\n    (2): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): Dropout(p=0.25, inplace=False)\n    (4): Linear(in_features=1536, out_features=512, bias=False)\n    (5): ReLU(inplace=True)\n    (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (7): Dropout(p=0.5, inplace=False)\n    (8): Linear(in_features=512, out_features=37, bias=False)\n  )\n)\n\n\n\n\nWhat’s All This Stuff?\nAlright, there’s a lot to digest here. Basically, the model is structured in layers upon layers. Here’s the breakdown:\nTimmBody: this contains most of the model architecture. Inside the TimmBody. you’ll find:\n\nModel: The main model components.\nStem: The initial layers that process the raw input.\nStages: There are further broken down into multiple blocks, each packed with convolutional layers. normalization layers, and more.\n\n\n\nLet’s Peek Inside a Layer\nTo dig deeper into what these layers contain, you can use a really convenient Pytorch method called get_submodule:\n\nl = m.get_submodule('0.model.stem.1')\nl\n\nLayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n\n\nAs you can see it return a LayerNorm2d layer. Wondering what this LayerNorm2d thing is all about? It comprises a mathematical function for normalization and bunch of parameters:\n\nprint(list(l.parameters()))\n\n\n\n[Parameter containing:\ntensor([ 1.2546e+00,  1.9191e+00,  1.2191e+00,  1.0385e+00, -3.7148e-04,\n         7.6571e-01,  8.8668e-01,  1.6324e+00,  7.0477e-01,  3.2892e+00,\n         7.8641e-01, -1.7453e-03,  1.0006e+00, -2.0514e-03,  3.2976e+00,\n        -1.2112e-03,  1.9842e+00,  1.0206e+00,  4.4522e+00,  2.5476e-01,\n         2.7248e+00,  9.2616e-01,  1.2374e+00,  4.3668e-03,  1.7875e+00,\n         5.4292e-01,  4.6268e+00,  1.1599e-02, -5.4437e-04,  3.4510e+00,\n         1.3520e+00,  4.1267e+00,  2.6876e+00,  4.1197e+00,  3.4007e+00,\n         8.5053e-01,  7.3569e-01,  3.9801e+00,  1.2851e+00,  6.3985e-01,\n         2.6897e+00,  1.1181e+00,  1.1699e+00,  5.5318e-01,  2.3341e+00,\n        -3.0504e-04,  9.7000e-01,  2.3409e-03,  1.1984e+00,  1.7897e+00,\n         4.0138e-01,  4.5116e-01,  9.7186e-01,  3.9881e+00,  6.5935e-01,\n         6.8778e-01,  9.8614e-01,  2.7053e+00,  1.2169e+00,  7.6268e-01,\n         3.3019e+00,  1.6200e+00,  9.5547e-01,  2.1216e+00,  6.2951e-01,\n         4.0349e+00,  8.9246e-01, -2.9147e-03,  4.0874e+00,  1.0639e+00,\n         1.3963e+00,  1.6683e+00,  4.6571e-04,  7.6833e-01,  8.8542e-01,\n         6.4305e-01,  1.3443e+00,  7.1566e-01,  5.4763e-01,  2.0902e+00,\n         1.1952e+00,  3.0668e-01,  2.9682e-01,  1.4709e+00,  4.0830e+00,\n        -7.8233e-04,  1.1455e+00,  3.8835e+00,  3.5997e+00,  4.8206e-01,\n         2.1703e-01, -1.6550e-04,  6.4791e-01,  3.0069e+00,  3.0463e+00,\n         4.6374e-03], device='cuda:0', requires_grad=True), Parameter containing:\ntensor([-9.8183e-02, -4.0191e-02,  4.1647e+00, -8.9313e-03,  3.7929e-03,\n        -2.7139e-02, -3.1174e-02, -7.9865e-02, -1.4053e-01, -6.3492e-02,\n         3.2160e-01, -3.3837e-01, -5.6851e-02, -4.0384e-03, -4.7630e-02,\n        -2.6376e-02, -4.0858e-02, -4.0886e-02,  8.7548e-03, -2.4149e-02,\n         8.5088e-03, -1.6333e-01, -4.0154e+00,  5.2989e-01, -5.3410e-01,\n         2.8046e+00,  3.5663e-02, -1.0321e-02, -1.1255e-03, -1.1721e-01,\n        -1.3768e-01,  1.8840e-02, -9.5614e-02, -1.3149e-01, -1.9291e-01,\n        -6.8939e-02, -3.6672e-02, -1.2902e-01,  1.5387e-01,  3.6398e-03,\n        -6.6185e-02,  5.8841e-02, -9.1987e-02, -1.1453e+00, -5.4502e-02,\n        -5.3649e-03, -1.8238e-01,  2.3167e-02,  3.8862e-02, -5.9394e-02,\n        -4.1380e-02, -5.6917e-02, -4.3903e-02, -1.2954e-02, -1.1092e-01,\n         7.0337e-03, -3.9300e-02, -1.5816e-01, -9.8132e-02, -1.8553e-01,\n        -1.1112e-01, -1.8186e-01, -3.4278e-02, -2.6474e-02,  1.4192e+00,\n        -3.1935e-02, -4.3245e-02, -2.7030e-01, -4.6695e-02, -6.4756e-04,\n         2.6561e-01,  1.8779e-01,  6.9716e-01, -3.0647e-01,  8.1973e-02,\n        -1.0845e+00,  1.4999e-02, -4.4244e-02, -8.0861e-02, -6.8972e-02,\n        -1.3070e-01, -1.7093e-02, -1.9623e-02, -3.9345e-02, -6.9878e-02,\n         1.2335e-02, -5.9947e-02, -3.5691e-02, -7.9831e-02, -7.4387e-02,\n        -9.5232e-03, -3.7763e-01, -1.1987e-02, -2.5113e-02, -6.2690e-02,\n        -3.0666e-04], device='cuda:0', requires_grad=True)]\n\n\nAnother example: Let’s inspect a layer deeper inside:\n\nl = m.get_submodule('0.model.stages.0.blocks.1.mlp.fc1')\nprint(l)\nprint(list(l.parameters()))\n\n\n\nLinear(in_features=96, out_features=384, bias=True)\n[Parameter containing:\ntensor([[ 0.0227, -0.0014,  0.0404,  ...,  0.0016, -0.0453,  0.0083],\n        [-0.1439,  0.0169,  0.0261,  ...,  0.0126, -0.1044,  0.0565],\n        [-0.0655, -0.0327,  0.0056,  ..., -0.0414,  0.0659, -0.0401],\n        ...,\n        [-0.0089,  0.0699,  0.0003,  ...,  0.0040,  0.0415, -0.0191],\n        [ 0.0019,  0.0321,  0.0297,  ..., -0.0299, -0.0304,  0.0555],\n        [ 0.1211, -0.0355, -0.0045,  ..., -0.0062,  0.0240, -0.0114]],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([-0.4049, -0.7419, -0.4234, -0.1651, -0.3027, -0.1899, -0.5534, -0.6270,\n        -0.3008, -0.4253, -0.5996, -0.4107, -0.2173, -1.7935, -0.3170, -0.1163,\n        -0.4483, -0.2847, -0.4343, -0.4945, -0.4064, -1.1403, -0.6754, -1.7236,\n        -0.2954, -0.2655, -0.2188, -0.3913, -0.4148, -0.4771,  0.2366, -0.7542,\n        -0.5851, -0.1821, -1.5273, -0.3625, -2.4688, -2.3461, -0.6110, -0.4114,\n        -0.6963, -0.5764, -0.5878, -0.0318, -2.0354, -0.2859, -0.3954, -0.8404,\n        -2.2399, -1.0874, -0.2296, -0.9002, -0.7585, -0.8834, -0.3753, -0.4548,\n        -0.3836, -0.4048, -2.0231, -1.0264, -0.4106, -1.1566, -0.2225, -0.4251,\n        -0.2496, -0.4224, -0.0975, -1.4017, -0.6887, -0.4370, -0.2931, -0.4643,\n        -0.4959, -1.2535, -1.0720, -1.2966, -0.6276, -1.4162, -2.3081, -2.4540,\n        -0.4258, -0.9987, -0.4638, -0.3147, -0.2417, -0.8744, -0.2828, -1.4208,\n        -0.3257, -0.3202, -0.0603, -0.1894, -0.2496, -0.6130, -0.2975, -2.1466,\n        -0.4129, -0.3677, -1.9813, -0.3814, -0.3785, -0.2294, -0.3698, -0.3256,\n        -0.5585, -2.4192, -0.4589, -1.7748, -0.3995, -0.4092, -0.3517, -0.5331,\n        -1.6535, -1.8190,  0.6264, -0.4059,  0.5873, -2.2074, -0.2438, -2.4539,\n        -0.2283, -0.6865,  0.6988,  0.6476, -0.6445, -0.3452, -0.3276, -0.5700,\n        -0.5173, -0.2775, -0.4089, -0.3020, -0.4872, -0.4952, -0.4072, -0.4356,\n        -0.5102, -0.4128, -2.0918, -0.2826, -0.5830, -1.5835,  0.6139, -0.8504,\n        -0.4669, -2.1358, -0.3418, -0.3767, -0.3345, -0.3960, -0.3886, -0.5667,\n        -0.2225, -1.3059, -0.4600, -0.3927, -0.4667, -0.4214, -0.4755, -0.2866,\n        -1.5805, -0.1787, -0.4367, -0.3172,  1.5731, -0.4046, -0.4838, -0.2576,\n        -0.5612, -0.4264, -0.2578, -0.3175, -0.4620, -1.9552, -1.9145, -0.3960,\n         0.3988, -2.3519, -0.9688, -0.2831, -1.9001, -0.4180,  0.0159, -1.1109,\n        -0.4921, -0.3177, -1.8909, -0.3101, -0.8136, -2.3345, -0.3845, -0.3847,\n        -0.1974, -0.4445, -1.6233, -2.5485, -0.3176, -1.2715, -1.1479,  0.6149,\n        -0.3748, -0.3949, -2.0747, -0.4657, -0.3780, -0.4957, -0.3282, -1.9219,\n        -2.0019, -0.5307, -0.2554, -1.1160, -0.3517, -2.2185, -1.1393,  0.5364,\n        -0.3217, -2.0389, -0.4655,  0.1850, -0.5830, -0.3128,  0.6180, -0.2125,\n        -2.3538, -0.9699, -0.9785, -0.3667, -0.4502, -1.9564, -0.2662, -1.1755,\n        -0.4198, -0.9024, -0.3605, -0.5172, -1.1879, -0.4190, -0.4770, -1.5560,\n        -0.4011, -0.6518, -0.4818, -0.2423,  0.6909, -0.5081, -0.4304, -0.6068,\n        -0.4000, -0.3329, -0.3596, -1.6108, -0.2371, -0.2467, -0.4545,  0.1807,\n        -0.3227, -0.3918, -0.3515, -0.3755, -1.2178, -0.3999, -0.3578, -0.2882,\n        -1.7483, -0.2363, -0.1599, -0.2640, -0.9769, -1.3065, -0.4148, -0.2663,\n        -0.3933, -0.4627, -0.2174,  0.2140, -0.5733, -0.2766, -0.3659, -0.5172,\n        -0.3484, -0.3362, -0.6445,  0.6866, -0.3738, -0.2902, -2.0863, -0.4882,\n        -0.2597, -1.0496, -1.6616, -0.3398, -0.5111, -0.5659, -0.3027, -0.5048,\n        -0.2877, -0.2841, -0.1982, -0.6910, -0.2873, -2.1121, -0.8927, -0.2301,\n        -1.5013, -0.4734, -2.2292, -0.4022, -0.2926, -0.4199,  0.6646, -0.3047,\n        -0.1688, -0.3749, -0.6433, -2.3348, -0.3101, -1.2730, -0.8193, -1.0593,\n        -0.0934, -1.6387,  0.3426, -0.8484, -0.4910, -0.5001, -1.0631, -0.3534,\n        -1.1564, -0.3842, -0.3172, -0.6432, -0.9083, -0.6567, -0.6490,  0.6337,\n        -0.2662, -1.3202, -1.1623, -1.2032, -2.0577, -0.3001, -1.3596, -0.4612,\n        -0.5024, -0.4950, -0.3156, -0.3272, -0.2669, -0.4279, -0.3296, -0.3011,\n        -1.6635,  0.6434, -0.9455,  0.6099, -0.4234,  0.3917, -0.4944, -0.4284,\n        -0.2587, -0.4952, -2.1991, -0.2601, -0.3934, -0.4565, -0.5816, -0.3487,\n        -0.7372, -0.3589, -0.4894, -2.0105,  0.4557, -0.8055, -1.7748, -0.3512,\n        -0.5359, -0.2101, -0.3955, -0.4782, -1.1457, -0.3974, -2.2115, -0.2838],\n       device='cuda:0', requires_grad=True)]\n\n\nWhat do these numbers mean? you say! These are the learned parameters of the model essentially, the weights that have been optimized during training. They’re the secret sauce that allows the model to identify whether an image is a basset hound, a tabby cat, or anything else.\nNext up, we will explore how neural networks really work under the hood. We’ll unravel the mysterious that turn these parameters into powerful predictions."
  },
  {
    "objectID": "posts/2024-07-28-from-neuron-to-gradient/index.html#how-neural-networks-really-work---the-magic-unveiled",
    "href": "posts/2024-07-28-from-neuron-to-gradient/index.html#how-neural-networks-really-work---the-magic-unveiled",
    "title": "From Neurons to Gradients: Unpacking FastAI Lesson 3",
    "section": "How Neural Networks Really Work - The Magic Unveiled!🧙‍♂️",
    "text": "How Neural Networks Really Work - The Magic Unveiled!🧙‍♂️\nTo answer the burning question from before, let’s dive into the marvels of neural networks. Yes, Jeremy Howard has an amazing notebook called “How does a neural net really work?” that’s perfect for beginners. But, I’m here to give you a walkthrough with a dash of humor!\nMachine learning models are like very smart shape-fitting artists. They find pattern in data and learn to recognize them. We’ll start simple - with a quadratic function. Let’s see how it all works:\n\n\nCode\nimport plotly\nimport plotly.express as px\nimport torch\nimport numpy as np\nfrom IPython.display import display, HTML\n\n # Tomas Mazak's workaround for MathJax in VSCode\nplotly.offline.init_notebook_mode()\ndisplay(HTML(\n    '&lt;script type=\"text/javascript\" async src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_SVG\"&gt;&lt;/script&gt;'\n)) \ndef plot_function(f, title=None, min=-2.1, max=2.1):\n    x = torch.linspace(min, max, steps=100)\n    y = f(x)\n    return px.line(x=x, y=y, title=title)\n\n\n\ndef f(x): return 3 * x**2 + 2 * x + 1\nplot_function(f, title=r\"$3x^2 + 2x + 1$\")\n\n\nfunction f(x, a, b, c) { return a*x**2 + b*x + c }\nx = { return Array.from({ length: 40 }, (_, i) =&gt; -2 + (i * (2 - (-2)) / (40 - 1))); }\ny = x.map(element =&gt; f(element, a, b, c))\n{\n  var trace1 = { x: x, y: y, mode: 'lines', name: 'quadratic'};\n  var data = [trace1];\n  var layout = { title: \"3x²+ 2x + 1\", xaxis: { title: 'x', zeroline: false, }, yaxis: { title: \"y\", } };\n  const div = DOM.element('div');\n  Plotly.newPlot(div, data, layout);\n  return div;\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat we want to do here is simple: imageine we don’t know the true mathematical function, and we’re trying to recreate it from some data. This is easier than trying to figure out if an image contains a basset hound or your grandma’s cat. Here’s the real function, and we’re going to try to mimic it using lot’s of different quadratic equations.\nCreating Quadratics on Demand\nIn Python, this magical thing called partial allows us to fix some values of a function and crete variation of it. It’s like having a playlist of your favorite songs, but you can change he lyric any time!\n\nfrom functools import partial\n\ndef quad(a, b, c, x): return a * x**2 + b * x + c\ndef mkquad(a, b, c): return partial(quad, a, b, c)\n\n\nIntroducing Noise\nIn real life, data never fits perfectly to a function. There’s always some noise, it’s often as messy and unpredictable as a doctor’s illegible handwriting. Let’s add some noise to our data:\n\ndef noise(x, scale): return np.random.normal(scale=scale, size=x.shape)\ndef add_noise(x, mult, add): return x * (1 + noise(x, mult)) + noise(x, add)\n\n\nnp.random.seed(42)\nx = torch.linspace(-2, 2, steps=40)\ny = add_noise(f(x), 0.15, 1.5)\npx.scatter(x=x, y=y)\n\n\ndata = FileAttachment(\"dataponts.csv\").csv()\nx_data = data.map(item =&gt; item.x);\ny_data = data.map(item =&gt; item.y);\n{\n  var trace1 = { x:  x_data , y:  y_data , mode: 'markers', name: 'data ponts'};\n  var data = [trace1];\n  var layout = { title:\"\", xaxis: { title: 'x', }, yaxis: { title:\"y\", } };\n  const div = DOM.element('div');\n  Plotly.newPlot(div, data, layout);\n  return div;\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis noisy data is inspired by the quadratic function but comes with a sprinkle of randomness.\nPlot Quadratics with Sliders: Interactive Fun\nEver played with sliders to adjust stuff? Here’s your chance to do the same with quadratics. You can tweak the coefficients a, b, and c to fit the noisy data manually.\n\nviewof a = Inputs.range([-1, 4], {label: \"a\", step: 0.1})\nviewof b = Inputs.range([-1, 4], {label: \"b\", step: 0.1})\nviewof c = Inputs.range([-1, 4], {label: \"c\", step: 0.1})\n{\n  var trace1 = { x: x, y: y, mode: 'lines', name: 'quadratic'};\n  var trace2 = { x:  x_data , y:  y_data , mode: 'markers', name: 'data ponts'};\n  var data = [trace1, trace2];\n  var layout = { title: `Interactive Quadratics`, xaxis: { title: 'x', zeroline: false, }, yaxis: { title: `${a}x² + ${b}x + ${c}`, } };\n  const div = DOM.element('div');\n  Plotly.newPlot(div, data, layout);\n  return div;\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBut who wants to be a human slider forever? We need a more scientific approach to measure how well our function fits the data. Enter loss functions - the unsung heroes of machine learning.\n\n\nMeet the Mean Squared Error (MSE)\nMSE stands for Mean Squared Error. It’s a way to measure how far off our predictions are from the actual values. Here’s how you define it:\n\ndef mse(preds, acts): return ((preds - acts)**2).mean()\n\nNow, let’s use MSE to evaluate our quadratics. This function will calculate the loss (how bad our predictions are) and give us a number we can use to improve our model.\n\nfunction mse(preds, acts) {\n  const squared_error = [];\n  for (let i=0; i &lt; preds.length; i++) {\n    const error = preds[i] - acts[i];\n    squared_error.push(error**2);\n  }\n  const mse = squared_error.reduce((acc, curr) =&gt; acc+curr, 0) / preds.length;\n  return mse;\n}\n_y = x.map(element =&gt; f(element, _a, _b, _c))\nviewof _a = Inputs.range([-1, 4], {label: \"a\", step: 0.1})\nviewof _b = Inputs.range([-1, 4], {label: \"b\", step: 0.1})\nviewof _c = Inputs.range([-1, 4], {label: \"c\", step: 0.1})\n{\n  const loss = mse(_y, y_data)\n  var trace1 = { x: x, y: _y, mode: 'lines', name: 'quadratic'};\n  var trace2 = { x:  x_data , y:  y_data , mode: 'markers', name: 'data ponts'};\n  var data = [trace1, trace2];\n  var layout = { title: `Loss: ${loss.toFixed(4)}`, xaxis: { title: 'x', zeroline: false, }, yaxis: { title: `${_a}x² + ${_b}x + ${_c}`, } };\n  const div = DOM.element('div');\n  Plotly.newPlot(div, data, layout);\n  return div;\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWith MSE, you don’t need to rely on your eyes to see if the fit is better. Numbers will tell you if you’re on the right track. But adjusting sliders manually is so last decade. We need a faster way…\n\n\nThe Power of Derivatives\nOne approach would be to try fitting different parameters manually, right? We could increase each parameter a bit and see if the loss (our way of measuring how bad the model’s predictions are) improves, and vice versa. But there’s a much faster way, and this magic trick is called the derivative\nThe derivative tells you, “Hey, if you tweak this parameter, the output will change this much.” Essentially, it’s like having a super-smart assistant that knows whether to turn the dial up or down to make the song sound better. This is also known as the slop or gradient.\n\n\nPytorch to the Rescue\nGood news: Pytorch can automatically calculate derivative for you. Here’s how:\nWe’ll define a function quad_mse, which computes the Mean Squared Error (MSE) between our noisy data and a quadratic function defined by a set of parameters ([a, b, c]):\n\ndef quad_mse(params):\n    f = mkquad(*params)\n    return mse(f(x), y)\n\nThis function takes the coefficients (a, b, c), creates a quadratic function, and then returns the MSE of the predicted values against the actual noisy data.\nTime to give it a whirl:\n\nquad_mse([1.5, 1.5, 1.5])\n\ntensor(6.7798, dtype=torch.float64)\n\n\nSpoiler alert: We get a MSE of 6.78, and yes, it’s a tenser (just a fancy array with some extra Pytorch powers). Let’s make it easier to hand:\n\nabc = torch.tensor([1.5, 1.5, 1.5])\nabc.requires_grad_()\n\ntensor([1.5000, 1.5000, 1.5000], requires_grad=True)\n\n\nNow, our tensor is ready to calculate gradients for these coefficients whenever used in computations. Pass this to quad_mse to verify:\n\nloss = quad_mse(abc)\nprint(loss)\n\ntensor(6.7798, dtype=torch.float64, grad_fn=&lt;MeanBackward0&gt;)\n\n\nAs expected, we get that magical tensor value 6.78. Nothing fancy yet? Hold on. We now tell Pytorch to store the gradients:\n\nloss.backward()\n\nNo fireworks, but something profound just happened. Run this:\n\nprint(abc.grad)\n\ntensor([-7.6934, -0.4701, -2.8031])\n\n\nVoila! You’ve got the gradients or slopes. Theyy tell us how much the loss changes if you tweak each parameter-perfect for finding the optimal values.\n\n\nUpdating Parameters Using Gradients\nTo bring our loss down, we adjust the parameters in the direction that reduces the loss. Essentially, we descend down the gradient:\n\nwith torch.no_grad():\n    abc -= abc.grad * 0.01\n    loss = quad_mse(abc)\nprint(loss)\n\ntensor(6.1349, dtype=torch.float64)\n\n\nThis subtracts a small portion of the gradient from each parameter to create a new set of parameters. And our loss improves from 6.78 to 6.13.\nRemember, with_torch.no_grad() ensures Pytorch doesn’t calculate the gradient for this piece (because it’s just us updating weights and biases, not a loss calculation)\n\n\nAutomating the Gradient Descent\nWhy do it manually when you’er a Pytorch Jedi? Here’s a loop to handle multiple steps of gradient descent:\n\nfor i in range(5):\n    loss = quad_mse(abc)\n    loss.backward()\n    with torch.no_grad():\n        abc -= abc.grad * 0.01\n        print(f\"Step {i}; loss: {loss:.2f} \")\n        abc.grad.zero_()  # Clear the gradient after each step\n\nStep 0; loss: 6.13 \nStep 1; loss: 5.05 \nStep 2; loss: 4.68 \nStep 3; loss: 4.37 \nStep 4; loss: 4.10 \n\n\n\nabc\n\ntensor([1.9329, 1.5305, 1.6502], requires_grad=True)\n\n\nAfter 5 steps of gradient descent, you’ll have a set or parameters edging closer to the optimal values. These number continually adjust to minimize the loss, effectively “learning”, the pattern in your data.\n\n\nWelcome to Optimization: Meet Gradient Descent\nThis whole process of tweaking parameters to minimize loss is called optimization, specifically gradient descent. Pretty much all machine learning models, including the fancy neural networks, use variations of this technique.\n\n\nThe Magic of ReLUs\nWe can’t just fit our model with simple quadratics, can we? The real world is way more complex-especially when it comes to discerning the subtle nuances of whether a pixel forms part of a basset hound or not. So, let’s up the complexity game, shall we?\nEnter the superhero of activation function: the Rectified Linear Unit(ReLUs). This tiny thing is like the ultimate building block for creating infinity flexible functions:\n\ndef rectified_linear(m, b, x):\n    y = m * x + b\n    return torch.clip(y, min=0.0)\n\nThis function is a simple line y = mx + b. The torch.clip() function takes anything blow zero and flatlines it at zero. Essentially, this turns any negative output into zero, while keeping positive values unchanged.\nHere’s what the ReLU looks like:\n\nplot_function(partial(rectified_linear, 1, 1))\n\n\nfunction rectified_linear(m, b, x) {\n  const y = m*x + b;\n  return Math.max(y, 0);\n}\nviewof m = Inputs.range([-1, 4], {label: \"m\", step: 0.1})\nviewof b_ = Inputs.range([-1, 4], {label: \"b\", step: 0.1})\n{\n  const _y = x.map(element =&gt; rectified_linear(m, b_, element));\n  var trace1 = {\n    x: x,\n    y: _y,\n    mode: 'lines',\n    name: 'ReLU',\n  };\n  var data = [trace1];\n  var layout = {\n    title: \"Rectified Linear Unit\",\n    xaxis: { title: \"x\", },\n    yaxis: { title: \"y\", range: [-1, 4] }\n  };\n  const div = DOM.element('div');\n  Plotly.newPlot(div, data, layout);\n  return div;\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImagine a line rising up at a 45-degree angle until it hits zero-at which point it surrenders to the great oblivion blow it. Now, you can adjust the coefficients m (slope) and b (intercept) and watch the magic happen.\n\n\nThe Power of Double ReLU: Fun With Functions\nWhy stop at one ReLU when you can have double the fun with two?\n\ndef double_relu(m1, b1, m2, b2, x):\n    return rectified_linear(m1, b1, x) + rectified_linear(m2, b2, x)\n\nThis function combines two ReLUs. Let’s plot this end see what unfolds:\n\nplot_function(partial(double_relu, 1, 1, -1, 1))\n\n\nfunction double_relu(m1,b1,m2,b2,x) {\n  return rectified_linear(m1,b1,x) + rectified_linear(m2,b2,x)\n}\nviewof m1 = Inputs.range([-2, 4], {label: \"m1\", step: 0.1})\nviewof b1 = Inputs.range([-2, 4], {label: \"b1\", step: 0.1})\nviewof m2 = Inputs.range([-2, 4], {label: \"m2\", step: 0.1})\nviewof b2 = Inputs.range([-2, 4], {label: \"b2\", step: 0.1})\n{\n  const _y = x.map(element =&gt; double_relu(m1,b1,m2,b2, element));\n  var trace1 = {\n    x: x,\n    y: _y,\n    mode: 'lines',\n    name: 'Double ReLU',\n  };\n  var data = [trace1];\n  var layout = {\n    title: \"Double Rectified Linear Unit\",\n    xaxis: { title: \"x\", },\n    yaxis: { title: \"y\", range: [-1, 4] }\n  };\n  const div = DOM.element('div');\n  Plotly.newPlot(div, data, layout);\n  return div;\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou’ll notice a downward slope that hooks upward into another slope. Tweak the coefficients m1, b1, m2, and b2, and watch the slopes and hooks dance around!\n\n\nInfinity Flexible ReLUs\nThink this is fun? Imagine adding a million ReLUs together. In face, you can add as many as you want to create function as wiggly and complex as you desire.\nBehold the power of ReLUs! 👀✨ With enough ReLUs, you can match any data pattern with incredible precision. you want a function that isn’t just 2D but spreads across multiply dimensions? You got it! ReLUs can do 3D, 4D, 5D…, nD.\n\n\nNeed Parameters? We’ve got Gradient Descent\nBut we need parameters to make magic happen, right? Here’s where gradient descent swoops in to save the day. By continuously tweaking these coefficients based on our loss calculations, we gradually descend towards the perfect parameter set.\n\n\nThe Big Picture: Adding ReLus and Gradient Descent === Deep Learning\nBelieve it or not, this is the essence of deep learning. Everything else-every other tweak is just about making this process faster and more efficient, sparking those “a-ha!” moments.\nQuoting Jeremy Howard:\n\n\n“Now I remember a few years ago when I said something like this in a class, somebody on the forum was like, this reminds me of that thing about how to draw an owl. Jeremy’s basically saying, okay, step one: draw two circles; step two: daw the rest of the owl”\n\nThis bit is pure gold because it distills deep learning down to its core components. When you have ReLUs being added together, gradient descent optimizing you parameters, and sample of inputs and outputs-voilà! The computer draws the owl.\nRemember when things get dense: keep coming back to what’s really happening. Deep learning, at it’s heart, is using gradient descent to tweak parameters, adding lots of ReLUs (or something similar) to match your data.\nAnd that’s it! You’ve just peeked under the hood of deep learning. Stay curious, keep playing with those ReLUs, and watch the neural magic unfold. 🚀"
  },
  {
    "objectID": "posts/2024-07-28-from-neuron-to-gradient/index.html#the-titanic-competition",
    "href": "posts/2024-07-28-from-neuron-to-gradient/index.html#the-titanic-competition",
    "title": "From Neurons to Gradients: Unpacking FastAI Lesson 3",
    "section": "The Titanic Competition",
    "text": "The Titanic Competition\n[updating …]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dai’s blog",
    "section": "",
    "text": "Sink or Swim: Navigating Deep Learning with the Titanic Competition\n\n\n\n\n\n\nkaggle\n\n\ncompetition\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\nAug 1, 2024\n\n\nBui Huu Dai\n\n\n\n\n\n\n\n\n\n\n\n\nFrom Neurons to Gradients: Unpacking FastAI Lesson 3\n\n\n\n\n\n\nblogging\n\n\nfastai\n\n\ntorch\n\n\n\n\n\n\n\n\n\nJul 28, 2024\n\n\nBui Huu Dai\n\n\n\n\n\n\n\n\n\n\n\n\nFrom Notebook to Web App: Deploying Your Models with fastai Lesson 2\n\n\n\n\n\n\nblogging\n\n\nfastai\n\n\nhuggingface spaces\n\n\ngradio\n\n\n\n\n\n\n\n\n\nJul 7, 2024\n\n\nBui Huu Dai\n\n\n\n\n\n\n\n\n\n\n\n\nFirst Step in AI: My Experience with fast.ai Lesson 1\n\n\n\n\n\n\nblogging\n\n\nfastai\n\n\n\n\n\n\n\n\n\nJul 4, 2024\n\n\nBui Huu Dai\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024-07-07-from-notebook-to-web-app/index.html",
    "href": "posts/2024-07-07-from-notebook-to-web-app/index.html",
    "title": "From Notebook to Web App: Deploying Your Models with fastai Lesson 2",
    "section": "",
    "text": "Welcome back to our deep learning adventure with fastai! In Lesson 2, we dive into the exciting world of putting model into production. Whether you’re a beginner looking to get your feet wet or an experienced practitioner wanting to brush up on your deployment skills, this lesson is packed with practical tips and hands-on techniques to take your models from the notebook to the real world.\nIn this blog post, we’ll cover everything from gathering images to training and deploying models, using tools like Jupyter Notebooks, Gradio, and Hugging Face Spaces. Get ready to explore essential concepts like how to clean your data and see how different deployment platforms stack up against each other.\nBuckle up and let’s get started on this journey and bring your deep learning models to life!"
  },
  {
    "objectID": "posts/2024-07-07-from-notebook-to-web-app/index.html#introduction",
    "href": "posts/2024-07-07-from-notebook-to-web-app/index.html#introduction",
    "title": "From Notebook to Web App: Deploying Your Models with fastai Lesson 2",
    "section": "",
    "text": "Welcome back to our deep learning adventure with fastai! In Lesson 2, we dive into the exciting world of putting model into production. Whether you’re a beginner looking to get your feet wet or an experienced practitioner wanting to brush up on your deployment skills, this lesson is packed with practical tips and hands-on techniques to take your models from the notebook to the real world.\nIn this blog post, we’ll cover everything from gathering images to training and deploying models, using tools like Jupyter Notebooks, Gradio, and Hugging Face Spaces. Get ready to explore essential concepts like how to clean your data and see how different deployment platforms stack up against each other.\nBuckle up and let’s get started on this journey and bring your deep learning models to life!"
  },
  {
    "objectID": "posts/2024-07-07-from-notebook-to-web-app/index.html#gathering-and-cleaning-data",
    "href": "posts/2024-07-07-from-notebook-to-web-app/index.html#gathering-and-cleaning-data",
    "title": "From Notebook to Web App: Deploying Your Models with fastai Lesson 2",
    "section": "Gathering and Cleaning Data",
    "text": "Gathering and Cleaning Data\nIn this section we’ll walk through the process of gathering and cleaning data, leveraging some handy tools and methods introduced in Lesson 2.\n\nImporting and Setting Up\nFirst ensure that you have all necessary libraries and modules in place. If you haven’t already, run the following command to install the fastbook module:\nconda install -y -c fastai fastbook\nNow you can import the required functions from fastbook:\n\nfrom fastbook import *\nfrom fastai.vision.widgets import *\n\n\n\nGathering Images with DuckDuckGo\nUsing DuckDuckGo(ddg) for image searches simplifies the process, as it doesn’t require an API key. Here’s the code to create our dataset of bear images:\n\nbear_types = 'grizzly', 'black', 'teddy'\npath = Path('bear')\nif not path.exists():\n    path.mkdir()\n    for o in bear_types:\n        dest = (path/o)\n        dest.mkdir(exist_ok=True)\n        results = search_images_ddg(f\"{o} bear\")\n        download_images(dest, urls=results)\n\nThis code snippet sets up directories for different bear types and download images into respective folders.\nNext, we verify and clean the downloaded images:\n\nfailed = verify_images(get_image_files(path))\nfailed\n\n(#24) [Path('bear/black/b8d71ddf-a84d-4054-8088-bb08e8cbd814.jpg'),Path('bear/black/bbe19bf5-3d28-4d7e-b8bf-3e7f8afff5af.jpg'),Path('bear/teddy/ee833f9f-ff26-4435-a4cc-89208236c442.jpg'),Path('bear/teddy/f6f4f901-c873-46a6-8ef7-bbd65da2c910.jpg'),Path('bear/teddy/e3c4fc0d-e494-4c8a-9dc5-8510f8407cac.jpg'),Path('bear/teddy/3309df18-2a7f-4d67-b2e8-9f08eea06025.jpg'),Path('bear/teddy/99377b04-a870-4798-9e6a-02543a495395.JPG'),Path('bear/teddy/fbf4430d-0643-444b-a0c5-7d13c43d92b6.jpg'),Path('bear/teddy/4c576f46-fa7e-4800-bb89-9eea7662ab10.jpg'),Path('bear/teddy/50d2e017-49b9-49a1-9538-5e402932e463.jpg')...]\n\n\n\nfailed.map(Path.unlink);\n\nThis step ensures that any currupt images are identified and removed.\n\n\nStructuring Data with DataBlock API\nWe use DataBlock API to structure our data, making it ready for training:\n\nbears = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=Resize(128)\n)\n\ndls = bears.dataloaders(path)\ndls.valid.show_batch(max_n=4, nrows=1)\n\n\n\n\n\n\n\n\nThis show a batch of images using the default resizing method. Different resizing startegies can impact the dataset in various ways."
  },
  {
    "objectID": "posts/2024-07-07-from-notebook-to-web-app/index.html#exploring-resizing-methods",
    "href": "posts/2024-07-07-from-notebook-to-web-app/index.html#exploring-resizing-methods",
    "title": "From Notebook to Web App: Deploying Your Models with fastai Lesson 2",
    "section": "Exploring Resizing Methods",
    "text": "Exploring Resizing Methods\nResizing plays a crucial role in preparing your images for model training. Let’s explore three different resizing methods:\n\nStandard Resize\nThe standard resize method adjust the image size for model while maintaining a specific aspect rato. Here, we pad the images with zeros(black) to ensure the entire image is included:\n\nbears = bears.new(item_tfms=Resize(128, ResizeMethod.Pad, pad_mode='zeros'))\ndls = bears.dataloaders(path)\ndls.valid.show_batch(max_n=4, nrows=1)\n\n\n\n\n\n\n\n\nThis approach, padding with zeros maintains the aspect rato and ensures that the entire image fit within the frame.\n\n\nRandomResizedCrop\nAnother effective method is RandomResizedCrop, which crops different parts of an image each time, providing varied views:\n\nbears = bears.new(item_tfms=RandomResizedCrop(128, min_scale=0.3))\ndls = bears.dataloaders(path)\ndls.train.show_batch(max_n=4, nrows=1, unique=True)\n\n\n\n\n\n\n\n\nRandomResizedCrop is excellent for generating diverse training data. It explores different regions of the same image, enhancing the robustness of your model."
  },
  {
    "objectID": "posts/2024-07-07-from-notebook-to-web-app/index.html#applying-data-augmentation",
    "href": "posts/2024-07-07-from-notebook-to-web-app/index.html#applying-data-augmentation",
    "title": "From Notebook to Web App: Deploying Your Models with fastai Lesson 2",
    "section": "Applying Data Augmentation",
    "text": "Applying Data Augmentation\nData Augmentation increases the diversity of your training data by applying various transformations, such as rotation and flipping:\n\nbears = bears.new(\n    item_tfms=Resize(128),\n    batch_tfms=aug_transforms(mult=2)\n)\ndls = bears.dataloaders(path)\ndls.train.show_batch(max_n=8, nrows=2, unique=True)\n\n\n\n\n\n\n\n\nUsing aug_transforms, we can dynamically modify images during training. This process, called data augmentation, helps the model generalize better by exposing it to various versions of the same image. The mult=2 parameter exaggerates the transformations for better visualization."
  },
  {
    "objectID": "posts/2024-07-07-from-notebook-to-web-app/index.html#training-the-model",
    "href": "posts/2024-07-07-from-notebook-to-web-app/index.html#training-the-model",
    "title": "From Notebook to Web App: Deploying Your Models with fastai Lesson 2",
    "section": "Training the Model",
    "text": "Training the Model\nWith our data ready, we can proceed to train a model using a pre-trained resnet18:\n\nbears = bears.new(\n    item_tfms=RandomResizedCrop(224, min_scale=0.5),\n    batch_tfms=aug_transforms()\n)\ndls = bears.dataloaders(path)\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(4)\n\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00&lt;00:00, 131MB/s] \n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.010127\n0.147814\n0.066038\n00:15\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.187663\n0.105568\n0.037736\n00:13\n\n\n1\n0.165576\n0.146279\n0.037736\n00:13\n\n\n2\n0.140621\n0.171902\n0.047170\n00:14\n\n\n3\n0.117243\n0.166212\n0.047170\n00:13\n\n\n\n\n\nIn this case we use RandomSizedCrop and aug_transforms to create robust data loaders. Training a model for four epochs results in an error rate of under five percent-quite impressive!"
  },
  {
    "objectID": "posts/2024-07-07-from-notebook-to-web-app/index.html#evaluating-the-model",
    "href": "posts/2024-07-07-from-notebook-to-web-app/index.html#evaluating-the-model",
    "title": "From Notebook to Web App: Deploying Your Models with fastai Lesson 2",
    "section": "Evaluating the Model",
    "text": "Evaluating the Model\nEvaluating the trained model is crucial for understanding its performance and identifying areas for improvement. Lesson 2 introduces several important techniques for model evaluation.\n\nConfusion Matrix Explanation\nThe confusion matrix is powerful tool for examining the performance of classification models. It provide insights into which categories are commonly confused by the model:\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe confusion matrix shows the model’s predictions against the actual labels. Here’s what it tells us:\n\nThe diagonal represents correct predictions (e.g., 29 black bears correctly predicted as black bears).\nOff-diagonal elements reveal misclassifications (e.g., 2 black bears predicted as grizzly bears)\n\nFor instance, if our bear classifier mislabels a grizzly bear as a black bear, the corresponding cell in the matrix indicates how often this mistaken occurs. It’s a visual representation of “where did we go wrong?” and is essential for refining the model.\nJeremy pointed out that such insight help:\n\nIdentify which categories are inherently difficult to distinguish(e.g., black bears and grizzly bears).\nUnderstand if certain errors systematics and need targeted improvements.\n\nHere’s my model shows:\n\nHigh accuracy in identifying grizzly bears and teddy bears: There are only a couple of misclassification.\nSome confusion between black bears and grizzly bears: This is evident from the few off-diagonal elements\n\nUnderstand these errors helps us focus on areas that need more training data or better distinguishing features.\n\n\nplot_top_losses Explanation\nThe plot_top_losses function highlights the individual images where your model made the worst predictions:\n\ninterp.plot_top_losses(5, nrows=1, figsize=(17,4))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe results show specific cases where the model was either:\n\nHighly confident but wrong: E.g., the model predicted “teddy” with high confidence when it actually “black”.\nCorrect but not confident: E.g., predicting the right class but with low confidence.\n\nBy examining these top losses, you gain insights into why the model might be confused. Here, my model misclassified a black bear as a grizzly bear with high confidence which might indicate that the features used to distinguish between these classes are not prominent enough.\nThese insights can help in refining your data and possibly augmenting it to address these specific weaknesses."
  },
  {
    "objectID": "posts/2024-07-07-from-notebook-to-web-app/index.html#clean-the-data-with-imageclassifiercleaner",
    "href": "posts/2024-07-07-from-notebook-to-web-app/index.html#clean-the-data-with-imageclassifiercleaner",
    "title": "From Notebook to Web App: Deploying Your Models with fastai Lesson 2",
    "section": "Clean the Data with ImageClassifierCleaner",
    "text": "Clean the Data with ImageClassifierCleaner\nOnce we’ve evaluated our model, the next important step is data cleaning. Surprisingly, Jeremy suggests cleaning the data after training the initial model. This counterintuitive approach allows the model to highlight problematic data points\n\nImageClassifierCleaner Demonstration\nThe ImageClassifierCleaner widget is a fantastic tool for this purpose. It helps you manually review and clean your dataset based on the model’s predictions:\n\ncleaner = ImageClassifierCleaner(learn)\ncleaner\n\n\nWhen you run this widget, it launches an interactive interface where you can:\n\nSort Image by Loss: Images are ordered by the model’s confidence, making it easy to identify incorrect or ambiguous labels.\nCorrect Labels: Reassign images to the correct categories if they were mislabeled.\nDelete Incorrect Images: Remove images that don’t belong in any category.\n\nJeremy explained how he used it to clean the bear dataset:\n\nBy selecting “teddy bears”, the widget displayed all images classified as teddy bears.\nHe manually review the images, reassigning or deleting those that were incorrectly labeled.\n\nHere’s how you can apply the changes:\n\nfns = get_image_files(path)\nfor idx in cleaner.delete(): cleaner.fns[idx].unlink()\nfor idx, cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat)\n\nThis code snippet updates the dataset based on your interaction with the ImageClassifierCleaner: - Delete: Removes files marked for deletion. - Move: Reassigns files to the correct categories.\n\n\nWhy Clean After Training?\nCleaning the data after training might seems backward, but it has significant advantages:\n\nModel-Assisted Cleaning: The initial model helps identify problematic data points that might be hard to spot manually.\nFocus on Hard Cases: The confusion matrix and top losses highlight the hight areas that need most attention, making your cleaning effort more efficient.\n\nThis process ensures a high quality dataset for subsequent training iterations, leading to better model performance.\nBy incorporating thorough evaluation and cleaning steps, you refine you dataset and improve your model’s accuracy and reliability. These insights are invaluable for building robust deep learning models that perform well on real-world data."
  },
  {
    "objectID": "posts/2024-07-07-from-notebook-to-web-app/index.html#deployment-building-and-deploying-a-model",
    "href": "posts/2024-07-07-from-notebook-to-web-app/index.html#deployment-building-and-deploying-a-model",
    "title": "From Notebook to Web App: Deploying Your Models with fastai Lesson 2",
    "section": "Deployment: Building and Deploying a Model",
    "text": "Deployment: Building and Deploying a Model\nAfter cleaning our data, the next exciting step is to put our model into production. While the book introduces Voilà for creating interactive web application using Jupyter Notebooks, there’s another powerful tool that’s becoming increasingly popular: HuggingFace Spaces. Together with Gradio, they offer an intuitive and powerful way to deploy machine learning models as web applications.\n\nIntroducing HuggingFace Spaces and Gradio.\nHuggingFace Spaces is a platform that allows you to host machine learning model and their interfaces for free. On the other hand, Gradio make it easy to create customizable web interfaces with a few lines of Python code.\n\n\nA Shortout of Tanishq Abraham\nBefore diving into technical details, let’s give a shortout to Tanishq Abraham, one of the most remarkable individuals in the fastai community. Known as a child prodigy, Tanishq has contributed immensely to the community, making complex topics accessible to everyone. I’ve learned a lot from his work and highly recommend checking out his website and his Twitter for more insightful resources.\nTanshiq has also written an excellent blog post the cover everything you need to know about using Gradio and HuggingFace Spaces. You can read his detail guide here\n\n\nSetting Up and Deploying Your Model using Gradio and HuggingFace Spaces\nTo deploy our model, we’ll use HuggingFace Spaces. The set up process is straightforward and free of charge. Follow these step to get started:\nStep 1: Sign Up and Create a New Space\n\nGo to the HuggingFace Spaces page and sign up for an account if you haven’t already.\nClick “Create a new space”.\nGive you space a name and chose a template (you can start with Gradio template).\n\nCongrats! You’ve created a new space. Now, what’s next?\nStep 2: Getting Familiar with Git\nHuggingFace Spaces works through Git, which many developers are already familiar with. Using Git is also a good practice, and Jeremy recommends using Github Desktop and WSL2. Refer to the guide for WSL2 installation to get started.\nCloning the Repository\nTo start working on HuggingFace Spaces you need to clone the repository locally, you have two options for cloning: HTTPs and SSH\n\n\n\n\n\n\n\n\nPros / Cons\nHTTPS\nSSH\n\n\n\n\nPros\nEasier for beginners, no SSH key setup required\nMore secure, no need to enter credentials each time\n\n\nCons\nRequires authentication each time you push\nRequires SSH key setup\n\n\n\nSince I’m using SSH, if you follow along please make sure your SSH key is properly set up in your HuggingFace Spaces user setting\n# Clone the repository using SSH\ngit clone git@huggingface.co:USERNAME/YOUR_REPO_NAME.git\ncd YOUR_REPO_NAME\nStep 3: Prepare Your Model\nMake sure to export your trained model from the notebook:\n\n# in your bear classifier notebook\nlearn.export(\"model.pkl\")\n\nThis saves the trained model as model.pkl, which you’ll need for the deployment.\nStep 4: Building the Gradio Interface\nGradio makes it easy to build an interactive interface. You can use Jupyter Notebook for experimentation and then use nbdev.export.nb_export to convert the notebook into Python script. This tool is very handy for such conversions. Alright, but first make sure you run pip install gradio in your terminal if you haven’t already.\n1. Import Required Libraries:\n\nimport gradio as gr\nfrom fastai.vision.all import *\n\nAnd so we can create a python image library image from that black bear\n\nim = PILImage.create('black.jpg')\nim.thumbnail((192, 192))\nim\n\n\n\n\n\n\n\n\nTurn it into slightly smaller one so it doesn’t overwhelm my whole screen and there’s is a picture of a black bear so we will use it for experimenting\n2. Load the Model:\n\nlearn = load_learner('model.pkl')\n\nOne of the methods that the learner has is a predict method\n\nlearn.predict(im)\n\n\n\n\n\n\n\n\n('black', tensor(0), tensor([9.9995e-01, 4.9545e-05, 3.8379e-06]))\n\n\nSo if you run it, you can see, even on a laptop, it’s basically instant. It took a really short time to figure out this is a black bear\n3. Define the Prediction Function:\nGradio requires us to give it a function that it’s going to call but first we need to know what labels do we have?\n\nlearn.dls.vocab\n\n['black', 'grizzly', 'teddy']\n\n\n\n#create our categories\ncategories = learn.dls.vocab\n\nSo here’s our function:\n\ndef classify_image(img):\n    img = PILImage.create(img)\n    pred, idx, probs = learn.predict(img)\n    return dict(zip(categories, map(float, probs)))\n\nSo we called predict and that returns three things: the prediction as a string, the index of that, and the probabilities of whether it’s black or grizzly or teddy bear. And what Gradio wants is it wants to get back a dictionary containing each of the possible categories-which is in this case grizzly, black and teddy bear-and the probabilities of each one.\n4. Create the Gradio Interface:\n\nimage = gr.Image(height=512, width=512)\nlabels = gr.Label()\nexamples = [\"grizzly.jpg\", \"black.jpg\", \"teddy.jpg\"]\n\nintf = gr.Interface(fn=classify_image, inputs=image, outputs=labels, examples=examples)\nintf.launch(inline=True)\n\nRunning on local URL:  http://127.0.0.1:7860\nTo create a public link, set `share=True` in `launch()`.\n\nThis code creates a simple Gradio interface where user can upload images, and the model will predict whether it’s a grizzly, black or teddy bear. you can run the interface inline in you Jupyter Notebook for testing.\nStep 5: Export the Notebook to a Python script:\nWe will use nbdev to convert the Jupyter Notebook to a Python script.\n1. Add Metadata and Export Tags:\n- Add `#| default_exp app` to the first cell\n- Add `#| export` to every cell you want to convert\n2. Run the Conversion:\n\nimport nbdev.export\nnbdev.export.nb_export('app.ipynb', '.')\n\nThis command converts the notebook app.ipynb to a python script app.py.\nStep 6: Push Your Changes to HuggingFace Spaces:\nHandle large files like model.pkl using Git LFS (Large File Storage).\n1. Set up Git LFS:\n\ngit lfs install\ngit lfs track \"*.pkl\"\ngit add .gitattributes\ngit commit -m \"Track .pkl files with Git LFS\" # To be honest, when initializing git lfs, the .gitattributes already supports .pkl files, and in my repo when initialized, it already had .gitattributes file. I don't know why, but I didn't need to commit it anyway, but I still write it here for the sake of completeness. 😉\n\n2. Commit and Push Your Changes:\n\ngit add .\ngit commit -m \"Deploy bear classifier with Gradio interface🐻🎉\"\ngit push"
  },
  {
    "objectID": "posts/2024-07-07-from-notebook-to-web-app/index.html#conclusion",
    "href": "posts/2024-07-07-from-notebook-to-web-app/index.html#conclusion",
    "title": "From Notebook to Web App: Deploying Your Models with fastai Lesson 2",
    "section": "Conclusion",
    "text": "Conclusion\nBy following this guide, you’ve successfully built and deployed your bear classifier using Gradio and HuggingFace Spaces. This powerful combination not only make you model accessible for everyone through the user-friendly web interface but also leverages cutting-edge tool to ensure it easy to maintain and extend.\nDeploying machine learning models in real-world applications is an exciting milestone. It transform your hard work and complex algorithms into actionable insights and tools that can be used by anyone, anywhere. Whether you’re a beginner or an experienced partitioner, the ability to take a model from Jupyter Notebook to a live web app is an invaluable skill in today’s AI driven world.\nIn case you want to explore more about Gradio and HuggingFace, here are some valuable resources:\n\nGradio Documentation: For more on Gradio, refer to the Gradio documentation.\nMy HuggingFace Spaces Bear Classifier: Checkout my deployed bear classifier on HuggingFace Spaces here.\nTanishq Abraham’s Blog: For an in-depth look at deploying model using Gradio and HuggingFace Spaces, make sure to read Tanishq’s excellent blog post.\n\nAlthough I initially planned to look into the HuggingFace Spaces API and deploying your own web app via JavasScript, sometimes technical hitches happened. Whether it’s a client-side issue or just part of the learning curve, don’t let it discourage you. Every challenge is learning opportunity, and with the fast-placed advancements in AI and deployment tools, there’s always something new and exciting around the corner.\nThank you for joining me on this journey to bring deep learning model to life. Embrace the power of open-source tools, keep experimenting, and never stop learning. Happy coding, and may your models always be accurate!"
  },
  {
    "objectID": "posts/2024-06-30-your-deep-learning-journey/index.html",
    "href": "posts/2024-06-30-your-deep-learning-journey/index.html",
    "title": "First Step in AI: My Experience with fast.ai Lesson 1",
    "section": "",
    "text": "Welcome to my deep dive in to the world of deep learning! In this blog post, I’ll be sharing my journey through the first lesson of fast.ai course an acclaimed program that makes learning AI accessible and enjoyable.\nFast.ai was created with the goal of making deep learning understandable for everyone, no matter their background, and Lesson 1 accomplishes that by having us build a simple yet fascinating model: a bird classifier. this exciting task not just introduces me to the basics of deep learning but also alow me to experience firsthand the power and simplicity of modern AI tools.\nJoin me as I walk you though key concept covered in the Lesson 1, from understanding how images are processed by computers to training and validating our model. I will also share some personal insights and reflections on the learning process, aiming to make this technical journey both informative and relatable.\nWhether you are a beginner in AI or someone looking for refresh your knowledge, I hope this post inspires and guides you in your own deep learning"
  },
  {
    "objectID": "posts/2024-06-30-your-deep-learning-journey/index.html#introduction",
    "href": "posts/2024-06-30-your-deep-learning-journey/index.html#introduction",
    "title": "First Step in AI: My Experience with fast.ai Lesson 1",
    "section": "",
    "text": "Welcome to my deep dive in to the world of deep learning! In this blog post, I’ll be sharing my journey through the first lesson of fast.ai course an acclaimed program that makes learning AI accessible and enjoyable.\nFast.ai was created with the goal of making deep learning understandable for everyone, no matter their background, and Lesson 1 accomplishes that by having us build a simple yet fascinating model: a bird classifier. this exciting task not just introduces me to the basics of deep learning but also alow me to experience firsthand the power and simplicity of modern AI tools.\nJoin me as I walk you though key concept covered in the Lesson 1, from understanding how images are processed by computers to training and validating our model. I will also share some personal insights and reflections on the learning process, aiming to make this technical journey both informative and relatable.\nWhether you are a beginner in AI or someone looking for refresh your knowledge, I hope this post inspires and guides you in your own deep learning"
  },
  {
    "objectID": "posts/2024-06-30-your-deep-learning-journey/index.html#the-xkcd-joke-and-debunking-deep-learning-myths",
    "href": "posts/2024-06-30-your-deep-learning-journey/index.html#the-xkcd-joke-and-debunking-deep-learning-myths",
    "title": "First Step in AI: My Experience with fast.ai Lesson 1",
    "section": "The XKCD Joke and Debunking Deep Learning Myths",
    "text": "The XKCD Joke and Debunking Deep Learning Myths\n\n\n\n\nXKCD Joke\n\n\nJeremy Howard kicked off the lesson with relatable XKCD Joke about how in 2015, detecting a bird in a photo was seen as a challenging task, almost a joke. Fast forward to today, and we can build such as system in mere minutes, showcasing how far deep learning has come.\nMany people believe that diving into deep learning requires extensive mathematical knowledge, huge datasets, and expensive hardware. However, these myths are far from the truth.\n\n\n\n\n\n\n\nMyth(Don’t need)\nTruth\n\n\n\n\nLots of math\nJust high school math is sufficient\n\n\nLots of data\nWe’ve seen record-breaking results with fewer than 50 items of data\n\n\nLots of expensive computer\nYou can perform state-of-the-art work with hardware available for free of minimal cost"
  },
  {
    "objectID": "posts/2024-06-30-your-deep-learning-journey/index.html#top-down-learning-approach",
    "href": "posts/2024-06-30-your-deep-learning-journey/index.html#top-down-learning-approach",
    "title": "First Step in AI: My Experience with fast.ai Lesson 1",
    "section": "Top-Down Learning Approach",
    "text": "Top-Down Learning Approach\nOne of the most refreshing aspects of fastai course is its top-down teaching approach. Traditional education often starts with the basics and slowly builds up to more complex topics. However, Jeremy Howard and Rachel Thomas believe that learning is more effective when you see the big picture first.\nIn the fastai course, we start by building practically applications from lesson one, allowing us to see immediate results and understanding the relevance of what we are doing. This approach mirrors how we learn many real-word skills, such as sport or cooking, where we start by trying out the activity and learn the details as needed.\nBy diving straight into creating a deep learning model, we get hands-on experience early on, which helps solidify our understanding and maintain our interest. As we process though the course, we gradually delve deeper into the underlying principles and theories, building a robust foundation along the way"
  },
  {
    "objectID": "posts/2024-06-30-your-deep-learning-journey/index.html#understanding-deep-learning",
    "href": "posts/2024-06-30-your-deep-learning-journey/index.html#understanding-deep-learning",
    "title": "First Step in AI: My Experience with fast.ai Lesson 1",
    "section": "Understanding Deep learning",
    "text": "Understanding Deep learning\nDeep learning is a technique for extracting and transforming data, with application ranging from speech recognition to image classification. It uses multiple layer of neural networks, where each layer refines the data received from the previous one. These layers are trained using the algorithms that minimize the errors and improve accuracy, enabling the network to learn specific tasks.\nDeep learning’s power, flexibility, and simplicity make it applicable across various field, including social science, medicine, finance, and more. For instance, despite lacking of medical background, Jeremy Howard founded Enlitic, a company leveraging deep learning to diagnose illnesses. Within months, their algorithm was more effective at identifying malignant tumors than radiologists.\nHere are some areas where deep learning excels:\n\nNatural Language Processing (NLP): Answering question, speech recognition, document summarization, and more.\nComputer Vision: Interpreting satellite images, face recognition, and autonomous vehicle navigation.\nMedicine: Analyzing radiology images, measuring features and medical scans, and diagnosing diseases.\nBiology: Protein folding, genomics tasks, and cell classification.\nImage Generation: Colorizing images, enhancing resolution, and converting images to artistic style.\nRecommendation System: Web search optimization, product recommendations, and personalized content layout.\nGaming: Mastering games like Chess, Go, and various video games.\nRobotics: Handling challenging objects and complex manipulation tasks.\nOther: Financial forecasting, text-to-speech conversion, and much more.\n\nThe versatility of deep learning lies in its foundation: neuron networks."
  },
  {
    "objectID": "posts/2024-06-30-your-deep-learning-journey/index.html#a-brief-history-of-deep-learning",
    "href": "posts/2024-06-30-your-deep-learning-journey/index.html#a-brief-history-of-deep-learning",
    "title": "First Step in AI: My Experience with fast.ai Lesson 1",
    "section": "A Brief History of Deep Learning",
    "text": "A Brief History of Deep Learning\n\n\n\n\nBiological Neurons vs. Artificial Neural Network\n\n\nDeep learning draws inspiration from human brain’s neural network. The concept of neural network isn’t new; it dates back to 1957 with the creation of the first neural network. The fundamental ideas remain the same today, but advances in hardware and data availability have significantly propelled the field forward."
  },
  {
    "objectID": "posts/2024-06-30-your-deep-learning-journey/index.html#the-sofware-pytorch-fastai-and-jupyter",
    "href": "posts/2024-06-30-your-deep-learning-journey/index.html#the-sofware-pytorch-fastai-and-jupyter",
    "title": "First Step in AI: My Experience with fast.ai Lesson 1",
    "section": "The Sofware: Pytorch, Fastai, and Jupyter",
    "text": "The Sofware: Pytorch, Fastai, and Jupyter\nAt fastai, after extensive testing of various machine learning packages and languages, they decided to adopt Pytorch in 2017 for their course, software development, and research. Pytorch has become the fastest-growing deep learning library and is widely used in academic research and industry. Its flexibility and expressiveness make it an excellent foundation for deep learning.\nThe fastai library builds on top of Pytorch, provide high-level functionality for deep learning. This layered architecture allows for a seamless learning experience, make it easier to understand both high-level concepts and low-level operations.\nHowever, the specific software you use a less important than understanding the core principles and techniques of deep learning. Learning to transition between the libraries is relatively quick, but mastering deep learning foundation is crucial.\nJupyter notebook, a powerful and reflexible tool for data science, will be our primary platform for experimentation. Its interaction with fastai and Pytorch makes it ideal for developing and testing deep learning model.\nReady to see it in action? Let’s train our first model!"
  },
  {
    "objectID": "posts/2024-06-30-your-deep-learning-journey/index.html#exploring-the-is-it-a-bird-classifier",
    "href": "posts/2024-06-30-your-deep-learning-journey/index.html#exploring-the-is-it-a-bird-classifier",
    "title": "First Step in AI: My Experience with fast.ai Lesson 1",
    "section": "Exploring the “Is it a Bird?” Classifier",
    "text": "Exploring the “Is it a Bird?” Classifier\nOne of the most exciting part of Lesson 1 was building our own image classifier to determine whether the a given image contains a bird. For this project, we used the fastai library along with pre-trained model to quickly and efficiently create our classifier. Let’s dive into the code walkthrough.\nThe basic steps we’ll need to do:\n\nUse DuckDuckGo for search images of “bird photos”\nUse DuckDuckGo to search for images of “forest photos”\nFine-tune a pre-trained neural network to recognize these two groups\nTry running this model on a picture of bird and see if it works.\n\n\nSearching for images: DuckDuckGo Search\nInstead of using a big search that requires an API key, we opted to DuckDuckGo, which doesn’t require an API key for image searches. This make the setup simpler and faster.\nBut make sure you run this command in your terminal before run the code to update DuckDuckGo\npip install -Uqq fastai duckduckgo_search\n\nfrom duckduckgo_search import DDGS\nfrom fastcore.all import *\n\nddgs = DDGS()\n\ndef search_images(term, max_images=30):\n    print(f\"Searching for '{term}'\")\n    return L(ddgs.images(keywords=term, max_results=max_images)).itemgot('image')\n\n\nurls = search_images('bird photos', max_images=1)\nurls[0]\n\nSearching for 'bird photos'\n\n\n'https://images.pexels.com/photos/326900/pexels-photo-326900.jpeg?cs=srgb&dl=wood-flight-bird-326900.jpg&fm=jpg'\n\n\nJeremy Howard mentioned that using import * in Jupyter notebooks is not the big deal because Jupyter only import what we use. This approach simplifies the code and keeps it clean.\nHere’s the quick explanation of the functions and libraries used in this snippet:\nDDGS from duckduckgo_search:\n\nduckduckgo_search: This library allows us to search for images using DuckDuckGo without the need for an API key. So no more begging Google for an API key.\nDDGS: The class that does the heavy lifting of searching for images.\n\nfastcore: - fastcore: A foundational library that make Python feel like a Lamborghini-sleek, powerful, and fast.\nL:\n\nL: A magical list from fastcore that does way more than the regular Python list. Think of it as a list on steroids.\n\nIn our example, search_images is a function that performs an image search using DuckDuckGo. It’s print out the search term being used and return a list of images URLs retrieved from the search results.\nfor more details on the tools, you can refer to the fastcore documentation and the duckduckgo_search documentation.\n\nfrom fastdownload import download_url\ndest = 'bird.jpg'\ndownload_url(urls[0], dest, show_progress=False)\n\nfrom fastai.vision.all import *\nim = Image.open(dest)\nim.to_thumb(256,256)\n\n\n\n\n\n\n\n\n\ndownload_url(search_images('forest photos', max_images=1)[0], 'forest.jpg', show_progress=False)\nImage.open('forest.jpg').to_thumb(256,256)\n\nSearching for 'forest photos'\n\n\n\n\n\n\n\n\n\nfastdownload and download_url:\n\nfastdownload: Think of this as your friendly neighborhood delivery service, but for files. It’s help with downloading files and datasets easier.\ndownload_url: A function that fetches the file you need from a URL. In our case, it says “Hey URL, gimme that picture!” and save it as bird.png\n\nfastai.vision.all:\n\nTThis module from the fastai library is like a Swiss Army knife for vision tasks, providing all the tools you need, from data loaders to model training utilities.\n\nto_thumb: - A method from the PIL.Image class, which is quite handy it resizes an image to a thumbnail while maintaining the aspect rato. Kind of like shrinking your favorite sweater but in a good way\nThese libraries and function streamline the process of getting and preparing the images for our model. For more detailed documentation, you can refer to the fastdownload, fastai vision, and Pillow documentation.\n\n\nDownloading and Preparing Images\nTo build our dataset, we need to download images for the categories we are interested in (‘forest’ and ‘bird’). Here’s how we did it:\n\nsearches = 'forest','bird'\npath = Path('bird_or_not')\nfrom time import sleep\n\nfor o in searches:\n    dest = (path/o)\n    dest.mkdir(exist_ok=True, parents=True)\n    download_images(dest, urls=search_images(f'{o} photo'))\n    sleep(10)  # Pause between searches to avoid over-loading server\n    download_images(dest, urls=search_images(f'{o} sun photo'))\n    sleep(10)\n    download_images(dest, urls=search_images(f'{o} shade photo'))\n    sleep(10)\n    resize_images(path/o, max_size=400, dest=path/o)\n\nSearching for 'forest photo'\nSearching for 'forest sun photo'\nSearching for 'forest shade photo'\nSearching for 'bird photo'\nSearching for 'bird sun photo'\nSearching for 'bird shade photo'\n\n\nPath:\n\nPath: An object-oriented way to work with filesystem paths. It makes handling files and directories as easy as pie.\n\ndownload_images:\n\ndownload_images: This function fetches a bunch of images from the internet and saves them in a specified directory. Like ordering a pizza, but instead of pizza, you get pictures.\n\nPausing Between Searches:\n\nPausing between searches (sleep(10)) is important to avoid overloading the server. Think of it as giving the server a coffee break between each request.\n\nresize_images:\n\nresize_images: A function from fastai that resizes images to a maximum specified size. This is useful for ensuring all images are of a consistent size before training the model.\n\nFor more details on these tools, you can refer to the pathlib, Vision utils documentation.\n\n\nVerifying and Leaning Images\nAfter download images, it’s essential to verify them and remove corrupt or invalid images.\n\nfailed = verify_images(get_image_files(path))\nfailed.map(Path.unlink)\nlen(failed)\n\n0\n\n\nverify_images:\n\nverify_images: Think of this as the bouncer for your image dataset, checking IDs to make sure no bad images get through.\n\nget_image_file:\n\nget_image_file: This function grabs all image paths in a directory. It’s like having someone fetch all your misplaced socks in the laundry room.\n\nPath.unlink:\n\nPath.unlink: A method to delete files. This is how we get rid of the bad apples in the bunch.\n\nFortunately, in my case, all downloaded images were valid, so len(failed) return 0–no bad apples in our dataset!\n\n\nThe DataBlock API\nCreating our data loader is a critical step. The DataBlock API in fastai allows us to define how to transform and manage our data easily.\n\ndls = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=[Resize(192, method='squish')]\n).dataloaders(path, bs=32)\n    \ndls.show_batch(max_n=6)\n\n\n\n\n\n\n\n\nHere’s the breakdown of the arguments in DataBlock:\nblocks:\n\nSpecifies the type of inputs and targets. In our case, we have images (ImageBlock) and categories (CategoryBlock). It’s like saying, “I have pictures of cats and dogs”\n\nget_items:\n\nFunction to get the list of items. Here we’re using get_image_file to retrieve all our image files.\n\nsplitter:\n\nDefines how to split the dataset into training and validation sets. RandomSplitter(valid_pct=0.2, seed=42) means 20% of the data will be used for validation. The seed ensures that every time we run the code we get the same split. Think of like setting your DVR to record your favorite show at the same time every week.\n\nget_y:\n\nFunction to get the target label from each item. We use parent_label to get the label from parent directory name (e.g., ‘forest’ or ‘bird’)\n\nitem_tfms:\n\nitem transformation to apply. We use Resize(129, method='squish') to resize images to 129x129 pixels by squishing them if necessary.\n\ndataloaders:\n\nCreates the data loaders for our dataset, with a batch size of 32. Data loaders are like conveyor belt that feed the data into your model in manageable chunks.\n\nThe show_batch method is handy way to visualize a batch of data items. It’s like a quick preview to make sure everything looks good.\nFor more details, checkout the fastai DataBlock API documentation.\n\n\nTraining the Model: Welcome to the Learner World\nAfter preparing our dataset, it’s time to train our model. We use the vision_learner function to setup a learner and the powerful fine_tune method to train the model.\n\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(3)\n\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n  0%|          | 0.00/44.7M [00:00&lt;?, ?B/s]100%|██████████| 44.7M/44.7M [00:00&lt;00:00, 145MB/s] \n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.120399\n1.209828\n0.411765\n00:01\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.185352\n0.054729\n0.029412\n00:01\n\n\n1\n0.102830\n0.023147\n0.000000\n00:01\n\n\n2\n0.072183\n0.049310\n0.029412\n00:01\n\n\n\n\n\nvision_learner:\n\nThis create a learner object that combines our data loaders(dls) and a pre-trained model(resnet18). We basically saying, “Hey, take this data and use this model to learn from it.”\n\nresnet18:\n\nA specific architecture of a Convolutional Neuron Network that’s been pre-trained on a large dataset. Think of it as seasoned detective who’s seen it all and just need to be briefed on this specific case.\n\nmetrics=error_rate:\n\nThis specifies that we want to use the error rate as a metric to evaluate our model’s performance. It’s like having a scoreboard to keep track of who’s winning.\n\n\nfine_tune(3):\n\nHere’s where the magic happens. Unlike the traditional fit method, fine_tune starts by refining the pre-trained model with our specific data. It’s like taking your detective and train them on a nuances of this particular mystery. The 3 indicates the number of epochs (full cycles through the training data).\n\nThe fine_tune method is particularly powerful because it starts with a model that already knows a lot (thanks to pre-training) and fine-tune it to specific task. This approach often yields better results, faster and with less data, compared to training a model from scratch.\n\n\nMaking Predictions\nFinally, let’s make our bird classifier predict whether or not an image contain a bird.\n\nis_bird,_,probs = learn.predict(PILImage.create('bird.jpg'))\nprint(f\"This is a: {is_bird}.\")\nprint(f\"Probability it's a bird: {probs[0]:.4f}\")\n\n\n\n\n\n\n\n\nThis is a: bird.\nProbability it's a bird: 0.9988\n\n\nPILImage.create:\n\nThis function create a image object from a file. It’s like saying “Hey, look at this picture I just took.”\n\nlearn.predict:\n\nThis method uses our train model to predict what’s in a image. It’s like asking your well-trained detective, “What do you see in this picture?”\nThe method returns three values:\n\nis_bird: The predicted label(whether it’s a bird or not).\nprobs: The probabilities associated with each class.\n\n\nWhen we print out the predicted label and the probability. If the model says it’s a bird with a high probability, you can feel pretty confident your model knows its bird!\nBuilding the “Is it a Bird?” classifier was hands-on way to introduce the principles of deep learning. By leveraging fastai and Pytorch, we could quickly create an effective model with minimal code. This approach of starting with practical, top-down learning ensures that we see immediately results and understand the real world applicability of deep learning from the get-go."
  },
  {
    "objectID": "posts/2024-06-30-your-deep-learning-journey/index.html#what-is-machine-learning",
    "href": "posts/2024-06-30-your-deep-learning-journey/index.html#what-is-machine-learning",
    "title": "First Step in AI: My Experience with fast.ai Lesson 1",
    "section": "What Is Machine Learning",
    "text": "What Is Machine Learning\nAh, the age-old question: What is the machine learning? Well, imagine if your computer was a child, and you were its teacher. Instead of giving it a a strict set of rules to follow(which, let’s be honest, kids hate), you give it examples from which it can learn. In essence, machine learning is about enabling computer to learn from data rather than being explicitly programmed. It’s like teaching your computer how to ride a bike by letting it practice, fall and get up again, rather than reading it a manual\nLet’s take a closer look at this with a series of visualizations:\n\nTraditional Programming\nIn traditional Programming we write explicit instructions-a program-that processes input to produce results.\n\n\n\n\n\n\n\n\n\nThink of it as following a recipe step-by-step: preheat the oven, mix the ingredients, bake for 30 minutes, and voilà, you have a cake.\n\n\nProgram Using Weight And Assignment\nIn machine learning, we use model with weights(parameters) that processes inputs to generates result.\n\n\n\n\n\n\n\n\n\nHere, the model is like a reflexible recipe that can adjust itself. The ingredients(inputs) are mixed differently depending on the weights, and the output is a delicious result that varies based on those adjustments.\n\n\nTraining a Machine Learning Model\nTraining a model involves feeding inputs through the model to produce results, measuring performance and updating the weights to improve accuracy.\n\n\n\n\n\n\n\n\n\nThink of it as trial and error. The model tries to bake a cake, and if it’s to salty, it adjusts the recipe (update the weights). Over time, it learns the perfect proportions.\n\n\nUsing a Trained Model\nOnce the model is trained, it can be used just like a traditional program, taking inputs and producing results predictably.\n\n\n\n\n\n\n\n\n\nNow, you have reliable recipe that consistently makes the perfect cake. The model processes new inputs(ingredients) and produces outputs(cakes) with the learned adjustments."
  },
  {
    "objectID": "posts/2024-06-30-your-deep-learning-journey/index.html#what-our-image-recognizer-learned",
    "href": "posts/2024-06-30-your-deep-learning-journey/index.html#what-our-image-recognizer-learned",
    "title": "First Step in AI: My Experience with fast.ai Lesson 1",
    "section": "What Our Image Recognizer Learned",
    "text": "What Our Image Recognizer Learned\nAt this stage, we have an image recognizer that works very well. But what is it actually doing? Although many people believe that deep learning results in impenetrable “black box” models (where predictions are given, but no one understand why), this isn’t entirely true. There is a vast body of research showing how to inspect deep learning model deeply and gain rich insights for them. However, all kind of machine learning model (including machine learning and traditional statistical models) can be challenging to fully understand, especially when dealing with new data that differs significantly from the training data.\nWhen we fine-tuned our pre-trained model, we adapted the last layers(originally trained on general features like flowers, humans, animals) to specialize in a birds versus non-birds problem. Imagine our model initially knew how to recognize the entire zoo, but now we’ve trained it to focus solely on recognizing birds. More generally, we could specialize such a pre-trained model on many different tasks."
  },
  {
    "objectID": "posts/2024-06-30-your-deep-learning-journey/index.html#beyond-image-classification-other-application-of-deep-learning",
    "href": "posts/2024-06-30-your-deep-learning-journey/index.html#beyond-image-classification-other-application-of-deep-learning",
    "title": "First Step in AI: My Experience with fast.ai Lesson 1",
    "section": "Beyond Image Classification: Other Application of Deep Learning",
    "text": "Beyond Image Classification: Other Application of Deep Learning\nDeep learning isn’t just about figuring out whether there’s bird in your photo. It’s way more powerful than that! Let’s explore a couple of areas where deep learning make significant strides:\n\nImage Segmentation:\nSegmentation is a process of identifying and labeling pixels in an image belonging to the same object. This is critically important for application like autonomous vehicles where the car needs to recognize and localize object such as pedestrians, other vehicles, and road signs. Instead of just saying, “Hey, there’s a cat in a picture”, segmentation says, “Here’s the outline of the cat in this picture”.\nNatural Language Processing (NLP): Deep learning has dramatically improved Natural Language Processing over the last few years. Now computers can:\n\nGenerate text: Write coherent and context-aware essays (but don’t trust them with your love letters just yet).\nTranslate languages: Turn English into Spanish, French, or Klingon (okay, maybe not Klingon…yet)\nAnalyze comments: Understand sentiments, detect sarcasm, and probably tell when you’re being a bit snarky.\nLabel words in sentences: Identify parts of speech (nouns, verbs, adjectives, etc.), entities (like names and places), and more.\n\n\nHere’s some cool code to classify the sentiment of a movie review better than anything available just a few years ago:\n\nfrom fastai.text.all import *\ndls = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid='test')\nlearn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\nlearn.fine_tune(4, 1e-2)\n\n\n\n\n\n\n    \n      \n      100.00% [144441344/144440600 00:03&lt;00:00]\n    \n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [105070592/105067061 00:01&lt;00:00]\n    \n    \n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.462561\n0.395122\n0.822320\n03:08\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.301779\n0.248262\n0.899480\n06:38\n\n\n1\n0.244484\n0.202708\n0.921480\n06:38\n\n\n2\n0.189148\n0.194167\n0.926160\n06:37\n\n\n3\n0.148741\n0.191470\n0.929720\n06:38\n\n\n\n\n\n\nlearn.predict(\"I really liked that movie!\")\n\n\n\n\n\n\n\n\n('pos', tensor(1), tensor([7.8042e-04, 9.9922e-01]))\n\n\nAnd boom! You have a state-of-art sentiment analyzer."
  },
  {
    "objectID": "posts/2024-06-30-your-deep-learning-journey/index.html#the-important-of-validation-and-test-sets",
    "href": "posts/2024-06-30-your-deep-learning-journey/index.html#the-important-of-validation-and-test-sets",
    "title": "First Step in AI: My Experience with fast.ai Lesson 1",
    "section": "The Important of Validation and Test Sets",
    "text": "The Important of Validation and Test Sets\nWe’ve trained our model and it’s looking pretty smart, but know how do we know it’s actually learned something useful? This is where validation and test sets come in.\n\nWhy Do We Need a Validation set?\nThe goal of a model is to make predictions about unseen data. If we trained a model with all our data and evaluated it using the same data, we wouldn’t really know how well it performs on new, unseen data. It could just memorize the training data(cheating basically). The model could get great results on your training data but bomb when given the data to analyze. To avoid this, we: - We split dataset: We divide our data into training and validation sets. The training set is used to teach the model, and the validation set is used to see how well it’s learning\n\n\nPreventing Overfitting with a Test set\nOverfitting is a common issue where the model preform exceptionally well on the training set but poorly on the validation set, meaning it has memorized the training data rather than learning the generalizable pattern.\nEven when your model hasn’t fully memorized all your data, it might memorized certain parts of it during earlier training stages. The longer you train, the better the accuracy on the training set, but eventually, the validation accuracy will start to decline. This is because your model is begins memorizing the training data instead of learning the pattern that generalize well. When this happens, we say the model is overfitting.\nHere’s an example to visualize overfitting:\n\n\n\n\nExample of overfitting\n\n\nThe Image shows what happens when you overfit, using a simplified example where we have just one parameter and some randomly generated data. Although the overfitted model’s prediction are accurate for the data near the observed data points, they are way off when outside of that range.\nOverfitting is the single most important and challenging issue when training machine learning models. It’s easy to create a model that does the great job at making predictions on the data it’s been trained on, but making accurate predictions on new data is much harder.\nFor instance, if you writing a handwritten digit classifier (as we will very soon) and use it to recognize numbers on checks, you won’t see the same numbers the model was trained on–checks will have different variations of handwriting to deal with."
  },
  {
    "objectID": "posts/2024-06-30-your-deep-learning-journey/index.html#wrapping-up",
    "href": "posts/2024-06-30-your-deep-learning-journey/index.html#wrapping-up",
    "title": "First Step in AI: My Experience with fast.ai Lesson 1",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nDeep learning is an exciting field that extends far beyond simple image classification. From understand speech to translate languages and detecting malware, it’s applications are vast. Through this blog post, we’ve seen how to build a bird classifier using the fastai library-an accessible, powerful tool that simplifies the complexities of machine learning.\nBy splitting our data into training and validation sets, we ensure our model doesn’t cheat and genuinely learns the task at hand. With powerful tools like fastai and the ability to handle the diverse tasks, deep learning truly has potential to transform numerous industries.\nI hope you enjoyed this journey as much as I did. Remember, the key to mastering deep learning is to keep experimenting and learning. So go ahead, build that next big thing, and maybe teach your computer to recognize your pet fish or translate cat’s meows!"
  },
  {
    "objectID": "posts/2024-06-30-your-deep-learning-journey/index.html#final-thoughts",
    "href": "posts/2024-06-30-your-deep-learning-journey/index.html#final-thoughts",
    "title": "First Step in AI: My Experience with fast.ai Lesson 1",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nThank you for joining me on this deep learning adventure! if you find this blog helpful or inspiring, please share it with others who might also be interested. Deep learning is a continuously evolving field with endless possibilities. Stay curious, keep learning, and don’t hesitate to dive deeper into the world of AI.\nFeel free to leave your comments, questions, or insights below. I’d love to hear your experiences, projects, and what you’re learning. Together, we can continue to explore and push the boundaries of what’s possible with deep learning.\nHappy coding, and may your models always be accurate!"
  }
]
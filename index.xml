<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Bui Huu Dai</title>
<link>https://bhdai.github.io/blog/</link>
<atom:link href="https://bhdai.github.io/blog/index.xml" rel="self" type="application/rss+xml"/>
<description>Dai&#39;s blog.</description>
<generator>quarto-1.7.33</generator>
<lastBuildDate>Wed, 13 Aug 2025 17:00:00 GMT</lastBuildDate>
<item>
  <title>Attentions and Transformers</title>
  <dc:creator>Bui Huu Dai</dc:creator>
  <link>https://bhdai.github.io/blog/posts/attention-and-transformers/</link>
  <description><![CDATA[ 






<section id="recurrent-neural-networks-and-the-bottleneck" class="level2">
<h2 class="anchored" data-anchor-id="recurrent-neural-networks-and-the-bottleneck">Recurrent neural networks and the bottleneck</h2>
<p>For the first time we step a way from static image and introduce this idea of Recurrent neural network, or RNN. These are really powerful and flexible class because they’re design to operate on sequential data.</p>
<p><a href="./images/rnn.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1"><img src="https://bhdai.github.io/blog/posts/attention-and-transformers/images/rnn.png" class="img-fluid"></a></p>
<p>We see all these different architectures patterns: one-to-many for tasks like image captioning, many-to-one for something like video classification, and many-to-many for things like machine translation. The core idea is this recurrent connection - the hidden state that gets passed from one time step to the next, which gives the model a kind of memory. But these sequential processing and the way gradient flow time also introduces some pretty significant challenge especially dealing with very long sequences.</p>
<p>So, today, we’re going to build on that and introduce two new, very important concepts that address these challenges head-on: <strong>Attention</strong> and <strong>Transformers</strong>. First we’ll talk about attention, you can think of this as a new kind of building block, a new primitive operation for our neural network. At a high level, it’s a mechanism that operates on a set of vectors. It allow the model to dynamically focus on the most relevant part of the inputs when it’s producing an output. And that brings us to the Transformer. A transformer is a full-brown neural network architecture that uses this attention mechanism everywhere. It’s a very powerful design that completely get rid of sequential recurrence of RNNs and instead relies entirely on attention to model dependencies inputs and output.</p>
<p>Now, it’s really hard to overstate the impact that Transformers have had. They are everywhere today. They are the fundamental architecture behind the large language models you’ve heard about, like GPT and BERT, and more and more, they’re being applied to computer vision, often outperforming the convolutional networks that we’ve spent so much time on. But, before we can really understand the Transformer, I think it’s critical to understand where it came from. It didn’t just appear out of thin air. It actually developed as an offshoot of the RNN-based sequence-to-sequence models. The attention mechanism was first introduced to fix a fundamental bottleneck in those models. So, to properly motivate and understand why Transformers work the way they do, we need to go back to that starting point. So let’s start there</p>
<p>Alright, so to really understand where attention came from, let’s first dive deep into the problem it was designed to solve. And that problem arises in what are called sequence-to-sequence models built with RNNs</p>
<p><a href="./images/sts-rnn.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2"><img src="https://bhdai.github.io/blog/posts/attention-and-transformers/images/sts-rnn.png" class="img-fluid"></a></p>
<p>So let’s set up the problem. The task is “sequence-to-sequence.” We have an input sequence of vectors, <em>x<sub>1</sub></em> through <em>x<sub>T</sub></em>, and we want to produce an output sequence, <em>y<sub>1</sub></em> through <em>y<sub>T’</sub></em>, where the lengths <em>T</em> and <em>T’</em> can be different. A great, motivating example for our whole discussion today is machine translation. Let’s say we want to translate a sentence from English to Italian. Our input sequence might be the words “we see the sky.” The standard way to tackle this with RNNs is with an architecture called an Encoder-Decoder.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ah_t%20=%20f_w(x_t,%20h_%7Bt-1%7D)%0A"></p>
<p>The first part of this model is the <strong>Encoder</strong>. Its job is to read, or “encode,” the entire input sequence. So we take our first input word “we,” pass it into an RNN cell, and it produces a hidden state <em>h1</em>. Then, at the next time step, we take the next word “see” and the previous hidden state <em>h1</em> to produce the new hidden state <em>h2.</em> And we just repeat this process for the entire input sequence, one word at a time, until we’ve processed the last word, “sky,” and produced our final hidden state <em>h4.</em> This is exactly the RNN behavior. So, once we’ve read the entire input sequence, what happens? Well, the whole idea here is that the final hidden state of the encoder, <em>h<sub>T</sub></em> (or <em>h<sub>4</sub></em> in this case), has to somehow summarize the meaning of the entire input sentence. We take this final hidden state and we call it the <strong>context vector</strong>, which we will label <em>c.</em> This single vector <em>c</em> is the only piece of information that gets passed from the encoder to the next part of the model. It’s supposed to be a fixed-size summary of the entirely input. We also use this context vector to define the initial hidden state <em>s<sub>0</sub>,</em> for the decoder</p>
<p><img src="https://latex.codecogs.com/png.latex?%0As_t%20=%20g_U(y_%7Bt-1%7D,%20s_%7Bt-1%7D,%20c)%0A"></p>
<p>This brings us to the second half of the model, the <strong>Decoder</strong>. The decoder is another RNN, and its job is to take that context vector <em>c</em> and generate the output sequence, word by word. So how does it start? It’s initialized with hidden state <em>s<sub>0</sub></em> that we got from context vector. To kick of the generation process, we feed a special [START] token as its first input, which we call <em>y<sub>0</sub>.</em> The decoder RNN then process its state <em>s<sub>0</sub></em> and the input <em>y<sub>0</sub>,</em> and it produces two things: an output <em>y<sub>1</sub>,</em> which is the first word of our translated sentence, “vediamo,” and a new hidden state, <em>s<sub>1</sub>.</em> Notice that the context vector <em>c</em> is used as an input at every step of the decoder, constantly reminding it of the original sentence. Okay, so what happens at the next time step? This is where it gets interesting. The model operates in what’s called an “auto-regressive” fashion. That means the output from the previous step becomes the input for the current step. So, we take the word we just generated, “vediamo” (<em>y<sub>1</sub></em>), and feed that back into the decoder as the input for the next time step. The decoder then takes its previous state <em>s<sub>1</sub></em> and this new input <em>y<sub>1</sub></em> to produce the next state <em>s<sub>2</sub></em> and the next output word, <em>y<sub>2</sub>,</em> which in this case is “il.” And this process just continues. We feed “il” back in as input, the decoder produces “cielo.” We feed “cielo” back in as input, and eventually, the decoder will produce a special [STOP] token, which tells us that the sentence is complete and we can stop the generation process.</p>
<p>So just summarizing the whole flow: the encoder reads the input sequence and squashes it all down into one context vector. The decoder then text that context vector and unrolls it, generating input sequence one word at a time, feeding its own prediction back in as input.</p>
<p>Alright, so this architecture seems pretty reasonable, right? It was state-of-the-art for several years. But there’s a really fundamental problem with this design. And the problem is <strong>the input sequence bottlenecks through a fixed-size context vector <em>c</em></strong>. Think about it. The entire meaning of the input sequence, all the word, their order, their grammatical relationships all of it has to be compressed and stuffed into this single vector. For a short sentence like “we see the sky,” maybe that’s plausible. But now, as a thought exercise, what if you’re translating a whole paragraph? What if your input sequence length <em>T</em> is 1000? You’re asking the model to summarize a thousand words of nuanced information into a single vector of maybe 512 or 1024 numbers. That is an immense information bottleneck. The model is almost certainly going to forget information, particularly from the beginning of the sequence. It puts a tremendous amount of pressure on the model to represent everything perfectly in this one vector.</p>
</section>
<section id="the-attention-mechanism" class="level2">
<h2 class="anchored" data-anchor-id="the-attention-mechanism">The attention mechanism</h2>
<p>So, what’s the solution? Well, what if we could remove this bottleneck entirely? The core idea, and this is really the key conceptual leap, is to allow the decoder to <strong>look back the whole input sequence at each time step of the output generation</strong>. Instead of relying on a single summary, at every time step where it’s trying to produce an output word, the decoder can have direct access to every single hidden state from encoder. This way it can dynamically decide which part of the input are most relevant to the specific word it’s about to generate. This is the central idea behind attention.</p>
<p><a href="./images/rnn-attention.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3"><img src="https://bhdai.github.io/blog/posts/attention-and-transformers/images/rnn-attention.png" class="img-fluid"></a></p>
<p>This is the Sequence to Sequence model with RNNs and Attention. This was first proposed in a paper <a href="https://arxiv.org/abs/1409.0473">“Neural Machine Translation by Jointly Learning to Align and Translate”</a> by Bahdanau et al.&nbsp;in 2015, and it was a landmark result for machine translation. The encoder part of the model is exactly the same as before. We run an RNN over the input sequence, “we see the sky,” and we compute a hidden state <em>h<sub>1</sub>,</em> <em>h<sub>2</sub>,</em> <em>h<sub>3</sub>,</em> <em>h<sub>4</sub></em> for each input token. But here’s the crucial difference: we are no longer going to discard <em>h<sub>1</sub>,</em> <em>h<sub>2</sub>,</em> and <em>h<sub>3</sub>.</em> We’re going to keep all of them. These encoder hidden states will be the values we “attend” to. We still use the final hidden state <em>h<sub>4</sub></em> to initialize the decoder’s first hidden state, <em>s<sub>0</sub>,</em> just to give it a starting point. So now we’re at the first step of the decoder. Our goal is to generate the first output word. To do this, we first need to figure out which of the input words are most relevant. We do this by computing a set of <strong>scale alignment scores</strong>. You’ll also see these called attention score. Let’s call them <em>e<sub>ti</sub>,</em> which represent the score between the t-th decoder step and the i-th encoder hidden state. How do we compute this? We take our current state of our decoder and we compare it to each of the encoder hidden state, <em>h<sub>1</sub></em> through <em>h<sub>4</sub>.</em> This comparison is done by a simple neural network, <em>f<sub>att</sub>.</em></p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ae_%7Bt,i%7D%20=%20f_%7Batt%7D(s_%7Bt-1%7D,%20h_i)%0A"></p>
<p>In practice, this is often just a simple linear layer, sometimes with a <em>tanh</em> activation function. It’s a trainable function that takes two vector, a decoder state and an encoder state and splits out a single scalar number that present how well they match or align. Now we have these raw alignment scores <em>e.</em> But they’re just arbitrary real numbers. What we want is a distribution. We want to know what proportion of our attention should go to each input word. So, the next step is to normalize these alignment scores to get <strong>attention weights</strong>, which we’ll call <em>a</em>. And the natural way to turn a set of arbitrary scores into a distribution is to use a <strong>softmax</strong> function. We apply a softmax over all the alignment scores for this time step. The resulting attention weights, <em>a<sub>11</sub></em> through <em>a<sub>14</sub>,</em> now have some nice properties: they are all between 0 and 1, and they all sum to 1. So they form a proper probability distribution over the input sequence.</p>
<p>Alright, so now we have these weights that tell us how much to focus on each input word. What do we do with them? The next step is to compute our <strong>context vector</strong> <em>c<sub>t</sub></em>, as a <strong>weighted sum</strong> of the encoder hidden states. So for our first time step, <em>c<sub>1</sub></em> will be <img src="https://latex.codecogs.com/png.latex?a_%7B11%7D%20%5Ccdot%20h_1%20+%20a_%7B12%7D%20%5Ccdot%20h_2%20+%20a_%7B13%7D%20%5Ccdot%20h_3%20+%20a_%7B14%7D%20%5Ccdot%20h_4">. This is the most critical part of the whole mechanism. Instead of one static context vector for the whole decoding process, we are now computing a new, dynamic context vector at every single time step. This context vector, <em>c<sub>t</sub>,</em> is tailored specifically for generating the t-th output word. It’s created by pooling information from the encoder states, weighted by how relevant they are for the current decoder state.</p>
<p>Now that we have this dynamically computed context vector <em>c<sub>1</sub>,</em> the rest of the decoder step proceeds much like before. We feed this new context vector <em>c<sub>1</sub></em> into our decoder RNN unit, along with the previous output <em>y<sub>0</sub></em> (the [START] token). The RNN then updates its hidden state from <em>s<sub>0</sub></em> to <em>s<sub>1</sub></em> and predicts the first output word, <em>y<sub>1</sub>,</em> which is “vediamo.” And the important thing to realize is that this whole process will repeat for the next step. To generate <em>y<sub>2</sub>,</em> we will first use the new decoder state <em>s<sub>1</sub></em> to re-calculate a whole new set of attention weights <em>a<sub>2i</sub>,</em> which gives us a new context vector <em>c<sub>2</sub>,</em> and so on. The context is no longer fixed, it’s recomputed for every single output token.</p>
<p>So let’s just pause for a second and build some intuition for what’s happening here. The context vector <em>c<sub>t</sub></em> now effectively <strong>attends to the relevant part of the input sequence</strong>. For our example, when we’re trying to generate the word “vediamo,” which corresponds to “we see,” it’s very likely that a well-trained model would learn to produce high attention weights <em>a<sub>11</sub></em> and <em>a<sub>12</sub></em> (for the inputs “we” and “see”) and low attention weights <em>a<sub>13</sub></em> and <em>a<sub>14</sub></em> (for “the” and “sky”). This means that the context vector <em>c<sub>1</sub></em> would be composed mostly of the information from <em>h<sub>1</sub></em> and <em>h<sub>2</sub>.</em> It allows the model to focus its “attention” precisely where it’s needed. This completely resolves the bottleneck problem we identified earlier. Now a question you might have is: Where is the supervision for the attention weights come from? Do we need to separate the dataset to tell the model which input words a align with which output word? And the answer, which is really quite beautiful, is <strong>no</strong>. There is <strong>no direct supervision on the attention weights</strong>. The entire attention mechanism—the <em>f<sub>att</sub></em> linear layer, the softmax, the weighted sum is a fully differentiable computation. We can just plug it into our network, and we can backpropagate the final loss (from predicting the correct output word) all the way back through everything, including the attention mechanism. The model learns to produce sensible attention weights entirely on its own, through end to end training, simply because doing so helps it minimize the overall loss function. It’s a learned, latent alignment that emerge as a side effect of trying to solve the main task.</p>
<p>This new architecture is better in two critical ways. First, and most importantly, the input sequence is <strong>no longer bottlenecked through a single vector</strong>. We’ve completely remove that constraint. The decoder has direct access to the full sequence of encoder hidden state at every step. And second, following from that, <strong>at each timestep of the decoder, the context vector “looks at” different parts of the input sequence</strong>. As you can see in this simplified diagram, we compute a different context vector <em>c<sub>1</sub>,</em> <em>c<sub>2</sub>,</em> <em>c<sub>3</sub></em> for each and every output step. This allows the model to create a soft, dynamic alignment between the source and target sequences, which is a much more powerful and flexible way to handle the relationship between them, especially for long and complex inputs. This idea of using attention to overcome the limitations of sequential processing was incredibly powerful. And it naturally leads to the next question: if attention is so good at relating different parts of a sequence, do we even need the recurrent part of the RNN at all? And that’s exactly the question that the Transformer paper set out to answer.</p>
<p><a href="./images/attention.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4"><img src="https://bhdai.github.io/blog/posts/attention-and-transformers/images/attention.png" class="img-fluid"></a></p>
<p>So here’s a famous visualization from that original <a href="https://arxiv.org/abs/1409.0473">Bahdanau attention paper</a>. What they did was train an English-to-French translation model and then plotted the attention weights, <em>a<sub>ti</sub>,</em> for a given sentence. What you’re looking at here is a matrix. The brightness of the pixel at position (<em>i</em>, <em>j</em>) tells you the strength of the attention weight. So, a bright pixel means that when the model was generating output word <em>j,</em> it was paying a lot of attention to input word <em>i.</em> Let’s look at the specific example here. The input sentence in English is: “The agreement on the European Economic Area was signed in August 1992.” And the model correctly translates this to the French output: “L’accord sur la zone économique européenne a été signé en août 1992.” Now, if you look at the structure of this attention map, you can see some really interesting patterns. For large parts of the sentence, the attention is strongly diagonal. You can see that when the model generates “L’accord,” it’s paying strong attention to “The agreement.” When it generates “sur la,” it’s looking at “on the.” Down at the end, when it generates “signé en août 1992,” it’s looking at “signed in August 1992.” This makes perfect sense because, for these parts of the sentence, the word order between English and French is pretty much the same. This diagonal alignment is a great sanity check; it shows us that the model has learned a very sensible, monotonic alignment between the two languages. But here’s where it gets really cool. Look at the part of the sentence that translates “European Economic Area.” In French, the word order is flipped: “zone économique européenne.” And look what the model does! The attention map is no longer diagonal here. When it generates the word “économique,” you can see a bright spot where it’s attending to the English word “Economic.” But then, when it generates the next word, “européenne,” it correctly looks back to the word “European” in the input. This is really powerful. It shows that the model isn’t just doing a simple, one-to-one mapping. The attention mechanism has allowed it to learn these non-trivial, non-monotonic alignments between the source and target languages. It figures out these complex word re-orderings all on its own, just through backpropagation.</p>
</section>
<section id="scaled-dot-product-attention" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="scaled-dot-product-attention">Scaled dot-product attention</h2>
<p>Let’s try to formalize this a bit using some new terminology. In this setup, we can think of having two sets of vectors. First, we have <strong>Query vectors</strong>. In our RNN example, these were the decoder hidden states (<em>s<sub>0</sub></em>, <em>s<sub>1</sub>,</em> etc.). At each step, the decoder state is forming a query that essentially asks, “Given my current context, what information from the input is most relevant for me to generate the next word?” Second, we have <strong>data vectors</strong>, which are the encoder RNN states (<em>h<sub>1</sub></em>, <em>h<sub>2</sub>,</em> etc.). These are the vectors that hold the information we want to retrieve. The query is posed against this set of data vectors. And the result of this operation is a set of output vectors, which in our case were the context vectors (<em>c<sub>1</sub></em>, <em>c<sub>2</sub>,</em> etc.). So the core operation is this: <strong>Each query attends to all data vectors and give one output vector</strong>. That output vector is a summary of the data vectors, weighted by how relevant they are to that specific query. This Query-Data-Output framework is the general abstraction of the attention mechanism, and it’s the key idea that will let us move beyond RNNs entirely</p>
<p>So, let’s define the input for this layer. The first input is a single <strong>Query vector</strong>, which we’ll call <em>q</em>. This vector have some dimensionality, let’s say <em>D<sub>Q</sub></em>. . In our running RNN example, the query vector at the first time step was the initial decoder state, <em>s<sub>0</sub>.</em> This is the vector that is “asking the question.”</p>
<p>The second input to our Attention Layer is a set of <strong>Data vectors</strong>, which we will represent as a matrix <em>X</em>. This matrix has <em>N<sub>X</sub></em> rows, where <em>N<sub>X</sub></em> is the number of data vectors, and <em>D<sub>X</sub></em> columns, for the dimensionality of each vector. In our RNN example, the data vectors were the set of <em>N<sub>X</sub></em> = 4 encoder hidden states, <em>h<sub>1</sub></em> through <em>h<sub>4</sub>.</em> These are the vectors containing the information that the query wants to selectively read from.</p>
<p>Okay, so those are the inputs: one query <em>q</em> and a set of data vectors <em>X.</em> Now let’s define the computation that happens inside the layer. The first step is to compute <strong>similarities</strong>. We take the query vector <em>q</em> and compare it to every one of the <em>N<sub>X</sub></em> data vectors <em>X<sub>i</sub>.</em> This gives us <em>N<sub>X</sub></em> similarity scores, <em>e<sub>i</sub>,</em> which we can collect into a vector <em>e.</em> Just like before, this similarity is calculated by some function <em>f<sub>att</sub>,</em> which is typically a simple learned linear layer.</p>
<p>Step two: Once we have these raw similarity scores <em>e,</em> we need to normalize them into a distribution. So, we compute the <strong>attention weights</strong>, <em>a,</em> by simply applying a softmax function to the entire vector of similarity scores. The result, <em>a,</em> is a vector of length <em>N<sub>X</sub></em> where all the elements sum to 1. These weights tell us how much importance to assign to each of the data vectors.</p>
<p>The final step is to compute the single <strong>Output vector</strong>, which we’ll call <em>y.</em> This is done by taking a weighted sum of the data vectors <em>X<sub>i</sub>,</em> using our just-computed attention weights <em>a<sub>i</sub></em> as the coefficients. So, <img src="https://latex.codecogs.com/png.latex?y%20=%20%5Csum_i%20a_i%20%5Ccdot%20X_i">. The dimensionality of this output vector <em>y</em> will be <em>D<sub>X</sub>,</em> the same as the data vectors, because it’s just a linear combination of them. In our RNN example, this output vector was the context vector <em>c<sub>1</sub>.</em> And that’s it! We have now defined a complete, self-contained Attention Layer. It’s a differentiable module that takes one query vector <em>q</em> and a set of data vectors <em>X</em> as input, and produces a single output vector <em>y</em> that is a summary of <em>X</em> from the perspective of <em>q.</em></p>
<p>Now, let’s start refining this layer to get to the specific formulation that’s used in the Transformer model. We’ll make a few small but important changes. The first change is how we compute the similarity. In our general definition, we just said we use some function <em>f<sub>att</sub>,</em> which is typically a small neural network. It turns out that in practice, we can do something much simpler and more efficient. We can just use the <strong>dot product</strong> between the query vector <em>q</em> and each data vector <em>X<sub>i</sub>.</em> So the new similarity calculation is simply <img src="https://latex.codecogs.com/png.latex?e_i%20=%20q%20%5Ccdot%20X_i">. This is computationally very fast, and it works well. For this to be possible, of course, the dimensionality of the query <em>D<sub>Q</sub></em> and the data vectors <em>D<sub>X</sub></em> must be the same.</p>
<p>Okay, so that’s a nice simplification. But it turns out that if you just use a plain dot product, you can run into problems during training, especially when the dimension of these vectors is large. So this brings us to our second change: we don’t just use the dot product, we use a <strong>scaled dot product</strong>. We compute the dot product just as before, but then we scale it down by dividing by the square root of the dimension of the data vectors, <img src="https://latex.codecogs.com/png.latex?%5Csqrt%7B%20D_X%20%7D">. So the similarity is <img src="https://latex.codecogs.com/png.latex?e_i%20=%20(q%20%5Ccdot%20X_i)%20/%20%5Csqrt%7B%20D_X%20%7D">. This might seem like a weird, magical detail, but it’s actually incredibly important for stabilizing the training process. So, let’s do a quick thought experiment to understand why this scaling is so critical. If we have very large similarity values, these will cause the softmax function to saturate, which in turn leads to vanishing gradients, making the model very hard to train. So, why would the dot product values become large? Well, let’s think about the dot product <img src="https://latex.codecogs.com/png.latex?q%20%5Ccdot%20X_i">. Suppose the components of <em>q</em> and <em>X<sub>i</sub></em> are independent random variables with zero mean and unit variance. Then the variance of their product is also 1. The dot product is a sum of <em>D<sub>X</sub></em> of these products. So, the variance of the dot product itself will be <em>D<sub>X</sub>.</em> This means that as the dimensionality of our vectors <em>D<sub>X</sub></em> grows, the magnitude of our dot products will also grow. And if the inputs to a softmax function are very large in magnitude, the softmax output will be pushed to be very hard, one value will be very close to 1, and all the others will be very close to 0. When the softmax is in this saturated state, its gradients are extremely small, close to zero. And if the gradients are zero, no learning can happen. So, by dividing by <img src="https://latex.codecogs.com/png.latex?%5Csqrt%7B%20D_X%20%7D">, we are effectively normalizing the variance of the dot product back to 1. This keeps the inputs to the softmax in a reasonable range, prevents saturation, and allows gradients to flow properly during training. It’s a crucial trick for making these models work.</p>
<p>Alright, so we’ve refined our similarity calculation. The next generalization is to handle not just one query vector at a time, but a whole set of them. So instead of a single query vector <em>q,</em> our input will now be a matrix <em>Q</em> of shape [<em>N<sub>Q</sub></em> x <em>D<sub>X</sub></em>], containing <em>N<sub>Q</sub></em> different query vectors. And the beauty of using dot products is that this entire computation can now be expressed as a single, highly efficient matrix multiplication. To get all the similarity scores <em>E,</em> we just compute <img src="https://latex.codecogs.com/png.latex?Q%20%5Ccdot%20X%5ET%20/%20%5Csqrt%7BD_X%7D">. The softmax is applied to each row of this similarity matrix. And the final output <em>Y</em> is just the attention weight matrix <em>A</em> times the data matrix <em>X.</em> So now our Attention Layer takes a set of <em>N<sub>Q</sub></em> queries and a set of <em>N<sub>X</sub></em> data vectors, and it produces a set of <em>N<sub>Q</sub></em> output vectors. Each output vector <em>Y<sub>i</sub></em> is a weighted sum of the data vectors, where the weights are determined by the i-th query.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>Inputs:</p>
<ul>
<li>Query vector: <em>Q</em> [<em>N<sub>Q</sub></em> x <em>D<sub>Q</sub></em>]</li>
<li>Data vectors: <em>X</em> [<em>N<sub>X</sub></em> x <em>D<sub>X</sub></em>]</li>
<li>Key matrix: <em>W<sub>K</sub></em> [<em>D<sub>X</sub></em> x <em>D<sub>Q</sub></em>]</li>
<li>Value matrix: <em>W<sub>V</sub></em> [<em>D<sub>X</sub></em> x <em>D<sub>V</sub></em>]</li>
</ul>
<p>Computation:</p>
<ul>
<li>Keys: <img src="https://latex.codecogs.com/png.latex?K%20=%20XW_K"> [<em>N<sub>X</sub></em> x <em>D<sub>Q</sub></em>]</li>
<li>Values: <img src="https://latex.codecogs.com/png.latex?V%20=%20XW_V"> [<em>N<sub>X</sub></em> x <em>D<sub>V</sub></em>]</li>
<li>Similarities: <img src="https://latex.codecogs.com/png.latex?E%20=%20QK%5ET%20/%20%5Csqrt%7B%F0%9D%90%B7_%F0%9D%91%84%7D"> [<em>N<sub>Q</sub></em> x <em>N<sub>X</sub></em>], <img src="https://latex.codecogs.com/png.latex?E_%7Bij%7D%20=%20Q_iK_j%20/%20%5Csqrt%7B%F0%9D%90%B7_%F0%9D%91%84%7D"></li>
<li>Attention weights: A = softmax(E, dim=1) [<em>N<sub>Q</sub></em> x <em>N<sub>X</sub></em>]</li>
<li>Output vector: <img src="https://latex.codecogs.com/png.latex?Y%20=%20AV"> [<em>N<sub>Q</sub></em> x <em>D<sub>V</sub></em>], <img src="https://latex.codecogs.com/png.latex?Y_i%20=%20%5Csum_j%20A_%7Bij%7D%20V_j"></li>
</ul>
</div></div><p>Okay, one final change, and this will bring us to the complete formulation of what is called Scaled Dot-Product Attention, which is the core building block of the Transformer. We’re going to introduce a distinction between Keys and Values. The motivation here is that for each item in our input set, the vector we use to compute similarity (the “key”) might not be the same as the vector we use to compute the output (the “value”). This gives the model more flexibility. So what we do is we take our original data vectors X and we project them through two different learned linear layers, two weight matrices W_K and W_V, to produce a Key matrix K and a Value matrix V. Now, the computation proceeds like this:</p>
<ol type="1">
<li>The Queries <em>Q</em> are compared against the Keys <em>K</em> to compute the similarity scores.</li>
<li>We use these scores to get our attention weights <em>A.</em></li>
<li>And finally, we use these attention weights <em>A</em> to take a weighted sum of the Values <em>V.</em></li>
</ol>
</section>
<section id="self-attention" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="self-attention">Self-Attention</h2>
<p>So, this entire operation that we’ve just visualized taking one set of vectors <em>Q</em> and another set <em>X</em> to produce an output <em>Y</em> has a name. This is called a <strong>Cross-Attention</strong> Layer. It’s called “cross-attention” because the queries are coming from one source, and the keys and values are coming from a different (or “cross”) source. Each query produces one output, and that output is a mixture of information from the data vectors, with the mixing proportions determined by the query’s similarity to the keys. This is precisely the type of attention we used in the sequence-to-sequence RNN model. The decoder states were the queries, and the encoder states were the source for the keys and values. Now, this naturally leads to a very interesting question: what happens if the queries, keys, and values all come from the same set of vectors?</p>
<div class="columns">
<div class="column" style="width:50%;">
<p>This brings us to our next topic. What if the two sets of vectors were actually the same? What if a sequence wanted to attend to itself? This brings us to what is arguably the most important component of the Transformer: the <strong>Self-Attention Layer</strong>. So let’s define this. In a Self-Attention Layer, we only have one set of input vectors, <em>X.</em> There’s no separate set of queries. Instead, the Queries, Keys, and Values are all derived from this same input set <em>X.</em> This is the central idea. For each input vector <em>X<sub>i</sub></em> in our sequence, we are going to generate a query <em>Q<sub>i</sub>,</em> a key <em>K<sub>i</sub>,</em> and a value <em>V<sub>i</sub></em> by projecting <em>X<sub>i</sub></em> through three separate, learnable weight matrices: <em>W<sub>Q</sub>,</em> <em>W<sub>K</sub>,</em> and <em>W<sub>V</sub>.</em> Because the queries and the keys/values now come from the same source, the shapes get a little simpler. The number of queries <em>N<sub>Q</sub></em> is now the same as the number of keys/values <em>N<sub>X</sub>,</em> so all our matrices like <em>E</em> and <em>A</em> will be square: [N x N]. And in practice, we almost always set the dimensions of the queries, keys, and values to be the same, so <em>D<sub>Q</sub></em> = <em>D<sub>K</sub></em> = <em>D<sub>V</sub>.</em> The core concept here is that each input produces one output, and that output is a mixture of information from all inputs in the sequence. It’s a mechanism for each token in a sentence to look at all the other tokens in that same sentence and build a new, more context-aware representation of itself.</p>
</div><div class="column" style="width:50%;">
<p><a href="./images/self-attention.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5"><img src="https://bhdai.github.io/blog/posts/attention-and-transformers/images/self-attention.png" class="img-fluid"></a></p>
</div>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<p>Inputs:</p>
<ul>
<li>Data vectors: <em>X</em> [<em>N</em> x <em>D<sub>in</sub></em>]</li>
<li>Key matrix: <em>W<sub>K</sub></em> [<em>D<sub>in</sub></em> x <em>D<sub>out</sub></em>]</li>
<li>Value matrix: <em>W<sub>V</sub></em> [<em>D<sub>in</sub></em> x <em>D<sub>out</sub></em>]</li>
<li>Query matrix: <em>W<sub>Q</sub></em> [<em>D<sub>in</sub></em> x <em>D<sub>out</sub></em>]</li>
</ul>
<p>Computation:</p>
<ul>
<li>Query: <img src="https://latex.codecogs.com/png.latex?Q%20=%20XW_Q"> [<em>N</em> x <em>D<sub>out</sub></em>]</li>
<li>Keys: <img src="https://latex.codecogs.com/png.latex?K%20=%20XW_K"> [<em>N</em> x <em>D<sub>out</sub></em>]</li>
<li>Values: <img src="https://latex.codecogs.com/png.latex?V%20=%20XW_V"> [<em>N</em> x <em>D<sub>out</sub></em>]</li>
<li>Similarities: <img src="https://latex.codecogs.com/png.latex?E%20=%20QK%5ET%20/%20%5Csqrt%7B%F0%9D%90%B7_%F0%9D%91%84%7D"> [<em>N</em> x <em>N</em>], <img src="https://latex.codecogs.com/png.latex?E_%7Bij%7D%20=%20Q_iK_j%20/%20%5Csqrt%7B%F0%9D%90%B7_%F0%9D%91%84%7D"></li>
<li>Attention weights: A = softmax(E, dim=1) [<em>N</em> x <em>N</em>]</li>
<li>Output vector: <img src="https://latex.codecogs.com/png.latex?Y%20=%20AV"> [<em>N</em> x <em>D<sub>out</sub></em>], <img src="https://latex.codecogs.com/png.latex?Y_i%20=%20%5Csum_j%20A_%7Bij%7D%20V_j"></li>
</ul>
</div></div><p>Now, just as a quick practical note. We said we generate Q, K, and V by passing our input X through three separate weight matrices. As an implementation optimization, these three linear projections are often fused into one larger operation. Instead of three separate matrix multiplies, you can concatenate the three weight matrices W_Q, W_K, and W_V into one big matrix. You then do a single, large matrix multiplication of X with this fused matrix. The result is a wider matrix that you can then just slice or split back into your Q, K, and V matrices. Why do this? Well, it turns out that modern hardware like GPUs are much more efficient at performing one large matrix multiplication than three smaller ones. It reduces overhead and better utilizes the parallel processing capabilities of the hardware. So while conceptually they are three separate operations, in code you will almost always see this fused implementation for performance reasons.</p>
<div class="columns">
<div class="column" style="width:50%;">
<p>Let’s consider <strong>permuting the inputs</strong>. On the right, our original input order was <em>X<sub>1</sub>,</em> <em>X<sub>2</sub>,</em> <em>X<sub>3</sub>.</em> Now, what happens if we feed the exact same vectors into the layer, but we just shuffle their order? Let’s say the new input order is <em>X<sub>3</sub>,</em> <em>X<sub>1</sub>,</em> <em>X<sub>2</sub>.</em> What’s going to happen to the output? Well, let’s trace the computation. The first step is to compute the queries, keys, and values. Since each <em>Q<sub>i</sub>,</em> <em>K<sub>i</sub>,</em> <em>V<sub>i</sub></em> triplet is generated only from the corresponding input <em>X<sub>i</sub>,</em> if we permute the inputs <em>X,</em> then the resulting sets of <em>Q,</em> <em>K,</em> and <em>V</em> vectors will contain the exact same vectors as before, just in that new permuted order. The set of queries is {<em>Q<sub>1</sub></em>, <em>Q<sub>2</sub>,</em> <em>Q<sub>3</sub></em>}, and it will still be that same set, just shuffled. Okay, what about the next step, computing similarities? The similarity matrix <em>E</em> is computed by taking the dot product of every query with every key. For example, <img src="https://latex.codecogs.com/png.latex?E_%7B1,1%7D%20=%20Q_1%20%5Ccdot%20K_1">. If we shuffle the inputs, the dot product between <em>Q<sub>1</sub></em> and <em>K<sub>1</sub></em> will be exactly the same, it will just appear in a different position in the new, shuffled similarity matrix. The set of all pairwise similarities is identical; it’s just that the rows and columns of the matrix have been permuted. Next, the attention weights. The attention weights <em>A</em> are computed by applying a softmax to the rows of the similarity matrix <em>E.</em></p>
</div><div class="column" style="width:50%;">
<p><a href="./images/permuting.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6"><img src="https://bhdai.github.io/blog/posts/attention-and-transformers/images/permuting.png" class="img-fluid"></a></p>
</div>
</div>
<p>Since the values in <em>E</em> are just shuffled, the resulting attention weights <em>A</em> will also just be a permuted version of the original attention matrix. And finally, the outputs. Each output <em>Y<sub>i</sub></em> is a weighted sum of the value vectors. Since the value vectors <em>V</em> are the same set (just shuffled) and the attention weights <em>A</em> are also correspondingly shuffled, the final set of output vectors <em>Y</em> will be the exact same set of vectors we got before, just permuted in the same way as the original inputs.</p>
<p>So, what have we just demonstrated? We’ve shown that Self-Attention is <strong>permutation equivariant</strong>. This is a very important formal property. What it means is that if you have a function <img src="https://latex.codecogs.com/png.latex?F"> (our self-attention layer) and you apply it to a permuted input <img src="https://latex.codecogs.com/png.latex?%5Csigma(X)">, the result is the same as applying the function to the original input <img src="https://latex.codecogs.com/png.latex?F(X)"> and then permuting the output in the same way <img src="https://latex.codecogs.com/png.latex?%5Csigma(F(X))">. The critical takeaway here is that this means Self-Attention naturally <strong>works on sets of vectors</strong>. It doesn’t have any built-in notion of order or position. It treats the input as an unordered bag of vectors, interacts them all with each other, and produces an output bag of vectors. Now, this property is both a strength and a major weakness. It’s a strength because it’s a very general and flexible operator. But for many of the tasks we care about, like processing language, the order of the sequence is absolutely critical. “The dog bit the man” means something very different from “The man bit the dog.” So we have a problem: <strong>Self-Attention does not know the order of the sequence</strong>. If we just feed in word embeddings, it has no idea which word came first.</p>
<p>So how do we fix this? We need to explicitly give the model information about the position of each element in the sequence. We do this by adding a <strong>positional encoding</strong> to each input vector. This positional encoding is just another vector. The key idea is that this vector is not learned; it’s a <strong>fixed function of the index</strong>* of the token. So, for the first input <em>X<sub>1</sub>,</em> we add a specific vector <em>E(1).</em> For the second input <em>X<sub>2</sub>,</em> we add a different specific vector <em>E(2),</em> and so on. By adding these unique positional markers to each input, we’ve broken the permutation equivariance. The input X1 + E(1) is now fundamentally different from X2 + E(2), even if X1 and X2 were identical. This gives the model the information it needs to understand and leverage the sequential order of the input. And we’ll talk more about how these encodings are actually constructed later.</p>
<p>Okay, so this brings us to the next important variant of self-attention, which is critical for the decoder part of a Transformer. But now let’s think about another scenario. What happens when we want to use self-attention for a task where the model is generating a sequence, one element at a time, and it’s not supposed to know what’s coming next? This brings us to a crucial variant of this layer. This is the <strong>Masked Self-Attention Layer</strong>. The guiding principle here is simple: <strong>Don’t let vectors “look ahead” in the sequence</strong>.</p>
<div class="columns">
<div class="column" style="width:50%;">
<p>Let’s think about why this is so important. Imagine you’re training a language model. The task is, given the first three words of a sentence, to predict the fourth word. If we used a standard self-attention layer, the representation for the third word would be computed by attending to all the words in the sentence, including the fourth word we’re trying to predict! The model would be cheating; it would have access to the answer. It would learn a trivial solution of just copying the answer, and it would be completely useless at test time when it actually has to generate a new word without knowing the future.</p>
<p>So, we need to enforce causality. We need to ensure that when we compute the output for position <em>i,</em> the model can only attend to inputs from positions 1 through <em>i,</em> and nothing further. How do we do this mechanically? The trick is very clever. It happens right after we compute the similarity matrix <em>E,</em> but before the softmax. We override the similarity scores for any connection that looks forward in time by setting them to negative infinity. Let’s look at the diagram. When we’re computing the output for position 2 (using <em>Q<sub>2</sub></em>), we want it to be able to look at position 1 and position 2, but not position 3. So, we would set the similarity score <em>E<sub>2,3</sub></em> to negative infinity. When we compute the output for position <em>1,</em> it should only be allowed to look at position 1. So we would set <em>E<sub>1,2</sub></em> and <em>E<sub>1,3</sub></em> to negative infinity.</p>
</div><div class="column" style="width:50%;">
<p><a href="./images/marked-self-attention.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7"><img src="https://bhdai.github.io/blog/posts/attention-and-transformers/images/marked-self-attention.png" class="img-fluid"></a></p>
</div>
</div>
<p>Now, why negative infinity? Think about the softmax function: <em>e<sup>x</sup>.</em> What is e raised to the power of negative infinity? It’s zero. So, after we apply the softmax, all of these masked positions will have an attention weight of exactly zero. You can see this in the attention matrix A. The connections we wanted to forbid now have a weight of zero. This means that when we compute the final output vectors as a weighted sum of the value vectors, the model is physically incapable of pulling information from those future positions. We have effectively “masked out” the future. And as we’ve been discussing, this is absolutely critical for auto-regressive tasks like <strong>language modeling</strong>, where the goal is to predict the next word. If our input is “Attention is very,” and we’re trying to predict the next words (“cool,” “important,” etc.), the masked self-attention mechanism ensures that when we compute the output vector for “is,” it only uses information from “Attention” and “is.” When we compute the output for “very,” it only uses information from “Attention,” “is,” and “very.” This preserves the causal, step-by-step nature of generation that we need for these kinds of tasks. This masking is what allows a Transformer’s decoder to generate a sequence one token at a time, just like the RNN decoder did, but without the sequential bottleneck of an RNN.</p>
</section>
<section id="multi-head-self-attention" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="multi-head-self-attention">Multi-head Self-Attention</h2>
<p>So far, we’ve talked about self-attention as a mechanism where a sequence interacts with itself. We saw that a single self-attention layer learns to compute one specific kind of relationship between the tokens in a sequence, based on the dot-product similarity of their query-key pairs. But this raises a question. What if there are multiple different kinds of relationships we want to capture? For example, in a sentence, one token might relate to another syntactically (e.g., subject to verb), while also relating to a different token semantically (e.g., being a synonym). A single set of <em>W<sub>Q</sub>,</em> <em>W<sub>K</sub>,</em> and <em>W<sub>V</sub></em> matrices might struggle to learn all these different types of relationships at once. This is the motivation for our next idea. This is the <strong>Multi-headed Self-Attention Layer</strong>. The core idea is deceptively simple, instead of having one self-attention mechanism, we’re going to run <em>H</em> copies of self-attention in parallel.</p>
<div class="columns">
<div class="column" style="width:50%;">
<p>We start with our same input vectors, <em>X<sub>1</sub>,</em> <em>X<sub>2</sub>,</em> <em>X<sub>3</sub>.</em> But now, we’re going to have, in this example, <em>H</em>=3 independent self-attention layers. Each of these layers is called an “attention head.” And the critical part is that each head has <strong>its own, completely independent set of weights</strong>. So, head 1 has its own <em>W<sub>Q1</sub>,</em> <em>W<sub>K1</sub>,</em> <em>W<sub>V1</sub>.</em> Head 2 has its own <em>W<sub>Q2</sub>,</em> <em>W<sub>K2</sub>,</em> <em>W<sub>V2</sub>,</em> and so on. Because they have different weights, each head is free to learn a different kind of relationship. You can think of this as being very analogous to a convolutional layer in a CNN. A single convolution layer doesn’t have just one filter; it has many filters, and each filter learns to detect a different kind of visual feature (an edge, a color blob, a texture). Here, each attention head can learn to specialize in detecting a different kind of relationship within the sequence.</p>
<p>Okay, so we run our input <em>X</em> through these <em>H</em> parallel heads. What do we get? Well for each input token <em>X<sub>i</sub></em>, we now have <em>H</em> different output vector. As the diagram show, for input <em>X<sub>1</sub></em>, we now have an input from head 1 (<em>Y<sub>1,1</sub></em>), and output from head 2 (<em>Y<sub>1,2</sub></em>), and an output from head 3 (<em>Y<sub>1,3</sub></em>). So the next logical step is to gather these up. We can think of this as stacking or concatenating the independent outputs for each input position</p>
</div><div class="column" style="width:50%;">
<p><a href="./images/multiheaded.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8"><img src="https://bhdai.github.io/blog/posts/attention-and-transformers/images/multiheaded.png" class="img-fluid"></a></p>
</div>
</div>
<p>But we can’t just leave these outputs concatenated. We started with one vector per position, and we want to end up with one vector per position so we can feed it into the next layer of the network. So, the final step of the multi-head attention layer is an <strong>output projection</strong>. We take the concatenated outputs from all the heads and we pass them through one more learned linear layer, which has a weight matrix <em>W<sub>O</sub>.</em> This final projection fuses the information learned by all the different heads back into a single output vector, <em>O<sub>i</sub>,</em> for each position. This projection layer learns the best way to combine the different specialized representations from each head.</p>
<p>Conceptually, we’re running <em>H</em> full self-attention layers in parallel. But that sounds computationally expensive. Let’s say our main model has an input dimension of <em>D.</em> Instead of each of the <em>H</em> heads working with vectors of dimension <em>D,</em> we first split that dimension up. Each head will work with smaller vectors of dimension <em>D<sub>H</sub>,</em> which we call the “head dimension.” Typically, we set <em>D<sub>H</sub></em> = <em>D</em> / <em>H.</em> So, we project our input <em>X</em> (dimension <em>D</em>) down into smaller <em>Q,</em> <em>K,</em> and <em>V</em> vectors (dimension <em>D<sub>H</sub></em>) for each head. Then we perform the scaled dot-product attention in parallel for all heads. Finally, we concatenate the <em>H</em> output vectors of size <em>D<sub>H</sub></em> back together, which gives us a vector of size <em>H</em> * <em>D<sub>H</sub></em> = <em>D.</em> Then we apply that final output projection <em>W<sub>O</sub>.</em></p>
<p>And in practice, all of this can be implemented incredibly efficiently. We don’t actually run H separate for-loops. Instead, we can reshape our Q, K, and V matrices to have an explicit “heads” dimension, and then we compute all H heads in a single pass using batched matrix multiply operations, which are highly optimized on GPUs. So the takeaway here is that multi-head attention isn’t just an optional add-on; it is the standard. It allows the model to simultaneously attend to information from different representation subspaces at different positions. This mechanism is used everywhere in practice and is fundamental to the power of the Transformer architecture.</p>
<p>Fundamentally, self-attention boils down to just <strong>four</strong> main learnable matrix multiplications. The first one is the <strong>QKV Projection</strong>. As we discussed, this is where we take our input <em>X</em> and project it into the queries, keys, and values for all heads. In practice, this is done with a single large matrix multiply from [<em>N</em> x <em>D</em>] to [<em>N</em> x <em>3HD<sub>H</sub></em>], which we then split and reshape to get our <em>Q,</em> <em>K,</em> and <em>V</em> tensors. So that’s our first learnable multiplication.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5BN%20%5Ctimes%20D%5D%20%5BD%20%5Ctimes%203HD_H%5D%20%5CRightarrow%20%5BN%20%5Ctimes%203HD_H%5D%0A"></p>
<p>The second big computation is the <strong>QK Similarity</strong>. This is where we multiply the <em>Q</em> and <em>K</em> tensors to get our similarity matrix <em>E.</em> This is the core of the attention mechanism, where every token is compared against every other token.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5BH%20%5Ctimes%20N%20%5Ctimes%20D_H%5D%20%5BH%20%5Ctimes%20D_H%20%5Ctimes%20N%5D%20%5CRightarrow%20%5BH%20%5Ctimes%20N%20%5Ctimes%20N%5D%0A"></p>

<div class="no-row-height column-margin column-container"><div class="">
<p>Inputs:</p>
<ul>
<li>Data vectors: <em>X</em> [<em>N</em> x <em>D</em>]</li>
<li>Key matrix: <em>W<sub>K</sub></em> [<em>D</em> x <em>HD<sub>H</sub></em>]</li>
<li>Value matrix: <em>W<sub>V</sub></em> [<em>D</em> x <em>HD<sub>H</sub></em>]</li>
<li>Query matrix: <em>W<sub>Q</sub></em> [<em>D</em> x <em>HD<sub>H</sub></em>]</li>
<li>Ouput matrix: <em>W<sub>O</sub></em> [<em>HD<sub>H</sub></em> x <em>D</em>]</li>
</ul>
<p>Computation:</p>
<ul>
<li>Query: <img src="https://latex.codecogs.com/png.latex?Q%20=%20XW_Q"> [<em>H</em> x <em>N</em> x <em>D<sub>H</sub></em>]</li>
<li>Keys: <img src="https://latex.codecogs.com/png.latex?K%20=%20XW_K"> [<em>H</em> x <em>N</em> x <em>D<sub>H</sub></em>]</li>
<li>Values: <img src="https://latex.codecogs.com/png.latex?V%20=%20XW_V"> [<em>H</em> x <em>N</em> x <em>D<sub>H</sub></em>]</li>
<li>Similarities: <img src="https://latex.codecogs.com/png.latex?E%20=%20QK%5ET%20/%20%5Csqrt%7B%F0%9D%90%B7_%F0%9D%91%84%7D"> [<em>H</em> x<em>N</em> x <em>N</em>]</li>
<li>Attention weights: A = softmax(E, dim=2) [<em>H</em> x <em>N</em> x <em>N</em>]</li>
<li>Head outputs: <img src="https://latex.codecogs.com/png.latex?Y%20=%20AV"> [<em>H</em> x <em>N</em> x <em>D<sub>H</sub></em>]</li>
<li>Outputs: <img src="https://latex.codecogs.com/png.latex?O%20=%20YW_o"> [N X D]</li>
</ul>
</div></div><p>The third major multiplication is the <strong>V-Weighting</strong>. After we compute the attention weights <em>A</em> via softmax, we multiply that attention matrix by the <em>V</em> tensor to produce the head outputs <em>Y.</em> This is where we actually aggregate the information from the value vectors based on our computed attention scores. And the fourth and final learnable matrix multiplication is the Output Projection. We take the concatenated head outputs and pass them through our final weight matrix W_O to produce the final output O of the layer.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5BH%20%5Ctimes%20N%20%5Ctimes%20N%5D%20%5BH%20%5Ctimes%20N%20%5Ctimes%20D_H%5D%20%5CRightarrow%20%5BH%20%5Ctimes%20N%20%5Ctimes%20D_H%5D%0A"></p>
<p>And the fourth and final learnable matrix multiplication is the <strong>Output Projection</strong>. We take the concatenated head outputs and pass them through our final weight matrix <em>W<sub>O</sub></em> to produce the final output <em>O</em> of the layer.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5BN%20%5Ctimes%20HD_H%5D%20%5BHD_H%20%5Ctimes%20D%5D%20%5CRightarrow%20%5BN%20%5Ctimes%20D%5D%0A"></p>
<p>So there you have it. The entire, complex-looking multi-head self-attention layer is really just these four core matrix multiply stages, glued together with some reshapes and a softmax.</p>
<p>Now, analyzing the computation this way allows us to ask some very important practical questions. For example: <strong>How does the amount of compute scale as the number of vectors N (our sequence length) increases?</strong> Let’s look at the four steps. The QKV projection and the Output projection are linear in <em>N.</em> If you double the sequence length, you double the work for those. But look at steps 2 and 3. The QK Similarity step involves multiplying a [… x <em>N</em> x <em>D<sub>H</sub></em>] matrix by a [… x <em>D<sub>H</sub></em> x <em>N</em>] matrix, which results in a giant [… x <em>N</em> x <em>N</em>] attention matrix. The V-Weighting step multiplies that [… x <em>N</em> x <em>N</em>] matrix by the [… x <em>N</em> x <em>D<sub>H</sub></em>] value matrix. Both of these operations are dominated by that <em>N</em> x <em>N</em> interaction. This means that the computational cost of self-attention is <em>O(N<sup>2</sup>),</em> quadratic in the sequence length. If you double the length of your input sequence, you quadruple the amount of computation. This is the single biggest architectural bottleneck of the Transformer. RNNs, by comparison, are linear, <em>O(N),</em> in their computational cost.</p>
<p>Now let’s ask a related but different question: <strong>How does the memory usage of this layer scale with N? Where is the biggest tensor we have to store in memory to do the computation and backpropagation?</strong> Again, the answer comes from looking at that intermediate attention matrix <em>A.</em> This is an [<em>H</em> x <em>N</em> x <em>N</em>] tensor. To compute the gradients during backpropagation, we need to have stored this matrix in GPU memory. And its size is also <em>O(N<sup>2</sup>).</em> This quadratic memory cost is an even bigger problem in practice than the compute cost. Let’s put some numbers on it. Suppose you’re working with a long sequence, say <em>N=100,000</em> tokens. Maybe it’s a long document, or a high-resolution image treated as a sequence of patches. And let’s say you have <em>H=64</em> heads. That <em>H x N x N</em> attention matrix would require 1.192 Terabytes of memory. That’s not a typo. Your standard high-end GPU might have 40 or 80 gigabytes of memory. You can’t even come close to fitting this matrix. This quadratic memory scaling is what has historically limited Transformers to relatively short sequence lengths, often just 512 or a few thousand tokens.</p>
<p>Now, I have to mention that this is a very active area of research, and there have been some incredible breakthroughs recently. An algorithm called <a href="https://arxiv.org/abs/2205.14135">FlashAttention</a>, which came out in 2022, is a great example. The key insight of FlashAttention is that you don’t actually need to write out the full <em>N x N</em> attention matrix to memory. By being very clever about the order of operations and how data is moved between the different levels of GPU memory (SRAM and HBM), it’s possible to compute the exact same output without ever instantiating that giant matrix. It computes the output in small blocks, fusing the softmax and the V-weighting steps together. This reduces the memory requirement from <em>O(N<sup>2</sup>)</em> down to <em>O(N).</em> This single algorithmic improvement has been transformative, allowing Transformers to be trained on much, much longer sequences than were possible before. It’s a beautiful example of how deep algorithmic understanding of hardware can unlock new capabilities for our models.</p>
<p><a href="./images/three-ways.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9"><img src="https://bhdai.github.io/blog/posts/attention-and-transformers/images/three-ways.png" class="img-fluid"></a></p>
<p>So if you look at this table, you see this fascinating set of trade-offs between these three fundamental operations. RNNs are slow but have an efficient <em>O(N)</em> scaling. Convolutions are fast but struggle with long-range dependencies. Self-attention is fast and directly models long-range dependencies, but has this expensive <em>O(N<sup>2</sup>)</em> scaling. And it was precisely by analyzing these trade-offs that the authors of the original Transformer paper made their big claim. They recognized the massive parallelization benefit of self-attention and saw that the long-range modeling was superior to convolutions. And they made a bet that they could build an entire architecture using only this self-attention mechanism, completely getting rid of recurrence and convolution. And this led to the landmark 2017 paper, famously titled: <a href="https://arxiv.org/abs/1706.03762">“Attention is All You Need.”</a>. And with that, we are finally ready to put all of these pieces together and look at the full Transformer architecture.</p>
</section>
<section id="the-transformer" class="level2">
<h2 class="anchored" data-anchor-id="the-transformer">The Transformer</h2>
<p>The core building block of the entire Transformer architecture is what’s called a <strong>Transformer Block</strong>. So let’s build one of these blocks up, step by step.</p>
<section id="transformer-block" class="level3">
<h3 class="anchored" data-anchor-id="transformer-block">Transformer block</h3>
<div class="columns">
<div class="column" style="width:50%;">
<p>The input to a Transformer block is a set of vectors <em>x.</em> Let’s say we have <em>N=4</em> input vectors. Remember, because of what we learned about permutation equivariance, these also need to have positional encodings added to them, but we’ll leave that detail aside for the moment and just focus on the architecture of the block itself. Okay, so the very first thing we do inside a Transformer block is the operation we just spent all this time on. We pass the entire set of vectors through a (multi-headed) Self-Attention layer. This is the communication part of the block. This is where every vector x_i gets to look at every other vector x_j in the input set and produce a new, updated representation of itself that incorporates context from the entire sequence. Now, if you remember back to our CNN architecture, what was one of the most important innovations that allowed us to build really deep networks? It was <strong>residual connections</strong>.</p>
</div><div class="column" style="width:50%;">
<p><a href="./images/transformer-block.png" class="lightbox" data-gallery="quarto-lightbox-gallery-10"><img src="https://bhdai.github.io/blog/posts/attention-and-transformers/images/transformer-block.png" class="img-fluid"></a></p>
</div>
</div>
<p>And the Transformer architecture uses them everywhere. So, after the self-attention layer, we add a residual connection. We take the original input vectors x and add them to the output of the self-attention layer. This “add” operation is done element-wise for each corresponding vector in the sequence. Just like in ResNets, this helps with gradient flow and makes it possible to train very deep stacks of these Transformer blocks. The next component in the block is <strong>Layer Normalization</strong>. We used Batch Normalization to stabilize training. Layer Normalization is another type of normalization that serves a similar purpose. But there’s a key difference. Batch Norm normalizes across the batch dimension for each feature. Layer Norm, on the other hand, normalizes across the feature dimension for each individual data point in the batch. So for each vector coming out of our residual connection, we compute the mean and standard deviation of its elements and use them to normalize that vector. This was found to work much better than Batch Norm for sequence models like the Transformer. So, we apply a Layer Normalization step after the residual connection.</p>
<p>Okay, so we’ve had the communication part of the block (self-attention). Now we need the computation part. After the first Layer Norm, we pass each vector in the sequence <strong>independently</strong> through a small Multi-Layer Perceptron, or <strong>MLP</strong>. This is also sometimes called a Feed-Forward Network or FFN in the Transformer literature. It’s typically a simple two-layer MLP. You have a linear layer that expands the dimension (e.g., from <em>D</em> to <em>4D</em>), followed by a ReLU activation, and then another linear layer that projects it back down from <em>4D</em> to <em>D.</em> The important thing here is that while the self-attention layer mixes information across the sequence, this MLP operates on each position separately. It’s a per-position computation that adds more expressive power and allows the model to do more complex processing on the features at each location.</p>
<p>And what do you think comes after the MLP? You guessed it. Another <strong>residual connection</strong>. We take the input to the MLP block which was the output of the first layer norm and we add it to the output of the MLP. Again, this is crucial for building deep models. And finally, to finish off our Transformer block, we apply one more <strong>Layer Normalization</strong> step after that second residual connection. This produces the final output vectors <em>y<sub>1</sub></em> through <em>y<sub>4</sub></em> of our block.</p>
<p>So, this block takes a set of vectors <em>x</em> as input and produces a set of vectors <em>y</em> as output, with the same number of vectors. And it does this using two main computational motifs. First, there’s the <strong>interaction step</strong>. The only place in this entire block where information is mixed between different vectors in the sequence is inside that multi-head self-attention layer. That is the sole communication hub. Second, there’s the <strong>per-position processing</strong>. The LayerNorm and the MLP components all work on each vector independently. They process each position in the sequence in parallel, without any interaction between them. And as we’ve discussed, the whole thing is <strong>highly scalable and parallelizable</strong>. If you count them up, the vast majority of the floating point operations in this block are contained in just six matrix multiplications: the four from the multi-head self-attention layer (the QKV projection, the QK similarity, the V-weighting, and the output projection), and the two from the two-layer MLP. And as we know, matrix multiplications are something that modern hardware is incredibly good at.</p>
<p>So, what is a Transformer? It is really just a stack of these identical Transformer blocks. You take the output from the first block and feed it directly as the input to the second block, and so on. And what’s really remarkable is that this fundamental block architecture has not changed much since it was introduced in 2017. All of the massive models you hear about today are, at their core, just stacks of this exact same block. What has changed is that they have gotten a lot, a lot bigger. Let’s look at the scaling trend just to appreciate the numbers. The <strong>original Transformer</strong> from the “Attention is All You Need” paper, often called the “base” model, had 12 of these blocks stacked up. The model dimension <em>D</em> was 1024, it used 16 heads, and it was trained on sequences of length <em>N=512.</em> This model had about 213 million parameters. Which, at the time, was a very large model. But then things started to escalate quickly. Just two years later, in 2019, OpenAI released <a href="https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe">GPT-2</a>. This was essentially just a larger version of the decoder part of the original Transformer. It had 48 blocks, a larger dimension of 1600, more heads, and could handle longer sequences of 1024 tokens. And this scaled the parameter count up to 1.5 billion. This was one of the first models that really captured public attention with its impressive text generation abilities. And the scaling just continued. The next year, OpenAI released <a href="https://arxiv.org/abs/2005.14165">GPT-3</a>. Now we’re talking about 96 blocks, a massive model dimension of over 12,000, and 96 attention heads. This model clocked in at 175 billion parameters. And of course, the models we see today have continued this trend, pushing into the trillions of parameters. The key takeaway here is that this basic Transformer block has proven to be an incredibly scalable architecture. It seems that just by making these models bigger, more layers, larger dimensions, more heads and training them on more and more data, their capabilities continue to improve. This scaling law has been one of the most powerful and driving forces in all of AI over the last several years.</p>
</section>
<section id="transformer-for-language-and-vision" class="level3">
<h3 class="anchored" data-anchor-id="transformer-for-language-and-vision">Transformer for language and vision</h3>
<p>Now let’s get concrete and see how we would use this architecture to build a Large Language Model, or LLM, like GPT. The first thing we need to do is get our input into the right format.</p>
<div class="columns">
<div class="column" style="width:50%;">
<p>Our Transformer blocks operate on sets of vectors, but our input is text-a sequence of words or tokens. So, the very first layer of the model needs to convert these words into vectors. We do this with a learnable embedding matrix. If our vocabulary has <em>V</em> unique tokens and our model’s hidden dimension is <em>D,</em> then this is simply a lookup table of shape [V x D]. For each word in our input sequence, like “Attention,” “is,” “all,” “you,” we look up its corresponding D-dimensional vector in this table. These embedding vectors are initialized randomly and are learned just like all the other weights in the network through backpropagation. This gives us our initial set of input vectors to feed into the first Transformer block. And of course, we would also add our positional encodings to these vectors at this stage.</p>
<p>Now, we feed this set of vectors through our stack of Transformer blocks. But there’s a critical detail we need to remember for language modeling. The task is to predict the next word in the sequence. This means the model must be causal; it cannot be allowed to look ahead. So, as we discussed earlier, we must use masked attention inside each of the self-attention layers in our Transformer blocks. This ensures that when the model is computing the output for token <em>i,</em> it can only attend to tokens 1 through <em>i</em> and is explicitly prevented from seeing any future tokens. Every self-attention layer in a model like GPT is a masked multi-head self-attention layer.</p>
<p>What we get out is a final set of contextualized output vectors, one for each input position.</p>
</div><div class="column" style="width:50%;">
<p><a href="./images/llm.png" class="lightbox" data-gallery="quarto-lightbox-gallery-11"><img src="https://bhdai.github.io/blog/posts/attention-and-transformers/images/llm.png" class="img-fluid"></a></p>
</div><p>Now we need to make a prediction. For each output vector, we want to predict what the next word in the sequence should be. To do this, we need to go from our D-dimensional representation space back to the vocabulary space. So, at the very end of the model, we add a final <strong>projection matrix</strong>. This is a linear layer with weights of shape [<em>D</em> x <em>V</em>]. It takes each D-dimensional output vector from the last Transformer block and projects it into a V-dimensional vector. We can interpret this V-dimensional vector as a set of raw scores, or logits, for every single word in our vocabulary. The final step is to turn these scores into probabilities and compute a loss. For each position <em>i,</em> we take the V-dimensional logit vector and pass it through a softmax function. This gives us a probability distribution over the entire vocabulary, representing the model’s prediction for the word at position <em>i+1.</em> We can then compare this predicted distribution to the actual next word in the training data using a standard <strong>cross-entropy loss</strong>. We compute this loss for every position in the sequence and average them together. That final loss signal is then backpropagated all the way through the entire stack of Transformer blocks and the embedding matrix to update all the weights.</p><p>Okay, so this has been all about language. So, the natural next question is, can we use these same ideas for images? For a while, it seemed like CNNs were the undisputed kings of vision, and Transformers were for language. But then, in 2021, a paper <a href="https://arxiv.org/abs/2010.11929">“An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale”</a> came out of Google that completely changed that landscape. And that brings us to the <strong>Vision Transformer</strong>, or <strong>ViT</strong>.</p><p><a href="./images/vit.png" class="lightbox" data-gallery="quarto-lightbox-gallery-12"><img src="https://bhdai.github.io/blog/posts/attention-and-transformers/images/vit.png" class="img-fluid"></a></p><p>The core question the ViT paper asked was: can we apply a standard Transformer directly to an image for a task like image classification? The challenge is that a Transformer expects a sequence of vectors as input. But an image isn’t a sequence; it’s a 2D grid of pixels. For a standard 224x224 image, if you treated every single pixel as a token, you’d have over 50,000 tokens. And given the <em>O(N<sup>2</sup>)</em> scaling of self-attention, that would be computationally infeasible. So we need a different way to turn an image into a sequence.</p><p>The solution they proposed was remarkably simple and effective. Instead of treating pixels as tokens, they decided to break the image into a grid of <strong>non-overlapping patches</strong>. For example, you can take a 224x224 image and break it down into a grid of 16x16 patches. For a 224x224 image and 16x16 patches, this gives us a 14x14 grid, for a total of 196 patches. Each patch is 16x16x3 pixels. The next step is to turn each of these patches into a vector. So we take each patch, <strong>flatten</strong> its pixels into one long vector (16 * 16 * 3 = 768), and then apply a <strong>linear transformation</strong> to project it into our desired model dimension, <em>D.</em> This gives us what the Transformer wants: a sequence of <em>N</em> vectors, where <em>N</em> is the number of patches. Now let me pause here and ask a question. This whole operation of breaking an image into patches, flattening each patch, and then applying a linear projection… does that sound familiar? Is there another way we could describe this exact same operation using concepts we already know? This whole “patchify and project” operation is mathematically equivalent to taking the original image and running it through a single <strong>convolutional layer</strong>. Specifically, it’s a convolution with a kernel size of 16x16 and a stride of 16. The stride of 16 ensures that the filter is applied to non-overlapping patches. The kernel size of 16 ensures it sees the whole patch. The number of input channels is 3 (for RGB), and the number of output channels would be <em>D,</em> our model dimension. So, you can think of the ViT’s input processing as just a very large-stride convolution. It’s a nice way to connect this new architecture back to concepts we’re already very familiar with.</p><p>So now we have our sequence of N patch embedding vectors. What do we do with them? We just feed them directly into a standard Transformer encoder, which is just a stack of our Transformer blocks that we’ve already defined. These D-dimensional vectors for each patch are the input <em>x</em> to our stack of Transformer blocks. But wait, we’ve forgotten a crucial detail. Self-attention is permutation equivariant. It doesn’t know where each patch came from. The patch from the top-left corner is treated the same as the patch from the bottom-right. So, just like with language, we need to add positional encodings. In ViT, these are learnable vectors that are added to the patch embeddings. There’s a unique <strong>positional encoding</strong> for each possible patch position (e.g., for each spot on the 14x14 grid), which tells the model the original 2D position of each patch.</p><p>Now, what kind of attention should we use? Should it be masked? For image classification, the answer is no. We want to classify the entire image. So, every patch should be able to freely communicate with and attend to every other patch in the image. We want global context. So we <strong>don’t use any masking</strong> inside the self-attention layers. Alright, so we pass our sequence of patch embeddings (with positional encodings added) through a deep stack of standard, unmasked Transformer blocks. The Transformer then does its thing, mixing information between all the patches. What we get out at the end is a set of N output vectors, one for each patch.</p><p>So now we have these N output vectors, but our goal is image classification, which requires a single prediction for the entire image. How do we get from a set of vectors to a single class score? The standard approach in ViT is to simply average pool all of the N output vectors from the final Transformer block. This gives you a single D-dimensional vector that represents the entire image. You then take this single vector and pass it through a final linear layer (a classification head) to predict the class scores. And that’s the Vision Transformer. It’s a surprisingly direct application of the Transformer architecture to images, with the key innovation being this “patchification” frontend. And it turned out to work remarkably well, often outperforming state-of-the-art CNNs, especially when trained on very large datasets.</p><section id="tweaking-transformers" class="level3">
<h3 class="anchored" data-anchor-id="tweaking-transformers">Tweaking Transformers</h3>
<p>So this brings us to the topic of <strong>Tweaking Transformers</strong>. As I’ve said a couple of times, the high-level architecture of the Transformer block has been remarkably stable since it was first introduced in 2017. The old block diagram is pretty much what people are still using today. But, like with any influential piece of engineering, people have tinkered with it over the years. And a few small changes have become very common because they’ve been found to improve performance or, more importantly, training stability. And when you’re spending millions of dollars to train a single model, stability is incredibly important.</p>
<p><a href="./images/tweaking.png" class="lightbox" data-gallery="quarto-lightbox-gallery-13"><img src="https://bhdai.github.io/blog/posts/attention-and-transformers/images/tweaking.png" class="img-fluid"></a></p>
<p>So let’s look at the original architecture from the “Attention is All You Need” paper, which is what’s shown here on the left. This is what’s known as a “Post-Norm” Transformer. And it has a kind of weird property if you look closely at the residual connections. The <strong>Layer normalization</strong> step happens outside the residual connection. It happens after you add the output of the self-attention block back to the input. Now, let’s do a little thought experiment. What’s the point of a residual connection? It’s to make it easy for a block to learn an identity function, so that we can stack many layers without performance degrading. So, what if our self-attention block learns to output all zeros? In a standard ResNet, the output of the block would be <em>x</em> + 0 = <em>x.</em> But here, the output is LayerNorm(<em>x</em> + 0), which is just LayerNorm(<em>x</em>). It’s not <em>x.</em> So, this architecture can’t actually learn a true identity function. The signal is always getting re-normalized. This can lead to some instabilities, especially at the beginning of training, as the gradients flowing back through the network can be a bit chaotic.</p>
<p>So, what’s the solution? It’s a simple fix that has become almost standard practice. You just move the layer normalization. Instead of putting it after the addition, you move the layer normalization before the Self-Attention and MLP blocks. So now it’s inside the residual branch. This is called a <strong>Pre-Norm Transformer</strong>. If the self-attention block outputs all zeros, the output of the entire sub-block is <em>x</em> + SelfAttention(LayerNorm(<em>x</em>)). If the self-attention part is zero, the output is x. It can now learn a perfect identity function. Empirically, this small change has been shown to make training significantly <strong>more stable</strong>, which is a huge win for these massive-scale models.</p>
<p>Alright, so that’s the first big tweak: move from Post-Norm to Pre-Norm. The next common tweak is to the normalization layer itself. People found that you can simplify Layer Normalization a bit and get even better results. This brings us to <strong>RMSNorm,</strong> which stands for Root-Mean-Square Normalization. If you recall, standard LayerNorm first subtracts the mean to center the data, and then divides by the standard deviation to scale it. RMSNorm gets rid of the centering step. It <strong>only scales</strong> the vector by its root-mean-square, as you can see in the formula. It still has a learnable gain parameter <img src="https://latex.codecogs.com/png.latex?%5Cgamma">, but it doesn’t have the learnable shift <img src="https://latex.codecogs.com/png.latex?%5Cbeta">.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0Ay_i%20&amp;=%20%5Cfrac%7Bx_i%7D%7BRMS(x)%7D%20%5Cgamma_i%20%5C%5C%0ARMS(x)%20&amp;=%20%5Csqrt%7B%5Cvarepsilon%20+%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi=1%7D%5EN%20x_i%5E2%7D%0A%5Cend%7Balign%7D%0A"></p>
<p>This makes the computation slightly simpler and faster. And for reasons that are still being actively studied, this also tends to make training a bit more stable than the full LayerNorm, especially in the Pre-Norm configuration. So, if you look at the source code of many modern open-source LLMs like Llama, you will find they use this exact Pre-Norm architecture with RMSNorm</p>
<p>Okay, so we’ve tweaked the normalization. The next common place for tweaks is inside that other part of the block, the feed-forward MLP. The <strong>classic MLP</strong> used in the original Transformer is a very simple two-layer feed-forward network. You take your input <em>X</em> of dimension [<em>N</em> x <em>D</em>] project it up to a wider dimension, typically <em>4D</em> using <em>W<sub>1</sub></em> [<em>D</em> x <em>4D</em>] and <em>W<sub>2</sub></em> [<em>4D</em> x <em>D</em>], apply a non-linearity like ReLU, and then project it back down to D</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cunderbrace%7BY%7D_%7BN%20%5Ctimes%20D%7D%20=%20%5Cmax(0,%20XW_1)W_2%0A"></p>
<p>Very straightforward. But it turns out there are other ways to design this MLP block that work a bit better. A very popular variant today is called the <strong>SwiGLU MLP</strong>. Instead of one big projection up, we now have two separate linear projections, <em>W<sub>1</sub></em> and <em>W<sub>2</sub>,</em> [<em>D</em> x <em>H</em>]. The output of the first projection, <em>XW<sub>1</sub>,</em> is passed through a Swish activation function <img src="https://latex.codecogs.com/png.latex?%5Csigma">. The output of the second projection, <em>XW<sub>2</sub>,</em> is not passed through an activation. We then take these two results and multiply them together element-wise. This is the “gating” part of the Gated Linear Unit, or GLU. Finally, that gated result is passed through a third projection matrix, <em>W<sub>3</sub></em> [<em>H</em> x <em>D</em>] to get the final output.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AY%20=%20(%5Csigma(XW_1)%20%5Codot%20XW_2)W_3%0A"></p>
<p>It’s a more complex interaction, but empirically, it just works better. To keep the number of parameters roughly the same as the classic MLP, you can set the intermediate dimension H to be 8D/3. Again, many modern LLMs like Llama use this exact SwiGLU formulation.</p>
<p>Now, you might ask, why does this work better? What’s the deep theoretical reason for this specific combination of projections and gating? And this is one of those moments where, as a researcher, I have to be very honest with you. The paper that introduced this, “GLU Variants Improve Transformers,” has this wonderfully candid line</p>
<blockquote class="blockquote">
<p>“We offer no explanation as to why these architectures seem to work; we attribute their success, as all else, to divine benevolence.”</p>
</blockquote>
<p>And I think that’s a perfect encapsulation of a lot of deep learning research. We have some high-level intuitions—gating lets the network control information flow more dynamically—but often, these specific architectural choices are found through extensive empirical exploration. We try a bunch of things, and some just work better than others, and then we try to understand why after the fact. It’s a testament to the fact that this is still very much an empirical science.</p>
<p>Okay, that brings us to the final, and perhaps most impactful, tweak to the modern Transformer architecture: the <strong>Mixture of Experts</strong>, or <strong>MoE</strong> introduced in paper <a href="https://arxiv.org/abs/1701.06538">“Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer”</a> by Shazeer et al, in 2017. This is a really big idea. We’ve talked about how these models have scaled up to hundreds of billions or even trillions of parameters. A major problem with that is that for every single input token, you have to do a matrix multiplication with all of those weights. The computational cost grows with the parameter count. The idea behind MoE is to decouple the number of parameters from the amount of compute. What if, instead of having one giant MLP in each block, we have E separate sets of MLP weights? Each of these MLPs is called an “expert.”</p>
<p>Here’s how it works. For each token that enters the MoE layer, we use a small, learnable “router” network to decide which of the E experts are most relevant for processing this specific token. The router predicts a probability distribution over the experts, and we then route the token to be processed by only a small number, <em>A,</em> of the top-scoring experts (where <em>A</em> is much smaller than <em>E</em>). These are the <strong>active experts</strong>. This is a breakthrough because it means we can have a model with a massive number of parameters (by having many experts, E), but the computational cost for any given token only depends on the small number of active experts, A. It <strong>massively increases</strong> the parameter count without a proportional <strong>increase in compute</strong>. It allows for specialization, where different experts can learn to handle different types of inputs.</p>
<p>And this MoE architecture is no longer a niche research idea. <strong>All of the biggest LLMs today</strong> GPT-4o, Claude 3.7, Gemini 2.5 Pro, all of them, almost certainly use some form of Mixture of Experts. This is how they are able to claim parameter counts in the trillions while still being trainable and runnable. The exact details are usually kept secret, but the general principle of sparse, conditional computation via MoE is the key enabling technology for the current generation of frontier models.</p>


</section>
</div>
</section>
</section>

 ]]></description>
  <category>Deep Learning</category>
  <category>Transformer</category>
  <category>Attention</category>
  <category>RNN</category>
  <guid>https://bhdai.github.io/blog/posts/attention-and-transformers/</guid>
  <pubDate>Wed, 13 Aug 2025 17:00:00 GMT</pubDate>
  <media:content url="https://bhdai.github.io/blog/posts/attention-and-transformers/images/cover.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>CNN architectures</title>
  <dc:creator>Bui Huu Dai</dc:creator>
  <link>https://bhdai.github.io/blog/posts/cnn-architectures/</link>
  <description><![CDATA[ 






<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./images/cover.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="“A small child is sitting on the ground in a brightly lit playground, surrounded by colorful toy blocks, legos. The child is focused on building a tall structure. The child’s expression is one of deep thought and gentle confusion, holding up several blocks. unsure which one to place next.” Generated by DALL-E 3"><img src="https://bhdai.github.io/blog/posts/cnn-architectures/images/cover.png" class="img-fluid figure-img" alt="“A small child is sitting on the ground in a brightly lit playground, surrounded by colorful toy blocks, legos. The child is focused on building a tall structure. The child’s expression is one of deep thought and gentle confusion, holding up several blocks. unsure which one to place next.” Generated by DALL-E 3"></a></p>
<figcaption>“A small child is sitting on the ground in a brightly lit playground, surrounded by colorful toy blocks, legos. The child is focused on building a tall structure. The child’s expression is one of deep thought and gentle confusion, holding up several blocks. unsure which one to place next.” Generated by DALL-E 3</figcaption>
</figure>
</div>
<section id="fundamental-cnn-layers" class="level2">
<h2 class="anchored" data-anchor-id="fundamental-cnn-layers">Fundamental CNN layers</h2>
<p>Remember from our previous blog post where we talk about <strong>fully connected layer</strong>. If we have, for example, a 32x32x3 image, the first thing we do is stretch or flatten it into a 3072x1 vector. This flattened vector then becomes the input to our layer. The layer computes <img src="https://latex.codecogs.com/png.latex?Wx">, where <img src="https://latex.codecogs.com/png.latex?W"> is a weight matrix. If we want 10 output activations (say, for 10 classes), then <img src="https://latex.codecogs.com/png.latex?W"> would be a 10x3072 matrix. Each row of <img src="https://latex.codecogs.com/png.latex?W"> can be thought of as a template. The output of this matrix multiplication is a 10x1 vector of activations. Looking a bit closer at how each of those 10 output activations is computed, each individual number in that output vector is the result of taking a dot product between one row of the weight matrix <img src="https://latex.codecogs.com/png.latex?W"> and the entire input vector <img src="https://latex.codecogs.com/png.latex?x">. So, if the input <img src="https://latex.codecogs.com/png.latex?x"> is 3072-dimensional, each output activation is a 3072-dimensional dot product. This means every output neuron is connected to every input neuron, hence “fully connected.”</p>
<section id="convolution-layer" class="level3">
<h3 class="anchored" data-anchor-id="convolution-layer">Convolution layer</h3>
<p>Now, let’s contrast this with the <strong>Convolution Layer</strong>. A fundamental difference is that the convolution layer aims to preserve the spatial structure of the input image. So, if we have a 32x32x3 image, we don’t flatten it. We treat it as a 3D volume of numbers: 32 in height, 32 in width, and 3 in depth (representing, for example, the red, green, and blue color channels).</p>
<p><a href="./images/convolution-layer.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2"><img src="https://bhdai.github.io/blog/posts/cnn-architectures/images/convolution-layer.png" class="img-fluid"></a></p>
<p>The core operation in a convolution layer involves a filter, also sometimes called a kernel. This filter is also a small volume of numbers. For example, we might have a 5x5x3 filter. The “3” here refers to the depth of the filter. The operation is <strong>to convolve the filter with the image</strong>. Conceptually, this means we “slide over the image spatially, computing dot products.” We’ll make this much more precise in a moment, but the key idea is that the filter interacts with local regions of the input image. A very important point here is that filters always extend the full depth of the input volume. So, if our input image is 32x32x<strong>3</strong>, then our filter, say a 5x5 filter, must also have a depth of <strong>3</strong>. It will be a 5x5x3 filter. This is critical. The filter isn’t just looking at a 2D patch of one channel, it’s looking at a 3D slice through the entire depth of the input volume at that spatial location. This allows the filter to learn patterns that involve combinations of information across all input channels simultaneously.</p>
<p>The input to a convolution layer is typically a batch of images with dimensions N x C<sub>in</sub> x H x W, where N is the batch size, C<sub>in</sub> is the number of input channels and H and W are the height and width of the input feature maps. The convolution layer itself is defined by a set of filters. If we want C<sub>out</sub> output channels (i.e., we want to produce C<sub>out</sub> activation maps), we will have C<sub>out</sub> filters. Each filter will have dimensions C<sub>in</sub> x K<sub>h</sub> x K<sub>w</sub>, where K<sub>h</sub> and K<sub>w</sub> are the height and width of the kernel (e.g., 5x5). Note that the depth of each filter, C<sub>in</sub>, must match the number of input channels of the volume it’s being convolved with. So, the collection of filters can be thought of as a tensor of shape C<sub>out</sub> x C<sub>in</sub> x K<sub>h</sub> x K<sub>w</sub>. There will also be a C<sub>out</sub>-dimensional bias vector, one bias term for each of the C<sub>out</sub> filters. The output of the convolution layer will then be a batch of output volumes with dimensions <strong>N x C<sub>out</sub> x H’ x W’</strong>. Here, C<sub>out</sub> is the number of output channels (equal to the number of filters), and H’ and W’ are the new height and width of the feature maps. The exact values of H’ and W’ will depend on the input H and W, the kernel size K<sub>h</sub> and K<sub>w</sub>, and also on other hyperparameters like stride and padding, which we will discuss shortly. This framework describes the fundamental operation of a convolution layer. It takes an input volume, applies a set of learned filters to it locally across space, and produces an output volume where each “slice” in depth corresponds to the response of one of those filters</p>
<p><a href="./images/convnet.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3"><img src="https://bhdai.github.io/blog/posts/cnn-architectures/images/convnet.png" class="img-fluid"></a></p>
<p>Okay, so now that we understand the mechanics of a single convolution layer, let’s see how they fit into a larger network. Essentially, a ConvNet is a neural network that incorporates Conv layers as its primary building blocks. , especially in the earlier stages responsible for feature extraction. So, we might start with an input volume, say a 32x32x3 image. We pass this through a first <strong>CONV layer</strong>. For example, this layer might use 6 filters, each of size 5x5x3. Assuming stride 1 and no padding, this would produce an output volume of size 28x28x6. The depth of 6 corresponds to the 6 filters used. This output volume then becomes the input to the next CONV layer. So, the 28x28x6 volume is fed into a second CONV layer. This layer might, for example, use 10 filters, each of size 5x5x6. Notice that the depth of these filters (6) must match the depth of the input volume (6). If these filters are also 5x5, then again assuming stride 1 and no padding, the output of this second CONV layer would be a volume of size 24x24x10. The depth of 10 corresponds to the 10 filters used in this layer. And this process can continue, stacking more CONV layers to learn increasingly complex and abstract features.</p>
<p>A very important point, which we haven’t explicitly shown in the diagrams until now but is absolutely crucial, is that ConvNets, like other neural networks, need non-linearities. So, a ConvNet is a neural network with Conv layers, with activation functions! Typically, an activation function, most commonly <strong>ReLU</strong>, is applied element-wise to the output of each CONV layer after the bias has been added. So, the flow would be: Input → CONV (filters + bias) → ReLU → Output Volume. This output volume then feeds into the next CONV → ReLU sequence, and so on. Without these non-linearities, stacking multiple CONV layers would be equivalent to a single, more complex CONV layer, and the network wouldn’t be able to learn the rich hierarchical features we desire.</p>
</section>
<section id="what-do-conv-filters-learn" class="level3">
<h3 class="anchored" data-anchor-id="what-do-conv-filters-learn">What do Conv filters learn?</h3>
<p>So, a natural question arises: <strong>What do these Conv filters actually learn?</strong> Let’s think back to our simpler models. With a Linear Classifier, we saw that it learned essentially one template per class. These templates were global, representing an average look for each category. When we moved to a Multi-Layer Perceptron (MLP), specifically a 2-layer neural network, the first layer (W1) learned a bank of whole-image templates. These were still operating on the flattened image, but the network could learn multiple templates that could then be combined by the second layer. These templates were more diverse than the single template per class of a linear classifier.</p>
<p><a href="./images/edges.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4"><img src="https://bhdai.github.io/blog/posts/cnn-architectures/images/edges.png" class="img-fluid"></a></p>
<p>Now, with ConvNets, the first-layer conv filters learn local image templates. Because the filters are small and slide across the image, they learn to detect small, localized patterns. Empirically, it’s often observed that these first-layer filters learn to detect things like <strong>oriented edges, or opposing colors</strong> (e.g., a filter that activates strongly when it sees a green region next to a red region, or a horizontal edge. The example shown here on the left is from the first layer of AlexNet, which had 64 filters, each of size 3x11x11 (operating on RGB input). You can see the variety of edge detectors and color blob detectors that have been learned. What about <strong>deeper conv layers?</strong> Visualizing what filters learn in deeper layers is harder, because they are no longer operating directly on image pixels but on the activation maps produced by previous layers. However, various visualization techniques suggest that deeper conv layers tend to learn larger, more complex structures. They combine the simpler features detected by earlier layers to represent more abstract concepts, for example, parts of objects like eyes, or even more complex textures or object parts, sometimes even letter-like shapes if trained on relevant data. The visualization here on the right, from <a href="https://arxiv.org/abs/1412.6806">Springenberg et al.&nbsp;(2015)</a>, attempts to show patterns that maximally activate neurons in a 6th layer conv layer of an ImageNet model. You can see more intricate and larger receptive field patterns</p>
</section>
<section id="spatial-dimension" class="level3">
<h3 class="anchored" data-anchor-id="spatial-dimension">Spatial dimension</h3>
<p>Let’s now focus on the Spatial Dimensions of the convolution operation. This is about understanding how the height and width of the activation map are determined by the input size and the filter size, as well as other hyperparameters. In general, if the input has a spatial dimension (width or height) of W (or H), and the filter has a spatial dimension of K (or K<sub>h</sub>, K<sub>w</sub>), and we are using a stride of 1 and no padding, then the output dimension will be W - K + 1. Now, this formula W - K + 1 reveals a <strong>Problem: Feature maps shrink with each layer!</strong> If we have a deep network with many convolution layers, and each layer reduces the spatial dimensions (e.g., from 32 to 28, then from 28 to 24, and so on), the feature maps can become very small quite quickly. This might be undesirable if we want to maintain spatial resolution for a while, or if we want to build very deep networks without the features vanishing spatially. This shrinking effect is something we often want to control. So, what’s the solution?</p>
<p>The Solution to this shrinking problem is to add padding around the input before sliding the filter. Usually, this padding consists of zeros. If we use P pixels of padding on each side, the effective input size becomes W + 2P. Then, applying a filter of size K, the output size becomes (W + 2P) - K + 1. So, our new formula for the output size is <strong>W - K + 1 + 2P</strong>. A very common setting for padding is to choose <strong>P = (K - 1) / 2</strong>. This is typically used when the filter size K is odd. If you plug this P into the output size formula W - K + 1 + 2P, you get W - K + 1 + 2 * (K - 1) / 2, which simplifies to W - K + 1 + K - 1, which equals W. This means that with this choice of padding, the <strong>output feature map has the same spatial size as the input feature map</strong>. This is often called “same” padding or “half” padding, and it’s very useful for building deep networks because it prevents the spatial dimensions from shrinking at each layer.</p>
</section>
<section id="receptive-fields" class="level3">
<h3 class="anchored" data-anchor-id="receptive-fields">Receptive fields</h3>
<p>Now, let’s talk about another important concept related to stacking convolution layers: <strong>Receptive Fields</strong>. The receptive field of a neuron in a convolutional network is the region in the input space (e.g., the original image) that a particular neuron “sees” or is affected by. For a single convolution layer with a kernel size K, each element in the output feature map depends on a K x K receptive field in the input to that layer.</p>
<p><a href="./images/receptive-fields.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5"><img src="https://bhdai.github.io/blog/posts/cnn-architectures/images/receptive-fields.png" class="img-fluid"></a></p>
<p>When we stack multiple convolution layers, the receptive field size grows. Each successive convolution adds K - 1 to the receptive field size (assuming stride 1). Consider the diagram: The purple output neuron in the third layer “sees” a 3x3 region in the orange layer (its direct input). Each of those orange neurons, in turn, sees a 3x3 region in the blue layer. So, the purple neuron’s receptive field in the blue layer is larger. More generally, <strong>with L layers, each using a KxK filter (and stride 1), the receptive field size in the original input is 1 + L * (K - 1).</strong> It’s important to be careful here: we distinguish between the receptive field in the input (meaning the original image) versus the receptive field in the previous layer. This growth of the receptive field is desirable because it allows neurons in deeper layers to capture information from larger and larger regions of the input image, enabling them to learn more global and abstract features. However, there’s a Problem: If we only use small KxK filters (like 3x3) and stride 1 convolutions, then <strong>for large images, we would need many, many layers for each output neuron to “see” the whole image</strong>, or at least a significant portion of it. For example, if K=3, each layer adds 2 to the receptive field size. To get a receptive field of, say, 100, you’d need roughly 50 layers. This can lead to very deep and computationally expensive networks if this is the only mechanism for increasing receptive fields. So, how do we address this problem of needing many layers for large receptive fields? One common solution is to <strong>downsample inside the network</strong>. If we reduce the spatial dimensions of the feature maps at certain points in the network, then subsequent convolution filters, even if they are spatially small (like 3x3), will cover a larger effective area of the original input image.</p>
<p>One way to downsample within the network and thus increase the effective receptive field size more quickly is by using <strong>Strided Convolution</strong>. In general, if the input has dimension W, the filter has dimension K, we’re using Padding P, and a Stride S, then the output dimension is given by the formula: <strong>(W - K + 2P) / S + 1</strong>. It’s important that (W - K + 2P) is divisible by S for this to work out cleanly without fractional pixels, or you need to decide on a rounding convention (floor or ceil). Most libraries will use a floor operation implicitly if it’s not perfectly divisible. So, strided convolutions give us a way to perform the convolution operation and downsample the feature map simultaneously. This is a very common technique used in many CNN architectures to reduce computational cost and increase receptive field sizes efficiently.</p>
<p>Okay, let’s provide a Convolution Summary to bring all these definitions and formulas together.</p>
<p>Input: A volume of size C<sub>in</sub> x H x W. Hyperparameters that define the convolution layer:</p>
<ul>
<li>Kernel size: K<sub>H</sub> x K<sub>W</sub> (often K<sub>H</sub> = K<sub>W</sub> = K, e.g., 3x3, 5x5).</li>
<li>Number of filters: C<sub>out</sub> (this determines the depth of the output volume).</li>
<li>Padding: P (number of zeros added to each side of the input spatial dimensions).</li>
<li>Stride: S (how many pixels the filter slides at each step).</li>
</ul>
<p>The Weight matrix (or tensor) can be thought of as having dimensions C<sub>out</sub> x C<sub>in</sub> x K<sub>H</sub> x K<sub>W</sub>. This represents C<sub>out</sub> filters, each of size C<sub>in</sub> x K<sub>H</sub> x K<sub>W</sub>.</p>
<p>The Bias vector has dimension C<sub>out</sub> (one bias per output filter/channel).</p>
<p>The Output size will be C<sub>out</sub> x H’ x W’, where:</p>
<ul>
<li>H’ = (H - K<sub>H</sub> + 2P) / S + 1</li>
<li>W’ = (W - K<sub>W</sub> + 2P) / S + 1</li>
</ul>
<p>Some common settings for these hyperparameters include:</p>
<ul>
<li>K<sub>H</sub> = K<sub>W</sub>: Using small, square filters is very common (e.g., 3x3, 5x5, sometimes 1x1).</li>
<li>P = (K - 1) / 2: This results in “Same” padding, where the output spatial dimensions match the input (assuming S=1).</li>
<li>C<sub>in</sub>, C<sub>out</sub>: Often chosen as powers of 2 (e.g., 32, 64, 128, 256, 512) and typically increase as we go deeper into the network.</li>
<li>K=3, P=1, S=1: A very common 3x3 convolution that preserves spatial resolution.</li>
<li>K=5, P=2, S=1: A 5x5 convolution that preserves spatial resolution.</li>
<li>K=1, P=0, S=1: This is a 1x1 convolution, which we’ll discuss separately as it has interesting properties.</li>
<li>K=3, P=1, S=2: A 3x3 convolution that downsamples the input by a factor of 2 (approximately, depending on exact input size and rounding). This is often used to reduce spatial dimensions.</li>
</ul>
</section>
<section id="pooling-layers" class="level3">
<h3 class="anchored" data-anchor-id="pooling-layers">Pooling layers</h3>
<p>Alright, let’s move on to <strong>Pooling Layers</strong>. These provide another effective way to downsample feature maps, often used in conjunction with convolution layers. The primary purpose of a pooling layer is to reduce the spatial dimensions of the input volume. Importantly, pooling is applied independently to each depth slice of the input. So, given an input C x H x W, the pooling operation will downsample each 1 x H x W plane separately. The number of channels C remains unchanged by the pooling operation itself.</p>
<p><a href="./images/pooling.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6"><img src="https://bhdai.github.io/blog/posts/cnn-architectures/images/pooling.png" class="img-fluid"></a></p>
<p>The Hyperparameters for a pooling layer are:</p>
<ul>
<li><strong>Kernel Size</strong>: This defines the spatial extent of the pooling window (e.g., 2x2).</li>
<li><strong>Stride</strong>: This dictates how much the pooling window slides at each step (e.g., a stride of 2 is common with a 2x2 kernel for non-overlapping pooling).</li>
<li><strong>Pooling function</strong>: This specifies the operation to perform within each pooling window. Common choices are max pooling or average pooling.</li>
</ul>
<p>A key property of pooling, especially max pooling, is that it gives some invariance to small spatial shifts in the input. If the exact location of a feature moves slightly within a pooling window, the output of max pooling might remain the same if the maximum value is still captured. Also, critically, pooling layers typically have no learnable parameters. The operation (max or average) is fixed.</p>
<p>Here’s a Pooling Summary:</p>
<p>Input: A volume C x H x W.</p>
<p>Hyperparameters:</p>
<ul>
<li>Kernel size: K (e.g., 2 for a 2x2 pooling window).</li>
<li>Stride: S (e.g., 2).</li>
<li>Pooling function: Commonly ‘max’ or ‘avg’.</li>
</ul>
<p>Output size: C x H’ x W’, where the formulas for H’ and W’ are the same as for convolution:</p>
<ul>
<li>H’ = (H - K) / S + 1 (assuming P=0, as padding is less common with pooling, though possible)</li>
<li>W’ = (W - K) / S + 1</li>
</ul>
<p>And a crucial point: No learnable parameters. A very common setting is max pooling with K=2 and S=2. This effectively gives 2x downsampling of the spatial dimensions, halving the height and width of the feature map.</p>
</section>
</section>
<section id="normalize-layers" class="level2">
<h2 class="anchored" data-anchor-id="normalize-layers">Normalize layers</h2>
<p>So, taking a broader view, we can identify the primary components of nearly all CNNs, We have our Convolution Layers, Pooling Layers, and typically, at the terminus of the network, one or more Fully-Connected Layers that perform the final classification. Interspersed throughout are the activation functions that introduce non-linearity. We also have regularization techniques like dropout, which we’ll discuss shortly. But now, I want to focus on a component that has become absolutely central to modern deep learning: <strong>Normalization Layers</strong>. Their introduction has been one of the key factors enabling the training of the very deep and high-performing networks we see today. They address some fundamental issues related to the optimization dynamics of deep models.</p>
<p>To develop our intuition, let’s begin with a specific example: Layer Normalization, or LayerNorm. The high-level idea behind it, and indeed behind most normalization layers, is a two-step process. First, you take the activations at some point in the network and you normalize them, typically to have a zero mean and unit variance. This step helps to mitigate the problem of “internal covariate shift,” where the distribution of each layer’s inputs changes during training as the parameters of the previous layers change. This can stabilize and accelerate the training process. However, rigidly enforcing a zero-mean, unit-variance distribution might be suboptimal. Perhaps the network would benefit from activations with a different mean or variance. Therefore, the second step is to introduce two new learnable parameters that allow the network to scale and shift the normalized data. This gives the network the expressive power to, if necessary, learn to reverse the normalization or, more generally, learn the optimal affine transformation for its activations</p>
<p>Consider a mini-batch of N inputs, where each input x is a D-dimensional vector.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%5Cmu,%20%5Csigma%20&amp;:%20N%20%5Ctimes%201%20%5C%5C%0A%5Cgamma,%20%5Cbeta%20&amp;:%201%20%5Ctimes%20D%20%5C%5C%0Ay%20=%20&amp;%5Cfrac%7B%5Cgamma%20(x%20-%20%5Cmu)%7D%7B%5Csigma%7D%20+%20%5Cbeta%0A%5Cend%7Balign%7D%0A"></p>
<p>For Layer Normalization, the key is how the statistics are computed. The mean, <img src="https://latex.codecogs.com/png.latex?%5Cmu">, and standard deviation, <img src="https://latex.codecogs.com/png.latex?%5Csigma">, are calculated per batch element. As you can see, their dimension is N x 1. This means that for each individual training example, we compute its mean and standard deviation across all of its D features. Once the data is normalized, we apply the learned parameters: a scaling factor <img src="https://latex.codecogs.com/png.latex?%5Cgamma"> and a shifting factor <img src="https://latex.codecogs.com/png.latex?%5Cbeta">. Both of these are D-dimensional vectors, meaning we learn a unique scale and shift for each feature dimension. These parameters are shared across all examples in the batch and are updated via backpropagation. The final output y is thus the normalized input, scaled by <img src="https://latex.codecogs.com/png.latex?%5Cgamma"> and shifted by <img src="https://latex.codecogs.com/png.latex?%5Cbeta">.</p>
<p><a href="./images/group-norm.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7"><img src="https://bhdai.github.io/blog/posts/cnn-architectures/images/group-norm.png" class="img-fluid"></a></p>
<p>Now, LayerNorm is just one member of a family of normalization techniques. This visualization from the <a href="https://arxiv.org/abs/1803.08494">Group Normalization paper</a> provides an excellent conceptual map of the different approaches. The blue region highlights the set of neurons over which the mean and standard deviation are calculated to normalize a single value. The most common variant in CNNs is Batch Normalization. Here, we normalize across the batch dimension (N) and the spatial dimensions (H, W), but we do so independently for each feature channel (C). Layer Normalization, which we just discussed, normalizes across all channel and spatial dimensions for a single example in the batch. It’s agnostic to the batch size, which can be advantageous. Instance Normalization is even more granular, normalizing over only the spatial dimensions for each channel and each batch example independently. This is often used in style transfer. And Group Normalization strikes a balance, normalizing over spatial dimensions and pre-defined groups of channels. The choice between these depends on the specific architecture, task, and practical considerations like batch size</p>
</section>
<section id="dropout" class="level2">
<h2 class="anchored" data-anchor-id="dropout">Dropout</h2>
<p>Now, let’s focus specifically on Dropout. While incredibly influential and effective, especially in fully-connected layers, its use has become somewhat more nuanced with the rise of techniques like Batch Normalization, which itself provides a slight regularizing effect. Nevertheless, understanding Dropout is fundamental. So, what is the mechanism of Dropout? The idea, proposed by Srivastava and his colleagues in 2014 <a href="https://jmlr.org/papers/v15/srivastava14a.html">“Dropout: A simple way to prevent neural networks from overfitting”</a>, is conceptually quite simple. During the training phase, for each forward pass through the network, we randomly set the activations of some neurons to zero. The probability of dropping a neuron, or conversely, the probability p of keeping a neuron, is a hyperparameter that we choose. A common value is 0.5, meaning half of the neurons in a given layer are randomly deactivated for each training example that passes through. The key here is that a different set of neurons is dropped for each forward pass. This means that for every mini-batch, the network is effectively a different, “thinned” version of the full architecture.</p>
<p>This raises a very natural question: How can randomly removing parts of your model possibly be a good idea? It seems counterintuitive, almost destructive. But there are a couple of powerful interpretations for why it works so well. The first interpretation is that it prevents the co-adaptation of features. In a standard network, it’s possible for a set of neurons to become highly dependent on one another. For instance, in classifying a cat, one neuron might become a very specific “has a tail” detector, and another neuron downstream might learn to only activate strongly when that specific tail-detector is active. This is a fragile dependency. Dropout breaks these dependencies. Since any neuron can be randomly dropped at any time, a given neuron cannot rely on the presence of any single one of its inputs. It is forced to learn more robust features that are redundant and useful in a variety of different contexts. It has to learn to use evidence from many input neurons, making the learned representation less brittle and more generalizable.</p>
<p>There is another, perhaps more powerful, interpretation. Dropout can be viewed as an efficient way to train a massive ensemble of neural networks. Every time we apply a different dropout mask, that is, a different pattern of dropped neurons, we are effectively training a unique, thinned sub-network. All of these distinct sub-networks, however, share the same underlying parameters. The scale of this ensemble is simply staggering. For a single fully-connected layer with 4096 units, there are 2<sup>4096</sup> possible dropout masks. This number is astronomically larger than the number of atoms in the universe. So, during training, we are sampling from this enormous set of models and taking a gradient step for each one. This provides an extremely potent form of model averaging, which is a well-established technique for improving performance and reducing overfitting.</p>
<p>This stochastic behavior during training introduces a critical consideration: <strong>What do we do at test time?</strong> At test time, we want a single, deterministic prediction. We want to leverage the full capacity of the network we’ve trained. So we do not drop any neurons, all the neurons are active. However this create a mismatch. During training the expected output of any given neuron was scaled down because it was only active with the probability of p.&nbsp;If we simply use the full network at test time, the magnitude of the activation will be systematically larger than the network experience during training, To correct for this, we must scale the activation at test time. Specifically, we multiply the output of each layer to which dropout was applied during training by the keep probability p.&nbsp;This ensure that the expected output of any neuron at test time matches the expected output during training, learning to well-calibrated prediction.</p>
<p>Here’s the “vanilla implementation” of dropout:</p>
<div class="sourceCode" id="annotated-cell-1" style="background: #f1f3f5;"><pre class="sourceCode python code-annotation-code code-with-copy code-annotated"><code class="sourceCode python"><span id="annotated-cell-1-1">p <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span></span>
<span id="annotated-cell-1-2"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> train_step(X):</span>
<span id="annotated-cell-1-3">  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># forward pass for example 3-layer network</span></span>
<span id="annotated-cell-1-4">  H1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.maximum(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, np.dot(W1, X) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> b1)</span>
<span id="annotated-cell-1-5">  U1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.rand(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>H1.shape) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> p <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># first drop mask</span></span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-1" data-target-annotation="1" onclick="event.preventDefault();">1</a><span id="annotated-cell-1-6" class="code-annotation-target">  H1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*=</span> U1 <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># drop!</span></span>
<span id="annotated-cell-1-7">  H2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.maximum(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, np.dot(W2, H1) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> b2)</span>
<span id="annotated-cell-1-8">  U2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.rand(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>H2.shape) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> p <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># second drop mask</span></span>
<span id="annotated-cell-1-9">  H2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*=</span> U2 <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># drop!</span></span>
<span id="annotated-cell-1-10">  out <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.dot(W3, H2) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> b3</span>
<span id="annotated-cell-1-11"></span>
<span id="annotated-cell-1-12">  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># backward pass: compute gradients... (not show)</span></span>
<span id="annotated-cell-1-13">  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># parameter update... (not show)</span></span>
<span id="annotated-cell-1-14"></span>
<span id="annotated-cell-1-15"></span>
<span id="annotated-cell-1-16"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> predict(X):</span>
<span id="annotated-cell-1-17">  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ensembled forward pass</span></span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-1" data-target-annotation="2" onclick="event.preventDefault();">2</a><span id="annotated-cell-1-18" class="code-annotation-target">  H1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.maximum(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, np.dot(W1, X) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> b1) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> p <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># scale the activations</span></span>
<span id="annotated-cell-1-19">  H2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.maximum(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, np.dot(W2, H1) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> b2) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> p <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># scale the activations</span></span>
<span id="annotated-cell-1-20">  out <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.dot(W3, H2) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> b3</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code></pre></div>
<dl class="code-annotation-container-grid">
<dt data-target-cell="annotated-cell-1" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-1" data-code-lines="6" data-code-annotation="1">We have to distinct mode of operation. During the training step we first compute the activation, then generate a random mask and apply it, effectively setting some neuron to zero, this is the ‘drop in training time’ phase.</span>
</dd>
<dt data-target-cell="annotated-cell-1" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-1" data-code-lines="18" data-code-annotation="2">Then at test time, we perform the standard forward pass, but after each layer computation, we scale the result by a keep probability p, this is the ‘scale at test time’ phase .</span>
</dd>
</dl>
<p>I should not that this is a “not recommended” implementation. A more common approach today is known as inverted dropout. In inverted dropout, the scaling is performed during training step by dividing the activation by p rather than at the test time. This has the practical advantage that the test time forward pass remain unchanged. Which simplify deployment. The net effect is the same, but the implement detail is important to be aware of.</p>
</section>
<section id="activation-functions" class="level2">
<h2 class="anchored" data-anchor-id="activation-functions">Activation functions</h2>
<p>We’ve talked about the operations that involve learnable parameters, like convolution and fully-connected layers. The activation function is the piece that follows these linear operations, and its role is profound. What is that role? The fundamental goal of an activation function is to introduce non-linearities into our model. This is not a minor detail; it is the very reason deep networks are powerful. If you were to stack any number of linear layers without any non-linearities in between, the entire network would collapse into a single, equivalent linear transformation. You would have a very deep, very computationally expensive linear classifier, which is no more expressive than the simple linear models It’s the non-linearity that allows the network to approximate arbitrarily complex functions and learn the hierarchical features that are the hallmark of deep learning.</p>
<p>Here’s a little toy example of a double ReLU function you can play around with to gain a better understanding of why non-linearity is necessary. This uses just 2 ReLU functions, imagine hundreds of these! You could approximate any function by increasing the number of ReLUs and tuning their hyperparameters. The double ReLU function is defined as:</p>
<p><img src="https://latex.codecogs.com/png.latex?f(x)%20=%20a_1%20%5Ccdot%20%5Cmax(0,%20x%20-%20b_1)%20+%20a_2%20%5Ccdot%20%5Cmax(0,%20x%20-%20b_2)"></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Design choice for scaling factor placement
</div>
</div>
<div class="callout-body-container callout-body">
<p>Note that I placed a<sub>i</sub> outside of the max function (rather than inside), which allows for negative slopes. For this educational demo, this provides more intuitive control and creates more visually interesting non-linear combinations.</p>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code hidden" id="cb1" data-startfrom="195" data-source-offset="-0" style="background: #f1f3f5;"><pre class="sourceCode js code-with-copy"><code class="sourceCode javascript" style="counter-reset: source-line 194;"><span id="cb1-195"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> {viewof a1<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> viewof b1<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> viewof a2<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> viewof b2<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> viewof showComponents<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> chart} <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"https://observablehq.com/d/d00db79bf2aca3bf"</span></span>
<span id="cb1-196"></span>
<span id="cb1-197"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Display the controls and chart</span></span>
<span id="cb1-198">viewof a1</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-1" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-2" data-nodetype="expression">

</div>
</div>
</div>
<div class="sourceCode cell-code hidden" id="cb2" data-startfrom="199" data-source-offset="-178" style="background: #f1f3f5;"><pre class="sourceCode js code-with-copy"><code class="sourceCode javascript" style="counter-reset: source-line 198;"><span id="cb2-199">viewof b1</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-3" data-nodetype="expression">

</div>
</div>
</div>
<div class="sourceCode cell-code hidden" id="cb3" data-startfrom="200" data-source-offset="-188" style="background: #f1f3f5;"><pre class="sourceCode js code-with-copy"><code class="sourceCode javascript" style="counter-reset: source-line 199;"><span id="cb3-200">viewof a2</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-4" data-nodetype="expression">

</div>
</div>
</div>
<div class="sourceCode cell-code hidden" id="cb4" data-startfrom="201" data-source-offset="-198" style="background: #f1f3f5;"><pre class="sourceCode js code-with-copy"><code class="sourceCode javascript" style="counter-reset: source-line 200;"><span id="cb4-201">viewof b2</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-5" data-nodetype="expression">

</div>
</div>
</div>
<div class="sourceCode cell-code hidden" id="cb5" data-startfrom="202" data-source-offset="-208" style="background: #f1f3f5;"><pre class="sourceCode js code-with-copy"><code class="sourceCode javascript" style="counter-reset: source-line 201;"><span id="cb5-202">viewof showComponents</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-6" data-nodetype="expression">

</div>
</div>
</div>
<div class="sourceCode cell-code hidden" id="cb6" data-startfrom="203" data-source-offset="-230" style="background: #f1f3f5;"><pre class="sourceCode js code-with-copy"><code class="sourceCode javascript" style="counter-reset: source-line 202;"><span id="cb6-203">chart</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-7" data-nodetype="expression">

</div>
</div>
</div>
</div>
<p>Let’s begin with a historically significant activation function: the <strong>Sigmoid</strong>.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Csigma(x)%20=%20%5Cfrac%7B1%7D%7B1%20+%20e%5E%7B-x%7D%7D%0A"></p>
<p>It has a very appealing property: it squashes any real-valued input into the range [0, 1]. This made it popular in the early days of neural networks because it provided a nice biological analogy to the “firing rate” of a neuron, which can be thought of as varying between a state of no activity (0) and maximum saturation (1).</p>
<div class="cell">
<div class="sourceCode cell-code hidden" id="cb7" data-startfrom="216" data-source-offset="-0" style="background: #f1f3f5;"><pre class="sourceCode js code-with-copy"><code class="sourceCode javascript" style="counter-reset: source-line 215;"><span id="cb7-216"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> {sigmoidChart} <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"https://observablehq.com/d/d00db79bf2aca3bf"</span></span>
<span id="cb7-217"></span>
<span id="cb7-218">sigmoidChart</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-2-1" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-2-2" data-nodetype="expression">

</div>
</div>
</div>
</div>
<p>However, it harbors a key problem, one that severely hampered the training of deep networks. When you stack many layers of sigmoid neurons, you can encounter an issue with gradients. Let me pose this question to you: looking at the shape of the function, in which regions does the sigmoid have a very small gradient? As the input x become very large, either negative or positive, the sigmoid function saturates. Its output approaches either 1 or 0, and the curve becomes flat. In these flat regions, the local gradient is virtually zero. During backpropagation, the gradients from downstream layers are multiplied by these local gradients. If a neuron is in a saturated state, its local gradient will effectively “kill” or “vanish” the gradient signal passing through it. In a deep network with many sigmoid layers, this effect compounds, leading to the infamous vanishing gradient problem, where gradients in the early layers of the network become so small that the weights are barely updated, and learning grinds to a halt.</p>
<div class="cell">
<div class="sourceCode cell-code hidden" id="cb8" data-startfrom="225" data-source-offset="-0" style="background: #f1f3f5;"><pre class="sourceCode js code-with-copy"><code class="sourceCode javascript" style="counter-reset: source-line 224;"><span id="cb8-225"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> {reluChart} <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"https://observablehq.com/d/d00db79bf2aca3bf"</span></span>
<span id="cb8-226"></span>
<span id="cb8-227">reluChart</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-3-1" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-3-2" data-nodetype="expression">

</div>
</div>
</div>
</div>
<p>This problem necessitated the search for better activation functions, which led to the widespread adoption of <strong>the Rectified Linear Unit</strong>, or <strong>ReLU</strong>. Its definition is elegantly simple: <img src="https://latex.codecogs.com/png.latex?f(x)%20=%20%5Cmax(0,%20x)">. It simply thresholds the input at zero. This function, introduced in the context of deep learning in the <a href="https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">AlexNet paper</a>, was a major breakthrough. It has several compelling advantages. First, and most importantly, it does not saturate in the positive region. For any positive input, the gradient is simply 1, allowing the gradient signal to flow unhindered during backpropagation, thus alleviating the vanishing gradient problem. Second, it is computationally trivial, it’s just a simple comparison to zero, it’s a very cheap operation just basically check the sign bit whether it on or off, which makes both the forward and backward passes very fast. The empirical result of these properties is dramatic: networks using ReLU often converge much faster than those using sigmoid or tanh. The AlexNet authors reported a 6x speedup in convergence on ImageNet. However, ReLU is not without its own set of issues. One issue is that its output is not zero-centered. This can introduce some undesirable dynamics during gradient descent, although this is often mitigated by techniques like Batch Normalization. A more significant annoyance is the <strong>Dead ReLU</strong> problem. Look at the negative region, for any input x &lt; 0, the output is 0, and importantly, the gradient is also 0. If a neuron, due to a large gradient update or poor initialization, gets pushed into a regime where its input is consistently negative, it will always output zero. The gradient flowing through it will also always be zero. Consequently, the weights feeding into that neuron will never again receive a gradient update. The neuron is effectively “dead” for the remainder of training, having become an inert part of the network.</p>
<div class="cell">
<div class="sourceCode cell-code hidden" id="cb9" data-startfrom="234" data-source-offset="-0" style="background: #f1f3f5;"><pre class="sourceCode js code-with-copy"><code class="sourceCode javascript" style="counter-reset: source-line 233;"><span id="cb9-234"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> {reluGeluChart} <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"https://observablehq.com/d/d00db79bf2aca3bf"</span></span>
<span id="cb9-235"></span>
<span id="cb9-236">reluGeluChart</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-4-1" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-4-2" data-nodetype="expression">

</div>
</div>
</div>
</div>
<p>The quest to find an activation that combines the benefits of ReLU while mitigating its drawbacks is an active area of research. One prominent successor is the <strong>Gaussian Error Linear Unit</strong>, or <strong>GELU</strong>.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Af(x)%20=%20x%20%5Ccdot%20%5CPhi(x)%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5CPhi(x)"> is the cumulative distribution function of the standard Gaussian distribution. This means that GELU weights inputs based on their value, allowing for a smoother transition compared to other activation functions.</p>
<p>GELU can be thought of as a smoother, probabilistic version of ReLU. As you can see from the plot, it closely tracks ReLU for positive values but smoothly curves below zero. It doesn’t have the hard zero-gradient “kink” that ReLU does. This smoothness around zero is empirically beneficial and can facilitate more stable training. Critically, it does not have a zero gradient for negative inputs, which helps avoid the Dead ReLU problem. However, this comes at a cost: it is more computationally expensive than the simple ReLU. And while it’s less prone to killing gradients, for large negative values, the gradient does still approach zero. GELU has become the standard in many state-of-the-art models, particularly in the domain of Transformers.</p>
<p>And there are a lot activation functions out there like Leaky ReLU introduces a small, fixed slope for negative inputs to prevent neurons from dying. ELU uses an exponential function for negative inputs to push the mean activation closer to zero. And we’ve just discussed GELU. Another interesting one is SiLU, or Swish, which is x times the sigmoid of x, creating a non-monotonic function that dips slightly below zero before rising. The main takeaway here is not to memorize every single one, but to recognize that while ReLU remains a very strong and common default choice, the selection of an activation function is a design decision with trade-offs between performance, computational cost, and training stability.</p>
<p>Now thinking about a standard CNN architecture… where are these activation functions actually used? They are generally placed immediately after the linear operators in the network. So, you would have a convolution layer, followed by an activation function. Or a fully-connected layer, followed by an activation function. Their role is to take the output of these linear transformations and inject the critical non-linearity before the data is passed to the next layer.</p>
</section>
<section id="cnn-architectures" class="level2">
<h2 class="anchored" data-anchor-id="cnn-architectures">CNN architectures</h2>
<p>The next natural step is to see how these pieces are assembled into full-scale, effective CNN architectures. And to understand the evolution of these architectures, there is no better lens than the ImageNet Large Scale Visual Recognition Challenge, or ILSVRC.</p>
<p><a href="./images/ILSVRC.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8"><img src="https://bhdai.github.io/blog/posts/cnn-architectures/images/ILSVRC.png" class="img-fluid"></a></p>
<p>What you see here are the winning top-5 error rates on the ImageNet challenge from 2010 through 2017. In the pre-deep learning era of 2010 and 2011, the methods were based on shallow feature engineering, and the error rates were quite high, around 28% and 26%. Then, in 2012, something remarkable happened. AlexNet, an 8-layer convolutional neural network, entered the competition and dramatically reduced the error rate to 16.4%. This was the watershed moment that convinced the computer vision community of the power of deep learning. Following this, we see a clear and consistent trend: year after year, the error rates fall, while the network depths steadily increase. We go from 8 layers, to 19, to 22, and then in 2015, a truly massive leap to 152 layers with ResNet, which for the first time achieved an error rate lower than the estimated human performance on this task. This chart is, in essence, a story of the community learning how to successfully build and train progressively deeper neural networks. The revolution, as I mentioned, began in 2012 with AlexNet. This 8-layer network, building on many of the components we’ve discussed like ReLU and Dropout, demonstrated definitively that deep, learned features could vastly outperform hand-engineered ones. It set the stage for all the architectural development that followed. Today, we’re going to pick up the story in 2014. After AlexNet’s success, the immediate research question was, “If 8 layers are good, are more layers better?” The winning entries from 2014, VGGNet and GoogLeNet, answered this with a resounding yes. They pushed network depth from 8 layers to 19 and 22 layers, respectively, and were rewarded with another significant drop in error, from over 11% in the previous year down to the 7% range. Let’s start by taking a closer look at the VGG architecture.</p>
<section id="vgg" class="level3">
<h3 class="anchored" data-anchor-id="vgg">VGG</h3>
<p><a href="https://arxiv.org/abs/1409.1556">VGGNet</a>, from Simonyan and Zisserman at Oxford. The core philosophy behind VGG was to explore the effect of depth using an architecture that was remarkably simple. Their central idea was this: “Small filters, Deeper networks.”</p>
<p><a href="./images/vgg.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9"><img src="https://bhdai.github.io/blog/posts/cnn-architectures/images/vgg.png" class="img-fluid"></a></p>
<p>If you look at the comparison with AlexNet on the left, you’ll see AlexNet used a mix of filter sizes, a large 11x11 filter in the first layer, followed by 5x5 filters. VGGNet, in contrast, made a radical design choice: it exclusively uses very small 3x3 convolutional filters throughout the entire network. This uniformity allowed them to stack these layers very deep, creating the 16 model shown here. The structure is a repeating motif: a block of two or three 3x3 convolutions, followed by a 2x2 max-pooling layer to reduce the spatial dimensions. By sticking to this simple rule, they went from 8 layers to 16, and with this added depth, they achieved a substantial improvement, reducing the top-5 error from 11.7% to 7.3%. This brings us to a critical design question. Why did they make this choice? Why use only these small 3x3 filters? Why not use a 5x5 or a 7x7 filter, which would have a larger receptive field and seemingly be able to capture larger spatial patterns in a single step? Let’s take a moment to consider the implications of this design.</p>
<p>Alright, let’s analyze this question quantitatively. What is the effective receptive field of stacking three consecutive 3x3 convolution layers, assuming a stride of 1? There are two profound advantages. First, and arguably most important, the stacked approach is deeper and incorporates more non-linearities. Between each of the 3x3 convolutions, we place an activation function, like a ReLU. So, in the stacked version, we apply three non-linearities over that 7x7 receptive field. A single 7x7 convolution layer would only have one non-linearity. This increased non-linearity allows the model to learn more complex and discriminative features, which is a key benefit of depth. The second advantage is a significant reduction in the number of parameters. Let’s assume the number of channels per layer is C. A single 7x7 conv layer would have 7 * 7 * C * C = 49 * C<sup>2</sup> parameters. A stack of three 3x3 conv layers has 3 * (3 * 3 * C * C) = 27 * C<sup>2</sup> parameters. This is a substantial reduction, making the network more efficient and less prone to overfitting. So, the VGG design gives us more expressive power with fewer parameters, a clear win-win.</p>
<p>The VGGNet philosophy, that deeper is better, especially when done efficiently set the stage for what came next. This trend of increasing depth continued, but in 2015, we saw a jump that was qualitatively different from what came before. Kaiming He and his colleagues introduced the Residual Network, or ResNet, which had a staggering 152 layers. This wasn’t just a simple extrapolation; it was a fundamental architectural innovation that enabled training at depths previously thought impossible. This truly marks the “Revolution of Depth.”</p>
</section>
<section id="resnet" class="level3">
<h3 class="anchored" data-anchor-id="resnet">Resnet</h3>
<p>This architecture was born from a very simple and direct research question: What happens when we just continue stacking deeper and deeper layers on a “plain” convolutional network, like a VGG-style architecture? One might naively assume that performance should just continue to improve, or at worst, plateau. The reality, as we will see, is surprisingly different, and it revealed a fundamental optimization problem that ResNet was designed to solve.</p>
<p>Kaiming He and his colleagues took a plain network architecture, similar in style to VGG, and trained two versions: a “shallower” 20-layer model and a much deeper 56-layer model. Here are the results</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./images/resnet-experiment.png" class="lightbox" data-gallery="quarto-lightbox-gallery-10" title="On the left, we see the test error, and on the right, the training error. The blue line represents the 20-layer model, and the red line represents the 56-layer model."><img src="https://bhdai.github.io/blog/posts/cnn-architectures/images/resnet-experiment.png" class="img-fluid figure-img" alt="On the left, we see the test error, and on the right, the training error. The blue line represents the 20-layer model, and the red line represents the 56-layer model."></a></p>
<figcaption>On the left, we see the test error, and on the right, the training error. The blue line represents the 20-layer model, and the red line represents the 56-layer model.</figcaption>
</figure>
</div>
<p>What we observe is something quite unexpected. The deeper 56-layer model performs worse than the 20-layer model. But what’s truly puzzling is that it performs worse not only on the test set, but also on the training set. The training error for the 56-layer model is higher than for the 20-layer model. This is a crucial observation. The fact that the deeper model has a higher training error means that this is not a problem of overfitting. If it were overfitting, we would expect the deeper model to achieve a very low training error by memorizing the training data, but then perform poorly on the test set. Here, the deeper model is failing to even fit the training data as well as its shallower counterpart. This phenomenon is known as the degradation problem.</p>
<p>This points to a fundamental difficulty. It is a fact that a deeper model, having more parameters, has strictly greater representational power than a shallower model. It can represent any function the shallower model can, plus many more. So, why does it perform worse? The hypothesis put forth by the ResNet authors is that this is not a representation problem, but an optimization problem. While the deeper model can theoretically represent better solutions, it is paradoxically much harder for our optimization algorithms, like stochastic gradient descent, to actually find those good parameter settings. The optimization landscape becomes much more complex and difficult to navigate.</p>
<p>Let’s formalize this with a thought experiment. Consider a well-trained shallow model. Now, imagine we construct a deeper model. What should this deeper model learn to be, at the absolute minimum, at least as good as the shallow model? Well, there’s a simple solution by construction. The deeper model could simply copy the learned layers from the shallow model for its initial layers, and then set all the additional layers to simply be identity mappings. An identity mapping is a function that just passes its input through unchanged. If the extra layers do nothing, the deeper model will produce exactly the same output as the shallow model, and thus have the same error. Since we know a solution exists that is at least this good, the fact that SGD fails to find it implies that learning the identity mapping with a stack of non-linear layers is surprisingly difficult. This insight is the absolute core of ResNet.</p>
<p><a href="./images/resnet-solution.png" class="lightbox" data-gallery="quarto-lightbox-gallery-11"><img src="https://bhdai.github.io/blog/posts/cnn-architectures/images/resnet-solution.png" class="img-fluid"></a></p>
<p>The conventional approach, what the paper calls “plain” layers, is to have a stack of layers try to learn some desired underlying mapping, H(x). For example, H(x) might be the ideal features for the next stage of the network. The ResNet solution is to reframe the problem. Instead of asking the layers to learn H(x) directly, let’s change what they are learning. We introduce what’s called a “shortcut” or “skip” connection, which takes the input to the block, x, and adds it to the output of the block. The stack of layers is now only asked to learn a residual function, F(x). The final output of the block is H(x) = F(x) + x. Now, consider our identity mapping problem. How can this block learn to be an identity mapping, so that H(x) = x? It’s trivial. The network just needs to learn to set the output of the residual path, F(x), to zero. It can accomplish this by simply driving the weights of the convolutional layers in the block towards zero. This is a much easier optimization target for SGD than trying to learn an identity mapping through a complex stack of non-linear transformations. So, to be explicit, we are changing the objective of the building block. Instead of learning H(x) directly, we use the layers to fit the residual F(x) = H(x) - x. We are hypothesizing that it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping. The skip connection performs the identity mapping, and the stacked layers learn the “correction” or “residual” that needs to be applied. This formulation proved to be the key that unlocked the training of extremely deep neural networks.</p>
<div class="columns">
<div class="column" style="width:60%;">
<p>The full ResNet architecture is essentially just a stack of these residual blocks. As you can see on the right, the network is composed of repeating modules. Each of these modules, or residual blocks, contains two 3x3 convolutional layers, which echoes the design philosophy of VGG that we just discussed. The crucial difference, of course, is the identity skip connection that bypasses these two layers and is added to their output before the final ReLU activation. The entire network, from start to finish, is constructed by composing these fundamental building blocks one after another.</p>
<p>Now, a critical detail in any deep CNN is how to handle the changes in spatial resolution and channel depth. A network can’t maintain the same spatial dimensions throughout, as that would be computationally intractable and would fail to build a hierarchy of features. ResNet addresses this in a very systematic way. Periodically, at the beginning of a new “stage” of the network, it does two things simultaneously: it doubles the number of filters, and it downsamples the feature map spatially. This downsampling is achieved not by a pooling layer, but by setting the stride of the first 3x3 convolution in that block to 2. This, of course, creates a dimensionality mismatch for the addition: the identity x has half the spatial resolution and half the channel depth of the output of the convolutional path F(x). To resolve this, when downsampling occurs, the skip connection is also modified. It typically consists of a 1x1 convolution with a stride of 2, which serves to downsample x and project it to the new, higher channel dimension, so that the element-wise addition can be performed. This is a very clean and effective way to manage the tensor dimensions as we go deeper into the network. Finally, there’s one more piece. Before the main stack of residual blocks, the network begins with an initial convolutional layer, sometimes called the “stem.” In the case of ResNet, this is a large 7x7 convolution with a stride of 2, followed by a max-pooling layer. The purpose of this stem is to aggressively reduce the spatial dimensions and quickly extract low-level features like edges and blobs from the input image before it enters the more complex residual stages.</p>
</div><div class="column" style="width:40%;">
<p><a href="./images/resnet.png" class="lightbox" data-gallery="quarto-lightbox-gallery-12"><img src="https://bhdai.github.io/blog/posts/cnn-architectures/images/resnet.png" class="img-fluid"></a></p>
</div>
</div>
<p>The elegance of this modular design is its scalability. By simply deciding how many residual blocks to stack in each stage of the network, the authors could easily construct a family of architectures of varying depths. The paper presented models of 18, 34, 50, 101, and the flagship 152-layer network for the ImageNet challenge. This systematic and principled approach to increasing depth was a key contribution. And the results were nothing short of revolutionary. The ability to train these very deep network using residual connections led to a new state-of-the-art. Their 152-layer model won the ILSVRC 2015 classification competition with a top-5 error of just 3.57%, which was a remarkable improvement and the first time a model surpassed the reported human-level performance benchmark on this dataset. The impact of ResNet extended far beyond image classification. The features learned by this architecture were so powerful and general that ResNet-based models swept nearly all major classification and detection competitions in 2015. For several years following its publication, the ResNet architecture became the de facto standard backbone for a vast array of computer vision tasks.</p>
</section>
</section>
<section id="weight-initialize" class="level2">
<h2 class="anchored" data-anchor-id="weight-initialize">Weight initialize</h2>
<p>We’ve discussed the layers, the activations, and the architectural patterns. Now, we must address the crucial, and often overlooked, topic of <strong>Weight Initialization</strong>. How we set the initial values of the network’s parameters is not a trivial detail, it can be the difference between a network that trains successfully and one whose gradients either vanish or explode. How should we initialize the weights in our neural network layers? It seems like a minor detail, but as we’ll see, it has profound implications for the trainability of deep models. Let’s explore this with a concrete example.</p>
<div class="sourceCode" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1">dims <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4096</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span></span>
<span id="cb10-2">hs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb10-3">x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.randn(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">16</span>, dims[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])</span>
<span id="cb10-4"></span>
<span id="cb10-5"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> Din, Dout <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">zip</span>(dims[:<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>], dims[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>:]):</span>
<span id="cb10-6">  W <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> np.random.randn(Din, Dout) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># small weight init</span></span>
<span id="cb10-7">  x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.maximum(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, np.dot(x, W)) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ReLU activation</span></span>
<span id="cb10-8">  hs.append(x)</span></code></pre></div>
<p><a href="./images/small-weight-init.png" class="lightbox" data-gallery="quarto-lightbox-gallery-13"><img src="https://bhdai.github.io/blog/posts/cnn-architectures/images/small-weight-init.png" class="img-fluid"></a></p>
<p>Here we have a simple Python snippet that simulates the forward pass through a 6-layer deep network. Each hidden layer has 4096 neurons, and we’re using a ReLU activation function. We’ll start with a naive initialization strategy: initializing the weights from a standard normal distribution, and then scaling them down by a small constant factor, in this case, 0.01. This seems plausible; we want the initial weights to be small to avoid starting in a highly non-linear, saturated regime. But let’s look at what happens. we see histograms of the activations in each of the six layers after a single forward pass with random input data. In the first layer, the activations have a reasonable distribution. But by the second layer, the mean and standard deviation have shrunk considerably. By the third, even more so. As we propagate through the network, the activations progressively collapse towards zero. By the time we reach the final layers, nearly all the activations are zero. What is the consequence of this for learning? If all activations are zero, what will the gradients be during the backward pass? They will also be zero. This is a form of the vanishing gradient problem induced not by the activation function itself, but by poor weight initialization. The signal dies as it propagates forward, and the gradient dies as it propagates backward. The network will not learn.</p>
<p>Okay, so maybe our initial scaling factor was too small. Let’s try making the weights a bit larger. We’ll change the scaling factor from 0.01 to 0.05. A modest increase.</p>
<div class="sourceCode" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">dims <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4096</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span></span>
<span id="cb11-2">hs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb11-3">x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.randn(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">16</span>, dims[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])</span>
<span id="cb11-4"></span>
<span id="cb11-5"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> Din, Dout <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">zip</span>(dims[:<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>], dims[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>:]):</span>
<span id="cb11-6">  W <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.05</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> np.random.randn(Din, Dout) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># large weight init</span></span>
<span id="cb11-7">  x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.maximum(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, np.dot(x, W)) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ReLU activation</span></span>
<span id="cb11-8">  hs.append(x)</span></code></pre></div>
<p><a href="./images/large-weight-init.png" class="lightbox" data-gallery="quarto-lightbox-gallery-14"><img src="https://bhdai.github.io/blog/posts/cnn-architectures/images/large-weight-init.png" class="img-fluid"></a></p>
<p>The result is just as catastrophic, but in the opposite direction. Now, looking at the activation statistics, we see the mean and standard deviation exploding as we move through the network. The activations are pushed far into the positive regime of the ReLU. While this doesn’t cause saturation in the same way as a sigmoid, this rapid growth in magnitude leads to extremely large gradients during the backward pass. This is the exploding gradient problem. It can cause the weight updates to be so large that the optimization process becomes unstable, with the loss oscillating wildly or diverging to infinity.</p>
<p>So, we’re in a bit of a Goldilocks situation. We need an initialization that is not too small and not too large. The key insight is that the correct scaling factor depends on the size of the layer. Specifically, it depends on the number of input neurons to the layer, which we often call the “fan-in.” The variance of the output of a linear layer is proportional to the variance of the input times the number of input connections times the variance of the weights. To keep the variance of the activations constant as we pass through the network, we need to scale our weight initialization to counteract the effect of the fan-in. A principled way to do this, specifically for networks using ReLU activations, was proposed in the same year as ResNet by Kaiming He and colleagues. Their solution, often called “Kaiming initialization” or “MSRA initialization,” is to scale the weights by the square root of 2 divided by the fan-in (Din).</p>
<div class="sourceCode" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1">dims <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4096</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span></span>
<span id="cb12-2">hs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb12-3">x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.randn(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">16</span>, dims[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])</span>
<span id="cb12-4"></span>
<span id="cb12-5"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> Din, Dout <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">zip</span>(dims[:<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>], dims[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>:]):</span>
<span id="cb12-6">  W <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.randn(Din, Dout) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> np.sqrt(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> Din)</span>
<span id="cb12-7">  x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.maximum(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, np.dot(x, W))</span>
<span id="cb12-8">  hs.append(x)</span></code></pre></div>
<p><a href="./images/just-right-weight-init.png" class="lightbox" data-gallery="quarto-lightbox-gallery-15"><img src="https://bhdai.github.io/blog/posts/cnn-architectures/images/just-right-weight-init.png" class="img-fluid"></a></p>
<p>When we use Kaiming initialization, the result is remarkable. Looking at the histograms, we see that the distribution of activations remains stable across all six layers. The mean and standard deviation are preserved as the signal propagates through the deep network. This is precisely what we want. It ensures that all layers have a healthy flow of information and receive meaningful gradients, which is essential for successful training. This type of principled initialization has become standard practice and is a critical component for training deep architectures from scratch.</p>
</section>
<section id="data-preparation" class="level2">
<h2 class="anchored" data-anchor-id="data-preparation">Data preparation</h2>
<section id="data-preprocessing" class="level3">
<h3 class="anchored" data-anchor-id="data-preprocessing">Data preprocessing</h3>
<p>We’ve built our network, but now we need to train it. We will now focus on the practical methodologies for how to train CNNs, starting with data preprocessing. For image data, the standard preprocessing step is normalization. Here is the TLDR. The universal practice in modern deep learning is to center and scale the data for each channel independently. This means we first compute the mean and standard deviation of the pixel values across the entire training dataset, but we do this separately for the Red, Green, and Blue channels. This gives us three mean values and three standard deviation values. Then, for every image we feed into the network (both at training and test time), we subtract the corresponding per-channel mean from each pixel and divide by the per-channel standard deviation. This process ensures that the input data for each channel has approximately zero mean and unit variance. This is crucial for stable training, as it puts the data into a well-behaved numerical range, which helps with gradient flow and prevents the first layer from having to learn to adapt to arbitrarily scaled inputs. Note that this requires a pre-computation step on your training set before you begin the main training loop.</p>
</section>
<section id="data-augmentation" class="level3">
<h3 class="anchored" data-anchor-id="data-augmentation">Data augmentation</h3>
<p>We will now discuss Data Augmentation. This is an extremely powerful and widely used technique for improving the generalization performance of your model, effectively a form of regularization. Before we look at specific data augmentation techniques, I want to highlight a common pattern that underlies many forms of regularization in deep learning. The general pattern is this: during the training phase, we introduce some form of stochasticity or randomness into the process. The model’s output y is a function not only of the weights W and the input x, but also of some random variable z.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ay%20=%20f_W(x,%20z)%0A"></p>
<p>Then, at test time, we want a deterministic prediction. The strategy here is to marginalize out, or average over, this randomness. We want to compute the expectation of the model’s output over the distribution of the random variable z. In practice, this often involves an approximation, such as sampling or using an analytical trick.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ay%20=%20f(x)%20=%20E_z%5Cleft%5Bf(x,%20z)%5Cright%5D%20=%20%5Cint%20p(z)f(x,%20z)dz%0A"></p>
<p>We’ve already seen a perfect example of this pattern: Dropout. During training, the randomness comes from the binary mask that randomly drops activations. At testing, we average out this randomness. The scaling of activations by the keep probability p that we discussed is a clever analytical way to compute the exact expected output of the ensemble of all possible sub-networks. So, training involves random dropping, and testing involves averaging. Data augmentation fits this regularization pattern perfectly. The core idea is to artificially enlarge the training dataset by applying random, label-preserving transformations to the input images. During training, for each image we load, we apply a random transformation—a slight rotation, a crop, a color shift—and then we feed this transformed image to the CNN to compute the loss. This forces the network to learn features that are invariant to these transformations. The source of randomness, z, is the choice of transformation.</p>
<p><a href="./images/data-augmentation.png" class="lightbox" data-gallery="quarto-lightbox-gallery-16"><img src="https://bhdai.github.io/blog/posts/cnn-architectures/images/data-augmentation.png" class="img-fluid"></a></p>
<p>Let’s look at some common examples. The simplest and one of the most effective is the horizontal flip. For most object categories in natural images, like this cat, the semantic label is invariant to a horizontal flip. A cat is still a cat when mirrored. So, during training, we can randomly flip each image horizontally with a 50% probability. This effectively doubles the size of our training set and teaches the model that left-right orientation is not a distinguishing feature for this class.</p>
<p>A more sophisticated and extremely powerful technique involves random crops and scales. The procedure described here is from the original ResNet paper. During training, you first randomly pick a scale L from a given range. You resize the image so its shorter side is L, and then you sample a random 224x224 patch from this resized image. This teaches the model to be robust to both variations in object scale and position within the frame. Then, at test time, we follow the pattern of averaging out the randomness. We perform what is called test-time augmentation (TTA). Instead of a single random crop, we create a deterministic, fixed set of crops. For example, we might resize the image to 5 different scales, and for each scale, we take 10 crops: one from the center, one from each of the four corners, and then the horizontal flips of all five. We run all 50 of these crops through the network and average their final predictions to get a single, robust prediction for the image.</p>
<p>Another very common technique is Color Jitter. The exact same object can appear very different under varying lighting conditions. To make our model robust to this, we can randomly perturb the color properties of the image during training. Simple approaches involve randomly adjusting the contrast and brightness of the image. More complex methods can involve perturbations in the PCA space of the RGB values, as was done in the AlexNet paper.</p>
<p>We can also think of regularization techniques that operate directly on the image space, analogous to Dropout. Techniques like Cutout, or Random Erasing, involve setting random rectangular regions of the input image to zero (or some other constant value). This forces the network to look at the entire object and not become overly reliant on one specific, salient feature. For example, to recognize this cat, it can’t just rely on seeing the eye; it has to learn to use information from the ears, the fur texture, and the overall shape, because any one of those features might be occluded by the random patch. This works very well, especially for smaller datasets like CIFAR where overfitting is a major concern.</p>
</section>
</section>
<section id="transfer-learning" class="level2">
<h2 class="anchored" data-anchor-id="transfer-learning">Transfer learning</h2>
<p>This is arguably one of the most important concepts in the practical application of deep learning today: <strong>Transfer Learning</strong>. This brings us to a very pragmatic question. Training these large models, like the ResNets we just discussed, on a dataset like ImageNet requires immense computational resources and vast amounts of labeled data. So, what do you do in a more common scenario? What if you don’t have a lot of data? Can you still leverage the power of these deep CNNs for your specific problem? The answer is a definitive yes, and the mechanism for doing so is Transfer Learning. The fundamental intuition behind transfer learning in computer vision is that the features learned by a network trained on a large, diverse dataset are often useful for other, related tasks.</p>
<p><a href="./images/transfer-learning.png" class="lightbox" data-gallery="quarto-lightbox-gallery-17"><img src="https://bhdai.github.io/blog/posts/cnn-architectures/images/transfer-learning.png" class="img-fluid"></a></p>
<p>If we visualize the learned filters from the very first convolutional layer of AlexNet, we see that they are not random noise. The network has learned to detect fundamental visual primitives like oriented edges, color blobs, and other Gabor-like patterns. These low-level features are not specific to the 1000 classes of ImageNet; they are generic building blocks for visual understanding. They are likely to be useful for almost any computer vision task. This principle holds true even as we go deeper into the network. If we take the feature vector from one of the last layers of a pre-trained network—in this case, the 4096-dimensional vector before the final classifier—and we look for nearest neighbors in this feature space, we see something remarkable. The feature space is semantically organized. A test image of a flower is closest to other images of flowers. An elephant is closest to other elephants. An aircraft carrier is closest to other aircraft carriers. This demonstrates that the network has learned a rich, high-level representation of the visual world that captures semantic similarity. The core idea of transfer learning is to leverage this pre-existing knowledge.</p>
<p>So, here is the standard workflow for transfer learning. Step one is performed once by the broader research community. A very large model, like a ResNet, is trained on a massive, diverse dataset like ImageNet, which contains millions of images across a thousand categories. This is a computationally intensive process that can take weeks on many GPUs. The result is a set of “pre-trained” weights. Now, let’s say you have a new task, for which you only have a small dataset. Perhaps you want to classify between 10 different types of flowers, and you only have a few hundred examples. The strategy is as follows: you take the pre-trained network, and you freeze the weights of all the convolutional layers. You treat this part of the network as a fixed feature extractor. Then, you remove the original final fully-connected layer (which was trained to classify 1000 ImageNet classes) and replace it with a new, randomly initialized fully-connected layer that has the correct number of outputs for your new task (e.g., 10 outputs for 10 flower classes). You then train only this new final layer on your small dataset. Since you are only training a small number of parameters, this is much less prone to overfitting.</p>
<p>Now, consider a different scenario. What if you have a bigger dataset for your new task? Perhaps you have tens of thousands of images. In this case, you have enough data to do more than just train the final layer. The strategy here is to initialize the entire network with the pre-trained weights, replace the final layer as before, and then finetune the entire model. This means you allow backpropagation to update all the weights in the network, but you typically do so with a much smaller learning rate than you would use for training from scratch. This allows the pre-trained features to be gently adapted and specialized for your new task, often leading to better performance than simply freezing the feature extractor.</p>
<p>So, what is the key takeaway for any applied deep learning you do in the future? If you have a dataset of interest, and it’s smaller than the massive, web-scale datasets, you should almost always leverage transfer learning. You should find a model pre-trained on a large, similar dataset (ImageNet is the default for natural images) and then transfer that knowledge to your specific task using the strategies we’ve discussed. You don’t need to train these large models yourself. Deep learning frameworks like <a href="https://github.com/pytorch/vision">PyTorch</a> and platforms like <a href="https://github.com/huggingface/pytorch-image-models">Hugging Face</a> provide a “Model Zoo” of pre-trained models that you can download and use with just a few lines of code. This is an incredibly powerful paradigm that has democratized access to high-performing computer vision models.</p>
</section>
<section id="hyperparameter-selection" class="level2">
<h2 class="anchored" data-anchor-id="hyperparameter-selection">Hyperparameter selection</h2>
<p>We’ve built the model, we have the data, but there are still many “knobs” to turn, learning rates, weight decay, dropout probabilities, architectural choices. This brings us to the crucial, and often challenging, task of Hyperparameter Selection. This is often more of an art guided by scientific principles than a rigid science itself. I’m going to provide you with a practical, step-by-step workflow for approaching this problem. Here is a sequence of steps that will serve you well.</p>
<p><strong>Step 1</strong>: First, some initial sanity checks. Check your initial loss. Before you even start training, do a single forward pass and make sure the loss is what you’d expect. For a softmax classifier on a C-class problem, the initial loss should be around log(C). If it’s not, something is likely wrong with your initialization or loss function implementation.</p>
<p><strong>Step 2</strong>: Overfit a small sample. Take a tiny subset of your data, maybe just a few mini-batches, and try to train your network to 100% accuracy on that small set. The goal here is to prove that your model and your optimization setup are capable of learning something. If you can’t even overfit a tiny slice of data, you have a bug somewhere.</p>
<p><strong>Step 3</strong>: Find a learning rate that makes the loss go down. Once you can overfit a small sample, take your full dataset, turn on a small amount of regularization (weight decay), and experiment with a wide range of learning rates—from 1e-1 down to 1e-5. All you’re looking for here is a learning rate that causes the loss to drop reliably within the first hundred or so iterations. This gives you a reasonable starting point.</p>
<p>Once you have a viable learning rate, you can move on to a more systematic search.</p>
<p><strong>Step 4</strong>: Perform a coarse search over a grid of hyperparameters. Now you’ll be tuning not just the learning rate, but also regularization strength, perhaps dropout probability, and other architectural choices. Sample these values, and for each combination, train for just a few epochs—maybe 1 to 5. You’re just trying to identify promising regions in the hyperparameter space.</p>
<p><strong>Step 5</strong>: Refine the grid. Take the best-performing hyperparameter settings from your coarse search and perform a more focused search in that narrower region, and this time, train the models for a longer duration.</p>
<p><strong>Step 6</strong>: which is crucial throughout this whole process, is to look at the loss and accuracy curves. These plots are your primary diagnostic tool for understanding what’s happening during training.</p>
<p>Let’s look at some examples.</p>
<div class="columns">
<div class="column" style="width:50%;">
<p>Here you see the training accuracy in blue and the validation accuracy in orange. Both curves are steadily increasing, and there’s a relatively small gap between them. This is the ideal scenario. It tells you that your model is still learning and has not yet plateaued. The prescription here is simple: you just need to train for longer.</p>
</div><div class="column" style="width:50%;">
<p><a href="./images/accuracy-curve1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-18"><img src="https://bhdai.github.io/blog/posts/cnn-architectures/images/accuracy-curve1.png" class="img-fluid"></a></p>
</div>
</div>
<div class="columns">
<div class="column" style="width:50%;">
<p>Now, consider this case. What is happening here? The training accuracy continues to climb, but the validation accuracy, after an initial increase, starts to decrease. This is the classic signature of overfitting. The large and growing gap between the training and validation accuracy means your model is learning to memorize the training set, but this knowledge is not generalizing to unseen data. The solution here is to increase regularization. This could mean adding more weight decay, increasing the dropout rate, or using more aggressive data augmentation. Alternatively, if possible, getting more training data is often the most effective remedy for overfitting.</p>
</div><div class="column" style="width:50%;">
<p><a href="./images/accuracy-curve2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-19"><img src="https://bhdai.github.io/blog/posts/cnn-architectures/images/accuracy-curve2.png" class="img-fluid"></a></p>
</div>
</div>
<div class="columns">
<div class="column" style="width:50%;">
<p>And finally, what about this scenario? Here, the training and validation accuracy are very close to each other, but both have flattened out at a suboptimal level. There’s no gap, which means the model is not overfitting. This is a sign of underfitting. Your model does not have enough capacity to capture the complexity of the data. The potential solutions are to train longer (though it looks like it’s already converged), or more likely, you need to use a bigger, more powerful model—for instance, moving from a ResNet-18 to a ResNet-34.</p>
</div><div class="column" style="width:50%;">
<p><a href="./images/accuracy-curve3.png" class="lightbox" data-gallery="quarto-lightbox-gallery-20"><img src="https://bhdai.github.io/blog/posts/cnn-architectures/images/accuracy-curve3.png" class="img-fluid"></a></p>
</div>
</div>
<p><strong>Step 7</strong>: This process is inherently iterative. You follow these steps, you look at your loss curves, you diagnose the problem, and that diagnosis informs the next action. Often, after refining your grid and training longer (Step 5), you’ll look at the curves (Step 6), and they will tell you that you’re now overfitting. That’s a GOTO Step 5: you go back, refine your regularization parameters, and train again. It’s a cycle of experimentation and analysis.</p>
<p>When you’re performing these hyperparameter searches, how should you sample the values? The traditional approach is Grid Search, where you define a fixed grid of values for each parameter. However, a paper by Bergstra and Bengio showed that Random Search is almost always more efficient.</p>
<p><a href="./images/random-search.png" class="lightbox" data-gallery="quarto-lightbox-gallery-21"><img src="https://bhdai.github.io/blog/posts/cnn-architectures/images/random-search.png" class="img-fluid"></a></p>
<p>The reason is that not all hyperparameters are equally important. As shown in these diagrams, some parameters have a much larger impact on performance than others. With a grid layout, you are testing each “unimportant” parameter value multiple times, which is wasteful. With a random layout, you are effectively testing a unique value for each hyperparameter in every trial. This allows you to explore the hyperparameter space much more effectively for the same computational budget, making it more likely you’ll find a good combination.</p>


</section>

 ]]></description>
  <category>Deep Learning</category>
  <category>CNN</category>
  <guid>https://bhdai.github.io/blog/posts/cnn-architectures/</guid>
  <pubDate>Sat, 09 Aug 2025 17:00:00 GMT</pubDate>
  <media:content url="https://bhdai.github.io/blog/posts/cnn-architectures/images/cover.png" medium="image" type="image/png" height="96" width="144"/>
</item>
<item>
  <title>Neural network and backpropagation</title>
  <dc:creator>Bui Huu Dai</dc:creator>
  <link>https://bhdai.github.io/blog/posts/neural-network-and-backprobagation/</link>
  <description><![CDATA[ 






<section id="deep-learning-today" class="level2">
<h2 class="anchored" data-anchor-id="deep-learning-today">Deep learning today</h2>
<p>“Deep learning” this is the term you hear everywhere now, and it’s really transformed not just computer vision, but many areas of artificial intelligence. And the progress, especially in the last few years, has been absolutely outstanding. Let’s just take a moment to appreciate some of the incredible capabilities that deep learning models have unlocked, particularly in the realm of image generation and understanding.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./images/dalle2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Images generated by DALL-E 2 on Sam Altman tweet and https://openai.com/dall-e-2/"><img src="https://bhdai.github.io/blog/posts/neural-network-and-backprobagation/images/dalle2.png" class="img-fluid figure-img" alt="Images generated by DALL-E 2 on Sam Altman tweet and https://openai.com/dall-e-2/"></a></p>
<figcaption>Images generated by DALL-E 2 on <a href="https://x.com/sama/status/1511724264629678084">Sam Altman tweet</a> and <a href="https://openai.com/dall-e-2/">https://openai.com/dall-e-2/</a></figcaption>
</figure>
</div>
<p>These images are generated by <strong>DALL-E 2</strong>. These are not photographs; they are synthesized by an AI from text prompts. The level of detail, the coherence of the scenes, and the creativity are just remarkable. These models are clearly understanding complex relationships between concepts and visual elements. In 2022 Ramesh et al.&nbsp;released <a href="https://arxiv.org/abs/2204.06125">“Hierarchical Text-Conditional Image Generation with CLIP Latents”</a> give us a glimpse into how some of these text-to-image models work. So basically it involves an image encoder, a text encoder, a prior model that learns to associate these representations, and then a decoder to generate the image, the CLIP objective, which learns joint embeddings of images and text, plays a crucial role here.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./images/dalle3.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Images from “Improving image generation with better captions” paper (2023)"><img src="https://bhdai.github.io/blog/posts/neural-network-and-backprobagation/images/dalle3.png" class="img-fluid figure-img" alt="Images from “Improving image generation with better captions” paper (2023)"></a></p>
<figcaption>Images from <a href="https://cdn.openai.com/papers/dall-e-3.pdf">“Improving image generation with better captions”</a> paper (2023)</figcaption>
</figure>
</div>
<p>More recently, we’ve seen <strong>DALL-E 3</strong>. These models are getting exceptionally good at interpreting nuanced textual descriptions. Notice the image on the right, the model not only generates the individual elements but also composes them into a coherent scene with multiple interacting characters, and it even captures the specified “graphic novel” style. You can see how it even annotates which parts of the image correspond to which parts of the prompt.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./images/gpt.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Thinking with images (image from https://openai.com/index/introducing-o3-and-o4-mini/)"><img src="https://bhdai.github.io/blog/posts/neural-network-and-backprobagation/images/gpt.png" class="img-fluid figure-img" alt="Thinking with images (image from https://openai.com/index/introducing-o3-and-o4-mini/)"></a></p>
<figcaption>Thinking with images (image from <a href="https://openai.com/index/introducing-o3-and-o4-mini/">https://openai.com/index/introducing-o3-and-o4-mini/</a>)</figcaption>
</figure>
</div>
<p>This isn’t just about image generation. Now with o3 and o4 mini, these model can integrate images directly into their chain of thought which they call “Visual reasoning in action”, they can reason, think about the image. This shows a deep level of visual reasoning.</p>
<blockquote class="blockquote">
<p>Thinking with images allows you to interact with ChatGPT more easily. You can ask questions by taking a photo without worrying about the positioning of objects, whether the text is upside down or there are multiple physics problems in one photo. Even if objects are not obvious at first glance, visual reasoning allows the model to zoom in to see more clearly.</p>
</blockquote>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./images/sam.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Segment Anything Model (SAM)"><img src="https://bhdai.github.io/blog/posts/neural-network-and-backprobagation/images/sam.png" class="img-fluid figure-img" alt="Segment Anything Model (SAM)"></a></p>
<figcaption>Segment Anything Model (SAM)</figcaption>
</figure>
</div>
<p>We also have models like the Segment Anything Model (SAM). SAM is remarkable because it can generate segmentation masks for any object in an image, just from a prompt or a click. Look at the density of masks it can produce, it can segment hundreds, even thousands of objects and parts of objects within a single image. This has huge implications for image editing, robotics, and a wide range of computer vision tasks.</p>
<div class="quarto-video"><video id="video_shortcode_videojs_video1" class="video-js vjs-default-skin vjs-fluid" controls="" preload="auto" data-setup="{}" title=""><source src="https://cdn.openai.com/tmp/s/title_0.mp4"></video></div>
<p>And then we move into video. This is an example from <strong>Sora</strong>, a recent model from OpenAI that generates video from text. The quality, coherence over time, and realism are truly groundbreaking. Sora can do things like <strong>animating images</strong> that were themselves generated by models like DALL-E. It can also do <strong>video-to-video editing</strong>, where you provide an input video and a text prompt, and it modifies the video accordingly. The model understands the content of the video and can plausibly alter its style or elements based on text.</p>
<div class="grid">
<div class="g-col-4 text-center">
<div class="quarto-video"><video id="video_shortcode_videojs_video2" class="video-js vjs-default-skin vjs-fluid" controls="" preload="auto" data-setup="{}" title=""><source src="https://cdn.openai.com/tmp/s/scaling_0.mp4"></video></div>
<p><em>Base compute</em></p>
</div>
<div class="g-col-4 text-center">
<div class="quarto-video"><video id="video_shortcode_videojs_video3" class="video-js vjs-default-skin vjs-fluid" controls="" preload="auto" data-setup="{}" title=""><source src="https://cdn.openai.com/tmp/s/scaling_1.mp4"></video></div>
<p><em>4x compute</em></p>
</div>
<div class="g-col-4 text-center">
<div class="quarto-video"><video id="video_shortcode_videojs_video4" class="video-js vjs-default-skin vjs-fluid" controls="" preload="auto" data-setup="{}" title=""><source src="https://cdn.openai.com/tmp/s/scaling_2.mp4"></video></div>
<p><em>32x compute</em></p>
</div>
</div>
<p>And a key theme, as with many of these large generative models, is the impact of <strong>more compute</strong>. These video show the output of Sora for the same prompt, but with increasing amounts of compute used for generation. You can clearly see how the quality, detail, and realism improve significantly as more computational resources are applied. This scaling hypothesis that bigger models and more data, trained with more compute, lead to better performance has been a driving force in deep learning.</p>
<p>So, these examples are just a taste of what deep learning is achieving. It’s creating tools that can understand, generate, and manipulate visual information in ways that were science fiction just a decade ago. The common thread underlying all these incredible advancements is the use of neural networks, often very deep ones, trained on massive datasets. And that’s what we’re here to understand today: what are these neural networks, and how do we train them?</p>
</section>
<section id="what-is-neural-network" class="level2">
<h2 class="anchored" data-anchor-id="what-is-neural-network">What is neural network?</h2>
<p>We’re going to build these up step-by-step. And perhaps surprisingly, we’ll start with something very familiar. Think about our <strong>original linear classifier</strong>. Before, when we talked about linear score functions, we had this simple equation: <img src="https://latex.codecogs.com/png.latex?f(x)%20=%20Wx">. Here <img src="https://latex.codecogs.com/png.latex?x"> is our input vector, say an image flattened into a D-dimensional vector so <img src="https://latex.codecogs.com/png.latex?x"> is in <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5ED">. And <img src="https://latex.codecogs.com/png.latex?W"> is our weight matrix, of size C x D, where C is the number of classes. This matrix <img src="https://latex.codecogs.com/png.latex?W"> projects our D-dimensional input into a C-dimensional space of class scores. This is the simplest possible “network” if you like - a single linear layer. Now, how do we make this more powerful? How do we build a neural network <strong>with, say, 2 layers</strong>? We take our original linear score function, <img src="https://latex.codecogs.com/png.latex?Wx">, and we are going to insert something in the middle. Now our new equation look like this:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Af(x)%20=%20W_2%20%20%5Cmax(0,%20W_1x)%0A"></p>
<p>We’re still starting with our input <img src="https://latex.codecogs.com/png.latex?x">. We first multiply it by a weight matrix, let’s call it <img src="https://latex.codecogs.com/png.latex?W_1">. So we compute <img src="https://latex.codecogs.com/png.latex?W_1x">. If x is <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5ED">, then <img src="https://latex.codecogs.com/png.latex?W_1"> will be a matrix of size H x D. H here represents the number of neurons or hidden units in this first layer. So, <img src="https://latex.codecogs.com/png.latex?W_1x"> gives us an H-dimensional vector. Then we apply a <strong>non-linear function</strong>. Here we’re using <img src="https://latex.codecogs.com/png.latex?max(0,%20...)"> which we will talk about this for a moment. The output of this non-linearity is now an H dimensional vector, we then multiply this vector by a second weight matrix, <img src="https://latex.codecogs.com/png.latex?W_2">. This <img src="https://latex.codecogs.com/png.latex?W_2"> matrix will have dimensions C x H, where C is still our number of output classes. So, <img src="https://latex.codecogs.com/png.latex?W_2"> takes the H-dimensional output of the first layer and maps it down to our C class scores. In practice we will usually add a learnable bias at each layer as well. So, more accurately, the operations would look like <img src="https://latex.codecogs.com/png.latex?W_1x%20+%20b_1"> and <img src="https://latex.codecogs.com/png.latex?W_2h%20+%20b_2"> (where h is the output of the first layer). We often absorb the bias into the weight matrix by augmenting <img src="https://latex.codecogs.com/png.latex?x"> with a constant 1, as we’ve discussed before, or handle it as a separate bias vector. For now, we’ll keep the notation a bit cleaner by omitting explicit biases, but remember they’re typically there.</p>
<p><a href="./images/non-linear.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5"><img src="https://bhdai.github.io/blog/posts/neural-network-and-backprobagation/images/non-linear.png" class="img-fluid"></a></p>
<p>Consider this example, We have some data points in a 2D space. The red points form a cluster in the center, and the blue points surround them. Can we separate these red and blue points with a linear classifier? No, you can’t draw a single straight line in this x, y space that perfectly separates the red from the blue. A linear classifier, by definition, can only learn linear decision boundaries. But what if we could transform our input space? This is where the idea of feature transformations comes in, and it’s closely related to what neural networks do. Imagine we apply a function <img src="https://latex.codecogs.com/png.latex?f(x,%20y)"> that maps our original Cartesian coordinates (x, y) to polar coordinates (<img src="https://latex.codecogs.com/png.latex?r(x,y)">, <img src="https://latex.codecogs.com/png.latex?%5Ctheta(x,y)">). So, <img src="https://latex.codecogs.com/png.latex?r"> is the radius from the origin, and <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> is the angle. Now in this new <img src="https://latex.codecogs.com/png.latex?(r,%20%5Ctheta)"> space, shown on the right. The red points, which were close to the origin, now have small <img src="https://latex.codecogs.com/png.latex?r"> values. The blue points, which were further out, now have larger <img src="https://latex.codecogs.com/png.latex?r"> values. In this transformed space, the red and blue points are linearly separable! We can now draw a simple horizontal line (a constant <img src="https://latex.codecogs.com/png.latex?r"> value) that separates them.</p>
<p>This is the core idea. The non-linearities in a neural network allow the network to learn these kinds of powerful feature transformations. The first layer <img src="https://latex.codecogs.com/png.latex?W_1x"> followed by the <img src="https://latex.codecogs.com/png.latex?%5Cmax(0,%20...)"> non-linearity effectively computes a new representation of the input. And then the next layer <img src="https://latex.codecogs.com/png.latex?W_2"> operates on this transformed representation. By stacking these layers with non-linearities, the network can learn increasingly complex and abstract features that make the original problem (like classification) easier, often making it linearly separable in some high-dimensional learned feature space.</p>
</section>
<section id="neural-network-architectures" class="level2">
<h2 class="anchored" data-anchor-id="neural-network-architectures">Neural network architectures</h2>
<p>You’ll hear these kinds of networks, <img src="https://latex.codecogs.com/png.latex?f%20=%20W_2%20%20%5Cmax(0,%20W_1x)">, referred to by a few names. While “Neural Network” is a very broad term, the specific architecture we’re discussing here is often more accurately called a <strong>“fully-connected network”</strong>. This is because, as we’ll see when we visualize them, every neuron (or unit) in one layer is connected to every neuron in the subsequent layer. Another term you might encounter, especially in older literature, is <strong>“multi-layer perceptron” or MLP</strong>. For our purposes, when we talk about a basic neural network with these stacked layers of matrix multiplies and non-linearities, we’re generally talking about a fully-connected network. And remember, we usually add learnable biases at each layer.</p>
<p>Now, there’s nothing stopping us from making these networks deeper. We can extend this to 3 layers, or indeed many more. Our 2-layer network was <img src="https://latex.codecogs.com/png.latex?f%20=%20W_2%20%20%5Cmax(0,%20W_1x)">. A 3-layer neural network simply inserts another layer of weights and non-linearity:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0Af%20&amp;=%20W_3%20%20%5Cmax(0,%20W_2%20%20%5Cmax(0,%20W_1x))%20%5C%5C%0Ax%20&amp;%5Cin%20%5Cmathbb%7BR%7D%5ED,%20W_1%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BH_1%20%5Ctimes%20D%7D,%20W_2%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BH_2%20%5Ctimes%20H_1%7D,%20W_3%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BC%20%5Ctimes%20H_2%7D%0A%5Cend%7Balign%7D%0A"></p>
<p>A quick note on terminology: when we say an “N-layer neural network,” we typically mean a network with N layers of weights, or N-1 hidden layers plus an output layer. So, our 2-layer Neural Network here has one hidden layer (the W_1 transformation produces its activations), and our 3-layer Neural Network has two hidden layers (activations produced after <img src="https://latex.codecogs.com/png.latex?W_1"> and after <img src="https://latex.codecogs.com/png.latex?W_2">). The input is sometimes called the input layer, but it doesn’t usually involve learnable weights or non-linearities in the same way.</p>
<p><a href="./images/twolayer.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6"><img src="https://bhdai.github.io/blog/posts/neural-network-and-backprobagation/images/twolayer.png" class="img-fluid"></a></p>
<p>We can visualize this as a form of <strong>hierarchical computation</strong>. So what’s the advantage of this hidden layer h? Think back to our linear classifier, <img src="https://latex.codecogs.com/png.latex?f%20=%20Wx">. Each row of <img src="https://latex.codecogs.com/png.latex?W"> could be interpreted as a “template” for one of the classes. For CIFAR-10, we’d learn 10 templates. Now, with a 2-layer neural network, the first weight matrix W_1 (e.g., 100x3072) can be thought of as learning many more templates – in this example, 100 templates. These aren’t necessarily templates for the final classes directly. Instead, they are like <strong>intermediate features or visual primitives</strong>. The hidden layer <img src="https://latex.codecogs.com/png.latex?h"> then represents the extent to which each of these 100 learned “templates” or features is present in the input image. Then, the second weight matrix <img src="https://latex.codecogs.com/png.latex?W_2"> (10x100) learns how to combine these 100 intermediate features to produce the final scores for the 10 classes. So, instead of learning just 10 direct templates, we learn, say, 100 more foundational templates, and these templates can be <strong>shared and re-weighted</strong> by W_2 to form the decision for each class. This gives the network much more flexibility and expressive power. It can learn a richer set of features and then learn complex combinations of those features.</p>
</section>
<section id="activation-functions" class="level2">
<h2 class="anchored" data-anchor-id="activation-functions">Activation functions</h2>
<p>Now, a key question arises from this construction, <strong>why do we want non-linearity</strong>? Let’s consider the function we just built: <img src="https://latex.codecogs.com/png.latex?f%20=%20W_2%20%20%5Cmax(0,%20W_1x)">. What if we didn’t have that <img src="https://latex.codecogs.com/png.latex?%5Cmax(0,%20...)"> non-linearity? What if our function was just <img src="https://latex.codecogs.com/png.latex?f%20=%20W_2%20%20W_1%20%20x">? Well, if <img src="https://latex.codecogs.com/png.latex?W_1"> is a matrix and <img src="https://latex.codecogs.com/png.latex?W_2"> is a matrix, then their product, <img src="https://latex.codecogs.com/png.latex?W_2%20%20W_1">, is just another matrix. Let’s call it <img src="https://latex.codecogs.com/png.latex?W_%7Bprime%7D">. So, <img src="https://latex.codecogs.com/png.latex?f%20=%20W_%7Bprime%7D%20%20x">. This is just a linear classifier again! Stacking multiple linear transformations without any non-linearity in between doesn’t actually increase the expressive power of our model beyond a single linear transformation. It collapses back down to a linear function. The non-linearity is what allows us to learn much more complex functions. So, what are some common choices for these activation functions?</p>
<p><a href="./images/activation.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7"><img src="https://bhdai.github.io/blog/posts/neural-network-and-backprobagation/images/activation.png" class="img-fluid"></a></p>
<p>The one we’ve been using, <img src="https://latex.codecogs.com/png.latex?%5Cmax(0,%20x)">, is called <strong>ReLU (Rectified Linear Unit)</strong>, and you can see its plot in the top left, highlighted in red. For any negative input, it outputs zero. For any positive input, it outputs the input itself. It’s simple, computationally efficient, and works remarkably well in practice. In fact, <strong>ReLU is a good default choice for most problems</strong> and is very widely used. But there are many other activation functions people have explored like <strong>Leaky ReLU</strong> This can sometimes help with issues where ReLU units “die” or <strong>Sigmoid</strong> This function squashes its input into the range (0, 1). It was historically very popular, especially in the early days of neural networks, because its output can be interpreted as a probability or a firing rate of a neuron. However, it suffers from vanishing gradients for very large positive or negative inputs, which can slow down learning in deep networks. <strong>Tanh</strong> this function squashes its input into the range (-1, 1). It’s zero-centered, which can sometimes be advantageous over sigmoid. Like sigmoid, it also suffers from vanishing gradients at the extremes. And a lot of ReLU variations like <strong>ELU</strong>, <strong>GELU</strong>, <strong>SiLU</strong>, or what ever it is just ending with <strong>LU</strong>. But you shouldn’t expect much from those activation functions, you might get a slightly better result when using this activation function over one another depend on the problem you’re trying to solve, but trust me just stick with ReLU and it will work pretty much with most of the problems you might come up with.</p>
</section>
<section id="a-note-on-network-size-and-regularization" class="level2">
<h2 class="anchored" data-anchor-id="a-note-on-network-size-and-regularization">A note on network size and regularization</h2>
<p>Okay, so we have these building blocks: layers composed of a linear transformation followed by a non-linear activation function. Now, let’s think about how we arrange these layers to form different neural network architectures.</p>
<p><a href="./images/neural_network.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8"><img src="https://bhdai.github.io/blog/posts/neural-network-and-backprobagation/images/neural_network.png" class="img-fluid"></a></p>
<p>You’ll often see neural networks visualized like this, with nodes and connections. We have a “3-layer Neural Net” or a “2-hidden-layer Neural Net”. The circles on the far left represent the input layer – these are just our raw input features, say the pixel values of an image. The next set of circles in the middle is the hidden layer. Each unit here computes a weighted sum of the inputs, adds a bias, and then passes it through an activation function (like ReLU). And finally, the circles on the right form the output layer, which produces our final class scores, or perhaps a regression value. The output of the first hidden layer becomes the input to the second hidden layer, which then feeds into the output layer. The key thing to notice in these diagrams are the lines connecting the nodes. These layers are typically <strong>Fully-connected layers</strong>. This means that every neuron in the input layer is connected to every neuron in the first hidden layer. And every neuron in the first hidden layer is connected to every neuron in the second hidden layer, and so on, up to the output layer. Each of these connections has an associated weight.</p>
<p>So, when we say an N-layer Neural Net, we generally mean there are N layers of weights and N layers of activations being computed or N-1 hidden layers plus an output layer. The input itself isn’t usually counted as a layer in this sense, as it doesn’t perform a computation with learnable weights. Let’s look at an example feed-forward computation for a neural network, to make this more concrete.</p>
<div class="sourceCode" id="annotated-cell-1" style="background: #f1f3f5;"><pre class="sourceCode python code-annotation-code code-with-copy code-annotated"><code class="sourceCode python"><a class="code-annotation-anchor" data-target-cell="annotated-cell-1" data-target-annotation="1" onclick="event.preventDefault();">1</a><span id="annotated-cell-1-1" class="code-annotation-target">f <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span> x: <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> np.exp(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>x))</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-1" data-target-annotation="2" onclick="event.preventDefault();">2</a><span id="annotated-cell-1-2" class="code-annotation-target">x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.randn(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-1" data-target-annotation="3" onclick="event.preventDefault();">3</a><span id="annotated-cell-1-3" class="code-annotation-target">h1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> f(np.dot(W1, x) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> b1)</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-1" data-target-annotation="4" onclick="event.preventDefault();">4</a><span id="annotated-cell-1-4" class="code-annotation-target">h2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> f(np.dot(W2, h1) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> b2)</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-1" data-target-annotation="5" onclick="event.preventDefault();">5</a><span id="annotated-cell-1-5" class="code-annotation-target">out <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.dot(W3, h2) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> b3</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code></pre></div>
<dl class="code-annotation-container-grid">
<dt data-target-cell="annotated-cell-1" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-1" data-code-lines="1" data-code-annotation="1">we define our activation function f.&nbsp;In this example, they’re using a sigmoid</span>
</dd>
<dt data-target-cell="annotated-cell-1" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-1" data-code-lines="2" data-code-annotation="2">we have some input x, which is a 3x1 vector.</span>
</dd>
<dt data-target-cell="annotated-cell-1" data-target-annotation="3">3</dt>
<dd>
<span data-code-cell="annotated-cell-1" data-code-lines="3" data-code-annotation="3">get the activations of the first hidden layer, <code>h1</code>. <code>W_1</code>would be a 4x3 weight matrix, and <code>b1</code> would be a 4x1 bias vector then we performs the matrix multiplication and add the bias <code>b1</code>. Finally we apply the activation function <code>f</code> element-wise to the result. So <code>h1</code> will be a 4x1 vector.</span>
</dd>
<dt data-target-cell="annotated-cell-1" data-target-annotation="4">4</dt>
<dd>
<span data-code-cell="annotated-cell-1" data-code-lines="4" data-code-annotation="4">similarly, for the second hidden layer activations, h2</span>
</dd>
<dt data-target-cell="annotated-cell-1" data-target-annotation="5">5</dt>
<dd>
<span data-code-cell="annotated-cell-1" data-code-lines="5" data-code-annotation="5">finally for the output neuron, <code>out</code>, <code>W3</code> would be a 1x4 matrix b3 a 1x1 bias. Notice that for the output layer, an activation function isn’t always applied, or a different one might be used depending on the task for example softmax for multi-class classification, sigmoid for binary classification, or no activation for regression.</span>
</dd>
</dl>
<p>This step-by-step process, from input to output, is called the forward pass. Now, it might seem like these networks are incredibly complex, but you might be surprised to learn that a full implementation of training a 2-layer Neural Network needs only about 20 lines of Python code using NumPy. This is, of course, a simplified example, but it captures all the essential components</p>
<div class="sourceCode" id="annotated-cell-2" style="background: #f1f3f5;"><pre class="sourceCode python code-annotation-code code-with-copy code-annotated"><code class="sourceCode python"><span id="annotated-cell-2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="annotated-cell-2-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> numpy.random <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> rand</span>
<span id="annotated-cell-2-3"></span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-2" data-target-annotation="1" onclick="event.preventDefault();">1</a><span id="annotated-cell-2-4" class="code-annotation-target">N, D_in, H, D_out <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">64</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span></span>
<span id="annotated-cell-2-5">x, y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> randn(N, D_in), randn(N, D_out)</span>
<span id="annotated-cell-2-6">w1, w2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> randn(D_in, H), randn(H, D_out)</span>
<span id="annotated-cell-2-7"></span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-2" data-target-annotation="2" onclick="event.preventDefault();">2</a><span id="annotated-cell-2-8" class="code-annotation-target"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> t <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2000</span>):</span>
<span id="annotated-cell-2-9">  h <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> np.exp(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>x.dot(w1)))</span>
<span id="annotated-cell-2-10">  y_pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> h.dot(w2)</span>
<span id="annotated-cell-2-11">  loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.square(y_pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> y).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>()</span>
<span id="annotated-cell-2-12">  <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(t, loss)</span>
<span id="annotated-cell-2-13"></span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-2" data-target-annotation="3" onclick="event.preventDefault();">3</a><span id="annotated-cell-2-14" class="code-annotation-target">  grad_y_pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">2.0</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> (y_pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> y)</span>
<span id="annotated-cell-2-15">  grad_w2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> h.T.dot(grad_y_pred)</span>
<span id="annotated-cell-2-16">  grad_h <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> grad_y_pred.dot(w2.T)</span>
<span id="annotated-cell-2-17">  grad_w1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> x.T.dot(grad_h <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> h <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> h))</span>
<span id="annotated-cell-2-18"></span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-2" data-target-annotation="4" onclick="event.preventDefault();">4</a><span id="annotated-cell-2-19" class="code-annotation-target">  w1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-4</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> grad_w1</span>
<span id="annotated-cell-2-20">  w2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-4</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> grad_w2</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code></pre></div>
<dl class="code-annotation-container-grid">
<dt data-target-cell="annotated-cell-2" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-2" data-code-lines="4,5,6" data-code-annotation="1">we define the network, set up our hyperparameters. We then create some random input data X and target labels y, we initialize our weight matrices <code>w1</code> and <code>w2</code></span>
</dd>
<dt data-target-cell="annotated-cell-2" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-2" data-code-lines="8,9,10,11,12" data-code-annotation="2">next we perform forward pass</span>
</dd>
<dt data-target-cell="annotated-cell-2" data-target-annotation="3">3</dt>
<dd>
<span data-code-cell="annotated-cell-2" data-code-lines="14,15,16,17" data-code-annotation="3">after the forward pass, we need to calculate the analytical gradients. This is the core of how we’ll update our weights. We’re essentially applying the chain rule to work backward from the loss and find how <code>w1</code> and <code>w2</code> affect that loss. We’ll dive much deeper into this process, called backpropagation, very soon.</span>
</dd>
<dt data-target-cell="annotated-cell-2" data-target-annotation="4">4</dt>
<dd>
<span data-code-cell="annotated-cell-2" data-code-lines="19,20" data-code-annotation="4">And the final step in our training loop is to perform the gradient descent update. We simply take our current weights and subtract the gradient (multiplied by a small learning rate, here 1e-4) to move in the direction that reduces the loss.</span>
</dd>
</dl>
<p>And that’s it! In about 20 lines, we have a complete training procedure for a 2-layer neural network. Now, an obvious question that arises when designing these networks is how do we go about <strong>setting the number of layers and their sizes?</strong> This is a fundamental question in neural network design, and it relates to the “capacity” of the model</p>
<p><a href="./images/model-capacity.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9"><img src="https://bhdai.github.io/blog/posts/neural-network-and-backprobagation/images/model-capacity.png" class="img-fluid"></a></p>
<p>The diagrams in the first row illustrate the decision boundary learned by a two layer neural network with a varying number of neurons in its single hidden layer, trying to classify some 2D points. With only <strong>3 hidden neurons</strong>, the network can learn a relatively simple decision boundary. When we increase to <strong>6 hidden neurons</strong>, the network has more capacity. It can learn a more complex, wigglier decision boundary, fitting the data a bit better. And with <strong>20 hidden neurons</strong>, it can learn an even more intricate boundary, potentially capturing finer details of the data distribution. The general principle here is that <strong>more neurons = more capacity</strong>. A network with more neurons (and more layers, which we’ll get to) can approximate more complex functions. It has more parameters, more knobs to turn, so it can fit more complicated patterns in the data. Now, if more neurons give more capacity, you might be tempted to think: “Great! I’ll just make my network huge, and it will fit the training data perfectly!”. But there’s a catch, and that’s overfitting. A network with too much capacity might learn the training data perfectly, including all its noise but fail to generalize to new and unseen data.</p>
<p>So how do we control this? One might think “Okay, I’ll just make my network smaller to prevent overfitting.” However, the general wisdom in the deep learning community now is: <strong>Do not use the size of the neural network as your primary regularizer. Use stronger regularization instead</strong>. What does this mean? It’s generally better to use a larger network that has the potential to learn the true underlying function well, and then control overfitting using explicit regularization techniques like L2 regularization, dropout, or data augmentation. Look at these plots at the bottom. They show the decision boundary for a network of a fixed, reasonably large size, but with different strengths of L2 regularization, controlled by <img src="https://latex.codecogs.com/png.latex?%5Clambda">. The idea is that it’s often better to make your network “big enough” (in terms of layers and neurons) to have sufficient capacity to represent the complexity of the true underlying function, and then use regularization techniques to prevent it from merely memorizing the training data. Trying to find the “perfect” small size for your network is often harder and less effective than using a larger network with good regularization.</p>
</section>
<section id="how-to-compute-gradients" class="level2">
<h2 class="anchored" data-anchor-id="how-to-compute-gradients">How to compute gradients?</h2>
<p>Okay, so now we have our neural network, which is essentially a more complex, nonlinear score function. For a 2-layer network, our scores s are given by <img src="https://latex.codecogs.com/png.latex?f(x;%20W_1,%20W_2)%20=%20W_2%20%20%5Cmax(0,%20W_1x)">. This function takes our input x and our learnable parameters <img src="https://latex.codecogs.com/png.latex?W_1"> and <img src="https://latex.codecogs.com/png.latex?W_2"> and produces class scores. Once we have these scores, the rest of the framework we developed for linear classifiers still applies. We need a loss function to tell us how good these scores are. For example, we can use the Hinge Loss (or SVM loss) on these predictions:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AL_i%20=%20%5Csum_%7Bj%20%5Cneq%20y_i%7D%20%5Cmax(0,%20s_j%20-%20s_%7By_i%7D%20+%201)%0A"></p>
<p>This is exactly the same hinge loss we saw before. It penalizes the network if the score <img src="https://latex.codecogs.com/png.latex?s_j"> for an incorrect class <img src="https://latex.codecogs.com/png.latex?j"> is not sufficiently lower than the score <img src="https://latex.codecogs.com/png.latex?s_%7By_i%7D"> for the correct class <img src="https://latex.codecogs.com/png.latex?y_i"> (by a margin of 1). We could just as easily use the Softmax loss (cross-entropy loss) here. The choice of loss function depends on the specific problem and desired properties, just like with linear models. Next, we also typically include a regularization term to prevent overfitting. A common choice is L2 regularization: <img src="https://latex.codecogs.com/png.latex?R(W)%20=%20%5Csum_k%20W_k%5E2">. This penalizes large values in our weight matrices.</p>
<p>And finally, the total loss <img src="https://latex.codecogs.com/png.latex?L"> is the average of the data loss <img src="https://latex.codecogs.com/png.latex?L_i"> over all N training examples, plus our regularization terms.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AL%20=%20%5Cfrac%7B1%7D%7BN%7D%20%20%5Csum_%7Bi=1%7D%5EN%20L_i%20+%20%5Clambda%20R(W_1)%20+%20%5Clambda%20R(W_2)%0A"></p>
<p>Notice that now we have regularization terms for each of our weight matrices, <img src="https://latex.codecogs.com/png.latex?W_1"> and <img src="https://latex.codecogs.com/png.latex?W_2"> and if we had more layers, we’d regularize all their weights as well. The <img src="https://latex.codecogs.com/png.latex?%5Clambda"> hyperparameter controls the strength of this regularization. So the overall structure is very familiar, 1) define a score function. 2) define a loss function. 3) add regularization. Our goal is still the same find the weight <img src="https://latex.codecogs.com/png.latex?W_1">, <img src="https://latex.codecogs.com/png.latex?W_2"> that minimize this total loss <img src="https://latex.codecogs.com/png.latex?L">. But now, our score function f is more complex. It’s a nested composition of functions with matrix multiplies and non-linearities. This brings us to the next big challenge: <strong>How to compute gradients?</strong></p>
<p>To use gradient descent, we need to compute the partial derivatives of the total loss <img src="https://latex.codecogs.com/png.latex?L"> with respect to each of our parameters. That is, we need <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20W_1%7D,%20%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20W_2%7D">, if we can compute these gradients then we can update <img src="https://latex.codecogs.com/png.latex?W_1"> and <img src="https://latex.codecogs.com/png.latex?W_2"> using our standard gradient descent rule: <img src="https://latex.codecogs.com/png.latex?W_1%20=%20W_1%20-%20learning%5C_rate%20%20%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20W_1%7D">, and similarly for <img src="https://latex.codecogs.com/png.latex?W_2">. So how do we get these gradients? One idea which turns out to be a <strong>bad idea</strong> is to try and <strong>derive <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20W%7D"> on paper</strong>, analytically, and for the entire complex function. Think back to our linear classifier with SVM loss. Even for that relatively simple case, where <img src="https://latex.codecogs.com/png.latex?s%20=%20f(x;W)%20=%20Wx">, and <img src="https://latex.codecogs.com/png.latex?L_i%20=%20%5Csum%20%5Cmax(0,%20s_j%20-%20s_%7By_i%7D%20+%201)">, deriving the full gradient <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20W%7D"> was already a bit involved. We had to expand it all out, as shown here, and then take derivatives.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0AL_i%20&amp;=%20%5Csum_%7Bj%20%5Cneq%20y_i%7D%20%5Cmax(0,%20s_j%20-%20s_%7By_i%7D%20+%201)%20%5C%5C%0A%20%20%20%20&amp;=%20%5Csum_%7Bj%20%5Cneq%20y_i%7D%20%5Cmax(0,%20W_%7Bj,:%7D%20%5Ccdot%20x%20+%20W_%7By_i,%20:%7D%20%5Ccdot%20x%20+%201)%20%5C%5C%0AL%20&amp;=%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi=1%7D%5EN%20L_i%20+%20%5Clambda%20%5Csum_k%20W_k%5E2%20%5C%5C%0A&amp;=%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi=1%7D%5EN%20%5Csum_%7Bj%20%5Cneq%20y_i%7D%20%5Cmax(0,%20W_%7Bj,:%7D%20%5Ccdot%20x%20+%20W_%7By_i,%20:%7D%20%5Ccdot%20x%20+%201)%20+%20%5Clambda%20%5Csum_k%20W_k%5E2%20%5C%5C%0A%5Cnabla_W%20L%20&amp;=%20%5Cnabla_W%20%5Cleft(%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi=1%7D%5EN%20%5Csum_%7Bj%20%5Cneq%20y_i%7D%20%5Cmax(0,%20W_%7Bj,:%7D%20%5Ccdot%20x%20+%20W_%7By_i,%20:%7D%20%5Ccdot%20x%20+%201)%20+%20%5Clambda%20%5Csum_k%20W_k%5E2%20%5Cright)%0A%5Cend%7Balign%7D%0A"></p>
<p>Now imagine doing this for our 2-layer neural network, where <img src="https://latex.codecogs.com/png.latex?s"> itself is <img src="https://latex.codecogs.com/png.latex?W_2%20*%20%5Cmax(0,%20W_1%20x)">. The expression for <img src="https://latex.codecogs.com/png.latex?L"> in terms of the raw inputs <img src="https://latex.codecogs.com/png.latex?x"> and weights <img src="https://latex.codecogs.com/png.latex?W_1">, <img src="https://latex.codecogs.com/png.latex?W_2"> would become much, much more complicated. It would be very <strong>tedious</strong>. It involves a lot of matrix calculus, and you’d need a lot of paper (or whiteboard space!). It’s very easy to make mistakes. What if we want to <strong>change our loss function</strong>? Say we initially derived everything for Hinge loss, and now we want to try Softmax loss. We’d have to <strong>re-derive everything from scratch</strong>! That’s not flexible at all. This approach is simply <strong>not feasible for very complex models</strong>. Modern deep neural networks can have tens, hundreds, or even thousands of layers, with various types of operations. Deriving the full analytical gradient for such a beast by hand is practically impossible and incredibly error-prone. So, we need a more systematic, more modular, and more scalable way to compute these gradients. And that leads us to a better idea: <strong>Computational graphs + Backpropagation</strong>.</p>
<p>The core idea here is to break down our complex function from input <img src="https://latex.codecogs.com/png.latex?x"> and weights <img src="https://latex.codecogs.com/png.latex?W"> all the way to the final loss <img src="https://latex.codecogs.com/png.latex?L"> into a sequence of simpler, elementary operations. We can represent this sequence as a computational graph</p>
<p><a href="./images/computational-graph.png" class="lightbox" data-gallery="quarto-lightbox-gallery-10"><img src="https://bhdai.github.io/blog/posts/neural-network-and-backprobagation/images/computational-graph.png" class="img-fluid"></a></p>
<p>Let’s look at the example, which is for our familiar linear classifier with hinge loss and regularization.</p>
<ol type="1">
<li>We start with our inputs <img src="https://latex.codecogs.com/png.latex?x"> and our weights <img src="https://latex.codecogs.com/png.latex?W">.</li>
<li>The first operation is computing the scores: <img src="https://latex.codecogs.com/png.latex?s%20=%20Wx">. This is represented by a node. It takes <img src="https://latex.codecogs.com/png.latex?W"> and <img src="https://latex.codecogs.com/png.latex?x"> as inputs and produces scores <img src="https://latex.codecogs.com/png.latex?S">.</li>
<li>These scores <img src="https://latex.codecogs.com/png.latex?S"> then feed into the hinge loss function <img src="https://latex.codecogs.com/png.latex?L_i%20=%20%5Csum%20%5Cmax(0,%20s_j%20-%20s_%7By_i%7D%20+%201)">. This is another node in our graph.</li>
<li>Separately, our weights <img src="https://latex.codecogs.com/png.latex?W"> also feed into a regularization function <img src="https://latex.codecogs.com/png.latex?R(W)">.</li>
<li>Finally, the output of the hinge loss (<img src="https://latex.codecogs.com/png.latex?L_i">, which would then be averaged over the batch) and the output of the regularization term <img src="https://latex.codecogs.com/png.latex?R(W)"> are added together (the + node) to give us our final total loss <img src="https://latex.codecogs.com/png.latex?L">.</li>
</ol>
<p>So, we’ve decomposed our complex loss calculation into a directed acyclic graph of basic operations. Each node in this graph takes some inputs and produces an output. The beauty of this approach is that if we know how to compute the local gradient for each elementary operation for example how does the output of the <img src="https://latex.codecogs.com/png.latex?Wx"> node change if <img src="https://latex.codecogs.com/png.latex?W"> changes a little?, we can then use the <strong>chain rule</strong> from calculus to systematically “backpropagate” the gradient of the final loss <img src="https://latex.codecogs.com/png.latex?L"> all the way back to our input parameters <img src="https://latex.codecogs.com/png.latex?W">. This “backpropagation” algorithm is the workhorse that allows us to efficiently compute gradients for arbitrarily complex neural networks. It’s modular because we only need to know the local derivative for each type of node (matrix multiply, addition, ReLU, sigmoid, loss function, etc.). And it’s systematic, meaning we can implement it once, and it will work for any network architecture we can express as a computational graph. This approach is essential for handling the complexity of modern deep learning models.</p>
<p>Think about <strong>Convolutional Network</strong> like <a href="https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">AlexNet</a>, which was a groundbreaking architecture for image recognition. Trying to write down the full analytical derivative of the loss with respect to all those different sets of weights in AlexNet would be an absolute nightmare. It’s just too complex. And it gets even crazier. Consider something like a <a href="https://arxiv.org/abs/1410.5401">Neural Turing Machine</a>. This is a type of neural network architecture that’s augmented with an external memory component that it can learn to read from and write to. The idea is to give the network capabilities closer to a traditional computer program. The computational graph for a Neural Turing Machine, especially if you unroll its operations over time becomes incredibly deep and complex. Each “time step” of the machine’s operation adds another layer to this graph. Again, attempting to derive gradients by hand for such a system is completely out of the question. So, for all these sophisticated architectures – from convolutional networks to these very deep and recurrent models – we absolutely need a robust, general, and efficient method for computing gradients. And that method is backpropagation, operating on these computational graphs.</p>
</section>
<section id="backpropagation" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="backpropagation">Backpropagation</h2>
<p>So the solution <strong>backpropagation</strong>. This is the algorithm that will allow us to compute these gradients efficiently and systematically. It’s essentially a practical application of the chain rule from calculus on a computational graph. Let’s illustrate backpropagation with a simple example. Consider the function <img src="https://latex.codecogs.com/png.latex?f(x,%20y,%20z)%20=%20(x%20+%20y)%20*%20z">. Our goal will be to compute the partial derivatives of <img src="https://latex.codecogs.com/png.latex?f"> with respect to <img src="https://latex.codecogs.com/png.latex?x">, <img src="https://latex.codecogs.com/png.latex?y">, and <img src="https://latex.codecogs.com/png.latex?z">, that is, <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20x%7D">, <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20y%7D">, and <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20z%7D">. First, let’s represent this function as a computational graph</p>
<p><a href="./images/backpropagation.png" class="lightbox" data-gallery="quarto-lightbox-gallery-11"><img src="https://bhdai.github.io/blog/posts/neural-network-and-backprobagation/images/backpropagation.png" class="img-fluid"></a></p>
<p>We have inputs <img src="https://latex.codecogs.com/png.latex?x"> and <img src="https://latex.codecogs.com/png.latex?y">. These are fed into an addition node. Let’s call the output of this addition <img src="https://latex.codecogs.com/png.latex?q">. So, <img src="https://latex.codecogs.com/png.latex?q%20=%20x%20+%20y">. Then, this intermediate value q and our third input <img src="https://latex.codecogs.com/png.latex?z"> are fed into a multiplication node. The output of this multiplication node is our final function value <img src="https://latex.codecogs.com/png.latex?f">. So, <img src="https://latex.codecogs.com/png.latex?f%20=%20q%20z">. This graph breaks the computation down into two simple steps: an addition and a multiplication. Now, let’s assign some concrete values to our inputs to trace the computation. Let’s say, for example, <img src="https://latex.codecogs.com/png.latex?x%20=%20-2">, <img src="https://latex.codecogs.com/png.latex?y%20=%205">, and <img src="https://latex.codecogs.com/png.latex?z%20=%20-4">, so <img src="https://latex.codecogs.com/png.latex?q%20=%203"> and <img src="https://latex.codecogs.com/png.latex?f%20=%20-12">. This is the forward pass: we compute the values of all nodes in the graph from inputs to output.</p>
<p>Now, for backpropagation, we need the local gradients for each node in our graph. That is, for each operation, we need to know how its output changes with respect to its inputs. Consider the first operation: <img src="https://latex.codecogs.com/png.latex?q%20=%20x%20+%20y">, <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20q%7D%7B%5Cpartial%20x%7D%20=%201"> so if <img src="https://latex.codecogs.com/png.latex?x"> changes by a small amount, <img src="https://latex.codecogs.com/png.latex?q"> changes by that same amount, similarly <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20q%7D%7B%5Cpartial%20y%7D%20=%201">. These are the local gradients for the addition gate. Next, consider the second operation: <img src="https://latex.codecogs.com/png.latex?f%20=%20q%20z">. <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20q%7D%20=%20z"> so if <img src="https://latex.codecogs.com/png.latex?q"> changes by a small amount <img src="https://latex.codecogs.com/png.latex?dq">, <img src="https://latex.codecogs.com/png.latex?f"> changes by <img src="https://latex.codecogs.com/png.latex?z%20%5Ccdot%20dq">. And <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20z%7D%20=%20q">, if z changes by a small amount <img src="https://latex.codecogs.com/png.latex?dz">, <img src="https://latex.codecogs.com/png.latex?f"> changes by <img src="https://latex.codecogs.com/png.latex?q%20%5Ccdot%20dz">. So, we have the local derivatives for each individual operation. Ultimately, what we want are the gradients of the final output <img src="https://latex.codecogs.com/png.latex?f"> with respect to the initial inputs <img src="https://latex.codecogs.com/png.latex?x">, <img src="https://latex.codecogs.com/png.latex?y">, and <img src="https://latex.codecogs.com/png.latex?z">: <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20x%7D">, <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20y%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20z%7D">.</p>
<p>Okay, we’re ready to start the backward pass. We work from the output <img src="https://latex.codecogs.com/png.latex?f"> back towards the inputs <img src="https://latex.codecogs.com/png.latex?x,%20y,%20z">. The very first gradient we consider is the gradient of the final output <img src="https://latex.codecogs.com/png.latex?f"> with respect to itself, <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20f%7D">. This is always 1, by definition. This ‘1’ is often called the initial “upstream gradient”, it’s the gradient that starts the whole backward flow. You can see it annotated on the graph in read next to <img src="https://latex.codecogs.com/png.latex?f">.</p>
<p>Now let’s move back one step, we want to find <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20z%7D">. <img src="https://latex.codecogs.com/png.latex?z"> is an input to the multiplication gate <img src="https://latex.codecogs.com/png.latex?f%20=%20qz"> The local gradient of <img src="https://latex.codecogs.com/png.latex?f"> with respect to <img src="https://latex.codecogs.com/png.latex?z"> is <img src="https://latex.codecogs.com/png.latex?q">. From our forward pass, we know <img src="https://latex.codecogs.com/png.latex?q%20=%203">. So, <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20z%7D%20=%203">. The upstream gradient <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20f%7D"> (which is 1) is multiplied by the local gradient <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20z%7D"> (which is q). So, <img src="https://latex.codecogs.com/png.latex?1%20q%20=%20q%20=%203">. This value 3 is now the gradient of the final output <img src="https://latex.codecogs.com/png.latex?f"> with respect to the node <img src="https://latex.codecogs.com/png.latex?z">.</p>
<p>Next, we want <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20q%7D">. <img src="https://latex.codecogs.com/png.latex?q"> is the other input to the multiplication gate <img src="https://latex.codecogs.com/png.latex?f%20=%20qz">. The local gradient of <img src="https://latex.codecogs.com/png.latex?f"> with respect to <img src="https://latex.codecogs.com/png.latex?q"> is <img src="https://latex.codecogs.com/png.latex?z">. From our forward pass, <img src="https://latex.codecogs.com/png.latex?z%20=%20-4">. So, <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20q%7D%20=%20-4">. Again the upstream gradient <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20f%7D"> (which is 1) multiplied by the local gradient <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20q%7D"> (which is z). So, <img src="https://latex.codecogs.com/png.latex?1%20%5Ccdot%20z%20=%20z%20=%20-4">. This <img src="https://latex.codecogs.com/png.latex?-4"> is the gradient of the final output <img src="https://latex.codecogs.com/png.latex?f"> with respect to the intermediate node <img src="https://latex.codecogs.com/png.latex?q">. This value will be crucial as we continue propagating backward, because <img src="https://latex.codecogs.com/png.latex?q"> itself depends on <img src="https://latex.codecogs.com/png.latex?x"> and <img src="https://latex.codecogs.com/png.latex?y">.</p>
<p>Now we need to go further back to get <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20y%7D">. <img src="https://latex.codecogs.com/png.latex?y"> is an input to the addition gate <img src="https://latex.codecogs.com/png.latex?q%20=%20x%20+%20y">. The node <img src="https://latex.codecogs.com/png.latex?y"> influences <img src="https://latex.codecogs.com/png.latex?f"> through <img src="https://latex.codecogs.com/png.latex?q">. This is where the chain rule comes into play. The chain rule tells us:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20y%7D=%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20q%7D%20%5Cfrac%7B%5Cpartial%20q%7D%7B%5Cpartial%20y%7D%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20q%7D"> is the gradient of the final output <img src="https://latex.codecogs.com/png.latex?f"> with respect to <img src="https://latex.codecogs.com/png.latex?q">. We just calculated this as <img src="https://latex.codecogs.com/png.latex?-4">. This is often called the upstream gradient coming into the <img src="https://latex.codecogs.com/png.latex?q"> node from later parts of the graph. <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20q%7D%7B%5Cpartial%20y%7D"> is the local gradient of the <img src="https://latex.codecogs.com/png.latex?q"> node with respect to its input <img src="https://latex.codecogs.com/png.latex?y">. From <img src="https://latex.codecogs.com/png.latex?q%20=%20x%20+%20y">, we know <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20q%7D%7B%5Cpartial%20y%7D%20=%201">. So, <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20y%7D%20=%20-4%20%5Ccdot%201%20=%20-4">. This <img src="https://latex.codecogs.com/png.latex?-4"> is now the gradient of the final output <img src="https://latex.codecogs.com/png.latex?f"> with respect to the input <img src="https://latex.codecogs.com/png.latex?y">.</p>
<p>Finally, let’s get <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20x%7D"> Similar to <img src="https://latex.codecogs.com/png.latex?y,%20x"> is an input to the addition gate <img src="https://latex.codecogs.com/png.latex?q%20=%20x%20+%20y">, and it influences <img src="https://latex.codecogs.com/png.latex?f"> through <img src="https://latex.codecogs.com/png.latex?q">, we apply the chain rule here <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20x%7D%20=%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20q%7D%20%5Cfrac%7B%5Cpartial%20q%7D%7B%5Cpartial%20x%7D">. So, <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20x%7D%20=%20-4%20%5Ccdot%201%20=%20-4">. This <img src="https://latex.codecogs.com/png.latex?-4"> is the gradient of the final output <img src="https://latex.codecogs.com/png.latex?f"> with respect to the input <img src="https://latex.codecogs.com/png.latex?x">.</p>
<p>So, to summarize, we’ve found:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20f%7D%20&amp;=%201%20%5C%5C%0A%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20z%7D%20&amp;=%20q%20=%203%20%5C%5C%0A%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20q%7D%20&amp;=%20z%20=%20-4%20%5C%5C%0A%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20y%7D%20&amp;=%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20q%7D%20%20%5Cfrac%7B%5Cpartial%20q%7D%7B%5Cpartial%20y%7D%20%20=%20-4%20%5Ccdot%201%20=%20-4%20%5C%5C%0A%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20x%7D%20&amp;=%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20q%7D%20%5Cfrac%7B%5Cpartial%20q%7D%7B%5Cpartial%20x%7D%20%20=%20-4%20%5Ccdot%201%20=%20-4%0A%5Cend%7Balign%7D%0A"></p>
<p>Let’s generalize this idea of backpropagation through a single computational gate.</p>
<p><a href="./images/local-gradient.png" class="lightbox" data-gallery="quarto-lightbox-gallery-12"><img src="https://bhdai.github.io/blog/posts/neural-network-and-backprobagation/images/local-gradient.png" class="img-fluid"></a></p>
<p>Imagine we have some generic function, or “gate,” <img src="https://latex.codecogs.com/png.latex?f">. This gate takes some inputs, let’s say <img src="https://latex.codecogs.com/png.latex?x"> and <img src="https://latex.codecogs.com/png.latex?y">, and it produces an output <img src="https://latex.codecogs.com/png.latex?z">. This <img src="https://latex.codecogs.com/png.latex?f"> could be an addition, a multiplication, a ReLU, a sigmoid, any elementary operation. For this gate to be part of a backpropagation process, we need to know its <strong>local gradients</strong>. These are the partial derivatives of the gate’s output <img src="https://latex.codecogs.com/png.latex?z"> with respect to each of its inputs, <img src="https://latex.codecogs.com/png.latex?x"> and <img src="https://latex.codecogs.com/png.latex?y"> So, we need to be able to compute <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20z%7D%7B%5Cpartial%20x%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20z%7D%7B%5Cpartial%20y%7D"> . These gradients tell us how a small change in x (or y) directly affects z, assuming all other inputs to this specific gate are held constant. Now, during the backward pass, this gate <img src="https://latex.codecogs.com/png.latex?f"> will receive an <strong>upstream gradient</strong>. This upstream gradient is the derivative of the final loss function <img src="https://latex.codecogs.com/png.latex?L"> which is at the very end of our entire computational graph with respect to the output of this gate, <img src="https://latex.codecogs.com/png.latex?z">. So, we receive <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20z%7D">. This <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20z%7D"> tells us how much the final loss <img src="https://latex.codecogs.com/png.latex?L"> changes if the output <img src="https://latex.codecogs.com/png.latex?z"> of this particular gate changes by a small amount. This gradient has been propagated backward from later parts of the graph. Our goal, when backpropagating through this gate, is to compute the <strong>Downstream gradients</strong>. These are the derivatives of the final loss <img src="https://latex.codecogs.com/png.latex?L"> with respect to the inputs of this gate, <img src="https://latex.codecogs.com/png.latex?x"> and <img src="https://latex.codecogs.com/png.latex?y">. So, we want to calculate <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20x%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20y%7D">. How do we do this? We use the chain rule. To find <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20x%7D">, we multiply the upstream gradient <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20z%7D"> by the local gradient <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20z%7D%7B%5Cpartial%20x%7D">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20x%7D=%20%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20z%7D%20*%20%5Cfrac%7B%5Cpartial%20z%7D%7B%5Cpartial%20x%7D%0A"></p>
<p>This tells us how much the final loss <img src="https://latex.codecogs.com/png.latex?L"> changes if the input <img src="https://latex.codecogs.com/png.latex?x"> to this gate changes by a small amount. And similarly for the input <img src="https://latex.codecogs.com/png.latex?y">. So, for any gate in our computational graph, if we know its local gradient and the upstream gradient coming into its output we can compute the downstream gradient which will then become the upstream gradients for the gate that produced <img src="https://latex.codecogs.com/png.latex?x"> and <img src="https://latex.codecogs.com/png.latex?y">. This process is then repeated for every gate in the graph, working backwards from the final loss. Each gate receives an upstream gradient from its successor(s) in the forward pass. It uses this, along with its own local gradients, to compute downstream gradients. These downstream gradients are then passed further back to its predecessor(s). This recursive application of the chain rule is what allows us to efficiently compute the gradient of the overall loss function with respect to all parameters and inputs in the network. This pattern of upstream gradient times local gradient gives downstream gradient is the fundamental computation performed at each node during backpropagation.</p>
<p>Okay, let’s look at another example This time, the function is</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Af(w,%20x)%20=%20%5Cfrac%7B1%7D%7B1%20+%20e%5E%7B-(w_0x_0%20+%20w_1x_1%20+%20w_2)%7D%7D%0A"></p>
<p>This should look familiar! It’s the sigmoid function applied to a linear combination of inputs <img src="https://latex.codecogs.com/png.latex?w_0x_0%20+%20w_1x_1%20+%20w_2">. This is exactly what one neuron in a neural network might compute if it’s using a sigmoid activation. Our goal will be to find the gradients of this f with respect to <img src="https://latex.codecogs.com/png.latex?w_0,%20x_0,%20w_1,%20x_1">, and <img src="https://latex.codecogs.com/png.latex?w_2">. And if we work it out like the previous example we would have this graph</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>Sigmoid function: <img src="https://latex.codecogs.com/png.latex?%5Csigma(x)%20=%20%5Cfrac%7B1%7D%7B1%20+%20e%5E%7B-x%7D%7D"></p>
</div></div><p><a href="./images/sigmoid.png" class="lightbox" data-gallery="quarto-lightbox-gallery-13"><img src="https://bhdai.github.io/blog/posts/neural-network-and-backprobagation/images/sigmoid.png" class="img-fluid"></a></p>
<p>Now, let’s take a step back and look at a section of this graph. The sequence of operations we performed <em>multiply</em> by <em>-1</em> then <em>exp</em> then add 1, then take reciprocal <em>(1/x)</em> – this entire chain, highlighted in the blue box, is actually the computation of the <strong>Sigmoid function</strong>. The input to this entire sigmoid block was the value 1.00 and the output of the sigmoid block was 0.73. An important point here is that the computational graph representation may not be unique. We can choose to break down functions into very fine-grained elementary operations, as we did, or we can group common sequences of operations into a single “macro” gate. The key is to choose a representation where the local gradients at each node can be easily expressed. So, instead of backpropagating through each of those four individual steps, we could have treated the entire sigmoid function as a single gate. To do that, we’d need the sigmoid local gradient. The derivative of the sigmoid function with respect to its input <img src="https://latex.codecogs.com/png.latex?x"> is a well-known result:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7Bd%20%5Csigma(x)%7D%7Bd%20x%7D%20=%20%5Cfrac%7Be%5E%7B-x%7D%7D%7B%5Cleft(1%20+%20e%5E%7B-x%7D%5Cright)%5E2%7D%20=%20%5Cleft(%20%5Cfrac%7B1%20+%20e%5E%7B-x%7D%20-%201%7D%7B1%20+%20e%5E%7B-x%7D%7D%5Cright)%20%5Cleft(%5Cfrac%7B1%7D%7B1%20+%20e%5E%7B-x%7D%7D%5Cright)%20=%20%5Cleft(1%20-%20%5Csigma(x)%5Cright)%20%5Csigma(x)%0A"></p>
<p>This is a very convenient form because it expresses the derivative in terms of the function’s own output value. So, if we treat the sigmoid as a single gate. The input to this sigmoid gate was <img src="https://latex.codecogs.com/png.latex?x%5C_in%20=%201.00">. The output was <img src="https://latex.codecogs.com/png.latex?%5Csigma(x%5C_in)%20=%200.73">. The <strong>upstream gradient</strong> coming into the output of the sigmoid gate was 1.00. The <strong>local gradient</strong> of the sigmoid function, evaluated at its input <img src="https://latex.codecogs.com/png.latex?x%5C_in=1.00">, is <img src="https://latex.codecogs.com/png.latex?%5Csigma(1.00)%20(1%20-%20%5Csigma(1.00))">. Since <img src="https://latex.codecogs.com/png.latex?%5Csigma(1.00)"> is approximately <img src="https://latex.codecogs.com/png.latex?0.73">, the local gradient is <img src="https://latex.codecogs.com/png.latex?0.73%20%5Ccdot%20(1%20-%200.73)%20=%200.73%20%5Ccdot%200.27%20%5Capprox%200.1971">. Then we calculate the downstream gradient which is <img src="https://latex.codecogs.com/png.latex?upstream%5C_gradient%20%5Ccdot%20(1-%5Csigma(1))%20%5Csigma(1)"> and this evaluates to approximately 0.2. And if you look back at our step-by-step backpropagation, the gradient we computed at the input of the multiply by -1 gate which was the start of our sigmoid block was indeed 0.20! This demonstrates the modularity of backpropagation. We can define common operations like sigmoid, ReLU, tanh, etc., as single “black-box” gates. As long as we know how to compute their output during the forward pass and their local gradient during the backward pass, we can easily plug them into any computational graph. Modern deep learning frameworks are built around this idea of a library of predefined layers or operations, each knowing its forward and backward computation. This ability to abstract away complex operations into single gates with well-defined local gradients is incredibly powerful and makes implementing backpropagation for complex architectures much more manageable.</p>
</section>
<section id="backpropagation-in-practice" class="level2">
<h2 class="anchored" data-anchor-id="backpropagation-in-practice">Backpropagation in practice</h2>
<p>Now, let’s look for some general patterns in how these gradients flow through different types of gates. Understanding these can give you a good intuition for how backpropagation works.</p>
<p><a href="./images/gates.png" class="lightbox" data-gallery="quarto-lightbox-gallery-14"><img src="https://bhdai.github.io/blog/posts/neural-network-and-backprobagation/images/gates.png" class="img-fluid"></a></p>
<p>First, let’s consider the <strong>add gate</strong>: <img src="https://latex.codecogs.com/png.latex?z%20=%20x%20+%20y">. The add gate acts as a gradient distributor. It takes the upstream gradient and simply passes it along (distributes it) to both of its inputs unchanged, because the local gradients are 1.</p>
<p>Next, the <strong>mul gate</strong> (multiplication gate): <img src="https://latex.codecogs.com/png.latex?z%20=%20x%20y">. The mul gate acts as a <strong>swap multiplier</strong>. The gradient <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7BdL%7D%7Bdx%7D"> is the upstream gradient times the other input <img src="https://latex.codecogs.com/png.latex?y">. And <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7BdL%7D%7Bdy%7D"> is the upstream gradient times the other input <img src="https://latex.codecogs.com/png.latex?x">. It’s like the inputs get swapped when they multiply the upstream gradient.</p>
<p>What about a <strong>copy gate</strong>? A copy gate is when a single input <img src="https://latex.codecogs.com/png.latex?x"> is used in multiple places further down the graph. During backpropagation, this means gradients from multiple paths will flow back to x. A copy gate (or a “fan-out” point) acts as a <strong>gradient adder</strong>. It sums up all the incoming upstream gradients. This is a direct consequence of the multivariate chain rule when a variable influences the loss through multiple paths.</p>
<p>Finally, consider a <strong>max gate</strong>: <img src="https://latex.codecogs.com/png.latex?z%20=%20%5Cmax(x,%20y)">. The max gate acts as a <strong>gradient router</strong>. It takes the upstream gradient and routes it entirely to the input that won (had the maximum value) during the forward pass. The gradient for the input that lost is zero. This makes sense if <img src="https://latex.codecogs.com/png.latex?x"> wasn’t the max, then small changes in x (as long as it stays not the max) won’t affect the output <img src="https://latex.codecogs.com/png.latex?z"> at all.</p>
<p>These patterns of distributor for add, swap multiplier for mul, adder for copy, and router for max are very useful to keep in mind when thinking about how gradients propagate through a network. ReLU, for example, <img src="https://latex.codecogs.com/png.latex?%5Cmax(0,%20x)">, is a special case of the max gate. If <img src="https://latex.codecogs.com/png.latex?x%20%3E%200">, the gradient passes through. If <img src="https://latex.codecogs.com/png.latex?x%20%3C=%200">, the gradient becomes zero.</p>
<p>So, we have this conceptual understanding of backpropagation using computational graphs. How do we actually implement this? One way is to write “Flat” code. This means we explicitly write out the sequence of operations for the forward pass and then, in reverse order, write out the corresponding operations for the backward pass. Let’s look at our sigmoid example again</p>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row quarto-layout-valign-center">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><a href="./images/sigmoid2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-15"><img src="https://bhdai.github.io/blog/posts/neural-network-and-backprobagation/images/sigmoid2.png" class="img-fluid"></a></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> f(w0, x0, w1, x1, w2):</span>
<span id="cb1-2">  s0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> w0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> x0  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Forward pass</span></span>
<span id="cb1-3">  s1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> w1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> x1</span>
<span id="cb1-4">  s2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> s0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> s1</span>
<span id="cb1-5">  s3 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> s2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> w2</span>
<span id="cb1-6">  L <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sigmoid(s3)</span>
<span id="cb1-7"></span>
<span id="cb1-8"></span>
<span id="cb1-9">  grad_L <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span> <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Backward pass</span></span>
<span id="cb1-10">  grad_s3 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> grad_L <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> L) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> L</span>
<span id="cb1-11">  grad_w2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> grad_s3</span>
<span id="cb1-12">  grad_s2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> grad_s3</span>
<span id="cb1-13">  grad_s0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> grad_s2</span>
<span id="cb1-14">  grad_s1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> grad_s2</span>
<span id="cb1-15">  grad_w1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> grad_s1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> x1</span>
<span id="cb1-16">  grad_x1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> grad_s1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> w1</span>
<span id="cb1-17">  grad_w0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> grad_s0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> x0</span>
<span id="cb1-18">  grad_x0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> grad_s0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> w0</span></code></pre></div>
</div>
</div>
</div>
<p>During this forward pass, we would typically store all the intermediate values <code>s0, s1, s2, s3</code> and the final output <code>L</code>, because we’ll need them for the backward pass. Now, for the Backward pass to compute the gradients we work backward from the last operation in the forward pass. And that’s it! We’ve computed all the gradients we needed. Notice how the backward pass code mirrors the forward pass code but in reverse order. Each line in the forward pass that computes some variable v will have a corresponding set of lines in the backward pass that compute grad_v (the gradient of the final loss with respect to v) and then use grad_v to compute gradients for the inputs to that operation. This “flat” style of implementation is straightforward for simple functions. However, for very deep or complex networks, writing out all these steps explicitly can become tedious and error-prone. We’d prefer a more modular approach.</p>
<p>A much better approach for practical deep learning systems is a <strong>modularized implementation</strong> using a well-defined <strong>forward / backward API</strong> for each type of gate or operation. The idea is that each elementary operation (like multiplication, addition, sigmoid, ReLU, convolution, etc.) is implemented as an object or a set of functions that knows two things: 1) how to compute its output given its inputs (the <strong>forward</strong> pass). 2) how to compute the gradients with respect to its inputs, given the gradient with respect to its output (the <strong>backward</strong> pass). Here’s an example of what this might look like, using syntax similar to what you’d find in PyTorch’s <code>autograd.Function</code>.</p>
<div class="sourceCode" id="annotated-cell-3" style="background: #f1f3f5;"><pre class="sourceCode python code-annotation-code code-with-copy code-annotated"><code class="sourceCode python"><span id="annotated-cell-3-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> Multiply(torch.autograd.Function):</span>
<span id="annotated-cell-3-2">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">@staticmethod</span></span>
<span id="annotated-cell-3-3">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> forward(ctx, x, y):</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-3" data-target-annotation="1" onclick="event.preventDefault();">1</a><span id="annotated-cell-3-4" class="code-annotation-target">    ctx.save_for_backward(x, y)</span>
<span id="annotated-cell-3-5">    z <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> y</span>
<span id="annotated-cell-3-6">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> z</span>
<span id="annotated-cell-3-7"></span>
<span id="annotated-cell-3-8">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">@staticmethod</span></span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-3" data-target-annotation="2" onclick="event.preventDefault();">2</a><span id="annotated-cell-3-9" class="code-annotation-target">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> backward(ctx, grad_z):</span>
<span id="annotated-cell-3-10">    x, y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ctx.saved_tensors</span>
<span id="annotated-cell-3-11">    grad_x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> grad_z</span>
<span id="annotated-cell-3-12">    grad_y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> grad_z</span>
<span id="annotated-cell-3-13">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> grad_x, grad_y</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code></pre></div>
<dl class="code-annotation-container-grid">
<dt data-target-cell="annotated-cell-3" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-3" data-code-lines="4" data-code-annotation="1">We need to cache some values for use in backward</span>
</dd>
<dt data-target-cell="annotated-cell-3" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-3" data-code-lines="9" data-code-annotation="2"><code>grad_z</code> is upstream gradient</span>
</dd>
</dl>
<p>This is the pattern. Every operation (or “layer” in a deep learning framework) will have a <code>forward</code> method that computes its output and stashes away anything needed for the backward pass, and a <code>backward</code> method that takes the upstream gradient and uses the stashed values and local derivatives to compute and return the downstream gradients.</p>
</section>
<section id="backpropagation-with-vectors-and-matrices" class="level2">
<h2 class="anchored" data-anchor-id="backpropagation-with-vectors-and-matrices">Backpropagation with vectors and matrices</h2>
<p>So far, we’ve mostly been talking about scalar inputs and outputs for these gates. But in neural networks, we usually deal with vectors, matrices and tensors. All the examples we’ve worked through like <img src="https://latex.codecogs.com/png.latex?f(x,y,z)%20=%20(x+y)z"> and the sigmoid example <img src="https://latex.codecogs.com/png.latex?f(w,x)%20=%20%5Csigma(w_0x_0%20+%20w_1x_1%20+%20w_2)"> involved scalar inputs and scalar intermediate values. But in real neural networks, our inputs x are often vectors (e.g., an image flattened into a vector), our weights W are matrices, and the outputs of layers are vectors or higher-dimensional tensors. So, the question is <strong>what about vector-valued functions? How does backpropagation work then</strong>? To answer this, let’s quickly recap vector derivatives.</p>
<p><strong>Scalar to Scalar</strong>, if we have a scalar input <img src="https://latex.codecogs.com/png.latex?x%20%5Cin%20%5Cmathbb%7BR%7D"> and a scalar <img src="https://latex.codecogs.com/png.latex?y%20%5Cin%20%5Cmathbb%7BR%7D">, then we have the <strong>regular derivative</strong> <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bdy%7D%7Bdx%7D">. This tells us: if x changes by a small amount, how much will y change? The derivative itself is also a scalar <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bdy%7D%7Bdx%7D%20%5Cin%20%5Cmathbb%7BR%7D">.</p>
<p><strong>Vector to Scalar</strong>, now what if our input <img src="https://latex.codecogs.com/png.latex?x%20%5Cin%20%5Cmathbb%7BR%7D%5EN"> (a vector) and <img src="https://latex.codecogs.com/png.latex?y%20%5Cin%20%5Cmathbb%7BR%7D"> (a scalar). The derivative is now the <strong>Gradient</strong> <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20y%7D%7B%5Cpartial%20x%7D%20%5Cin%20%5Cmathbb%7BR%7D"> where n-th component is <img src="https://latex.codecogs.com/png.latex?%5Cleft(%5Cfrac%7B%5Cpartial%20y%7D%7B%5Cpartial%20x%7D%5Cright)_n%20=%20%5Cfrac%7B%5Cpartial%20y%7D%7B%5Cpartial%20x_n%7D">. This gradient answers the question for each element of <img src="https://latex.codecogs.com/png.latex?x">, if it changes by a small amount, then how much will <img src="https://latex.codecogs.com/png.latex?y"> change? Our loss function <img src="https://latex.codecogs.com/png.latex?L"> is a scalar, and our weights <img src="https://latex.codecogs.com/png.latex?W"> (and inputs x to various layers) are often vectors or matrices. So, when we compute <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20W%7D">, we are computing a gradient.</p>
<p><strong>Vector to Vector</strong>, what if both our input <img src="https://latex.codecogs.com/png.latex?x%20%5Cin%20%5Cmathbb%7BR%7D%5EN"> (a vector) and our output <img src="https://latex.codecogs.com/png.latex?y%20%5Cin%20%5Cmathbb%7BR%7D%5EM"> (a vector). The derivative is now the <strong>Jacobian</strong> matrix <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20y%7D%7B%5Cpartial%20x%7D%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BN%20%5Ctimes%20M%7D">. This is an <img src="https://latex.codecogs.com/png.latex?M%20%5Ctimes%20N"> matrix, where the element at row <img src="https://latex.codecogs.com/png.latex?m">, column <img src="https://latex.codecogs.com/png.latex?n"> is <img src="https://latex.codecogs.com/png.latex?%5Cleft(%5Cfrac%7B%5Cpartial%20y%7D%7B%5Cpartial%20x%7D%5Cright)_%7Bn,m%7D%20=%20%5Cfrac%7B%5Cpartial%20y_m%7D%7B%5Cpartial%20x_n%7D">. This Jacobian answers the question for each element of <img src="https://latex.codecogs.com/png.latex?x">, if it changes by a small amount, then how much will each element of y change? Many layers in a neural network can be thought of as vector-to-vector functions (e.g., a fully connected layer maps an input vector to an output vector of activations). The chain rule still applies, but now it involves matrix-vector multiplications or Jacobian-vector products. Let’s see how this plays out.</p>
<p><a href="./images/backprop-vector.png" class="lightbox" data-gallery="quarto-lightbox-gallery-16"><img src="https://bhdai.github.io/blog/posts/neural-network-and-backprobagation/images/backprop-vector.png" class="img-fluid"></a></p>
<p>We still have our gate <img src="https://latex.codecogs.com/png.latex?f"> which takes inputs, say <img src="https://latex.codecogs.com/png.latex?x"> and <img src="https://latex.codecogs.com/png.latex?y">, and produces an output <img src="https://latex.codecogs.com/png.latex?z."> Crucially, the final Loss <img src="https://latex.codecogs.com/png.latex?L"> is still a scalar! We are always trying to minimize a single scalar value representing how bad our network’s predictions are. During the backward pass, this gate will receive an <strong>Upstream gradient</strong>. This is the gradient of the scalar Loss L with respect to the vector output z of this gate. So, it’s <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20z%7D">. Since <img src="https://latex.codecogs.com/png.latex?L"> is a scalar and <img src="https://latex.codecogs.com/png.latex?z"> is a D_z-dimensional vector, this upstream gradient <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20z%7D"> will also be a D_z-dimensional vector. It tells us: “For each element of <img src="https://latex.codecogs.com/png.latex?z">, how much does that element influence the final Loss <img src="https://latex.codecogs.com/png.latex?L">?”. Now, the “local gradients” for this gate <img src="https://latex.codecogs.com/png.latex?f"> describe how its output z changes with respect to its inputs <img src="https://latex.codecogs.com/png.latex?x"> and <img src="https://latex.codecogs.com/png.latex?y">. <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20z%7D%7B%5Cpartial%20x%7D">: Since <img src="https://latex.codecogs.com/png.latex?z"> is D_z-dim and <img src="https://latex.codecogs.com/png.latex?x"> is D_x-dim, this is the Jacobian matrix of <img src="https://latex.codecogs.com/png.latex?z"> with respect to <img src="https://latex.codecogs.com/png.latex?x"> and same with <img src="https://latex.codecogs.com/png.latex?y">. The shapes of these Jacobians:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20z%7D%7B%5Cpartial%20x%7D"> will be a D_x x D_z matrix (derivative of each component of <img src="https://latex.codecogs.com/png.latex?z"> w.r.t. each component of <img src="https://latex.codecogs.com/png.latex?x">).</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20z%7D%7B%5Cpartial%20y%7D"> will be a D_y x D_z matrix.</li>
</ul>
<p>The <strong>Downstream gradients</strong> we want to compute are <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20x%7D"> (a D_x-dim vector) and <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20y%7D"> (a D_y-dim vector). Using the chain rule:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20x%7D%20&amp;=%20%5Cfrac%7B%5Cpartial%20z%7D%7B%5Cpartial%20x%7D%20%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20z%7D%20%20%5C%5C%0A%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20y%7D%20&amp;=%20%5Cfrac%7B%5Cpartial%20z%7D%7B%5Cpartial%20y%7D%20%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20z%7D%0A%5Cend%7Balign%7D%0A"></p>
<p>An important principle to remember: Gradients of variables with respect to the loss have the same dimensions as the original variable.</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?x"> is a D_x-dimensional vector. Its gradient <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20x%7D"> is also a D_x-dimensional vector.</li>
<li><img src="https://latex.codecogs.com/png.latex?y"> is a D_y-dimensional vector. Its gradient <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20y%7D"> is also a D_y-dimensional vector.</li>
<li><img src="https://latex.codecogs.com/png.latex?z"> is a D_z-dimensional vector. Its upstream gradient <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20z%7D"> is also a D_z-dimensional vector</li>
</ul>
<p>This makes intuitive sense: for every element in a variable (like a weight matrix or an activation vector), we need a corresponding gradient value that tells us how changing that specific element affects the final scalar loss. This consistency of shapes is critical for implementing backpropagation correctly in code</p>
<p>Let’s walk through a concrete example of backprop with vectors, using an element-wise ReLU function, <img src="https://latex.codecogs.com/png.latex?f(x)%20=%20%5Cmax(0,x)"></p>
<p><a href="./images/backprop-vector-example.png" class="lightbox" data-gallery="quarto-lightbox-gallery-17"><img src="https://bhdai.github.io/blog/posts/neural-network-and-backprobagation/images/backprop-vector-example.png" class="img-fluid"></a></p>
<p>We have a 4D input vector x as shown in the image, and the function max(0,x) is applied element-wise, and the output vector z. Now, for the backward pass, let’s assume we’ve received the upstream gradient <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20z%7D"> This <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20z%7D"> is also a 4D vector. Let’s say it’s <img src="https://latex.codecogs.com/png.latex?%5Cleft%5B4,%20-1,%205,%209%5Cright%5D%5ET">. This tells us, for example, that if the first element of <img src="https://latex.codecogs.com/png.latex?z"> changes by a small amount, the loss <img src="https://latex.codecogs.com/png.latex?L"> changes by 4 times that amount. To compute the downstream gradient <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20x%7D">, we need the Jacobian matrix <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20z%7D%7B%5Cpartial%20x%7D">. Since <img src="https://latex.codecogs.com/png.latex?z_i%20=%20%5Cmax(0,%20x_i)">, the output <img src="https://latex.codecogs.com/png.latex?z_i"> only depends on the corresponding input <img src="https://latex.codecogs.com/png.latex?x_i">. It does not depend on <img src="https://latex.codecogs.com/png.latex?x_j"> for <img src="https://latex.codecogs.com/png.latex?j%20%5Cneq%20i">. This means the Jacobian matrix will be <strong>diagonal</strong>.</p>
<ul>
<li>If <img src="https://latex.codecogs.com/png.latex?x_i%20%3E%200">, then <img src="https://latex.codecogs.com/png.latex?z_i%20=%20x_i">, so <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20z_i%7D%7B%5Cpartial%20x_i%7D%20=%201">.</li>
<li>If <img src="https://latex.codecogs.com/png.latex?x_i%20%5Cleq%200">, then <img src="https://latex.codecogs.com/png.latex?z_i%20=%200">, so <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20z_i%7D%7B%5Cpartial%20x_i%7D%20=%200">.</li>
</ul>
<p>So, the Jacobian <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20z%7D%7B%5Cpartial%20x%7D"> is a 4x4 diagonal matrix. Now, we compute <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20x%7D"> = <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20z%7D%7B%5Cpartial%20x%7D"> <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20z%7D">. This is a matrix-vector multiplication</p>
<p>Notice something important here. For element-wise operations like ReLU, the Jacobian is sparse, and specifically, it’s diagonal. The off-diagonal entries are always zero. Because of this, we Never explicitly form the full Jacobian matrix in practice for these types of operations. That would be very inefficient, especially for high-dimensional vectors. Storing a large D x D diagonal matrix is wasteful if only D elements are non-zero. Instead, we use implicit multiplication. We know that multiplying by a diagonal matrix is equivalent to an element-wise product with the diagonal elements</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cleft(%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20x%7D%5Cright)_i%20=%0A%5Cbegin%7Bcases%7D%0A%5Cleft(%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20z%7D%5Cright)_i%20&amp;%20%5Ctext%7B%20if%20%7D%20x_i%20%3E%200%20%5C%5C%0A0%20&amp;%20%5Ctext%7Botherwise%7D%0A%5Cend%7Bcases%7D%0A"></p>
<p>This is far more efficient than forming the full Jacobian and doing a matrix-vector multiply. Most deep learning frameworks will implement backpropagation through element-wise operations like this, using element-wise masking or multiplication. So, while the concept of Jacobians is important for understanding the general case, for many common operations, we can exploit their structure to make the backward pass much more efficient.</p>
<p>Now, what about when our variables are not just vectors, but matrices or even higher-dimensional tensors? So let’s extend this to <strong>backprop with matrices (or tensors)</strong>. The good news is that the core principles remain exactly the same. We still have our gate f, Our inputs x and y can now be matrices (or higher-order tensors). Let’s say x is D_x x M_x and y is D_y x M_y. The output z is also a matrix, say D_z x M_z. The Loss L is still a scalar! This is always true. And remember, <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20x%7D"> will always have the same shape as x. So <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20x%7D"> will be a D_x x M_x matrix of gradients.</p>
<p><a href="./images/backprop-tensor.png" class="lightbox" data-gallery="quarto-lightbox-gallery-18"><img src="https://bhdai.github.io/blog/posts/neural-network-and-backprobagation/images/backprop-tensor.png" class="img-fluid"></a></p>
<p>The local gradients are still conceptually Jacobian matrices, but now they relate elements of input matrices to elements of the output matrix. The chain rule, downstream = local * upstream, still holds. However, explicitly forming these Jacobians when <img src="https://latex.codecogs.com/png.latex?x">, <img src="https://latex.codecogs.com/png.latex?y">, and <img src="https://latex.codecogs.com/png.latex?z"> are matrices would involve thinking about derivatives of matrices with respect to matrices, which can lead to 4th-order tensors and becomes very cumbersome to write down and implement directly. Instead, what usually happens in practice is that we consider the matrix operations themselves (like matrix multiplication, element-wise operations on matrices, etc.) and derive the gradient expressions for those specific operations directly, often by thinking about how a small change in an input matrix element affects an element in the output matrix, and then summing up contributions to the scalar loss. The result is usually an expression for the gradient matrix <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20X%7D"> that has the same shape as <img src="https://latex.codecogs.com/png.latex?X"> and can be computed efficiently. The dimensions of these abstract Jacobians would be:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20z%7D%7B%5Cpartial%20x%7D"> relating a (D_x x M_x) input to a (D_z x M_z) output.</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20z%7D%7B%5Cpartial%20y%7D"> relating a (D_y x M_y) input to a (D_z x M_z) output.</li>
</ul>
<p>These would be very high-dimensional objects if we thought of them explicitly. The key takeaway is that even though the underlying math involves Jacobians of matrix-valued functions, for common operations like matrix multiplication, element-wise functions, convolutions, etc., we have pre-derived, efficient formulas for computing the gradient <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20X%7D"> given <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20Z%7D"> and the original inputs, and these formulas maintain the correct shapes.</p>
<p>Let’s look at a very common example: matrix multiplication.</p>
<p><a href="./images/matrices-example.png" class="lightbox" data-gallery="quarto-lightbox-gallery-19"><img src="https://bhdai.github.io/blog/posts/neural-network-and-backprobagation/images/matrices-example.png" class="img-fluid"></a></p>
<p>We have an input matrix <img src="https://latex.codecogs.com/png.latex?x"> of size N x D, and a weight matrix <img src="https://latex.codecogs.com/png.latex?w"> of size D x M. The output <img src="https://latex.codecogs.com/png.latex?y"> is their product: <img src="https://latex.codecogs.com/png.latex?y%20=%20x%20w">. This output <img src="https://latex.codecogs.com/png.latex?y"> will be an N x M matrix. The formula for an element <img src="https://latex.codecogs.com/png.latex?y_%7Bn,m%7D"> of the output is <img src="https://latex.codecogs.com/png.latex?y_%7Bn,m%7D%20=%20%5Csum_d%20(x_%7Bn,d%7D%20w_%7Bd,m%7D)">. This is the standard definition of matrix multiplication – the dot product of the n-th row of <img src="https://latex.codecogs.com/png.latex?x"> with the m-th column of <img src="https://latex.codecogs.com/png.latex?w."> We are given the upstream gradient <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20y%7D">, which is an N x M matrix, having the same shape as <img src="https://latex.codecogs.com/png.latex?y">. Our goal is to find <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20x%7D"> (an N x D matrix) and <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20w%7D"> (a D x M matrix). Now, if we were to think about the Jacobians explicitly, the Jacobian <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20y%7D%7B%5Cpartial%20x%7D"> would relate an (N x D) input to an (N x M) output, the Jacobian <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20y%7D%7B%5Cpartial%20z%7D"> would relate a (D x M) input to an (N x M) output. These would be 4D tensors! For example, if N=64 (batch size), D=M=4096 (common hidden layer sizes), then NxD is roughly 250,000, and NxM is also roughly 250,000. So the Jacobian <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20y%7D%7B%5Cpartial%20x%7D"> would have (N<em>D) </em> (N*M) elements, which is about (2.5e5)^2 = 6.25e10 elements. If each is a 4-byte float, that’s 250 GB of memory just for one Jacobian! Clearly, we <strong>must work with them implicitly</strong>! We cannot afford to form these giant Jacobian tensors. So, let’s reason about it element by element. What parts of <img src="https://latex.codecogs.com/png.latex?y"> are affected by one element of <img src="https://latex.codecogs.com/png.latex?x">? Let’s consider <img src="https://latex.codecogs.com/png.latex?x_%7Bn,d%7D">. The element <img src="https://latex.codecogs.com/png.latex?x_%7Bn,d%7D"> affects the whole n-th row of y (e.g., <img src="https://latex.codecogs.com/png.latex?y_%7Bn,:%7D">), this is because <img src="https://latex.codecogs.com/png.latex?x_%7Bn,d%7D"> participates in the calculation of <img src="https://latex.codecogs.com/png.latex?y_%7Bn,m%7D"> for all values of <img src="https://latex.codecogs.com/png.latex?m">. So, to find <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20y%7D%7B%5Cpartial%20x_%7Bn,d%7D%7D">, we need to sum up the influence of <img src="https://latex.codecogs.com/png.latex?x_%7Bn,d%7D"> on each <img src="https://latex.codecogs.com/png.latex?y_%7Bn,m%7D"> and then how each <img src="https://latex.codecogs.com/png.latex?y_%7Bn,m%7D"> influences <img src="https://latex.codecogs.com/png.latex?L">. Using the chain rule:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%20y%7D%7B%5Cpartial%20x_%7Bn,d%7D%7D%20=%20%5Csum_m%7B%20%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20y_%7Bn,m%7D%7D%20%5Cfrac%7B%5Cpartial%20y_%7Bn,m%7D%7D%7B%5Cpartial%20x_%7Bn,d%7D%7D%7D%0A"></p>
<p>Now, let’s look at the local derivative <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20y_%7Bn,m%7D%7D%7B%5Cpartial%20x_%7Bn,d%7D%7D">. From <img src="https://latex.codecogs.com/png.latex?y_%7Bn,m%7D%20=%20%5Csum_k%7B%20x_%7Bn,k%7D%20%20w_%7Bk,m%7D%7D">, if we vary <img src="https://latex.codecogs.com/png.latex?x_%7Bn,d%7D">, how does <img src="https://latex.codecogs.com/png.latex?y_%7Bn,m%7D"> change? Well, <img src="https://latex.codecogs.com/png.latex?x_%7Bn,d%7D"> is multiplied by <img src="https://latex.codecogs.com/png.latex?w_%7Bd,m%7D"> to contribute to <img src="https://latex.codecogs.com/png.latex?y_%7Bn,m%7D">. So</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%5Cfrac%7B%5Cpartial%20y_%7Bn,m%7D%7D%7B%5Cpartial%20x_%7Bn,d%7D%7D%20&amp;=%20w_%7Bd,m%7D%20%5C%5C%0A%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20x_%7Bn,d%7D%7D%20&amp;=%20%5Csum_m%20%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20y_%7Bn,m%7D%7D%20w_%7Bd,m%7D%0A%5Cend%7Balign%7D%0A"></p>
<p>And that sum is exactly the formula for the (n,d)-th element of the matrix product <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20y%7D%20w%5ET">. So, in matrix form:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20x%7D%20=%20%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20y%7D%20%20*%20w%5ET%0A"></p>
<p>Let’s check shapes</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Coverbrace%7B%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20x%7D%7D%5E%7BN%20%5Ctimes%20D%7D%0A=%20%5Cunderbrace%7B%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20y%7D%7D_%7BN%5Ctimes%20M%7D%5C;%0A%20%20%5Coverbrace%7Bw%5E%7B%5C!T%7D%7D%5E%7BM%5Ctimes%20D%7D%0A"></p>
<p>By similar logic for <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20w%7D"> , we can derive:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Coverbrace%7B%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20w%7D%7D%5E%7B%20D%20%5Ctimes%20M%20%7D%20=%0A%5Cunderbrace%7Bx%5ET%7D_%7BD%20%5Ctimes%20N%7D%0A%5Coverbrace%7B%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20y%7D%7D%5E%7BN%20%5Ctimes%20M%7D%0A"></p>
<p>And there’s a nice mnemonic: they are the only way to make the shapes match up! If you remember the input and output shapes, and that you need to use <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20y%7D"> and the other input matrix (or its transpose), you can often re-derive these just by making the dimensions work out for matrix multiplication. So, for a matrix multiply gate, given the upstream gradient <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20y%7D"> , we can directly compute the downstream gradients <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20x%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20w%7D"> using these matrix multiplication formulas, without ever forming the explicit giant Jacobian. This is how it’s done in practice.</p>


</section>

 ]]></description>
  <category>Deep Learning</category>
  <category>Backpropagation</category>
  <category>Neural Networks</category>
  <guid>https://bhdai.github.io/blog/posts/neural-network-and-backprobagation/</guid>
  <pubDate>Thu, 07 Aug 2025 17:00:00 GMT</pubDate>
  <media:content url="https://bhdai.github.io/blog/posts/neural-network-and-backprobagation/images/cover.png" medium="image" type="image/png" height="81" width="144"/>
</item>
<item>
  <title>Regularization and Optimization</title>
  <dc:creator>Bui Huu Dai</dc:creator>
  <link>https://bhdai.github.io/blog/posts/regularization_and_optimization/</link>
  <description><![CDATA[ 






<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./images/cover.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="“An explorer seen from behind, standing on a high rocky ridge looking through binoculars. The figure overlooks a vast valley rendered as a topographical map with intricate, glowing contour lines. At the bottom of this valley, a radiant orb pulses with energy. Two paths descend the landscape: one is a chaotic, jagged red line, and the other is a smooth river of glowing blue light flowing directly to the orb,” generated by DALL-E 3"><img src="https://bhdai.github.io/blog/posts/regularization_and_optimization/images/cover.png" class="img-fluid figure-img" alt="“An explorer seen from behind, standing on a high rocky ridge looking through binoculars. The figure overlooks a vast valley rendered as a topographical map with intricate, glowing contour lines. At the bottom of this valley, a radiant orb pulses with energy. Two paths descend the landscape: one is a chaotic, jagged red line, and the other is a smooth river of glowing blue light flowing directly to the orb,” generated by DALL-E 3"></a></p>
<figcaption>“An explorer seen from behind, standing on a high rocky ridge looking through binoculars. The figure overlooks a vast valley rendered as a topographical map with intricate, glowing contour lines. At the bottom of this valley, a radiant orb pulses with energy. Two paths descend the landscape: one is a chaotic, jagged red line, and the other is a smooth river of glowing blue light flowing directly to the orb,” generated by DALL-E 3</figcaption>
</figure>
</div>
<section id="we-have-a-loss-function-now-what" class="level2">
<h2 class="anchored" data-anchor-id="we-have-a-loss-function-now-what">We have a loss function, now what?</h2>
<p>In the last blog post we started by framing our core problem <strong>Image Classification</strong>. This is a fundamental task in computer vision. Given an image, our goal is to assign it one label from a predefined set of categories, the model needs to output the correct label. And we talked about why this is so challenging, right? It’s not trivial for a computer. We have immense variability due to viewpoint changes, different illumination conditions, deformations (cats are particularly good at this!), occlusion where parts of the object are hidden, clutter in the background, and of course, massive intraclass variation, think of all the different breeds and appearances of cats. To tackle this, we introduced the data-driven approach. Instead of trying to explicitly code rules for every variation, we collect a large dataset of images and their labels. We then use machine learning to learn the patterns. We talk about a powerful, parametric approach the <strong>Linear Classifier</strong>. Here, the idea is to learn a set of parameters, or weights, denoted by <img src="https://latex.codecogs.com/png.latex?W"> (and a bias term <img src="https://latex.codecogs.com/png.latex?b">). For an input image <img src="https://latex.codecogs.com/png.latex?x">, which we typically flatten into a vector, our score function is <img src="https://latex.codecogs.com/png.latex?f(x,%20W)%20=%20Wx%20+%20b">. This function computes scores for each class. We looked at this from a few different perspectives. The <strong>Algebraic Viewpoint</strong>: just a matrix multiplication and a bias addition. The <strong>Visual Viewpoint</strong>: where each row of W can be seen as a template for a class. The model is trying to learn what an “ideal” cat or dog template looks like. And the <strong>Geometric Viewpoint</strong>: where each row of W defines a hyperplane, and the classifier is essentially carving up the high-dimensional image space with these hyperplanes.</p>
<p>Okay, so we have these scores from our linear classifier. The question is: How good are these scores? How good is our current set of parameters <img src="https://latex.codecogs.com/png.latex?W">? For this, we introduced the concept of a <strong>loss function</strong>. A loss function, often denoted <img src="https://latex.codecogs.com/png.latex?L_i"> for a single example, quantify how unhappy we are with the prediction of that example. Given a dataset of N examples <img src="https://latex.codecogs.com/png.latex?%7B(x_i,%20y_i)%7D"> where <img src="https://latex.codecogs.com/png.latex?x_i"> is the image and <img src="https://latex.codecogs.com/png.latex?y_i"> is its true integer label, we can compute the total loss. Typically the overall loss <img src="https://latex.codecogs.com/png.latex?L"> is the average of the individual loss <img src="https://latex.codecogs.com/png.latex?L_i"> over all the training example. This loss function is our guide. It tells us how well our current classifier is performing. A high loss means our classifier is doing poorly; a low loss means it’s doing well. So we have a way to score images and a way to measure how good those score are. The next natural question are: how do we find the parameters <img src="https://latex.codecogs.com/png.latex?W"> that minimize this loss? And how do we make sure our model doesn’t just memorize the training data but actually learn to generalize? And that precisely where we’re headed today with <strong>Regularization and Optimization</strong> We’ll start by looking at regularization.</p>
<p>So far, here’s our loss function:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AL(W)%20=%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi%20=%201%7D%5EN%20L_i(f(x_i,%20W),%20y_i)%0A"></p>
<p>Currently our loss function just has the <strong>data loss</strong>. This term, the average of <img src="https://latex.codecogs.com/png.latex?L_i"> over our <img src="https://latex.codecogs.com/png.latex?N"> training examples, measure how well our model’s prediction, <img src="https://latex.codecogs.com/png.latex?f(x_i,%20W)">, match the true labels, <img src="https://latex.codecogs.com/png.latex?y_i"> on the training data. Our goal is to make this data loss small. However, if we only focus on minimizing the data loss, we can run into a problem called overfitting. Our model might become too good at fitting the training data, including its noise and then fail to generalize to new, unseen data.</p>
</section>
<section id="keeping-model-honest" class="level2">
<h2 class="anchored" data-anchor-id="keeping-model-honest">Keeping model honest</h2>
<p>This is where <strong>regularization</strong> comes in. We modify our loss function to include an additional term, often denoted as <img src="https://latex.codecogs.com/png.latex?R(W)">, which is the regularization penalty. So, our full loss function now become the sum of the data loss and this regularization term, scaled by a hyperparameter <strong>lambda (<img src="https://latex.codecogs.com/png.latex?%5Clambda">)</strong></p>
<p><img src="https://latex.codecogs.com/png.latex?%0AL(W)%20=%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi%20=%201%7D%5EN%20L_i(f(x_i,%20W),%20y_i)%20+%20%5Clambda%20R(W)%0A"></p>
<p>The data loss part as before, pushes the model to fit the training data. The new <strong>regularization term</strong>, <img src="https://latex.codecogs.com/png.latex?R(W)">, is designed to penalize model complexity. Its job is essentially to prevent the model from doing too well on the training data, or more precisely, fitting the training data in an overly complex way. Lambda, the regularization strength, control the trade-off between these two terms: fitting the data well versus keeping the model simple.</p>
<p>This preference for simpler models, as we discussed, is nicely encapsulated by <strong>Occam’s Razor</strong>. The principle, attributed to William of Ockham, states that among multiple competing hypotheses, the simplest one is generally the best. Our regularization term <img src="https://latex.codecogs.com/png.latex?R(W)"> is our way of mathematically encoding a preference for certain types of “simpler” weight matrices <img src="https://latex.codecogs.com/png.latex?W">.</p>
</section>
<section id="a-penalty-for-complexity" class="level2">
<h2 class="anchored" data-anchor-id="a-penalty-for-complexity">A penalty for complexity</h2>
<p>So what are some common form of this regularization term <img src="https://latex.codecogs.com/png.latex?R(W)">? Here are a few classic examples:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AR(W)%20=%20%5Csum_%7Bk%7D%20%5Csum_%7Bl%7D%20W%5E%7B2%7D_%7Bk,l%7D%0A"></p>
<p><strong>L2 regularization</strong>, also known as weight decay or Ridge regression in other contexts. Here, <img src="https://latex.codecogs.com/png.latex?R(W)"> is the sum of the squares of all the individual weight elements <img src="https://latex.codecogs.com/png.latex?W_%7Bk,l%7D">. This penalty discourages very large weights. It prefers to distribute weights among many features rather than having a few features with very large weights. It leads to diffuse, small weight vector</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AR(W)%20=%20%5Csum_%7Bk%7D%20%5Csum_%7Bl%7D%20%7CW_%7Bk,l%7D%7C%0A"></p>
<p><strong>L1 regularization</strong>, also known as Lasso. Here, R(W) is the sum of the absolute values of the weights. L1 regularization has an interesting property: it tends to produce sparse weight vectors, meaning many of the weights will become exactly zero. This can be useful for feature selection, as it effectively tells you which input features the model deems unimportant.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AR(W)%20=%20%5Csum_%7Bk%7D%20%5Csum_%7Bl%7D%20%5Cbeta%20W%5E%7B2%7D_%7Bk,l%7D%20+%20%7CW_%7Bk,l%7D%7C%0A"></p>
<p><strong>Elastic Net regularization</strong> is simply a linear combination of L1 and L2 regularization. This tries to get the best of both worlds, offering a balance between the diffuse weights of L2 and the sparsity of L1. The <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> here would be another hyperparameter controlling the mix.</p>
<p>These are some of the most common, traditional forms of regularization that directly penalize the magnitudes of the weights. But the concept of regularization is broader than just L1 and L2 penalty on weights. There are many other techniques, particularly in deep learning, that have a regularizing effect, even if they don’t explicitly appear as an R(W) term added to the loss. For examples,</p>
<ul>
<li><strong>Dropout</strong>: This involves randomly setting some neuron activations to zero during training. It prevents co-adaptation of neurons and encourages more robust feature learning</li>
<li><strong>Batch Normalization</strong>: This normalized the activations between mini-batch. While primarily introduced to help with optimization and training stability, it also has a slight regularizing effect.</li>
<li>And there are others like <strong>Stochastic Depth</strong>(randomly drop entire layer during training) or <strong>fractional pooling</strong>, which introduce stochasticity or constrains during training to improve regularization.</li>
</ul>
<p>So, while we often start by thinking about L1 or L2, keep in mind that regularization is a general concept of adding something to your training process to prevent overfitting and improve performance on unseen data.</p>
<p>Note that one of the reason that I think really interesting about why we regularize is to improve optimization by adding curvature. This is a more subtle point, especially relevant for L2 regularization. Sometimes, the loss landscape can be very flat in certain directions, making optimization difficult. Adding an L2 penalty (which is a quadratic bowl shape) can add curvature to the loss surface, potentially making it easier for optimization algorithms like gradient descent to find a good minimum.</p>
</section>
<section id="finding-the-bottom-of-the-valley" class="level2">
<h2 class="anchored" data-anchor-id="finding-the-bottom-of-the-valley">Finding the bottom of the valley</h2>
<p>So, we’ve defined what it means for a set of weights W to be “good”, it should result in a low values for the total loss <img src="https://latex.codecogs.com/png.latex?L">. The big question that remain is: <strong>How do we find the best <img src="https://latex.codecogs.com/png.latex?W"></strong>? This is not a trivial task, especially <img src="https://latex.codecogs.com/png.latex?W"> is consist of millions or even billions, of parameters in modern neural networks. We’re searching in an incredibly high-dimensional space. And this leads us to our major topic for today: <strong>Optimization</strong></p>
<p><a href="./images/valley.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2"><img src="https://bhdai.github.io/blog/posts/regularization_and_optimization/images/valley.png" class="img-fluid"></a></p>
<p>To build some intuition, let’s think about a simpler analogy. Imagine this beautiful mountain landscape. You can think of the terrain here as representing our loss function. The east-west and north-south directions could represent, say, two of our weights, W1 and W2 (though in reality, we have many more dimensions). And the altitude at any point (W1, W2) represents the value of our loss function L(W) for those particular weight settings. Our goal, then, is to find the lowest point in this valley – the point where the loss is minimized. So if we imagine ourselves, or this intrepid hiker, standing somewhere in the landscape, what’s our strategy for getting to the bottom of the valley? We’re trying to find the minimum. This is precisely what optimization algorithms are designed to do: to provide a systematic way to navigate this loss landscape and find a good set of parameters <img src="https://latex.codecogs.com/png.latex?W">. Now, how might we go about this? What strategy could we employ to find this minimum? Let’s consider a few approaches, starting with a very simple one, perhaps naive, one.</p>
<section id="strategy-1-random-search" class="level3">
<h3 class="anchored" data-anchor-id="strategy-1-random-search">Strategy 1: Random search</h3>
<p>This is generally a very bad idea for optimizing a complex models, but it’s a simple baseline to start with. The idea is straightforward</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># assume X_train is the data where each column is an example (e.g. 3073 x 50,000)</span></span>
<span id="cb1-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># assume Y_train are the labels (e.g. 1D array of 50,000)</span></span>
<span id="cb1-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># assume the function L evaluates the loss function</span></span>
<span id="cb1-4"></span>
<span id="cb1-5">bestloss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'inf'</span>)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Python assigns the highest possible float value</span></span>
<span id="cb1-6"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> num <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">xrange</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>):</span>
<span id="cb1-7">    W <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.randn(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3073</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.0001</span>  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># generate random parameters</span></span>
<span id="cb1-8">    loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> L(X_train, Y_train, W)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># get the loss over the entire training set</span></span>
<span id="cb1-9">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> bestloss:  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># keep track of the best solution</span></span>
<span id="cb1-10">        bestloss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> loss</span>
<span id="cb1-11">        bestW <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> W</span>
<span id="cb1-12">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'in attempt </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%d</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;"> the loss was </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%f</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">, best </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%f</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span> (num, loss, bestloss))</span></code></pre></div>
<pre><code>in attempt 0 the loss was 9.401632, best 9.401632
in attempt 1 the loss was 8.959668, best 8.959668
in attempt 2 the loss was 9.044034, best 8.959668
in attempt 3 the loss was 9.278948, best 8.959668
in attempt 4 the loss was 8.857370, best 8.857370
in attempt 5 the loss was 8.943151, best 8.857370
in attempt 6 the loss was 8.605604, best 8.605604
... (truncated: continues for 1000 lines)</code></pre>
<p>As you can see from the example printout, with each attempt, we got a loss value. Sometimes we find a better <img src="https://latex.codecogs.com/png.latex?W">, sometimes we don’t. We just keep trying random configurations and hope to stumble upon a good one. This is, as you might imagine, highly inefficient. The space of possible W matrices is astronomically vast. Just randomly sampling points in this space is like trying to find a specific grain of sand on all the beaches of the world by randomly picking up grains. But, let’s humor ourselves. Suppose we run this random search for a while and get our bestW. How well does it actually perform on unseen data?</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># assume X_test is [3073 x 10000A], y_test [10000 x 1]</span></span>
<span id="cb3-2">scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> bestW.dot(X_test) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 10 x 10000, the class scores for all test examples</span></span>
<span id="cb3-3">y_pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.argmax(scores, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb3-4">np.mean(y_pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> y_test)</span></code></pre></div>
<pre><code>0.1555</code></pre>
<p>And the result? We get 15.5% accuracy! Now, for a 10-class classification problem like CIFAR-10, random guessing would give you 10% accuracy. So, 15.5% is… well, it’s better than random! One might sarcastically say “not bad!” given the simplicity of the approach. However, if we compare this to the state-of-the-art (SOTA) for image classification on datasets like CIFAR-10, which can be well above 90%, even approaching 99.7% for sophisticated models, 15.5% is clearly terrible. It highlights that simply guessing random parameters is not a viable strategy for training effective machine learning models. So, random search is a non-starter for any serious application. We need a more intelligent way to navigate the loss landscape. We need a strategy that uses information about the landscape itself to guide the search. This brings us to our next, much more sensible, strategy. We want to “follow the slope.”</p>
</section>
<section id="strategy-2-follow-the-slope" class="level3">
<h3 class="anchored" data-anchor-id="strategy-2-follow-the-slope">Strategy 2: Follow the slope</h3>
<p>Going back to our mountain analogy, if you’re standing on a hillside and want to get to the bottom of the valley, what’s the most intuitive thing to do? You look around, feel for the direction where the ground slopes downwards most steeply, and you take a step in that direction. Right? You follow the path of steepest descent. This intuitive idea of “slope” has a precise mathematical counterpart.</p>
<p>In 1-dimension, if we have a function <img src="https://latex.codecogs.com/png.latex?f(x)">, the concept of slope is captured by derivative <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bdf%7D%7Bdx%7D">. As you’ll recall from calculus, the derivative is defined as</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7Bdf(x)%7D%7Bdx%7D%20=%20%5Clim_%7Bh%20%5Cto%200%7D%20%5Cfrac%7Bf(x%20+%20h)%20-%20f(x)%7D%7Bh%7D%0A"></p>
<p>It tells us how much the function <img src="https://latex.codecogs.com/png.latex?f(x)"> changes for an infinitesimally small change in <img src="https://latex.codecogs.com/png.latex?x">. A positive derivative means the function is increasing, and a negative derivative means it’s decreasing. Now our loss function <img src="https://latex.codecogs.com/png.latex?L(W)"> is not a function of a single variable; it’s a function of many variables (all the elements of our weight matrix <img src="https://latex.codecogs.com/png.latex?W">). So, we’re in a multiple-dimension scenario. In this case, the equivalent of the derivative is the <strong>gradient</strong>. The gradient of <img src="https://latex.codecogs.com/png.latex?L"> with respect to <img src="https://latex.codecogs.com/png.latex?W"> often denoted as <img src="https://latex.codecogs.com/png.latex?%5Cnabla%20L(W)"> or <img src="https://latex.codecogs.com/png.latex?%5Cnabla_W%20L">, is a <strong>vector of partial derivatives</strong>. Each elements of this gradient vector tells us the slope of the loss function along the direction of the corresponding weight. For example <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cnabla%20L%7D%7B%5Cnabla%20W_%7Bij%7D%7D"> tells us how the loss <img src="https://latex.codecogs.com/png.latex?L"> change if we make a tiny change to the weight <img src="https://latex.codecogs.com/png.latex?W_%7Bij%7D">, keeping other weights constant.</p>
<p>So, the gradient vector <img src="https://latex.codecogs.com/png.latex?%5Cnabla%20L(W)"> points in the direction in which the loss function L(W) increases the fastest. How do we find the slope in any arbitrary direction? If we have some direction vector <img src="https://latex.codecogs.com/png.latex?u">, the slope in that direction is given by the dot product of that direction vector <img src="https://latex.codecogs.com/png.latex?u"> with the gradient vector <img src="https://latex.codecogs.com/png.latex?%5Cnabla%20L(W)">. And most importantly for our goal of minimizing the loss: the direction of steepest descent – the direction in which the loss function decreases the fastest it’s simple but happens to be true is the negative gradient, i.e., <img src="https://latex.codecogs.com/png.latex?-%5Cnabla%20L(W)">. So, if we can compute this gradient vector, we have a clear instruction: to reduce the loss, take a small step in the direction opposite to the gradient. This is the fundamental idea behind one of the most important optimization algorithms in machine learning.</p>
<p>This method is called the <strong>Numeric Gradient</strong>. And it has a couple of important properties: 1) It’s <strong>Slow</strong>! We need to loop over all dimensions (all parameters) and perform a full loss computation for each one. For models with millions of parameters, this is completely impractical for training. 2) It’s <strong>Approximate</strong>. Because we’re using a finite h instead of the true limit as h approaches zero, what we calculate is an approximation of the true gradient. The choice of h is also a bit tricky – too small and you hit numerical precision issues; too large and the approximation is poor. So, while the numeric gradient is conceptually simple and directly follows from the definition of a derivative, it’s not how we typically train our models due to its inefficiency. However, it’s an incredibly useful tool for debugging our more efficient gradient calculation methods, which we’ll get to. If you implement a more complex way to calculate the gradient (like backpropagation), you can compare its output to the numeric gradient on a small example to check if your implementation is correct. This is called a gradient check. But for actually doing the optimization, we need something much faster. The loss function <img src="https://latex.codecogs.com/png.latex?L(W)"> is, after all, just a mathematical function of <img src="https://latex.codecogs.com/png.latex?W">. Can’t we use calculus to find an exact, analytical expression for the gradient?</p>
<p>The key insight is that our <strong>loss function <img src="https://latex.codecogs.com/png.latex?L"> is just a function of <img src="https://latex.codecogs.com/png.latex?W"></strong></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0AL%20%20%20&amp;=%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi=1%7D%5E%7BN%7D%20L_i%20+%20%5Csum_%7Bk%7D%20W_%7Bk%7D%5E2%20%5C%5C%0AL_i%20&amp;=%20%5Csum_%7Bi%20%5Cneq%20y_i%7D%20%5Cmax(0,%20s_j%20-%20s_%7By_i%7D%20+%201)%20%5Ctext%7B%20or%20%7D%20L_i%20=%20-%5Clog(%5Cfrac%7Be%5E%7Bs_%7By_i%7D%7D%7D%7B%5Csum_%7Bj%7De%5E%7Bs_j%7D%7D)%20%5C%5C%0As%20%20&amp;=%20f(x,W)%20=%20Wx%20%5C%5C%0A&amp;%5Ctext%7B%20want%20%7D%20%5Cnabla_W%20L%0A%5Cend%7Balign%7D%0A"></p>
<p>So you can see <img src="https://latex.codecogs.com/png.latex?L"> ultimately a mathematical function of <img src="https://latex.codecogs.com/png.latex?W">, the input <img src="https://latex.codecogs.com/png.latex?x"> and the true label <img src="https://latex.codecogs.com/png.latex?y_i"> are fix constant from our dataset, the only thing we’re changing is <img src="https://latex.codecogs.com/png.latex?W">. What we want is the gradient of this entire expression L with respect to <img src="https://latex.codecogs.com/png.latex?W"> denoted as <img src="https://latex.codecogs.com/png.latex?%5Cnabla_W%20L">. Instead of numerically approximating this gradient by adjusting each <img src="https://latex.codecogs.com/png.latex?W_k">, we should be able to use the rules of calculus.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./images/calculus.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Newton and Leibniz"><img src="https://bhdai.github.io/blog/posts/regularization_and_optimization/images/calculus.png" class="img-fluid figure-img" alt="Newton and Leibniz"></a></p>
<figcaption>Newton and Leibniz</figcaption>
</figure>
</div>
<p>Given that we have these well-defined mathematical functions, we can <strong>use calculus to compute an analytic gradient</strong>. This means we can derive a direct mathematical formula for <img src="https://latex.codecogs.com/png.latex?%5Cnabla_W">. This is where our friend Newton and Leibniz, the inventors of calculus, come into play. Their work allow us to find these gradient efficiently and exactly (up to machine precision), without any of this iteration perturbation. Computing the analytic gradient involves applying the chain rule repeatedly, because our loss function is a composition of several functions (linear score function, then the loss calculation like hinge or softmax, then averaging, then adding regularization). For complex, deep neural networks, this process of applying the chain rule systematically is known as backpropagation which we will talk about very soon in other post. For now, let’s appreciate that deriving an analytic formula for the gradient is the efficient and exact way to go. It’s much faster than the numerical gradient because we just evaluate one formula, rather than N+1 evaluations of the loss function. So now instead of numerically calculating <img src="https://latex.codecogs.com/png.latex?dW"> element by element, if we had derived the analytic gradient, <img src="https://latex.codecogs.com/png.latex?dW"> would be given by some function that depends on our input data and the current <img src="https://latex.codecogs.com/png.latex?W">. We would plug our data and current <img src="https://latex.codecogs.com/png.latex?W"> into this derived formula, and it would directly give us the entire gradient matrix <img src="https://latex.codecogs.com/png.latex?dW"> in one go.</p>
<p>So we have two ways to think about computing gradient:</p>
<ul>
<li><strong>Numerical gradient</strong>:
<ul>
<li>It’s <strong>approximate</strong> (due to the finite h).</li>
<li>It’s very <strong>slow</strong> (requires N+1 evaluations of the loss function for N parameters).</li>
<li>However, it’s generally <strong>easy to write</strong> the code for, as it directly implements the definition of a derivative.</li>
</ul></li>
<li><strong>Analytic gradient</strong>:
<ul>
<li>It’s <strong>exact</strong> (up to machine precision).</li>
<li>It’s very <strong>fast</strong> (typically a single pass of computation once the formula is derived).</li>
<li>However, deriving and implementing the formula, especially for complex models, can be <strong>error-prone</strong>. It’s easy to make a mistake in the calculus or in the code.</li>
</ul></li>
</ul>
<p>So, what does this lead us to in practice? <strong>In practice: Always use the analytic gradient for training your models because of its speed and exactness.</strong> However, because it’s error-prone to implement, you should <strong>check your implementation with the numerical gradient</strong>. This crucial debugging step is called a gradient check. How does gradient check work? You implement your analytic gradient. Then for a small test case like a small <img src="https://latex.codecogs.com/png.latex?W"> and a few data point, you also compute the numerical gradient. You then compare the two results. If they are very close, you can be reasonably confident that your analytic gradient implementation is correct. If they differ significantly, you have a bug in your analytic gradient derivation or code.</p>
</section>
</section>
<section id="sgd" class="level2">
<h2 class="anchored" data-anchor-id="sgd">SGD</h2>
<p>Okay, so now we have an efficient and exact way to compute the gradient <img src="https://latex.codecogs.com/png.latex?%5Cnabla_W%20L">, which tells us the direction of steepest ascent. To minimize the loss, we want to go in the opposite direction. This brings us to the core algorithm for optimization in deep learning: Gradient Descent. The core idea is remarkably simple, here is a simple implementation of “Vanilla Gradient Decent”.</p>
<div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">while</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>:</span>
<span id="cb5-2">  weights_grad <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> evaluate_gradient(loss_fun, data, weights)</span>
<span id="cb5-3">  weights <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> step_size <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> weights_grad <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># perform parameter update</span></span></code></pre></div>
<p>So basically we start with some initial guess for our weights, then we enter a loop that (conceptually) runs “True” or until some stopping criterion is met. Inside the loop, the first crucial step is <code>weights_grad = evaluate_gradient(loss_fun, data, weights)</code>. This is where we compute the analytic gradient of our loss function (loss_fun) with respect to the current weights, using our training data. This weights_grad is our <img src="https://latex.codecogs.com/png.latex?%5Cnabla_W%20L">. Next is parameter update, we take the weights_grad multiply it by - step_size the negative sign is because we want to move in the direction opposite to the gradient (the direction of steepest descent). The step_size (also known as the learning rate) is a small positive scalar hyperparameter. It controls how far we step in that negative gradient direction. If it’s too large, we might overshoot the minimum. If it’s too small, training will be very slow. We then add this scaled negative gradient to our current weights to get the updated weights. And we repeat. We re-evaluate the gradient at the new weights, take another step, and so on, iteratively moving “downhill” on the loss surface.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./images/gd.gif" class="lightbox" data-gallery="quarto-lightbox-gallery-4"><img src="https://bhdai.github.io/blog/posts/regularization_and_optimization/images/gd.gif" class="img-fluid quarto-figure quarto-figure-center figure-img"></a></p>
</figure>
</div>
<p>This is Vanilla Gradient Descent. The “vanilla” part implies there are more sophisticated variants, which we’ll get to. One important detail in this vanilla version is that <code>evaluate_gradient</code> typically involves computing the gradient over the entire training dataset to get the true gradient of <img src="https://latex.codecogs.com/png.latex?L">. This can be very computationally expensive if our dataset is large. This brings us to <strong>Stochastic Gradient Descent (SGD)</strong>, or more commonly in practice, <strong>Minibatch Gradient Descent</strong>.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0AL(W)%20&amp;=%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi=1%7D%5E%7BN%7D%20L_i(x_i,%20y_i,%20W)%20+%20%5Clambda%20R(W)%5C%5C%0A%5Cnabla_W%20L%20&amp;=%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi%20=%201%7D%5EN%20%5Cnabla_W%20L_i(x_i,%20y_i,%20W)%20+%20%5Clambda%20%5Cnabla_W%20R(W)%0A%5Cend%7Balign%7D%0A"></p>
<p>The problem is that computing this full sum over all N examples for <img src="https://latex.codecogs.com/png.latex?%5Cnabla_W%20L_i"> is expensive when <img src="https://latex.codecogs.com/png.latex?N"> is large. If you have millions of training examples, calculating the gradient across all of them just to make one tiny step with your weights is very inefficient. You’d be spending a lot of computation for a potentially very small update. The core idea of Stochastic Gradient Descent is to <strong>approximate this sum using a small random subset of the data called a minibatch</strong>. Instead of summing the gradients over all N examples, we sum them over just a small batch of, say, 32, 64, or 128 examples. Common minibatch sizes often range from 32 to 256, sometimes larger, depending on memory constraints and the specific problem. So, the gradient we compute on a minibatch is not the true gradient of the total loss L, but it’s an estimate or an approximation. Since the minibatch is sampled randomly, this estimate is unbiased on average, and it’s much, much faster to compute.</p>
<div class="sourceCode" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Vanilla Minibatch Gradient Descent</span></span>
<span id="cb6-2"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">while</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>:</span>
<span id="cb6-3">  data_batch <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sample_training_data(data, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">256</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># sample 256 examples</span></span>
<span id="cb6-4">  weights_grad <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> evaluate_gradient(loss_fun, data_batch, weights)</span>
<span id="cb6-5">  weights <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> step_size <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> weights_grad <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># perform parameter update</span></span></code></pre></div>
<p>The key difference from Vanilla Gradient Descent is the first line inside the loop: <code>data_batch = sample_training_data(data, 256)</code>. Here, instead of using the full data, we randomly sample a data_batch (e.g., 256 examples). This approach is “stochastic” because each gradient estimate is noisy and depends on the specific random minibatch chosen. However, by taking many such noisy steps, we still tend to move towards the minimum of the loss function, often much faster in terms of wall-clock time than full batch gradient descent because each step is so much cheaper. The term “Stochastic Gradient Descent (SGD)” technically refers to the extreme case where the minibatch size is 1 – you update the weights after seeing just one example. In practice, when people say “SGD” in deep learning, they almost always mean minibatch gradient descent. Using minibatches not only speeds up training but can also sometimes help the optimization process escape poor local minima or saddle points due to the noise in the gradient estimates, leading to better generalization.</p>
<p>So, SGD with minibatches is the workhorse for training almost all large-scale deep learning models. However, it’s not without its own set of challenges and nuances.</p>
<section id="problem-1-with-sgd-dealing-with-ill-conditioned-loss-landscapes" class="level3">
<h3 class="anchored" data-anchor-id="problem-1-with-sgd-dealing-with-ill-conditioned-loss-landscapes">Problem #1 with SGD: Dealing with ill-conditioned loss landscapes</h3>
<p><a href="./images/sgd.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5"><img src="https://bhdai.github.io/blog/posts/regularization_and_optimization/images/sgd.png" class="img-fluid"></a></p>
<p>Consider a scenario where our loss function landscape looks like this: a long, narrow valley, or a ravine. The ellipses here represent level sets of the loss function, and the smiley face is at the minimum we want to reach. The question is: <strong>What if the loss changes quickly in one direction and slowly in another?</strong> For example, along the w2 direction (vertically), the valley is very steep – the loss changes rapidly. But along the w1 direction (horizontally), the valley is very shallow and elongated – the loss changes slowly. If we start at the red dot and apply standard SGD, what will happen? What gradient descent does is it always points in the direction of steepest descent. In such a ravine, the steepest direction is mostly perpendicular to the valley floor, pointing across the steep walls. So, SGD will tend to take large steps that oscillate back and forth across the narrow valley (the w2 direction, where the gradient is large). Because of these large oscillations, if we use a single learning rate, it has to be small enough to prevent divergence in this steep direction. However, this small learning rate means that progress along the shallow direction of the valley (the w1 direction, where the gradient component is small) will be agonizingly slow. The result is very slow progress along the shallow dimension, and a lot of jitter or oscillation along the steep direction, as shown by the red zigzag path. We’re making very inefficient progress towards the true minimum. This kind of behavior is characteristic of loss functions that are ill-conditioned.</p>
</section>
<section id="problem-2-with-sgd-local-minima-and-saddle-points" class="level3">
<h3 class="anchored" data-anchor-id="problem-2-with-sgd-local-minima-and-saddle-points">Problem #2 with SGD: Local Minima and Saddle Points</h3>
<p>The loss functions for deep neural networks are highly non-convex. This means they can have many local minima – points where the loss is lower than all its immediate neighbors, but not necessarily the global minimum (the lowest possible loss overall). They can also have saddle points.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./images/saddlepoint.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="Saddle point, image source: https://en.wikipedia.org/wiki/Saddle_point"><img src="https://bhdai.github.io/blog/posts/regularization_and_optimization/images/saddlepoint.png" class="img-fluid figure-img" alt="Saddle point, image source: https://en.wikipedia.org/wiki/Saddle_point"></a></p>
<figcaption>Saddle point, image source: <a href="https://en.wikipedia.org/wiki/Saddle_point">https://en.wikipedia.org/wiki/Saddle_point</a></figcaption>
</figure>
</div>
<p>At a local minimum, or at a saddle point, the gradient is zero (or very close to zero). If the gradient is zero, then weights_grad in our update <code>rule weights += - step_size * weights_grad</code> is zero. This means the weights stop updating. Gradient descent gets stuck! It thinks it has found the bottom of a valley, but it might just be a small dip, or a point that’s flat in some directions but goes up in others.</p>
<p>Now, for a long time, people were very concerned about local minima being a major impediment to training deep networks. The fear was that SGD would get trapped in a poor local minimum and never find a good solution. However, more recent research, like <a href="https://arxiv.org/abs/1406.2572">the paper</a> by Dauphin et al.&nbsp;(2014) cited here, suggests that in very high-dimensional spaces (which is where our weight matrices W live), <strong>saddle points are actually much more common than local minima</strong>. For a point to be a local minimum, the loss function needs to curve upwards in all dimensions around that point. For it to be a local maximum, it needs to curve downwards in all dimensions. For it to be a saddle point, it needs to curve upwards in some dimensions and downwards in others. In high dimensions, it’s statistically much more likely to have a mix of curvatures than for all curvatures to go in the same direction. So, while getting stuck is a problem, it’s often saddle points, rather than bad local minima, that are the primary culprits for slowing down or halting SGD. At a saddle point, the gradient is zero, but it’s not a minimum.</p>
<p>So, that’s the second major issue: points with zero or near-zero gradient (local minima and, more commonly, saddle points) can trap or significantly slow down SGD.</p>
</section>
<section id="problem-3-with-sgd-noisy-gradients" class="level3">
<h3 class="anchored" data-anchor-id="problem-3-with-sgd-noisy-gradients">Problem #3 with SGD: Noisy Gradients</h3>
<p>This problem is inherent in the “S” of SGD – the stochasticity. As we discussed, our loss <img src="https://latex.codecogs.com/png.latex?L(W)"> is an average over all <img src="https://latex.codecogs.com/png.latex?N"> training examples (plus regularization, which we’ll ignore for this point for simplicity). And it’s true gradient is:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cnabla_W%20L%20=%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi%20=%201%7D%5EN%20%5Cnabla_W%20L_i(x_i,%20y_i,%20W)%0A"></p>
<p>However, with minibatch SGD, we don’t compute this full sum. We compute the gradient on a small minibatch. This minibatch gradient is an estimate of the true gradient. Because the minibatch is a random sample, this estimate will be noisy. It won’t point exactly in the direction of the true steepest descent. If you take many different minibatches at the same point <img src="https://latex.codecogs.com/png.latex?W">, you’ll get slightly different gradient vectors.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./images/noisy-grad.gif" class="lightbox" data-gallery="quarto-lightbox-gallery-7"><img src="https://bhdai.github.io/blog/posts/regularization_and_optimization/images/noisy-grad.gif" class="img-fluid quarto-figure quarto-figure-center figure-img"></a></p>
</figure>
</div>
<p>The noisy gradients mean that our steps won’t be as smooth as the idealized animation we saw earlier. They’ll be a bit erratic. While this noise can sometimes be helpful (e.g., for escaping sharp local minima or some saddle points), it also means that the convergence path can be jittery, and finding the exact minimum can be harder. The optimization path might dance around the minimum without settling perfectly.</p>
<p>So, these three problems – ill-conditioned landscapes (ravines), local minima/saddle points, and noisy gradients – are key challenges that standard SGD faces. This has motivated the development of more advanced optimization algorithms that try to mitigate these issues. One of the first and most important enhancements is the idea of momentum.</p>
</section>
</section>
<section id="momentum" class="level2">
<h2 class="anchored" data-anchor-id="momentum">Momentum</h2>
<p>The first major improvement we’ll discuss is adding Momentum to SGD. If SGD gets to a flat region where the gradient is zero, it stops. Momentum, like a ball rolling downhill, has inertia. It might be able to “roll through” small local minima or flat regions of saddle points because it has built up speed from previous gradients. In that zigzagging scenario, the gradient components across the ravine tend to cancel out on average over iterations due to the oscillation. However, the small gradient components along the valley consistently point in the same direction. Momentum helps to average out these oscillations and amplify the consistent movement along the valley floor. You can see the blue line (SGD+Momentum) makes much more direct progress towards the minimum compared to the black zigzagging SGD line. The noise in minibatch gradients causes SGD to jitter. Momentum helps to smooth out these noisy updates by taking a running average of the gradients. This leads to a more stable and often faster convergence towards the minimum</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./images/momentum.gif" class="lightbox" data-gallery="quarto-lightbox-gallery-8"><img src="https://bhdai.github.io/blog/posts/regularization_and_optimization/images/momentum.gif" class="img-fluid quarto-figure quarto-figure-center figure-img"></a></p>
</figure>
</div>
<p>Now, for SGD + Momentum. The key idea is to continue moving in the general direction as the previous iterations. We introduce a “velocity” term, v, which accumulates a running mean of the gradients.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0Av_%7Bt+1%7D%20&amp;=%20%20%5Crho%20v_t%20+%20%5Cnabla%20f(x_t)%20%5C%5C%0Ax_%7Bt+1%7D%20&amp;=%20x_t%20-%20%5Calpha%20v_%7Bt+1%7D%0A%5Cend%7Balign%7D%0A"></p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?v_%7Bt%7D"> is the velocity vector from the previous step. It’s initialized to zero.</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Crho"> is the momentum coefficient (or friction term). It’s typically a value like 0.9 or 0.99. It determines how much of the previous velocity is retained. If ρ=0, we recover standard SGD (almost, as we’ll see the learning rate is applied differently).</li>
<li>In the first equation, we update the velocity: we take the old velocity <img src="https://latex.codecogs.com/png.latex?v_t">, scale it down by <img src="https://latex.codecogs.com/png.latex?%5Crho">, and then add the current gradient <img src="https://latex.codecogs.com/png.latex?%5Cnabla%20f(x_t)">. So, the velocity v is essentially an exponentially decaying moving average of the past gradients.</li>
<li>In the second equation, we update the parameters <img src="https://latex.codecogs.com/png.latex?x"> by taking a step in the direction of this new velocity <img src="https://latex.codecogs.com/png.latex?v_t+1">, scaled by the learning rate <img src="https://latex.codecogs.com/png.latex?%5Calpha">. Crucially, we are now stepping with the velocity, not directly with the current gradient.</li>
</ul>
<p>The intuition is that we build up “velocity” as a running mean of gradients. If gradients consistently point in the same direction, the velocity in that direction grows. If gradients oscillate, those components of the velocity tend to be dampened. The term <img src="https://latex.codecogs.com/png.latex?%5Crho"> acts like friction. If <img src="https://latex.codecogs.com/png.latex?%5Crho"> is close to 1 (e.g., 0.99), we have high momentum, and the velocity persists for a long time. If <img src="https://latex.codecogs.com/png.latex?%5Crho"> is smaller (e.g., 0.5), it’s like having more friction, and the velocity relies more on recent gradients. <a href="https://proceedings.mlr.press/v28/sutskever13.pdf">The paper</a> by Sutskever et al.&nbsp;(2013) is a classic reference highlighting the importance of momentum in deep learning</p>
<p>Here’s how this might look in code:</p>
<div class="sourceCode" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">vx <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span> <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># initialize velocity to zero</span></span>
<span id="cb7-2"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">while</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>:</span>
<span id="cb7-3">  dx <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> compute_gradient(x) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># current gradient</span></span>
<span id="cb7-4">  vx <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> rho <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> vx <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> dx       <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># update velocity</span></span>
<span id="cb7-5">  x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-=</span> learning_rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> vx  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># update parameters</span></span></code></pre></div>
<p>Momentum is a very powerful and widely used technique. But it’s not the only improvement over vanilla SGD. Other methods try to adapt the learning rate on a per-parameter basis, which can be particularly helpful for those ill-conditioned ravine-like loss surfaces.</p>
</section>
<section id="smarter-steps-with-adaptive-optimizer" class="level2">
<h2 class="anchored" data-anchor-id="smarter-steps-with-adaptive-optimizer">Smarter steps with adaptive optimizer</h2>
<p>Okay, so SGD with Momentum helps to accelerate in consistent directions and dampen oscillations. But what about that problem of different curvatures in different dimensions – the ravines? Momentum helps, but can we do even better? This leads us to optimizers that try to adapt the learning rate on a per-parameter basis. One popular example is <strong>RMSProp</strong>. Recall SGD+Momentum: we compute a velocity <code>vx</code> and update <code>x</code> using <code>learning_rate * vx</code>. Now, for RMSProp, developed by Tieleman and Hinton:</p>
<div class="sourceCode" id="annotated-cell-8" style="background: #f1f3f5;"><pre class="sourceCode python code-annotation-code code-with-copy code-annotated"><code class="sourceCode python"><a class="code-annotation-anchor" data-target-cell="annotated-cell-8" data-target-annotation="1" onclick="event.preventDefault();">1</a><span id="annotated-cell-8-1" class="code-annotation-target">grad_square <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="annotated-cell-8-2"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">while</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>:</span>
<span id="annotated-cell-8-3">  dx <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> compute_gradient(x)</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-8" data-target-annotation="2" onclick="event.preventDefault();">2</a><span id="annotated-cell-8-4" class="code-annotation-target">  grad_square <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> decay_rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> grad_square <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> decay_rate) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> dx <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> dx</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-8" data-target-annotation="3" onclick="event.preventDefault();">3</a><span id="annotated-cell-8-5" class="code-annotation-target">  x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-=</span> learning_rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> dx <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> (np.sqrt(grad_squared) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-7</span>)</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code></pre></div>
<dl class="code-annotation-container-grid">
<dt data-target-cell="annotated-cell-8" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-8" data-code-lines="1" data-code-annotation="1">We maintain a variable <code>grad_squared</code>. This variable will keep track of an exponentially decaying average of the squared gradients for each parameter. It’s initialized to zero.</span>
</dd>
<dt data-target-cell="annotated-cell-8" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-8" data-code-lines="4" data-code-annotation="2">This is an exponentially weighted moving average. <code>decay_rate</code> is a hyperparameter, typically something like 0.9 or 0.99. We’re taking the previous <code>grad_squared</code>, decaying it, and adding the newly computed <code>dx * dx</code> (element-wise square of the current gradient), scaled by <code>1 - decay_rate</code>. So, <code>grad_squared</code> accumulates information about the typical magnitude of recent gradients for each parameter.</span>
</dd>
<dt data-target-cell="annotated-cell-8" data-target-annotation="3">3</dt>
<dd>
<span data-code-cell="annotated-cell-8" data-code-lines="5" data-code-annotation="3">this is the parameter update rule, we take our square root of our accumulated <code>grad_squared</code> this gives us something like the Root Mean Square of recent gradients, add a small epsilon (like 1e-7) for numerical stability to prevent division by zero if grad_squared is tiny, and then we divide the current gradient <code>dx</code> by this term.</span>
</dd>
</dl>
<p>The effect of this division by <code>np.sqrt(grad_squared)</code> is crucial. If a particular parameter has consistently had large gradients in the past (meaning its corresponding element in <code>grad_squared</code> is large), then <code>np.sqrt(grad_squared)</code> will be large, and the effective step size for that parameter <code>learning_rate / (np.sqrt(grad_squared) + epsilon)</code> will be smaller. Conversely, if a parameter has consistently had small gradients (so grad_squared for it is small), then <code>np.sqrt(grad_squared)</code> will be small, and its effective step size will be larger. This effectively gives us <strong>per-parameter learning rates</strong> or <strong>adaptive learning rates.</strong> The learning rate is adapted for each parameter based on the history of its gradient magnitudes.</p>
<p><strong>What happens with RMSProp?</strong> Specifically, how does this help with that ravine problem? What happens is that progress along “steep” directions is damped, and progress along “flat” directions is accelerated. Think back to our ravie. In the steep direction (e.g., <code>w2</code>), the gradients dx are large. This means <code>dx * dx</code> will be large, and <code>grad_squared</code> will accumulate to a large value for that dimension. Consequently, when we divide <code>dx</code> by <code>np.sqrt(grad_squared)</code>, we significantly reduce the step size in that steep direction. This helps to prevent the oscillations. In the flat, shallow direction (e.g., <code>w1</code>), the gradients <code>dx</code> are small. <code>dx * dx</code> will be small, and <code>grad_squared</code> will be small for that dimension. When we divide <code>dx</code> by a small <code>np.sqrt(grad_squared)</code>, the effective step size for that direction is relatively larger (or at least not as suppressed). This helps to make faster progress along the shallow valley. So, RMSProp automatically adjusts the learning rate for each parameter, making bigger steps in directions where gradients have been small and smaller steps in directions where gradients have been large.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./images/RMSProp.gif" class="lightbox" data-gallery="quarto-lightbox-gallery-9" title="blue line: RMSProp, red line: SGD+Momentum, black line: SGD"><img src="https://bhdai.github.io/blog/posts/regularization_and_optimization/images/RMSProp.gif" class="img-fluid quarto-figure quarto-figure-center figure-img" alt="blue line: RMSProp, red line: SGD+Momentum, black line: SGD"></a></p>
</figure>
</div>
<figcaption>blue line: RMSProp, red line: SGD+Momentum, black line: SGD</figcaption>
</figure>
</div>
<p>RMSProp is a very effective optimizer and was a popular choice for quite some time. It directly addresses the issue of different learning rates being needed for different parameter dimensions. Now, what if we could combine the benefits of momentum with the adaptive learning rates of RMSProp? That leads us to <strong>Adam</strong>.</p>
<p>Here’s a look at the core of the Adam optimizer, presented as “Adam (almost)” because it’s missing one small but important detail we’ll get to. This was introduced by Kingma and Ba in their 2015 ICLR paper.</p>
<div class="sourceCode" id="annotated-cell-9" style="background: #f1f3f5;"><pre class="sourceCode python code-annotation-code code-with-copy code-annotated"><code class="sourceCode python"><span id="annotated-cell-9-1">first_moment <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="annotated-cell-9-2">second_moment <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="annotated-cell-9-3"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">while</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>:</span>
<span id="annotated-cell-9-4">  dx <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> compute_gradient(x)</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-9" data-target-annotation="1" onclick="event.preventDefault();">1</a><span id="annotated-cell-9-5" class="code-annotation-target">  first_moment <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> beta1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> first_moment <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> beta1) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> dx</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-9" data-target-annotation="2" onclick="event.preventDefault();">2</a><span id="annotated-cell-9-6" class="code-annotation-target">  second_moment <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> beta2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> second_moment <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> beta2) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> dx <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> dx</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-9" data-target-annotation="3" onclick="event.preventDefault();">3</a><span id="annotated-cell-9-7" class="code-annotation-target">  x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-=</span> learning_rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> first_moment <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> (np.sqrt(second_moment) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-7</span>)</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code></pre></div>
<dl class="code-annotation-container-grid">
<dt data-target-cell="annotated-cell-9" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-9" data-code-lines="5" data-code-annotation="1">This is an exponentially decaying moving average of the gradient itself. <code>beta1</code> is a decay rate (e.g., 0.9). This looks very much like the velocity update in SGD+Momentum! It’s capturing the “momentum” aspect which is an estimate of the mean of the gradients.</span>
</dd>
<dt data-target-cell="annotated-cell-9" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-9" data-code-lines="6" data-code-annotation="2">This is an exponentially decaying moving average of the squared gradients. <code>beta2</code> is another decay rate (e.g., 0.999). This looks exactly like the <code>grad_squared</code> update in RMSProp! It’s capturing information about the variance (or uncentered second moment) of the gradients.</span>
</dd>
<dt data-target-cell="annotated-cell-9" data-target-annotation="3">3</dt>
<dd>
<span data-code-cell="annotated-cell-9" data-code-lines="7" data-code-annotation="3">Here, we’re using the <code>first_moment</code> (our momentum term) in the numerator, and we’re dividing by the square root of the <code>second_moment</code> (our RMSProp-like adaptive scaling term) in the denominator.</span>
</dd>
</dl>
<p>It’s <strong>sort of like RMSProp with momentum</strong>. It’s trying to get the best of both worlds: the acceleration benefits of momentum and the per-parameter adaptive learning rates. Remember, <code>first_moment</code> and <code>second_moment</code> are initialized to zero. What’s the implication of this?</p>
<p>At the very first timestep (and for the first few timesteps), because <code>first_moment</code> and <code>second_moment</code> start at zero and the decay rates <code>beta1</code> and <code>beta2</code> are typically close to 1 (e.g., 0.9, 0.999), these moving average estimates will be biased towards zero. They haven’t had enough gradient information to “warm up” to their true values. This can cause the initial steps to be undesirably small. This is where the “almost” part is resolved. The full form of Adam includes a bias correction step to address this initialization bias.</p>
<div class="sourceCode" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1">first_moment <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb8-2">second_moment <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb8-3"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> t <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, num_iterations): <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># t is the timestep</span></span>
<span id="cb8-4">  dx <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> compute_gradient(x)</span>
<span id="cb8-5">  first_moment <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> beta1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> first_moment <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> beta1) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> dx</span>
<span id="cb8-6">  second_moment <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> beta2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> second_moment <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> beta2) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> dx <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> dx</span>
<span id="cb8-7"></span>
<span id="cb8-8">  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Bias correction</span></span>
<span id="cb8-9">  first_unbias <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> first_moment <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> beta1<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span>t)</span>
<span id="cb8-10">  second_unbias <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> second_moment <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> beta2<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span>t)</span>
<span id="cb8-11"></span>
<span id="cb8-12">  x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-=</span> learning_rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> first_unbias <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> (np.sqrt(second_unbias) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-7</span>)</span></code></pre></div>
<p>Here <code>t</code> is the current iteration number (timestep). At the beginning with small <code>t</code>, <code>beta1**t</code> and <code>beta2**t</code> are close to <code>beta1</code> and <code>beta2</code>. So, <code>(1 - beta1**t)</code> and <code>(1 - beta2**t)</code> are small numbers, which effectively scales up the <code>first_moment</code> and <code>second_moment</code> estimates, counteracting their initial bias towards zero. As <code>t</code> gets large, <code>beta1**t</code> and <code>beta2**t</code> approach zero (since <code>beta1</code>, <code>beta2</code> &lt; 1), so <code>(1 - beta1**t)</code> and <code>(1 - beta2**t)</code> approach 1, and the bias correction has less and less effect. The parameter update then uses these bias-corrected <code>first_unbias</code> and <code>second_unbias</code> estimates. This bias correction is for the fact that the first and second moment estimates start at zero and would otherwise be biased, especially during the early stages of training.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./images/adam.gif" class="lightbox" data-gallery="quarto-lightbox-gallery-10" title="blue line: RMSProp, red line: SGD+Momentum, black line: SGD, purple: Adam"><img src="https://bhdai.github.io/blog/posts/regularization_and_optimization/images/adam.gif" class="img-fluid quarto-figure quarto-figure-center figure-img" alt="blue line: RMSProp, red line: SGD+Momentum, black line: SGD, purple: Adam"></a></p>
</figure>
</div>
<figcaption>blue line: RMSProp, red line: SGD+Momentum, black line: SGD, purple: Adam</figcaption>
</figure>
</div>
<p>The Adam optimizer, with its combination of momentum-like behavior and RMSProp-like adaptive scaling, plus this bias correction, has proven to be very effective and robust across a wide range of deep learning tasks and architectures. The original paper suggests default values for the hyperparameters:</p>
<ul>
<li><code>beta1</code> = 0.9 (for the first moment/momentum)</li>
<li><code>beta2</code> = 0.999 (for the second moment/RMSProp-like scaling)</li>
<li><code>learning_rate</code> typically in the range of 1e-3 (0.001) or 5e-4 (0.0005).</li>
<li>The epsilon for numerical stability is often 1e-7 or 1e-8.</li>
</ul>
<p>And indeed, Adam with these default settings (<code>beta1</code>=0.9, <code>beta2</code>=0.999, and a <code>learning_rate</code> like 1e-3) is often a great starting point for many models! It’s frequently the first optimizer people try, and it often works quite well out of the box without extensive hyperparameter tuning, though tuning the learning rate is still important. Adam is a fantastic general-purpose optimizer. However, like all things, it’s not perfect, and some subtleties have emerged, particularly regarding how it interacts with weight decay (L2 regularization).</p>
<p>This leads us to <strong>AdamW</strong>: an Adam Variant with Weight Decay. The question to consider is: <strong>How does regularization interact with the optimizer? (e.g., L2)</strong></p>
<p>Remember that our full loss function is typically <img src="https://latex.codecogs.com/png.latex?L_%7Bdata%7D%20+%20%5Clambda%20R(W)">. If <img src="https://latex.codecogs.com/png.latex?R(W)"> is L2 regularization, then its gradient is <img src="https://latex.codecogs.com/png.latex?2%5Clambda%20W."> This gradient of the regularization term gets added to the gradient of the data loss term to form the total <code>dx = compute_gradient(x)</code>. So, how does this interaction play out with an adaptive optimizer like Adam? The answer, perhaps unsurprisingly, is: <strong>It depends!</strong> Specifically, it depends on how the L2 regularization (weight decay) is implemented with respect to the adaptive learning rate mechanism. In Standard Adam (and many other adaptive gradient methods like RMSProp or AdaGrad), the L2 regularization term <img src="https://latex.codecogs.com/png.latex?%5Clambda%20W"> is typically added to the gradient of the data loss before the moment calculations. So, <code>dx = gradient_of_data_loss + gradient_of_L2_regularization</code>. This combined <code>dx</code> is then used during the moment calculations (for <code>first_moment</code> and <code>second_moment</code>). What this means is that the L2 regularization gradient gets scaled by the adaptive learning rates (the <code>1/sqrt(second_moment)</code> term). If <code>second_moment</code> is large for a particular weight (meaning its gradients have been large), then the effective weight decay for that parameter is also reduced. This coupling means that the strength of the weight decay is not uniform and can be different for different parameters, and can change over time in a way that might not be optimal. Larger gradients lead to smaller effective weight decay.</p>
<p>AdamW introduced by Loshchilov and Hutter, 2017, in <a href="https://arxiv.org/abs/1711.05101">“Decoupled Weight Decay Regularization”</a> proposes a different approach. Instead of adding the L2 regularization gradient into <code>dx</code> before the moment calculations, AdamW performs weight decay after the moment updates, directly on the weights themselves. So, the <code>first_moment</code> and <code>second_moment</code> are computed using only the gradient of the data loss. Then, the weight decay is applied as a separate step, effectively by subtracting <code>learning_rate * weight_decay_coefficient * weights</code> from the weights after the Adam step that uses <code>first_unbias</code> and <code>second_unbias</code>. So, AdamW is now often preferred over standard Adam when L2 regularization is used, as it implements weight decay in a way that is often more effective and closer to its original intention. Many deep learning libraries now offer AdamW as a distinct optimizer.</p>
</section>
<section id="learning-rate-schedules" class="level2">
<h2 class="anchored" data-anchor-id="learning-rate-schedules">Learning rate schedules</h2>
<p>Now, let’s turn our attention to a hyperparameter that is critical for all of them: the <strong>learning rate</strong>, often referred to as step_size. In the vanilla gradient descent update, <code>weights += - step_size * weights_grad</code>, the learning rate determines how big of a step we take in the negative gradient direction. All the optimizers we’ve discussed – SGD, SGD+Momentum, RMSProp, Adam, AdamW all have the learning rate as a crucial hyperparameter. Choosing the right learning rate is often one of the most important parts of getting good performance from a deep learning model.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./images/lr.png" class="lightbox" data-gallery="quarto-lightbox-gallery-11" title="How learning rate effect our training?"><img src="https://bhdai.github.io/blog/posts/regularization_and_optimization/images/lr.png" class="img-fluid figure-img" alt="How learning rate effect our training?"></a></p>
<figcaption>How learning rate effect our training?</figcaption>
</figure>
</div>
<p>So, the question is: <strong>Which one of these learning rates is best to use?</strong> Is there a single magic value? The answer is: <strong>In reality, all of these could be good learning rates… at different stages of training</strong>. This is a key insight. A single, fixed learning rate throughout the entire training process might not be optimal. Early in training, when you’re far from a good solution, a relatively larger learning rate can help you make rapid progress across the loss landscape. However, as you get closer to a minimum, that same large learning rate might cause you to overshoot and bounce around the minimum, preventing you from settling into the very bottom of the valley. In this later stage, a smaller learning rate is often beneficial to fine-tune the weights and converge more precisely. This idea that the optimal learning rate might change during training leads directly to the concept of <strong>learning rate schedules</strong>, also known as learning rate decay. We don’t just pick one learning rate and stick with it; we dynamically adjust it as training progresses.</p>
<p>One common strategy is <strong>Step Decay</strong>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./images/lr-decay.png" class="lightbox" data-gallery="quarto-lightbox-gallery-12" title="training loss epoch, notice the distinct drops in loss at certain epochs"><img src="https://bhdai.github.io/blog/posts/regularization_and_optimization/images/lr-decay.png" class="img-fluid figure-img" alt="training loss epoch, notice the distinct drops in loss at certain epochs"></a></p>
<figcaption>training loss epoch, notice the distinct drops in loss at certain epochs</figcaption>
</figure>
</div>
<p>The strategy here is to reduce the learning rate at a few fixed points during training. For example, a common schedule for training ResNets on ImageNet is to start with a certain learning rate, and then multiply it by 0.1 after, say, 30 epochs, then again by 0.1 after 60 epochs, and perhaps one more time after 90 epochs. You can see that after the learning rate is reduced (indicated by the arrows), the loss often plateaus for a bit and then finds a new, lower level. This happens because with a smaller learning rate, the optimizer can more finely navigate the bottom of the loss valley it has reached</p>
<p>Step decay is quite effective, but it requires choosing when to drop the learning rate and by how much. Are there smoother ways to decay the learning rate? Yes! Another very popular schedule is <strong>Cosine Decay</strong>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./images/cosine-decay.png" class="lightbox" data-gallery="quarto-lightbox-gallery-13" title="Cosine Decay (or cosine annealing)"><img src="https://bhdai.github.io/blog/posts/regularization_and_optimization/images/cosine-decay.png" class="img-fluid figure-img" alt="Cosine Decay (or cosine annealing)"></a></p>
<figcaption>Cosine Decay (or cosine annealing)</figcaption>
</figure>
</div>
<p>The formula is:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Calpha_t%20=%20%5Cfrac%7B1%7D%7B2%7D%20%5Calpha_0%20%5Cleft(1%20+%20%5Ccos%5Cleft(%5Cfrac%7Bt%7D%7BT%7D%20%5Cpi%5Cright)%5Cright)%0A"></p>
<p>Where:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Calpha_0"> is initial learning rate</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Calpha_t"> is the learning rate at epoch <img src="https://latex.codecogs.com/png.latex?t"></li>
<li><img src="https://latex.codecogs.com/png.latex?T"> is the total number of epochs you plan to train for</li>
</ul>
<p>The plot shows how this learning rate evolves over epochs. It starts at <img src="https://latex.codecogs.com/png.latex?%5Calpha_0"> and smoothly decreases, following a cosine curve, until it reaches near zero at the end of training (epoch <img src="https://latex.codecogs.com/png.latex?T">). This smooth decay is often found to work very well in practice and is less sensitive to the exact choice of drop points compared to step decay. Many recent state-of-the-art models use cosine annealing, sometimes with warm restarts which we’re not covering in detail here but involves periodically resetting the learning rate and re-annealing, as in the Loshchilov and Hutter paper. And on the right is an example of what the training loss might look like when using a cosine decay schedule. You see a generally smooth decrease in loss over a larger number of epochs, without the sharp drops associated with step decay. It just gradually refines the solution as the learning rate smoothly anneals.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./images/linear-inverse.png" class="lightbox" data-gallery="quarto-lightbox-gallery-14" title="left: Linear Decay, right: Inverse Square Root Decay"><img src="https://bhdai.github.io/blog/posts/regularization_and_optimization/images/linear-inverse.png" class="img-fluid figure-img" alt="left: Linear Decay, right: Inverse Square Root Decay"></a></p>
<figcaption>left: Linear Decay, right: Inverse Square Root Decay</figcaption>
</figure>
</div>
<p>Another simple schedule is <strong>Linear Decay</strong>, the formula is:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Calpha_t%20=%20%5Calpha_0%20(1%20-%20%5Cfrac%7Bt%7D%7BT%7D)%0A"></p>
<p>The learning rate decreases linearly from <img src="https://latex.codecogs.com/png.latex?%5Calpha_0"> down to 0 over <img src="https://latex.codecogs.com/png.latex?T"> epochs. This is also a common choice, used in famous models like BERT, for example. The plot shows this straight-line decrease.</p>
<p>And one more example: <strong>Inverse Square Root Decay</strong>. The formula is:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Calpha_t%20=%20%5Cfrac%7B%5Calpha_0%7D%7B%5Csqrt%7Bt%7D%7D%0A"></p>
<p>The learning rate decreases proportionally to the inverse square root of the epoch number <img src="https://latex.codecogs.com/png.latex?t">. This schedule makes the learning rate drop relatively quickly at the beginning and then more slowly later on. This was famously used in the original <a href="https://arxiv.org/abs/1706.03762">“Attention is All You Need”</a> Transformer paper.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./images/linear-warmup.png" class="lightbox" data-gallery="quarto-lightbox-gallery-15" title="Linear Warmup"><img src="https://bhdai.github.io/blog/posts/regularization_and_optimization/images/linear-warmup.png" class="img-fluid figure-img" alt="Linear Warmup"></a></p>
<figcaption>Linear Warmup</figcaption>
</figure>
</div>
<p>There’s one more important trick related to learning rates, especially when using large initial learning rates or large batch sizes: <strong>Linear Warmup</strong>. The idea is that high initial learning rates can sometimes make the loss explode right at the beginning of training, especially if the initial weights are far from good and produce very large gradients. The model can become unstable. To prevent this, a common practice is to linearly increase the learning rate from 0 (or a very small value) up to your target initial learning rate <img src="https://latex.codecogs.com/png.latex?%5Calpha_0"> over the first few thousand iterations (e.g., the first ~5,000 iterations or the first epoch). The plot shows this: the learning rate starts at 0, ramps up linearly to a peak (this is our <img src="https://latex.codecogs.com/png.latex?%5Calpha_0">), and then a decay schedule like cosine or step takes over. This “warmup” phase allows the model to stabilize a bit with small updates before the more aggressive learning rate kicks in. It’s a very common and effective technique, particularly in training large models like Transformers. There’s also an interesting <strong>empirical rule of thumb</strong> often cited (e.g., from Goyal et al.’s paper <a href="https://www.semanticscholar.org/paper/Accurate%2C-Large-Minibatch-SGD%3A-Training-ImageNet-in-Goyal-Doll%C3%A1r/0d57ba12a6d958e178d83be4c84513f7e42b24e5">“Accurate, Large Minibatch SGD”</a>): If you increase the batch size by N, you should also scale the initial learning rate by N (or <img src="https://latex.codecogs.com/png.latex?%5Csqrt%7BN%7D"> in some cases), often in conjunction with a warmup. The intuition is that larger batches give more stable gradient estimates, so you can afford to take larger steps.</p>
<p>So, managing the learning rate is not just about picking a single value, but often about defining a schedule that includes an initial phase (like warmup) and a decay phase. This is a critical part of the “art” of training deep neural networks.</p>
<p>So, in practice:</p>
<ul>
<li><strong>Adam(W) is a good default choice in many cases</strong>. It often works reasonably well even with a constant learning rate (though a schedule is still generally better) and requires less manual tuning of the learning rate compared to SGD with momentum. AdamW is preferred if you’re using L2 regularization.</li>
<li><strong>SGD+Momentum can outperform Adam but may require more tuning of the learning rate and its schedule</strong>. There’s a bit of an ongoing debate and empirical evidence suggesting that with careful tuning, SGD+Momentum can sometimes find solutions that generalize better than those found by Adam, especially for certain types of vision models. However, “careful tuning” is the operative phrase.</li>
</ul>


</section>

 ]]></description>
  <category>Deep Learning</category>
  <category>Optimization</category>
  <guid>https://bhdai.github.io/blog/posts/regularization_and_optimization/</guid>
  <pubDate>Mon, 04 Aug 2025 17:00:00 GMT</pubDate>
  <media:content url="https://bhdai.github.io/blog/posts/regularization_and_optimization/images/cover.png" medium="image" type="image/png" height="96" width="144"/>
</item>
<item>
  <title>Image classification with linear classifiers</title>
  <dc:creator>Bui Huu Dai</dc:creator>
  <link>https://bhdai.github.io/blog/posts/linear_classification/</link>
  <description><![CDATA[ 






<p>Image classification is truly a core task in computer vision. It’s the simplest and most fundamental problem, and mastering it forms the basis for more complex tasks like object detection and segmentation.</p>
<section id="what-makes-a-cat-a-cat" class="level2">
<h2 class="anchored" data-anchor-id="what-makes-a-cat-a-cat">What makes a cat a cat?</h2>
<p>Let’s explicitly define image classification. The task is: given an input image, assign it one label from a predefine set of possible categories. If we have an image of a cat. Our system would label ‘cat’. Crucially for this task, we assume we are given a set of possible labels, like {dog, cat, truck, plane, …}. The model job is to pick the most appropriate label from this list for any given input image. We’re not asking it for generate new label or descriptions, just to categorize.</p>
<p>Now, this simple task of image to label hides a fundamental challenge in computer vision, what we call <strong>Semantic Gap</strong>. With an image what you see is a beautiful, tabby cat maybe with green eyes, you immediately understand it’s a living creature a pet, specifically a cat. But what the computer sees is a grid of numbers. The <strong>semantic gap</strong> is precisely this challenge: how do we bridge the enormous gap between these raw numerical pixel values (what computer sees) and the high-level semantic concepts that we human effortlessly understand? That’s the core problem we’re trying to solve in image classification, and indeed, in much of computer vision.</p>
<p>Now let’s look at why this is so hard for a computers. The world is messy, and images with a lot of variability.</p>
<p>One of the primitive challenge is <strong>Viewpoint variation</strong>. Take our friendly cat again. You see if from one angle. But what if the camera moved slightly? Or what if you’re looking at a different photo of a same cat from a completely different side, or from above, blow? The camera illustrated around the cat show different potential viewpoints. From these different angles, even if it’s the exact same physical cat, the resulting image (the grid of pixel values) will be drastically different. For us it’s still ‘a cat’. For a computer it’s a completely new set of numbers. Our model need to learn that all these wildly different arrangements of pixels still correspond to the same underlying object. This invariance viewpoints is a huge challenge.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./images/illumination.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Challenges: Illumination"><img src="https://bhdai.github.io/blog/posts/linear_classification/images/illumination.png" class="img-fluid figure-img" alt="Challenges: Illumination"></a></p>
<figcaption>Challenges: Illumination</figcaption>
</figure>
</div>
<p>Another major hurdle is <strong>Illumination</strong>. The way an object is lit dramatically changes its appearance in an image. All of these are cat, but the pixel values, the color, the contrast they are completely different across these images due to varying light conditions. Our model must be robust enough to recognize a cat regardless of whether it’s in bright sunlight, deep snow, or under artifact light.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./images/background-clutter.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Challenges: Background Clutter"><img src="https://bhdai.github.io/blog/posts/linear_classification/images/background-clutter.png" class="img-fluid figure-img" alt="Challenges: Background Clutter"></a></p>
<figcaption>Challenges: Background Clutter</figcaption>
</figure>
</div>
<p>Then we have <strong>Background Clutter</strong>. Objects in real world rarely appears against a plain, uniform background. They are embedded in complex often messy environments. The challenge here is for the computer to distinguish the object of interest from everything else in the image. It needs to focus on the relevant features and ignore the distractors, even when the background is cluttered or has similar textures of colors.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./images/occulusion.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Challenges: Occlusion"><img src="https://bhdai.github.io/blog/posts/linear_classification/images/occulusion.png" class="img-fluid figure-img" alt="Challenges: Occlusion"></a></p>
<figcaption>Challenges: Occlusion</figcaption>
</figure>
</div>
<p>A very common and difficult challenge is <strong>Occlusion</strong>. This occurs when parts of the object you’re trying to recognize are hidden from view from other objects. Despite missing large portion of the object, as human, we can still easily identify the cat. For a computer, dealing with these partial view and reasoning about what’s missing is incredibly difficult. It needs to learn to recognize an object even when only some of its characteristic features are present, or when they are distorted by being partially covered.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./images/deformation.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Challenges: Deformation"><img src="https://bhdai.github.io/blog/posts/linear_classification/images/deformation.png" class="img-fluid figure-img" alt="Challenges: Deformation"></a></p>
<figcaption>Challenges: Deformation</figcaption>
</figure>
</div>
<p>Another significant challenge is <strong>deformation</strong>. Many objects especially animate ones like animals and people, are not rigid They can change their shape, their pose, their configuration in countless ways. These are all cats, but their body shapes and the relative position of their limbs are drastically different. This isn’t just about camera moving around a rigid object, the object itself is deforming. Our visual system needs to be able to recognize an object class despite these non-rigid transformation. A simple template or a fix set of geometric rule would struggle immensely with these level of variability.</p>
<p>And if deformation wasn’t enough, we also have the huge challenge of <strong>Intraclass variation</strong>. What this means is that even within a single category, like ‘cat’ there can be an enormous amount of visual diversity. Think about different breed of cats look very different, they come in all sorts of colors and patterns. A good image classifier need to learn a concept of ‘cat’ that is general enough that encompass all these variation. It can’t just memorize one specific type of cat, it has to understand the underlying shared characteristic that define the entire class, despite the wide range of appearances. This is a core reason why simple, rule-based approaches often fail and why we need powerful learning algorithms.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./images/context.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="Challenges: Context"><img src="https://bhdai.github.io/blog/posts/linear_classification/images/context.png" class="img-fluid figure-img" alt="Challenges: Context"></a></p>
<figcaption>Challenges: Context</figcaption>
</figure>
</div>
<p>And that final challenge is <strong>Context</strong>. Humans are incredibly adept at using context to understand the world. We don’t just see objects in isolation; we see them within a scene, and that surrounding information heavily influences our perception. So what went wrong here? Here our model tend to latch onto local patterns, the stripping alone is enough to push our model prediction toward ‘tiger’. Our model didn’t sufficiently incorporate the fact this is on pavement, in front of a gate, and it turns out that it’s just someone’s small dog, and the tiger-stripes are the shadows cast by the iron bars of the gate across its fur. So, understanding and leveraging context is crucial for robust visual intelligence, but it’s also very difficult for current models to do this effectively. They often rely too heavily on local features and can miss the bigger picture or be fooled by misleading contextual cues.</p>
<p>Given all these hurdles, it should be clear that trying to write a program with explicit rules to identify, say, a ‘cat’ in all its possible manifestations and situations is an almost impossible task. We’d be writing <code>if-else</code> statements for years!</p>
</section>
<section id="the-three-faces-of-a-classifier" class="level2">
<h2 class="anchored" data-anchor-id="the-three-faces-of-a-classifier">The three faces of a classifier</h2>
<p>So now, we’re going to move on to a type of classifier, and arguably one of the most fundamental building blogs in machine learning and deep learning: the <strong>Linear Classifier</strong>. Linear classifier fall under what’s known as the <strong>Parametric Approach</strong>. This is a key distinction from K-Nearest Neighbors, which is a non-parametric method. In the parametric approach, we define a <strong>score function</strong>, let’s call it <img src="https://latex.codecogs.com/png.latex?f(x,%20W)">, that map the raw input data(our image x) to class scores. The crucial part here is this <img src="https://latex.codecogs.com/png.latex?W">. <img src="https://latex.codecogs.com/png.latex?W"> represents a set of <strong>parameters</strong> or <strong>weights</strong> that the model uses. The “learning” process in a parametric model involves finding the optimal values for these parameters <img src="https://latex.codecogs.com/png.latex?W"> using the training data. Once we’ve learned these parameters, we can effectively discard the training data itself! To make a prediction for a new image, we just need the learned parameters <img src="https://latex.codecogs.com/png.latex?W"> and the function <img src="https://latex.codecogs.com/png.latex?f">. This is very different from K-NN where we had to keep all the training data around.</p>
<p>Now, let’s get specific. What form does this function <img src="https://latex.codecogs.com/png.latex?f(x,W)"> take for a linear classifier? It’s beautifully simple:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Af(x,%20W)%20=%20Wx%0A"></p>
<p>That’s it! It’s a matrix-vector multiplication. The result of <img src="https://latex.codecogs.com/png.latex?Wx"> will be a vector of class scores. There’s one more small addition we typically make to this linear score function. We often add a <strong>bias term</strong>, <img src="https://latex.codecogs.com/png.latex?b"> (though often we just write f(x,W) and assume W implicitly includes b, or b is handled separately).. So, the full form of our linear classifier’s score function becomes:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Af(x,W,b)%20=%20Wx%20+%20b%0A"></p>
<p>What is this bias <img src="https://latex.codecogs.com/png.latex?b">? It allows the score function to have some class-specific preferences that are independent of the input image <img src="https://latex.codecogs.com/png.latex?x">. Think of it as shifting the baseline score for each class. For example, if in our training data, cats are just generally more common than airplanes, the bias term for “cat” might learn to be slightly higher, giving cats a bit of an advantage even before looking at the image pixels. It’s also common practice to sometimes absorb the bias term into the weight matrix <img src="https://latex.codecogs.com/png.latex?W"> by appending a constant 1 to the input vector <img src="https://latex.codecogs.com/png.latex?x">, and adding an extra column to <img src="https://latex.codecogs.com/png.latex?W"> to hold the bias values. This just makes the notation a bit cleaner, <img src="https://latex.codecogs.com/png.latex?f(x,W)%20=%20Wx">, but conceptually, it’s useful to think of <img src="https://latex.codecogs.com/png.latex?W"> (the part that multiplies <img src="https://latex.codecogs.com/png.latex?x">) and <img src="https://latex.codecogs.com/png.latex?b"> (the additive part) separately for a moment.</p>
<p>When it comes to interpreting a linear classifier, I think it’s easier for us to look at it from a few different perspectives</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./images/algebraic-viewpoint.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="Algebraic Viewpoint"><img src="https://bhdai.github.io/blog/posts/linear_classification/images/algebraic-viewpoint.png" class="img-fluid figure-img" alt="Algebraic Viewpoint"></a></p>
<figcaption>Algebraic Viewpoint</figcaption>
</figure>
</div>
<p>First is <strong>Algebraic Viewpoint</strong>. We need to produce 3 scores (one for cat, one for dog, one for ship). So our output should be a 3x1 vector. This means our weight matrix <img src="https://latex.codecogs.com/png.latex?W"> must have dimensions [3 x 4] (3 rows for 3 classes, 4 columns to match the 4 pixels in <img src="https://latex.codecogs.com/png.latex?x">). And our bias vector <img src="https://latex.codecogs.com/png.latex?b"> will be [3 x 1]. The first row of <img src="https://latex.codecogs.com/png.latex?W"> are the weights for the “cat” class. The second row are the weights for the “dog” class. The third row are for the “ship” class. To get the scores, we perform the matrix multiplication <img src="https://latex.codecogs.com/png.latex?Wx"> and then add <img src="https://latex.codecogs.com/png.latex?b."> So, for this input image and these particular weights <img src="https://latex.codecogs.com/png.latex?W"> and biases <img src="https://latex.codecogs.com/png.latex?b">, we get scores: Cat: -96.8, Dog: 437.9, Ship: 61.95. Based on these scores, which class would our linear classifier predict? It would predict “Dog,” because 437.9 is the highest score. The “learning” process, which we haven’t discussed yet, would be about finding values for <img src="https://latex.codecogs.com/png.latex?W"> and <img src="https://latex.codecogs.com/png.latex?b"> such that for an image that is a cat, the cat score is highest; for an image that is a dog, the dog score is highest, and so on.</p>
<p>Okay, we’ve seen the algebra. But can we get a more Visual Viewpoint of what these weights W actually represent? For CIFAR-10, each row of <img src="https://latex.codecogs.com/png.latex?W"> is a vector of 3072 weights. Since our input <img src="https://latex.codecogs.com/png.latex?x"> is an image, we can reshape each row of <img src="https://latex.codecogs.com/png.latex?W"> back into a 32x32x3 image. What do these “images” corresponding to the rows of <img src="https://latex.codecogs.com/png.latex?W"> look like?</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./images/visual-vewpoint.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="Visual Viewpoint"><img src="https://bhdai.github.io/blog/posts/linear_classification/images/visual-vewpoint.png" class="img-fluid figure-img" alt="Visual Viewpoint"></a></p>
<figcaption>Visual Viewpoint</figcaption>
</figure>
</div>
<p>What the linear classifier is learning is essentially a single template for each class. When a new image comes in, it’s like the classifier is “matching” that input image against each of these 10 templates using a dot product. If the input image has, say, a lot of blue pixels in the regions where the “plane” template has blue pixels, and not many red pixels where the “plane” template has red pixels, that will contribute to a high score for the “plane” class. These templates are often very blurry and try to capture the “average” appearance of objects in that class. For example, cars can be many colors, but if there are more red cars in the training set, the “car” template might end up looking reddish. If planes are often pictured against a blue sky, the “plane” template might pick up on that blue. This also highlight a limitation: a linear learns only one template per class. But a class like “car” has many variations (red cars, blue cars, trucks, cars viewed from the side, car viewed from the front). A single template will struggle to capture this variability. It might learn a sort “average car”.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./images/geometric-viewpoint.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8" title="Geometric Viewpoint"><img src="https://bhdai.github.io/blog/posts/linear_classification/images/geometric-viewpoint.png" class="img-fluid figure-img" alt="Geometric Viewpoint"></a></p>
<figcaption>Geometric Viewpoint</figcaption>
</figure>
</div>
<p>Finally let’s consider the <strong>Geometric Viewpoint</strong>. Our score function is <img src="https://latex.codecogs.com/png.latex?s%20=%20Wx%20+%20b">. For each class c, the score for that class is <img src="https://latex.codecogs.com/png.latex?s_c%20=%20W_c%20%E2%8B%85%20x%20+%20b_c">, where <img src="https://latex.codecogs.com/png.latex?W_c"> is the c-th row of <img src="https://latex.codecogs.com/png.latex?W">. This is the equation of a line (or a hyperplane in higher dimensions). So, linear classifier is learning a set of hyperplane in the high-dimensional pixel space. Each hyperplane correspond to a class. The decision boundary between any two classes, say class <img src="https://latex.codecogs.com/png.latex?i"> and class <img src="https://latex.codecogs.com/png.latex?j">, occurs where their score are equal: <img src="https://latex.codecogs.com/png.latex?W_ix%20+%20b_i%20=%20W_jx%20+%20b_j">. This equation also defines a hyperplane.</p>
<p>The 3D plot on the bottom left shows the actual score surfaces for three classes. Each colored plane represents the scores for one class as a function of a 2D input. The decision boundaries are where these planes intersect. You can see that the regions where one plane is highest correspond to the classification regions for that class. These regions are always convex polygons (or polyhedra in higher dimensions) for a linear classifier. So, geometrically, a linear classifier is carving up the high-dimensional input space using these hyperplanes. It’s trying to find orientations and positions for these hyperplanes such that most of the training examples for a given class fall on the “correct” side of their respective decision boundaries.</p>
<p>These three viewpoints – algebraic, visual (template matching), and geometric (hyperplanes) – all describe the same underlying mathematical operation <img src="https://latex.codecogs.com/png.latex?Wx%20+%20b">. Understanding it from these different angles helps build a much richer intuition for what a linear classifier is doing, what it can learn, and also, importantly, what its limitations might be.</p>
</section>
<section id="hard-cases-for-a-linear-classifier" class="level2">
<h2 class="anchored" data-anchor-id="hard-cases-for-a-linear-classifier">Hard cases for a linear classifier</h2>
<p>There are scenarios where, no matter how you orient your lines (or hyperplanes in higher dimensions), you simply cannot perfectly separate the classes.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./images/hard_cases.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9" title="Case 1 (left): The XOR problem. Case 2 (middle): The donut problem. Case 3 (right): Multiple modes."><img src="https://bhdai.github.io/blog/posts/linear_classification/images/hard_cases.png" class="img-fluid figure-img" alt="Case 1 (left): The XOR problem. Case 2 (middle): The donut problem. Case 3 (right): Multiple modes."></a></p>
<figcaption>Case 1 (left): The XOR problem. Case 2 (middle): The donut problem. Case 3 (right): Multiple modes.</figcaption>
</figure>
</div>
<p>In case 1 we have the XOR problem where class 1 is in the first and third quadrants, class 2 is in the second and forth quadrants. Try to draw a single straight line, that separate the class 1 and class 2. You can do it! You’d need at least two lines, or a non-linear boundary. A linear classifier will fall here, it will make mistakes no matter what i places its decision boundary.</p>
<p>Case 2 is the donut problem, here, class 1 is an annulus or a ring – points whose L2 norm is between 1 and 2. Class 2 is everything else (inside the inner circle and outside the outer circle). Again, can you separate the blue ring from the pink regions with a single straight line? No.&nbsp;You’d need something like a circular boundary, which is non-linear.</p>
<p>Case 3 is Multiple modes, Class 1 consists of three distinct, separated circular regions. Class 2 is everything else. A single linear classifier tries to learn one template per class. If a class has multiple, well-separated “prototypes” or modes in the feature space, a linear classifier will struggle. It can’t draw a line to nicely encapsulate all three blue regions while excluding the pink. It might try to find a single hyperplane that does its best, but it will inevitably misclassify many points.</p>
</section>
<section id="choose-a-good-w" class="level2">
<h2 class="anchored" data-anchor-id="choose-a-good-w">Choose a good W</h2>
<p>So, given that we have this linear score function <img src="https://latex.codecogs.com/png.latex?f(x,W)%20=%20Wx%20+%20b">, and we understand its capabilities and limitations, the central question becomes: How do we choose a good <img src="https://latex.codecogs.com/png.latex?W"> (and <img src="https://latex.codecogs.com/png.latex?b">)? We need a systematic way to 1) define a loss function (or cost function or objective function). This function will take our current <img src="https://latex.codecogs.com/png.latex?W"> and <img src="https://latex.codecogs.com/png.latex?b">, run our classifier on the training data, look at the scores it produces, and tell us how “unhappy” we are with those scores. A high loss means our <img src="https://latex.codecogs.com/png.latex?W"> and <img src="https://latex.codecogs.com/png.latex?b"> are bad (producing scores that lead to incorrect classifications or low confidence in correct classifications). A low loss means our <img src="https://latex.codecogs.com/png.latex?W"> and <img src="https://latex.codecogs.com/png.latex?b"> are good. 2) Once we have this loss function, we need to come up with a way of efficiently finding the parameters (<img src="https://latex.codecogs.com/png.latex?W"> and <img src="https://latex.codecogs.com/png.latex?b">) that minimize this loss function. This is an optimization problem. We want to search through the space of all possible <img src="https://latex.codecogs.com/png.latex?W">’s and <img src="https://latex.codecogs.com/png.latex?b">’s to find the set that makes our classifier perform best on the training data, according to our loss function.</p>
<p>More formally, we are given a dataset of <img src="https://latex.codecogs.com/png.latex?N"> training examples:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%7B%20(x_i,%20y_i)%20%7D_%7Bi=1%7D%5E%7BN%7D%0A"></p>
<p>Where <img src="https://latex.codecogs.com/png.latex?x_i"> is an image (our input vector) and <img src="https://latex.codecogs.com/png.latex?y_i"> is its true integer label (e.g., 0 for cat, 1 for car, 2 for frog). Typically, the total loss over the entire dataset, <img src="https://latex.codecogs.com/png.latex?L">, is the average of the losses computed for each individual training example:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AL%20=%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_i%20L_i(f(x_i,%20W),%20y_i)%0A"></p>
<p>Here:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?f(x_i,%20W)"> are the scores our current classifier (with weights <img src="https://latex.codecogs.com/png.latex?W">) produces for the i-th training image <img src="https://latex.codecogs.com/png.latex?x_i">.</li>
<li><img src="https://latex.codecogs.com/png.latex?y_i"> is the true label for that i-th training image.</li>
<li><img src="https://latex.codecogs.com/png.latex?L_i"> is the loss function calculated for that single i-th example. It takes the predicted scores and the true label and tells us how bad the prediction was for that one example.</li>
<li>We sum these individual losses <img src="https://latex.codecogs.com/png.latex?L_i"> over all <img src="https://latex.codecogs.com/png.latex?N"> training examples and then divide by <img src="https://latex.codecogs.com/png.latex?N"> to get the average loss</li>
</ul>
<p>Our goal will be to find the <img src="https://latex.codecogs.com/png.latex?W"> that minimizes this total loss <img src="https://latex.codecogs.com/png.latex?L">. Now, the crucial piece we still need to define is: what exactly is this <img src="https://latex.codecogs.com/png.latex?L_i"> function? How do we take a vector of scores and a true label and turn that into a single number representing the loss for that example? There are several ways to do this, and we’ll look at a very common one for classification next: the Softmax classifier (which uses cross-entropy loss), and also the SVM (hinge) loss.</p>
</section>
<section id="softmax-classifier" class="level2">
<h2 class="anchored" data-anchor-id="softmax-classifier">Softmax classifier</h2>
<p>The core idea behind the Softmax classifier is that we want to interpret the raw classifier scores as probabilities. Our linear classifier <img src="https://latex.codecogs.com/png.latex?s%20=%20f(x_i,%20W)"> produces these raw scores. For example our cat image example, we had scores: Cat: 3.2, Car: 5.1, Frog: -1.7. These are just arbitrary real numbers. They could be positive, negative, large, small. They don’t look like probabilities, which should be between 0 and 1 and sum to 1. The Softmax function is what allows us to convert these raw scores into a valid probability distribution over the classes. The formula for the probability of the true class <img src="https://latex.codecogs.com/png.latex?k"> given the input image <img src="https://latex.codecogs.com/png.latex?x_i"> (and our weights <img src="https://latex.codecogs.com/png.latex?W">) is:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AP(Y%20=%20k%20%7C%20X%20=%20x_i)%20=%20%5Cfrac%7Be%5E%7Bs_k%7D%7D%7B%5Csum_j%20e%5E%7Bs_j%7D%7D%0A"></p>
<p>Where:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?s_k"> is the raw score for class <img src="https://latex.codecogs.com/png.latex?k"> (e.g., 3.2 for cat).</li>
<li><img src="https://latex.codecogs.com/png.latex?s_j"> are the raw scores for all classes <img src="https://latex.codecogs.com/png.latex?j"> (cat, car, frog).</li>
<li>We exponentiate each score (<img src="https://latex.codecogs.com/png.latex?e%5E%7Bs_k%7D">). This makes all the numbers positive, which is good for probabilities.</li>
<li>Then we normalize by dividing by the sum of all these exponentiated scores. This ensures that the resulting probabilities for all classes sum to 1.</li>
</ul>
<p>The raw scores s that go into the Softmax function (our 3.2, 5.1, -1.7) are often called logits or unnormalized log-probabilities. The term “logit” comes from logistic regression, of which Softmax is a generalization to multiple classes</p>
<p>Okay, so we’ve used the Softmax function to get a probability distribution over the classes for a given input image <img src="https://latex.codecogs.com/png.latex?x_i."> Now, how do we define the loss <img src="https://latex.codecogs.com/png.latex?L_i"> for this single example? If the true class for our example image (the cat) is y_i (let’s say “cat” is class 0), then the loss for this example is defined as the negative log probability of the true class:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AL_i%20=%20-%5Clog%7BP(Y%20=%20y_i%7CX%20=%20x_i)%7D%0A"></p>
<p>Why this particular form for the loss? Probabilities <img src="https://latex.codecogs.com/png.latex?P"> are between <img src="https://latex.codecogs.com/png.latex?0"> and <img src="https://latex.codecogs.com/png.latex?1">. <img src="https://latex.codecogs.com/png.latex?log(P)"> will be negative (or <img src="https://latex.codecogs.com/png.latex?0"> if <img src="https://latex.codecogs.com/png.latex?P=1">). If the probability of the true class is very high (close to <img src="https://latex.codecogs.com/png.latex?1">, e.g., <img src="https://latex.codecogs.com/png.latex?P=0.99">), then <img src="https://latex.codecogs.com/png.latex?log(P)"> is close to <img src="https://latex.codecogs.com/png.latex?0">, and <img src="https://latex.codecogs.com/png.latex?-log(P)"> is also close to <img src="https://latex.codecogs.com/png.latex?0"> (a small loss, which is good!). If the probability of the true class is very low (close to <img src="https://latex.codecogs.com/png.latex?0">, e.g., <img src="https://latex.codecogs.com/png.latex?P=0.01">), then <img src="https://latex.codecogs.com/png.latex?log(P)"> is a large negative number, and <img src="https://latex.codecogs.com/png.latex?-log(P)"> will be a large positive number (a high loss, which is bad!). So, this <img src="https://latex.codecogs.com/png.latex?-log(P_true_class)"> loss function does what we want: it penalizes the model heavily if it assigns low probability to the correct answer, and penalizes it very little if it assigns high probability to the correct answer. Minimizing this loss will push the model to make the probability of the true class as close to <img src="https://latex.codecogs.com/png.latex?1"> as possible. This approach also known as <strong>Maximum Likelihood Estimation (MLE)</strong>. We are choosing the parameters <img src="https://latex.codecogs.com/png.latex?W"> to maximize the likelihood (or equivalently, the log-likelihood) of observing the true labels <img src="https://latex.codecogs.com/png.latex?y_i"> given the input data <img src="https://latex.codecogs.com/png.latex?x_i."> Taking the negative log-likelihood turns it into a loss minimization problem.</p>
<p>We can also think about this loss from an information theory perspective. The “correct probabilities” for our cat image would be: Cat: 1.00, Car: 0.00, Frog: 0.00. This is a probability distribution where all the mass is on the true class. Let’s call this target distribution <img src="https://latex.codecogs.com/png.latex?P">. Our Softmax classifier produced the distribution Q: Cat: 0.13, Car: 0.87, Frog: 0.00. The loss function we are using, <img src="https://latex.codecogs.com/png.latex?-log%20P(Y=y_i%20%7C%20X=x_i)">, is actually equivalent to the Kullback-Leibler (KL) divergence between the true distribution P (which is 1 for the correct class and 0 otherwise) and the predicted distribution Q from our Softmax. The KL divergence <img src="https://latex.codecogs.com/png.latex?D_KL(P%20%7C%7C%20Q)"> measures how different the distribution <img src="https://latex.codecogs.com/png.latex?Q"> is from the distribution <img src="https://latex.codecogs.com/png.latex?P">. It’s defined as <img src="https://latex.codecogs.com/png.latex?%5Csum_y%20P(y)%5Clog%5Cfrac%7BP(y)%7D%7BQ(y)%7D">. When P is a one-hot distribution (1 for the true class <img src="https://latex.codecogs.com/png.latex?y_i">, <img src="https://latex.codecogs.com/png.latex?0"> elsewhere), this simplifies to <img src="https://latex.codecogs.com/png.latex?-logQ(y_i)">, which is exactly our loss function!</p>
<p>So, the loss <img src="https://latex.codecogs.com/png.latex?L_i%20=%20-log%20P(Y=y_i%20%7C%20X=x_i)"> for a Softmax classifier is often called the Cross-Entropy loss between the true distribution (one-hot encoding of the correct label) and the predicted probability distribution from the Softmax.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AH(P,Q)%20=%20-%20%5Csum_y%20P(y)%20log%20Q(y)%0A"></p>
<p>When <img src="https://latex.codecogs.com/png.latex?P"> is one-hot, <img src="https://latex.codecogs.com/png.latex?P(y)"> is <img src="https://latex.codecogs.com/png.latex?1"> for the true class <img src="https://latex.codecogs.com/png.latex?y_i"> and <img src="https://latex.codecogs.com/png.latex?0"> otherwise. So, the sum collapses to a single term: <img src="https://latex.codecogs.com/png.latex?-1%20log%20Q(y_i)%20=%20-log%20Q(y_i)">.</p>
<p>So, whether you think of it as maximizing the log-likelihood of the correct class, or minimizing the KL divergence, or minimizing the cross-entropy between the true and predicted distributions, it all leads to the same loss function for the Softmax classifier: <img src="https://latex.codecogs.com/png.latex?L_i%20=%20-log(%5Ctext%7Bprobability%5C_of%5C_true%5C_class%7D)">. This is a cornerstone loss function for classification problems in deep learning. The overall loss for the dataset, L, would then be the average of these L_i’s over all training examples. Our goal is to find the W that minimizes this total cross-entropy loss.</p>
<p>So, just to recap:</p>
<ol type="1">
<li>We start with raw scores <img src="https://latex.codecogs.com/png.latex?s%20=%20f(x_i,%20W)"> from our linear classifier.</li>
<li>We want to interpret these as probabilities, so we use the Softmax function: <img src="https://latex.codecogs.com/png.latex?P(Y%20=%20k%20%7C%20X%20=%20x_i)%20=%20%5Cfrac%7Be%5E%7Bs_k%7D%7D%7B%5Csum_j%20e%5E%7Bs_j%7D%7D">. This gives us a probability for each class <img src="https://latex.codecogs.com/png.latex?k">.</li>
<li>Our goal is to maximize the probability of the correct class <img src="https://latex.codecogs.com/png.latex?y_i">.</li>
<li>The loss function <img src="https://latex.codecogs.com/png.latex?L_i"> for a single example is the negative log probability of the true class: <img src="https://latex.codecogs.com/png.latex?L_i%20=%20-log%20P(Y%20=%20y_i%20%7C%20X%20=%20x_i)">.</li>
</ol>
<p>Putting it all together, we can write the loss for a single example <img src="https://latex.codecogs.com/png.latex?x_i"> with true label <img src="https://latex.codecogs.com/png.latex?y_i"> and scores <img src="https://latex.codecogs.com/png.latex?s_j"> (where <img src="https://latex.codecogs.com/png.latex?s_j"> is the score for class <img src="https://latex.codecogs.com/png.latex?j">) directly as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%20L_i%20=%20-log%20(%20%5Cfrac%7Be%5E%7B%20s_%7By_i%7D%20%7D%7D%7B%5Csum_j%20e%5E%7B%20s_j%20%7D%7D%20)"></p>
<p>Here, <img src="https://latex.codecogs.com/png.latex?s_yi"> is the score for the true class <img src="https://latex.codecogs.com/png.latex?y_i">. This formula combines the Softmax calculation and the negative log operation into one expression for the loss. Minimizing this <img src="https://latex.codecogs.com/png.latex?L_i"> will push the score of the correct class <img src="https://latex.codecogs.com/png.latex?s_yi"> to be high relative to the scores of the other classes <img src="https://latex.codecogs.com/png.latex?s_j">.</p>
<p>Now, let’s think about some properties of this Softmax loss <img src="https://latex.codecogs.com/png.latex?L_i">. Two good questions to consider: What is the minimum and maximum possible Softmax loss <img src="https://latex.codecogs.com/png.latex?L_i">? And at initialization, our weights W are typically small random numbers. So, the initial scores <img src="https://latex.codecogs.com/png.latex?s_j"> for all classes will be approximately equal (and close to zero). What will the Softmax loss <img src="https://latex.codecogs.com/png.latex?L_i"> be in this scenario, assuming there are C classes?</p>
<p>Answer for the first question: <strong>Minimum possible loss</strong>, the loss is <img src="https://latex.codecogs.com/png.latex?L_i%20=%20-log(%5Ctext%7BP%5C_correct%5C_class%7D)">. To minimize <img src="https://latex.codecogs.com/png.latex?L_i">, we need to maximize P_correct_class. The maximum possible probability for the correct class is 1 (i.e., the classifier is 100% certain and correct). If <img src="https://latex.codecogs.com/png.latex?P%5C_correct%5C_class%20=%201">, then <img src="https://latex.codecogs.com/png.latex?log(1)%20=%200">, so <img src="https://latex.codecogs.com/png.latex?L_i%20=%20-0%20=%200">. Thus, the minimum possible Softmax loss is 0. This occurs when the model perfectly predicts the true class with probability 1. <strong>Maximum possible loss</strong>, to maximize <img src="https://latex.codecogs.com/png.latex?L_i">, we need to minimize P_correct_class. The minimum possible probability for the correct class is something very close to 0 (it can’t be exactly 0 because of the exponentiation, but it can be arbitrarily small if the score for the correct class is very, very negative relative to other scores). As P_correct_class approaches 0 from the positive side, log(P_correct_class) approaches negative infinity. Therefore, <img src="https://latex.codecogs.com/png.latex?-log(%5Ctext%7B%20P%5C_correct%5C_class%20%7D)"> approaches positive infinity. So, the Softmax loss can, in theory, go to infinity if the model is extremely confident in a wrong class and assigns vanishingly small probability to the true class.</p>
<p>Now for question 2: If all <img src="https://latex.codecogs.com/png.latex?s_j"> are approximately equal (say, <img src="https://latex.codecogs.com/png.latex?s_j%20%5Capprox%20s%5C_constant">), then <img src="https://latex.codecogs.com/png.latex?e%5E%7Bs_j%7D"> will also be approximately equal for all <img src="https://latex.codecogs.com/png.latex?j">. The probability for any given class k will be:</p>
<p><img src="https://latex.codecogs.com/png.latex?%20P(Y=k%20%7C%20X=x_i)%20=%20%5Cfrac%7Be%5E%7Bs_k%7D%7D%7B%5Csum_j%20e%5E%7Bs_j%7D%7D%20"></p>
<p>Since all <img src="https://latex.codecogs.com/png.latex?e%5E%7Bs_j%7D"> are roughly the same, let’s say <img src="https://latex.codecogs.com/png.latex?e%5E%7Bs%5C_constant%7D">, the sum in the denominator will be <img src="https://latex.codecogs.com/png.latex?C%20e%5E%7Bs%5C_constant%7D">. So, <img src="https://latex.codecogs.com/png.latex?P(Y=k%20%7C%20X=x_i)%20%5Capprox%20%5Cfrac%7Be%5E%7Bs%5C_constant%7D%7D%7BC%20e%5E%7B%20s%5C_constant%20%7D%7D%20=%20%5Cfrac%7B1%7D%7BC%7D">.</p>
<p>This makes intuitive sense: if the classifier has no information yet (all scores are equal), it should assign an equal probability of 1/C to each of the C classes. This is a uniform distribution. Now, the loss for any example <img src="https://latex.codecogs.com/png.latex?x_i"> (regardless of its true class <img src="https://latex.codecogs.com/png.latex?y_i">, since the probability assigned to every class is 1/C) will be:</p>
<p><img src="https://latex.codecogs.com/png.latex?%20L_i%20=%20-log(P%5C_correct%5C_class)%20=%20-log(%5Cfrac%7B1%7D%7BC%7D)%20"></p>
<p>Using the logarithm property <img src="https://latex.codecogs.com/png.latex?log(%5Cfrac%7B1%7D%7BC%7D)%20=%20log(1)%20-%20log(C)%20=%200%20-%20log(C)%20=%20-log(C)">. So, <img src="https://latex.codecogs.com/png.latex?L_i%20=%20-(-log(C))%20=%20log(C)">. Therefore, at initialization, when the classifier is essentially guessing uniformly, the Softmax loss per example will be approximately <img src="https://latex.codecogs.com/png.latex?log(C)">, where <img src="https://latex.codecogs.com/png.latex?C"> is the number of classes. For example, if we have <img src="https://latex.codecogs.com/png.latex?C%20=%2010"> classes (like in CIFAR-10), then the initial loss we expect to see is <img src="https://latex.codecogs.com/png.latex?L_i%20=%20log(10)"> (natural logarithm of 10), which is approximately 2.3. This is a very useful “sanity check”! When you’re implementing a Softmax classifier and you initialize your weights, if you compute the loss on your first batch of data and it’s wildly different from <img src="https://latex.codecogs.com/png.latex?log(C)">, you might have a bug somewhere in your loss calculation or your Softmax implementation. For CIFAR-10, you should see an initial loss around 2.3. If you see a loss of, say, 20 or 0.01 at the very start, something is likely wrong.</p>
</section>
<section id="multiclass-svm-loss" class="level2">
<h2 class="anchored" data-anchor-id="multiclass-svm-loss">Multiclass SVM loss</h2>
<p>Alright, so we’ve seen the Softmax classifier and its cross-entropy loss. Now, let’s turn to the other major type of loss function for linear classifiers: the <strong>Multiclass SVM loss</strong>, also known as the hinge loss. The Multiclass SVM loss has a different philosophy than Softmax. The SVM wants the score of the correct class <img src="https://latex.codecogs.com/png.latex?s_%7By_i%7D"> to be greater than the score of any incorrect class <img src="https://latex.codecogs.com/png.latex?s_j"> (where <img src="https://latex.codecogs.com/png.latex?j%20%E2%89%A0%20y_i">) by at least a certain fixed margin, which is commonly denoted by <img src="https://latex.codecogs.com/png.latex?%5CDelta"> and often set to 1. Here’s the form of the loss L_i for a single example:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AL_i%20=%20%5Csum_%7Bj%20%5Cne%20y_i%7D%0A%5Cbegin%7Bcases%7D%0A0%20&amp;%20%5Ctext%7Bif%20%7D%20s_%7By_i%7D%20%5Cgeq%20s_j%20+%201%20%5C%5C%0As_j%20-%20s_%7By_i%7D%20+%201%20&amp;%20%5Ctext%7Botherwise%7D%0A%5Cend%7Bcases%7D%0A=%20%5Csum_%7Bj%20%5Cne%20y_i%7D%20%5Cmax(0,%20s_j%20-%20s%7By_i%7D%20+%201)%0A"></p>
<p>So, for each training example, we sum up these margin violation penalties over all incorrect classes. If all incorrect classes have scores that are at least <img src="https://latex.codecogs.com/png.latex?%5CDelta"> less than the score of the correct class, then <img src="https://latex.codecogs.com/png.latex?L_i"> will be 0 for that example.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./images/svm.png" class="lightbox" data-gallery="quarto-lightbox-gallery-10" title="SVM loss"><img src="https://bhdai.github.io/blog/posts/linear_classification/images/svm.png" class="img-fluid figure-img" alt="SVM loss"></a></p>
<figcaption>SVM loss</figcaption>
</figure>
</div>
<p>Let’s visualize what one term <img src="https://latex.codecogs.com/png.latex?%5Cmax(0,%20s_j%20-%20s_%7By_i%7D%20+%201)"> of this loss looks like. The x-axis here is representing <img src="https://latex.codecogs.com/png.latex?s_%7By_i%7D%20-%20s_j">, which is the difference between the score of the correct class and the score of one particular incorrect class <img src="https://latex.codecogs.com/png.latex?j">. The SVM loss wants the score of the correct class <img src="https://latex.codecogs.com/png.latex?s_%7By_i%7D"> to be greater than the score of an incorrect class <img src="https://latex.codecogs.com/png.latex?s_j"> by at least the margin <img src="https://latex.codecogs.com/png.latex?%5CDelta"> (which is 1 in our plot). So, if <img src="https://latex.codecogs.com/png.latex?s_%7By_i%7D%20-%20s_j%20%5Cgeq%201">, meaning the correct score is already beating the incorrect score by the desired margin then the loss for that pair is 0. You can see the loss function is flat at 0 on the right side of the plot, where <img src="https://latex.codecogs.com/png.latex?s_%7By_i%7D%20-%20s_j%20%5Cgeq%201">. The SVM doesn’t care how much better the correct score is, as long as it’s better by at least the margin. It doesn’t try to push the correct score infinitely higher or the incorrect scores infinitely lower, unlike Softmax which is never fully satisfied. However, if <img src="https://latex.codecogs.com/png.latex?s_%7By_i%7D%20-%20s_j%20%5Clt%201"> (i.e., the margin is violated), then the loss becomes positive. The loss increases linearly as the difference <img src="https://latex.codecogs.com/png.latex?s_%7By_i%7D%20-%20s_j"> gets smaller (or more negative, meaning <img src="https://latex.codecogs.com/png.latex?s_j"> is much larger than <img src="https://latex.codecogs.com/png.latex?s_%7By_i%7D">). This is the hinge shape - zero loss if the margin is met and then a linear penalty for violations. The total <img src="https://latex.codecogs.com/png.latex?L_i"> for an example is the sum of these hinge losses over all incorrect classes <img src="https://latex.codecogs.com/png.latex?j">. This encourages the score of the true class <img src="https://latex.codecogs.com/png.latex?y_i"> to stand out from all other incorrect class scores by at least <img src="https://latex.codecogs.com/png.latex?%5CDelta">. This is a fundamentally different way of thinking about loss compared to Softmax. Softmax wants to correctly estimate the probability distribution; SVM wants to find a decision boundary that separates classes with a good margin.</p>
<p>So <strong>what is the min/max possible SVM loss L_i?</strong> <strong>Minimum possible loss</strong>, if all margins are satisfied (i.e., for all incorrect classes <img src="https://latex.codecogs.com/png.latex?j">, <img src="https://latex.codecogs.com/png.latex?s_%7By_i%7D%20%5Cgeq%20s_j%20+%20%5CDelta">), then every term in the <img src="https://latex.codecogs.com/png.latex?%5Csum%20%5Cmax(0,%20s_j%20-%20s_%7By_i%7D%20+%20%5CDelta)"> will be 0. So, the minimum SVM loss <img src="https://latex.codecogs.com/png.latex?L_i"> is 0. <strong>Maximum possible loss</strong>, if the score for the correct class <img src="https://latex.codecogs.com/png.latex?s_%7By_i%7D"> is extremely negative, and scores for incorrect classes <img src="https://latex.codecogs.com/png.latex?s_j"> are very positive, then <img src="https://latex.codecogs.com/png.latex?s_j%20-%20s_%7By_i%7D%20+%20%5CDelta"> can become arbitrarily large and positive for each incorrect class. Since we sum these terms, the total <img src="https://latex.codecogs.com/png.latex?L_i"> can go to positive infinity.</p>
<p><strong>At initialization, <img src="https://latex.codecogs.com/png.latex?W"> is small, so all scores <img src="https://latex.codecogs.com/png.latex?s_j%20%5Capprox%200">. What is the loss <img src="https://latex.codecogs.com/png.latex?L_i">, assuming <img src="https://latex.codecogs.com/png.latex?C"> classes?</strong> If all <img src="https://latex.codecogs.com/png.latex?s_j%20%5Capprox%200"> (including <img src="https://latex.codecogs.com/png.latex?s_%7By_i%7D%20%5Capprox%200">), then for each of the <img src="https://latex.codecogs.com/png.latex?C-1"> incorrect classes j, the term is:</p>
<p><img src="https://latex.codecogs.com/png.latex?s_j%20-%20s_%7By_i%7D%20+%20%5CDelta%20%E2%89%88%200%20-%200%20+%20%5CDelta%20=%20%5CDelta"></p>
<p>So, <img src="https://latex.codecogs.com/png.latex?%5Cmax(0,%20%5CDelta)"> is just <img src="https://latex.codecogs.com/png.latex?%5CDelta"> (assuming our margin <img src="https://latex.codecogs.com/png.latex?%5CDelta"> is positive, e.g., <img src="https://latex.codecogs.com/png.latex?%5CDelta=1">). We sum this <img src="https://latex.codecogs.com/png.latex?%5CDelta"> over all <img src="https://latex.codecogs.com/png.latex?C-1"> incorrect classes. Therefore, <img src="https://latex.codecogs.com/png.latex?L_i%20%E2%89%88%20(C-1)%20%5CDelta">. If <img src="https://latex.codecogs.com/png.latex?%5CDelta=1">, then the initial loss <img src="https://latex.codecogs.com/png.latex?L"> is approximately <img src="https://latex.codecogs.com/png.latex?C-1">. For CIFAR-10 with <img src="https://latex.codecogs.com/png.latex?C=10"> classes and <img src="https://latex.codecogs.com/png.latex?%5CDelta=1">, the initial SVM loss per example would be around 9. This is another good sanity check for your implementation. If you see an initial SVM loss far from <img src="https://latex.codecogs.com/png.latex?C-1">, you might have an issue.</p>
<p><strong>What if we used a squared term for the loss: <img src="https://latex.codecogs.com/png.latex?L_i%20=%20%5Csum_%7Bj%20%5Cneq%20y_i%7D%20%5Cmax(0,%20s_j%20-%20s_%7By_i%7D%20+%20%5CDelta)%5E2">?</strong> This is known as the squared hinge loss (or L2-SVM). The standard (L1) hinge loss <img src="https://latex.codecogs.com/png.latex?%5Cmax(0,%20margin%5C_violation)"> penalizes any violation linearly. Every unit of margin violation contributes equally to the loss. The squared (L2) hinge loss <img src="https://latex.codecogs.com/png.latex?%5Cmax(0,%20margin%5C_violation)%5E2"> penalizes larger violations much more heavily than smaller ones due to the squaring. It’s more sensitive to outliers or examples that are very wrong. This can sometimes lead to different solutions. Both have been used, though the L1 hinge loss is perhaps more standard for the classic SVM. The L2 version is smoother (differentiable even when the margin violation is zero, though the max(0,…) still introduces a non-differentiability point when the argument to max is exactly zero), which can sometimes be beneficial for certain optimization algorithms.</p>
<p>Here’s a python function that calculates the SVM loss for a single example x with true label y, given weight W.</p>
<div class="sourceCode" id="annotated-cell-1" style="background: #f1f3f5;"><pre class="sourceCode python code-annotation-code code-with-copy code-annotated"><code class="sourceCode python"><span id="annotated-cell-1-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> L_i_vecterized(x, y, W):</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-1" data-target-annotation="1" onclick="event.preventDefault();">1</a><span id="annotated-cell-1-2" class="code-annotation-target">  scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> W.dot(x)</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-1" data-target-annotation="2" onclick="event.preventDefault();">2</a><span id="annotated-cell-1-3" class="code-annotation-target">  margins <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.maximum(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> score[y] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-1" data-target-annotation="3" onclick="event.preventDefault();">3</a><span id="annotated-cell-1-4" class="code-annotation-target">  margins[y] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="annotated-cell-1-5">  loss_i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(margin)</span>
<span id="annotated-cell-1-6">  <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> loss_i</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code></pre></div>
<dl class="code-annotation-container-grid">
<dt data-target-cell="annotated-cell-1" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-1" data-code-lines="2" data-code-annotation="1">calculate scores</span>
</dd>
<dt data-target-cell="annotated-cell-1" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-1" data-code-lines="3" data-code-annotation="2">then calculate the margins <img src="https://latex.codecogs.com/png.latex?s_j%20-%20s_%7By_i%7D%20+%201"></span>
</dd>
<dt data-target-cell="annotated-cell-1" data-target-annotation="3">3</dt>
<dd>
<span data-code-cell="annotated-cell-1" data-code-lines="4" data-code-annotation="3">only sum <img src="https://latex.codecogs.com/png.latex?j"> is not <img src="https://latex.codecogs.com/png.latex?y_i"> so when <img src="https://latex.codecogs.com/png.latex?j%20=%20y_i">, set to zero</span>
</dd>
</dl>
<p>This is a nice, compact vectorized implementation. To get the loss for a whole batch of data, you’d typically loop over your batch, call this function for each example, and then average the results.</p>
<p>SVM is a margin-based loss. Softmax is a probabilistic loss that cares about the full distribution. In practice, both are widely used, and sometimes one might perform slightly better than the other depending on the dataset and task, but often their performance is quite similar when used in deep networks. The choice can also come down to whether you explicitly need probability outputs from your model.</p>


</section>

 ]]></description>
  <category>Deep Learning</category>
  <guid>https://bhdai.github.io/blog/posts/linear_classification/</guid>
  <pubDate>Fri, 01 Aug 2025 17:00:00 GMT</pubDate>
  <media:content url="https://bhdai.github.io/blog/posts/linear_classification/images/cover.png" medium="image" type="image/png" height="80" width="144"/>
</item>
<item>
  <title>A brief history of computer vision and deep learning</title>
  <dc:creator>Bui Huu Dai</dc:creator>
  <link>https://bhdai.github.io/blog/posts/history-of-computer-vision-and-deep-learning/</link>
  <description><![CDATA[ 






<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./images/cover_image.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="“A split scene: on one side, an old master painter (think Da Vinci) painting a realistic portrait, and on the other, a futuristic robot painting a digital image using code or neural networks as its brush strokes”, generated by DALL·E 3"><img src="https://bhdai.github.io/blog/posts/history-of-computer-vision-and-deep-learning/images/cover_image.png" class="img-fluid figure-img" alt="“A split scene: on one side, an old master painter (think Da Vinci) painting a realistic portrait, and on the other, a futuristic robot painting a digital image using code or neural networks as its brush strokes”, generated by DALL·E 3"></a></p>
<figcaption>“A split scene: on one side, an old master painter (think Da Vinci) painting a realistic portrait, and on the other, a futuristic robot painting a digital image using code or neural networks as its brush strokes”, generated by DALL·E 3</figcaption>
</figure>
</div>
<p>My goal over the next ten weeks or so is to have a deep, foundational understanding of the principles and practices that are driving the state-of-the-art in visual intelligence. So to begin our journey, I find it useful to first situate what we will be studying within a broader intellectual landscape. We can start with the most encompassing field: Artificial Intelligence.</p>
<section id="our-place-on-the-ai-map" class="level2">
<h2 class="anchored" data-anchor-id="our-place-on-the-ai-map">Our place on the AI map</h2>
<p>AI is the grand, overarching ambition. It’s the quest to build machines that can perform tasks that have historically required human intelligence (tasks like reasoning, planning, and perception). It’s a field with a long and rich history, full of profound philosophical questions and formidable engineering challenges.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./images/a-broad-view.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Where are we at? A broad view of the field of AI - Image inspired by Justin Johnson"><img src="https://bhdai.github.io/blog/posts/history-of-computer-vision-and-deep-learning/images/a-broad-view.png" class="img-fluid figure-img" style="width:80.0%" alt="Where are we at? A broad view of the field of AI - Image inspired by Justin Johnson"></a></p>
<figcaption>Where are we at? A broad view of the field of AI - Image inspired by Justin Johnson</figcaption>
</figure>
</div>
<p>Now, AI is an enormous domain. Within it, we can delineate several major sub-disciplines. Two of the most significant are <strong>Machine Learning</strong> and <strong>Computer Vision</strong>. <strong>Machine learning</strong> is a specific approach to achieving AI. Instead of explicitly programming a machine with a set of handcrafted rules to solve a task, the machine learning paradigm is to develop algorithms that allow machine to learn the rules by itself, by analyzing data. This shift from rule-based system to data-driven system is a fundamental concept that we will return to again and again. Then we have <strong>Computer Vision</strong>. This is the scientific and engineering discipline dedicated to a different goal: enabling machines to see. That is, to take in visual information from the world, from images, from video and to derive understanding from it. These two fields have a significant and ever-growing intersection. While there exists a body of classical computer vision work that does not rely on machine learning, think of the techniques from computational geometry or signal processing but the most powerful and prevalent methods in modern computer vision are fully rooted in machine learning.</p>
<p>Now let’s zoom in one level deeper. Within Machine Learning, a particular subfield has emerged over the last decade or so that has completely revolutionized the landscape. And that is <strong>Deep Learning</strong>. Deep learning is a specific class of machine learning algorithms. The defining characteristic is the use of neural networks with many layers, hence “deep” networks. These architectures, as we will go into great detail, have proven to be exceptionally effective at learning intricate patterns and hierarchical representations from vast amounts of data.</p>
<p>This brings us to the core focus of our discussion. The intersection of <strong>Deep Learning</strong> and <strong>Computer Vision</strong>. The red area on the diagram above is where we will spend our time. Our objective is to understand and implement deep learning architectures and methodologies that are purpose-built to solve computer vision problems. This convergence is responsible for nearly all of the dramatic breakthroughs in visual perception you may have seen in recent years.</p>
<p>However, it’s crucial to understand that while our focus is on vision, deep learning is not exclusively a tool for computer vision. It is a general-purpose computational paradigm that has had a similar transformative impact on other fields of AI. For example, another major subfield is <strong>Natural Language Processing</strong>, or NLP, which deals with enabling computers to understand and generate human language. And a closely related field is <strong>Speech Recognition</strong>, which focuses on converting spoken language into text. Both NLP and Speech have been fundamentally reshaped by the application of deep learning models.</p>
<p>We can further expand our map to include fields like <strong>Robotics</strong>. Robotics is an inherently integrated discipline. A truly autonomous robot must perceive its environment (which is a core computer vision problem) and then decide how to act, which often evolves from experience(a machine learning problem). Therefore, robotics draws heavily from both computer vision and machine learning and increasingly, deep learning is the unifying methodology.</p>
<p><strong>Mathematics</strong>, particularly linear algebra, probability, and calculus, provides the formal language and the core tools we use to define and optimize our models. <strong>Neuroscience</strong> and <strong>Psychology</strong> provide the biological inspiration for our network architectures and offer insights into the nature of intelligence itself. We also have <strong>Physics</strong> because we need to understand optics and image formation and how images are actually formed. We need to understand <strong>Biology</strong> and <strong>Psychology</strong> how the animal brain physically sees and processes visual information. And of course, all of this is built upon the substrate of <strong>Computer Science</strong> which gives us the algorithms, data structures, and high-performance computing systems necessary to make these computationally intensive ideas a reality.</p>
<p>Finally, it’s imperative to recognize that none of these fields exists in a vacuum. They are built upon and draw inspiration from a wide array of fundamental scientific disciplines. So while we will live in that red intersection of deep learning and computer vision, I want you to maintain this broader perspective. The work we do here connects to a rich and interdisciplinary tapestry of human knowledge.</p>
</section>
<section id="why-vision-from-first-eye-to-billion-cameras" class="level2">
<h2 class="anchored" data-anchor-id="why-vision-from-first-eye-to-billion-cameras">Why vision? From first eye to billion cameras</h2>
<p>Alright, so that gives you the sense of the intellectual landscape so let’s begin with the history. And to truly appreciate the motivation of our field i find it instructive to go back… quite a long way.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./images/the-dawn-of-vision.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Early multicellular life and the dawn of vision. All images from Wikipedia / CC‑BY. Left: Burgess Shale trilobite fossil preserving antennae and legs. Top right: Dickinsonia costata, a quilted Ediacaran organism of unknown affinity. Bottom right: Artistic reconstruction of Opabinia, the five‑eyed Cambrian critter that helped ignite interest in the Cambrian explosion."><img src="https://bhdai.github.io/blog/posts/history-of-computer-vision-and-deep-learning/images/the-dawn-of-vision.png" class="img-fluid figure-img" style="width:80.0%" alt="Early multicellular life and the dawn of vision. All images from Wikipedia / CC‑BY. Left: Burgess Shale trilobite fossil preserving antennae and legs. Top right: Dickinsonia costata, a quilted Ediacaran organism of unknown affinity. Bottom right: Artistic reconstruction of Opabinia, the five‑eyed Cambrian critter that helped ignite interest in the Cambrian explosion."></a></p>
<figcaption>Early multicellular life and the dawn of vision. All images from <a href="https://en.wikipedia.org/wiki/Cambrian_explosion">Wikipedia / CC‑BY</a>. Left: Burgess Shale trilobite fossil preserving antennae and legs. Top right: Dickinsonia costata, a quilted Ediacaran organism of unknown affinity. Bottom right: Artistic reconstruction of Opabinia, the five‑eyed Cambrian critter that helped ignite interest in the Cambrian explosion.</figcaption>
</figure>
</div>
<p>Roughly 540 million years ago, our planet experienced a period of unprecedentedly rapid diversification of complex, multicellular life. This is known as the Cambrian Explosion. And a leading scientific hypothesis for what acted as the primary catalyst for this “big bang” of evolution… was the advent of vision.</p>
<p>The development of the first primitive eyes created an enormous new set of evolutionary pressures. For the very first time, organisms could actively hunt, evade predators, and navigate their environment with a richness of information that was previously unimaginable. In a very real sense the ability to see changed the rules of life on Earth, and may have been the driving force behind the development and much of the biological complexity we see today.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./images/eyes.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Octopus camera-type eye, insect compound eye, chameleon turret eye, human binocular eye."><img src="https://bhdai.github.io/blog/posts/history-of-computer-vision-and-deep-learning/images/eyes.png" class="img-fluid figure-img" style="width:80.0%" alt="Octopus camera-type eye, insect compound eye, chameleon turret eye, human binocular eye."></a></p>
<figcaption>Octopus camera-type eye, insect compound eye, chameleon turret eye, human binocular eye.</figcaption>
</figure>
</div>
<p>And the legacy of that ancient innovation is all around us. Vision is a powerful example of convergent evolution. It has been independently invented by nature dozens of times across the tree of life. From the compound eyes of insects, which excel at detecting motion, to the incredibly sophisticated camera-like eyes of octopus, to the remarkable independently moving eyes of a chameleon… and of course, to our own visual system. The fact that evolution has arrived at the solution of “the eye” so many times underscores it profound utility as a mechanism for interacting with the world</p>
<p>For most of history vision was a purely biological phenomenon. But humanity has long been obsessed with capturing what we see, with creating an external record of our vision perception.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./images/camera-obscura.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="The Camera Obscura. Top right: first published picture of camera obscura, in Gemma Frisius’ 1545. Bottom right: Leonardo da Vinci, 16th Century AD. Left: camera obscura in Encyclopedia, 18th Century (all images from From Wikipedia, the free encyclopedia)"><img src="https://bhdai.github.io/blog/posts/history-of-computer-vision-and-deep-learning/images/camera-obscura.png" class="img-fluid figure-img" style="width:100.0%" alt="The Camera Obscura. Top right: first published picture of camera obscura, in Gemma Frisius’ 1545. Bottom right: Leonardo da Vinci, 16th Century AD. Left: camera obscura in Encyclopedia, 18th Century (all images from From Wikipedia, the free encyclopedia)"></a></p>
<figcaption>The Camera Obscura. Top right: first published picture of camera obscura, in Gemma Frisius’ 1545. Bottom right: Leonardo da Vinci, 16th Century AD. Left: camera obscura in Encyclopedia, 18th Century (all images from <a href="https://en.wikipedia.org/wiki/Camera_obscura">From Wikipedia, the free encyclopedia</a>)</figcaption>
</figure>
</div>
<p>This quest leads us to one of the most foundational principles in the history of imaging: The Camera Obscura, which is Latin for “dark chamber”. As early as the 16th century, and with principles understood even in antiquity, scholars and artists recognized that if you have a darkened enclosure with a small aperture, an inverted image of the external scene is projected into the opposite wall. This is the fundamental principle upon which all photography and even modern cameras is built. It represents the first critical step in humanity’s attempt to externalize the scene of sight.</p>
<p>Now, if we fast-forward from the simple pinhole in a dark room to the 21st century, the consequence of that is… staggering.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./images/cv-everywhere.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="Computer vision is now everywhere. First row, left to right: [1], [2], [3], [4]. Second row, left to right: [1], [2], [3], [4]. Third row, left to right: [1], [2], [3], [4]"><img src="https://bhdai.github.io/blog/posts/history-of-computer-vision-and-deep-learning/images/cv-everywhere.png" class="img-fluid figure-img" alt="Computer vision is now everywhere. First row, left to right: [1], [2], [3], [4]. Second row, left to right: [1], [2], [3], [4]. Third row, left to right: [1], [2], [3], [4]"></a></p>
<figcaption>Computer vision is now everywhere. First row, left to right: <a href="https://www.flickr.com/photos/sskennel/466632815">[1]</a>, <a href="https://pixabay.com/en/camera-lens-photographer-photo-193664/">[2]</a>, <a href="https://pixabay.com/en/drone-aerial-photo-djee-1142182/">[3]</a>, <a href="https://www.pexels.com/photo/red-hand-iphone-smartphone-80673/">[4]</a>. Second row, left to right: <a href="https://www.pexels.com/photo/woman-holding-a-white-samsung-galaxy-android-smartphone-taking-a-photo-of-hallway-38266/">[1]</a>, <a href="https://pixabay.com/en/selfie-couple-photography-dragooste-1363970/">[2]</a>, <a href="https://www.flickr.com/photos/gsfc/8145474144">[3]</a>, <a href="https://pixabay.com/p-1566884/">[4]</a>. Third row, left to right: <a href="https://www.pexels.com/photo/police-blue-sky-security-surveillance-96612/">[1]</a>, <a href="https://www.flickr.com/photos/dkeats/6363420863">[2]</a>, <a href="https://commons.wikimedia.org/wiki/File:Dashcams_P1210466.JPG">[3]</a>, <a href="https://commons.wikimedia.org/wiki/File:Google_Glass_detail.jpg">[4]</a></figcaption>
</figure>
</div>
<p>The reason we have a field called computer vision today is, in large part, because the sensors of vision (cameras) are utterly ubiquitous. They are in our pocket, in our cars, in our homes, attached to drones, flying through the air, and even roving the surface of other planets.</p>
<p>The proliferation of inexpensive, high-resolution digital cameras has resulted in an unprecedented deluge of visual data. More images are now captured every two minutes than were captured in the entire 19th century. This vast sea of pixels is the raw material, the fuel, that powers the deep learning models we will talk about a lot.</p>
<p>So this brings us to a critical question. We have this deep, biological imperative for vision, and we have this modern technological reality of ubiquitous cameras generating near-infinite data. Given this perfect storm of motivation and raw material… how did the scientific engineering discipline of <em>Computer Vision</em> actually come to be? Where did we, as a field, come from?</p>
</section>
<section id="neuroscience-lights-the-way" class="level2">
<h2 class="anchored" data-anchor-id="neuroscience-lights-the-way">Neuroscience lights the way</h2>
<p>The story often begins not in computer science but in neuroscience. In 1959, two neuroscientists, David Hubel and Torsten Wiesel, conducted a series of now-famous experiments for which they would later win the Nobel Prize. They sought to understand the architecture of the mammalian visual system. They did this by inserting microelectrodes into the primary visual cortex—the first cortical area to receive input from the eyes of an anesthetized cat. They then presented the cat with very simple visual stimuli on a screen—things like bars of light, dots, or oriented edges.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./images/neuroscience.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="Hubel &amp; Wiesel’s classic 1959 cat–visual‑cortex experiment (image inspired by Justin Johnson)"><img src="https://bhdai.github.io/blog/posts/history-of-computer-vision-and-deep-learning/images/neuroscience.png" class="img-fluid figure-img" style="width:80.0%" alt="Hubel &amp; Wiesel’s classic 1959 cat–visual‑cortex experiment (image inspired by Justin Johnson)"></a></p>
<figcaption>Hubel &amp; Wiesel’s classic 1959 cat–visual‑cortex experiment (image inspired by Justin Johnson)</figcaption>
</figure>
</div>
<p>What they discovered was remarkable. They found that individual neurons in the brain region were not responding to complex concepts like “a mouse” or “a food bowl”. Rather, they were highly specialized feature detectors. They identified two principal classes of cells. First, <strong>simple cells</strong>. A given simple cell would fire vigorously as shown in the top response graph, only when a bar of light with a very specific orientation appeared at a very specific location in the visual field. If the orientation was wrong, or if the stimulus was just a dot, the neuron remained silent. Then they found <strong>complex cells</strong>. These cells also respond to oriented edges, but they were invariant to the precise location of that edge within their receptive field. As you can see on the diagram, the bar can move, or translate, and the complex cell continues to fire. Many were also tuned to the direction of motion.</p>
<p>This discovery was profoundly influential. It provided the first biological evidence for a hierarchical visual processing system. Where the initial stages are dedicated to detecting simple, local features like oriented edges. This idea of building up complex recognition from a hierarchy of simple feature detectors is a cornerstone of modern computer vision, and as we will see, it is the fundamental architectural principle behind convolutional neural networks.</p>
<p>Just a few years later, inspired in part by this new understanding of biological vision, the field of computer vision had its genesis. Larry Robert’s 1963 PhD thesis MIT is widely considered to be the seminal work.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./images/edge-detector.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8" title="Roberts’s 1963 “block world” vision pipeline (image from epicsysinc)"><img src="https://bhdai.github.io/blog/posts/history-of-computer-vision-and-deep-learning/images/edge-detector.png" class="img-fluid figure-img" style="width:50.0%" alt="Roberts’s 1963 “block world” vision pipeline (image from epicsysinc)"></a></p>
<figcaption>Roberts’s 1963 “block world” vision pipeline (image from <a href="https://www.epicsysinc.com/blog/machine-vision-history-3/">epicsysinc</a>)</figcaption>
</figure>
</div>
<p>His system aimed to solve what seems like a simple problem: understanding the 3D geometry of simple “block world” scenes from a single 2D image. His approach was a pipeline. First take the original image. Second compute a “differentiated picture” which is a computational method for finding sharp changes in intensity in other words, an edge detector. This is a direct computational analog of what Hubel and Wiesel’s simple cell was doing. Finally, from this edge map, he would select feature points like corners and junctions and use geometric reasoning to infer the 3D shape. This was the start: a non-learning, rule-based system that decomposed vision into a series of explicit steps: find edges, find junctions, infer geometry.</p>
<p>This early success bred a great deal of optimism. So much so that in 1966, a group at MIT, led by Seymour Papert, proposed what is now famously known as <a href="https://dspace.mit.edu/handle/1721.1/6125">“The Summer Vision Project”</a>. The idea was, now we’ve got digital cameras, now they can detect edges, and Hubel and Wiesel told us how the brain works so basically what he wanted to do is hang a couple undergrads put them to work over the summer and after the summer we show it we should be able to construct a significant portion of visual system. The ambition was, in essence, to largely solve the problem of vision in a single summer by breaking it down into sub-problems. This, of course, turned out to be a profound underestimation of the problem’s difficulty. Now it’s clearly the computer vision was not solved and nearly 60 years later we’re still plugging away trying to achieve this what they thought they could do it in a summer with few undergrads. But it speaks to the excitement and perceived tractability of the field in its infancy.</p>
<p>Following this period of excitement and subsequent realization of the problem’s true depth, the field entered a phase of more systematic, theoretical thinking. The most influential of this era was David Marr.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./images/recognition-via-part.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9" title="Stages of Visual Representation, David Marr, 1970s. Bottom right: Recognition via Parts (1970s)"><img src="https://bhdai.github.io/blog/posts/history-of-computer-vision-and-deep-learning/images/recognition-via-part.png" class="img-fluid figure-img" alt="Stages of Visual Representation, David Marr, 1970s. Bottom right: Recognition via Parts (1970s)"></a></p>
<figcaption>Stages of Visual Representation, David Marr, 1970s. Bottom right: Recognition via Parts (1970s)</figcaption>
</figure>
</div>
<p>In the 1970s, Marr proposed a comprehensive framework for how a visual system should be structured. He argued for a staged, bottom-up pipeline. You start with an input image just an array of pixel intensities. The first stage is to compute what he called the <strong>Primal Sketch</strong>. This is a representation of 2D image structure, identifying primitive elements like zero-crossing, edges, bars, and blobs. Again, you see the direct intellectual lineage from Hubel and Wiesel. From the Primal Sketch, the system would then compute the <strong>2.5-D Sketch</strong>. This is a viewer-centric representation that captures local surface orientation and depth discontinuities. It’s not a full 3D model, but rather a map of how surfaces are angled relative to the observer. Finally from the 2.5-D Sketch, the system would construct a full, object-centered <strong>3-D Model Representation</strong>, describing the shapes and their spatial arrangement in a way that is independent from the viewpoint. This framework was immensely influential and guided vision research for many years.</p>
<p>Marr’s ideas spurred a great deal of research into how one might actually represent these 3D models. One popular idea from the 1970s was “Recognition via Parts”. One formulation of this was the idea of <strong>Generalized Cylinders</strong> proposed by Brooks and Binfold. The concept is to represent complex objects as a composition of simple, parameterized volumetric primitives like cylinders. A human figure can be modeled as an articulated collection of these cylinders. Another related idea was that of <strong>Pictorial Structure</strong>, from Fischler and Elshlager. Here, an object is represented as a collection of parts arranged in a deformable configuration, like nodes, connected by springs. This captures both the appearance of the parts and their plausible spatial relationships. Both of these are instantiations of the core idea that object recognition proceeds by identifying constituent parts and their arrangement.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./images/canny-edge-detection.png" class="lightbox" data-gallery="quarto-lightbox-gallery-10" title="Recognition via Edge Detection (1980s) - Edge detection algorithms by John Canny, 1986"><img src="https://bhdai.github.io/blog/posts/history-of-computer-vision-and-deep-learning/images/canny-edge-detection.png" class="img-fluid figure-img" style="width:80.0%" alt="Recognition via Edge Detection (1980s) - Edge detection algorithms by John Canny, 1986"></a></p>
<figcaption>Recognition via Edge Detection (1980s) - Edge detection algorithms by John Canny, 1986</figcaption>
</figure>
</div>
<p>Throughout the 1980s, much of the field’s energy was focused on perfecting the very first stage of Marr’s pipeline: edge detection. The thinking was that if we could just produce a perfect line drawing of the world from an image, as you see on the right, the subsequent steps of recognition would be much more tractable. This led to seminal work on edge detection algorithms, most famously by John Canny in 1986, whose algorithm is still baseline today, and also by David Lowe, whom we will encounter again later. The field became very good at turning images of things like those razors into learn edge maps.</p>
<p>Now, zooming out to the broader context of artificial intelligence during this period… something important was happening. The field was entering what became known as an “AI winter”. The massive enthusiasm and, critically, the government funding for AI research began to dwindle. This was largely the dominant paradigm of the time, so-called “Expert Systems” which tried to encode human expertise in vast, handcrafted rule-bases had failed on their very grandiose promise. However, this didn’t mean that research stopped. Instead, the subfield of AI, like computer vision, NLP, and robotics, continued to mature. They grew into more distinct disciplines, focusing on their own specific problems and developing their own specialized techniques, often with less of the grand, unifying ambition of the early AI pioneers.</p>
<p>But in the meantime.. while this entire arc of “classical” computer vision was unfolding, from Hubel and Wiesel to Marr to edge detector… another set of ideas, also with roots in neuroscience and cognitive science, was developing in parallel. And it is this other thread of history that will ultimately lead us to the “deep learning” part.</p>
</section>
<section id="learning-to-find-faces-in-a-crowd" class="level2">
<h2 class="anchored" data-anchor-id="learning-to-find-faces-in-a-crowd">Learning to find faces in a crowd</h2>
<p>Throughout the 1970s and 80s, cognitive scientists were conducting experiments that revealed just how complex and sophisticated the human system truly is, often in ways that these early models couldn’t account for.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./images/irving-biederman-experiment.png" class="lightbox" data-gallery="quarto-lightbox-gallery-11" title="Irving Biederman’s experiment in the early 1970s."><img src="https://bhdai.github.io/blog/posts/history-of-computer-vision-and-deep-learning/images/irving-biederman-experiment.png" class="img-fluid figure-img" alt="Irving Biederman’s experiment in the early 1970s."></a></p>
<figcaption>Irving Biederman’s experiment in the early 1970s.</figcaption>
</figure>
</div>
<p>One such piece of work comes from Irving Biederman in the early 1970s. He represented subjects with images like the one you see on the left—a coherent, real-world scene. Unsurprisingly, people can recognize this scene and its constituent objects almost instantaneously. But then he would show them an image like the one on the right, which contains the exact image patches, but jumped into a non-sensical configuration. Recognition of the individual objects in this jumbled scene is significantly slower and more difficult. This simple but elegant experiment demonstrates a crucial point: our visual system doesn’t just recognize isolated parts. It relies heavily on the global context and the plausible spatial arrangement of those parts. The “whole” is more, and is processed differently than, the sum of its parts. This posed a significant challenge to a purely bottom-up, part-based recognition pipeline.</p>
<p>Another line of inquiry focused on the sheer speed of human vision. A common experimental paradigm used to study this is called Rapid Serial Visual Perception, or RSVP. The setup is simple: a subject fixates on a cross at the center of a screen, and images are flashed in very rapid succession often for only a few tens of milliseconds each.</p>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row quarto-layout-valign-center">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./images/RSVP.png" class="lightbox" data-gallery="quarto-lightbox-gallery-12" title="EEG signal corresponding to the brain’s responses"><img src="https://bhdai.github.io/blog/posts/history-of-computer-vision-and-deep-learning/images/RSVP.png" class="img-fluid figure-img" alt="EEG signal corresponding to the brain’s responses"></a></p>
<figcaption>EEG signal corresponding to the brain’s responses</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><a href="./images/RSVP.gif" class="lightbox" data-gallery="quarto-lightbox-gallery-13"><img src="https://bhdai.github.io/blog/posts/history-of-computer-vision-and-deep-learning/images/RSVP.gif" class="img-fluid"></a></p>
</div>
</div>
</div>
<p>In 1996, Thorpe and colleagues used this RSVP paradigm in conjunction with electroencephalography, or EEG, which measures electrical activity in the brain with very high temporal resolution. They flashed images of animals and non-animals and asked subjects to perform a simple categorization task. What they found, as you can see on the plot, was outstanding. The EEG signal corresponds to the brain’s response to “animal” images, shown in darkest line, significantly diverged from the signal for “non-animal” images, shown in lightest line, at approximately 150 milliseconds after the image was presented. 150 milliseconds. To put that in perspective, a single blink of an eye takes about 300 to 400 milliseconds. This implies that the core computation underlying object recognition (from photons hitting the retina to a high-level semantic distinction) happens in a fraction of a blink. This is a critical insight that will strongly inform the design of the deep neural network we will talk about later.</p>
<p>And where in the brain is this happening? The advent of <a href="https://en.wikipedia.org/wiki/Functional_magnetic_resonance_imaging">functional Magnetic Resonance Imaging</a>, or fMRI, in the 1990s allowed researchers to start answering this question. While fMRI has poor temporal resolution, it has good spatial resolution, allowing us to see what brain regions are active during a task. Seminal work by Nancy Kanwisher and her colleagues called <a href="https://royalsocietypublishing.org/doi/10.1098/rstb.2006.1934">“The fusiform face area: a cortical region specialized for the perception of faces”</a> identified specific regions in human brain that show preferential activation for specific high-level categories. For instance they discovered a region in the fusiform gyrus, which they termed the Fusiform Face Area or FFA, that responds quickly to faces than to other objects like houses. Conversely, they found another region, the Parahippocampal Place Area or PPA, that shows opposite preference: it responds strongly to scenes like houses, but not to faces. This provided concrete evidence for semantic organization and specialization within the higher level of the visual cortex.</p>
<p>So taking stock of these findings from neuroscience and cognitive science, a clear picture emerges. Visual recognition is a fundamental, core competency of visual intelligence. And the biological solution to this problem is incredibly fast, it exploited global context, and it appears to culminate in specialized representations for semantically meaningful categories. This understanding began to shift the focus of the computer vision community itself.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./images/1997-2001-face-detection.png" class="lightbox" data-gallery="quarto-lightbox-gallery-14" title="Toward face recognition (~1997–2001). Top row: output of segmentation methods (normalized‑cuts in 1997) showing region grouping into visually coherent “blobs”. Middle of timeline: SIFT (1999) introduces keypoint detection and robust local feature description based on invariant orientation and scale matching. Bottom-right: Viola–Jones face detection framework (2001) applies a cascade of Haar‑like feature classifiers to group evidence into fast, reliable face detections in real time"><img src="https://bhdai.github.io/blog/posts/history-of-computer-vision-and-deep-learning/images/1997-2001-face-detection.png" class="img-fluid figure-img" alt="Toward face recognition (~1997–2001). Top row: output of segmentation methods (normalized‑cuts in 1997) showing region grouping into visually coherent “blobs”. Middle of timeline: SIFT (1999) introduces keypoint detection and robust local feature description based on invariant orientation and scale matching. Bottom-right: Viola–Jones face detection framework (2001) applies a cascade of Haar‑like feature classifiers to group evidence into fast, reliable face detections in real time"></a></p>
<figcaption>Toward face recognition (~1997–2001). Top row: output of segmentation methods (normalized‑cuts in 1997) showing region grouping into visually coherent “blobs”. Middle of timeline: SIFT (1999) introduces keypoint detection and robust local feature description based on invariant orientation and scale matching. Bottom-right: Viola–Jones face detection framework (2001) applies a cascade of Haar‑like feature classifiers to group evidence into fast, reliable face detections in real time</figcaption>
</figure>
</div>
<p>Coming out of the AI winter and into the 1990s, the field began to move away from pure edge detection and towards tackling the recognition problem more directly. One prominent approach was what we called “Recognition via Grouping”. The idea here is that a critical step towards recognition is to segment the image into perceptually meaningful regions. A landmark algorithm in this era was <a href="https://dl.acm.org/doi/abs/10.5555/794189.794502">Normalized Cuts</a> developed by Jianbo Shi and Jitendra Malik in 1997. As you can see, it takes an input image and groups pixels into coherent segments, effectively partitioning the image into a foreground object and a background. The underlying principle is based on graph theory, finding a cut in the pixel graph that minimizes a particular normalized cost. The thinking was, if we can achieve a good segmentation, recognition of the isolated object becomes a much simpler problem.</p>
<p>Then as we moved into the 2000s, another paradigm emerged that would become incredibly dominant: “Recognition via Matching”. The quintessential work here is <a href="https://en.wikipedia.org/wiki/Scale-invariant_feature_transform">David Lowe’s Scale-Invariant Feature Transform</a>, or SIFT, from 1999. The core innovation of SIFT was a procedure to find a set of local, high distinctive keypoints in an image and to describe them in a way that is invariant to transformation like changes in scale, image rotation, and to some extent, illumination. Recognition then becomes a task of matching these keypoint descriptors between a query image, and a database of known objects. As you can see here, the algorithm can robustly find corresponding points to the stop sign, even though it’s viewed from a different angle and at a different scale. For about a decade, feature-based methods like SIFT were the state-of-the-art for many object recognition tasks.</p>
<p>And right at the turn of the millennium, in 2001, we see a truly landmark achievement that pointed to the future. This was <a href="https://faculty.cc.gatech.edu/~hic/CS7616/Papers/Viola-Jones-2004.pdf">the face detector</a> developed by Paul Viola and Michael Jones. This was one of the first truly robust and real-time objective detections. It was so effective that it was quickly incorporated into consumer digital cameras, enabling the auto-focus-on-faces feature that we now take for granted. What was so revolutionary about the Viola-Jones detector was that it was one of the most highly successful applications of machine learning to a core computer vision problem. Instead of a human engineer meticulously designing feature to find faces, their algorithm learned a cascade of very simple rectangular features using a machine learning algorithm called AdaBoost, trained on a large dataset of positive examples (faces) and negative examples (non-faces). This was a critical turning point. It demonstrated, in a practical and impactful way, the power of a data-driven, learning-based approach over purely hand-engineered systems. And it’s this learning-based philosophy that, when taken to its extreme, will lead us to the deep learning revolution.</p>
</section>
<section id="the-rise-fall-and-return-of-neural-networks" class="level2">
<h2 class="anchored" data-anchor-id="the-rise-fall-and-return-of-neural-networks">The rise, fall, and return of Neural Networks</h2>
<p>So, we have now traced this timeline of computer vision up to the mid-2000s, We’ve seen the influence of neuroscience, the Marr paradigm, the focus on features like SIFT, and the nascent rise of machine learning. Now to understand what happens next, to understand the “deep learning” revolution we need to pause this timeline rewind all the way to the beginning, and pick up a completely different intellectual thread that was developing in parallel. This second thread also begins in the late 1950s, concurrent with Hubel and Wiesel’s discovery, in 1958, a psychologist named Frank Rosenblatt developed the <strong>Perceptron</strong>. The Perceptron was a simple computational model of a single biological neuron. It took a set of inputs, multiplied each by a corresponding weight, summed them up and if that sum exceeded a certain threshold it would output a “1”, otherwise “0”. It was simple, linear classifiers. And crucially, Rosenblatt devised a learning rule to automatically adjust the weights based on training examples.</p>
<p>However, this early enthusiasm for Perceptrons was dealt a severe blow in 1969 with the publication of the book Perceptron by Marvin Minsky and Seymour Papert. In this highly influential critique, they rigorously analyzed the mathematical properties of the single-layer Perceptron. They famously showed that there are certain, seemingly simple functions that a Perceptron is fundamentally incapable of learning. The canonical example is the logical XOR function.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./images/perceptron-fall.png" class="lightbox" data-gallery="quarto-lightbox-gallery-15" title="The perceptron’s inability to solve XOR and its critique by Minsky &amp; Papert."><img src="https://bhdai.github.io/blog/posts/history-of-computer-vision-and-deep-learning/images/perceptron-fall.png" class="img-fluid figure-img" alt="The perceptron’s inability to solve XOR and its critique by Minsky &amp; Papert."></a></p>
<figcaption>The perceptron’s inability to solve XOR and its critique by Minsky &amp; Papert.</figcaption>
</figure>
</div>
<p>As you can see, the XOR function is true if one of its true inputs is true, but not both. If you plot the four possible input pairs, you find that you can not draw a single straight line to separate the ‘1’ outputs from ‘0’ outputs. Because the Perceptron is a linear classifier, it is mathematically impossible for it to solve this non-linearly problem. This critique was so powerful that it led to a significant decline in funding and research into neural networks, contributing to that first “AI winter” we discussed.</p>
<p>Despite this, some research continued, and in 1980, Kunihiko Fukushima in Japan developed a model called Neocognitron in a paper <a href="https://www.cs.princeton.edu/courses/archive/spr08/cos598B/Readings/Fukushima1980.pdf">“Neocognitron: A Self-organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position”</a>. This is a truly remarkable piece of work, because it’s arguably the direct architectural ancestor of modern convolutional neural networks. The Neocognitron was explicitly and directly inspired by Hubel and Wiesel’s hierarchical of the visual cortex. It consisted of multiple layers, alternating between what Fukushima called S-cells and C-cells. The S-cells or simple cells, perform pattern matching using operations that are mathematically equivalent to what we now call <strong>convolution</strong>. The C-cells or complex cells, then provided spatial invariance by performing an operation analogous to what we now call <strong>pooling</strong> or subsampling. This is the fundamental architectural motif of a modern ConvNet. However, the Neocognitron had a critical limitation: it lacked a principled, end-to-end training algorithm. It was largely trained layer-by-layer with an unsupervised learning rule, and much of it was still hand-designed.</p>
<p>The missing piece of the puzzle arrived in 1986. In a landmark paper <a href="https://direct.mit.edu/books/edited-volume/5431/chapter-abstract/3958547/1986-David-E-Rumelhart-Geoffrey-E-Hinton-and?redirectedFrom=fulltext">“Learning representations by back-propagating errors”</a>, David Rumelhart, Geoffrey Hinton, and Ronald Williams popularized the backpropagation algorithm. Backpropagation is, in essence, an efficient method for computing the gradient of a loss function with respect to the weights of a multi-layered neural network. It’s a clever application of the chain rule from calculus. This algorithm provided the key that Minsky and Papert had pointed out was missing: a way to assign credit, or blame, to each neuron in each network, allowing one to systematically adjust the weights to improve performance. For the first time, it was possible to successfully train perceptrons with multiple layers, enabling them to learn non-linear function like XOR.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./images/lenet.png" class="lightbox" data-gallery="quarto-lightbox-gallery-16" title="Top: Lenet, applied backprop algorithm to a Neocognitron-like architecture, learned to recognize handwritten digits. Bottom: Unsupervised pre-training technique by Hinton, Bengio, and others"><img src="https://bhdai.github.io/blog/posts/history-of-computer-vision-and-deep-learning/images/lenet.png" class="img-fluid figure-img" alt="Top: Lenet, applied backprop algorithm to a Neocognitron-like architecture, learned to recognize handwritten digits. Bottom: Unsupervised pre-training technique by Hinton, Bengio, and others"></a></p>
<figcaption>Top: Lenet, applied backprop algorithm to a Neocognitron-like architecture, learned to recognize handwritten digits. Bottom: Unsupervised pre-training technique by Hinton, Bengio, and others</figcaption>
</figure>
</div>
<p>Now, we see the synthesis, in 1998, Yann LeCun and his colleagues took the Neocognitron architecture (with its alternating layers of convolution and pooling) and applied the backpropagation algorithm to train it from end-to-end on a real world task: recognizing handwritten digits. The resulting model, known as LeNet-5, was a tremendous success. It archived state-of-the-art performance and was deployed commercially by AT&amp;T to read handwritten checks. If you look at this architecture diagram, it is strikingly similar to the convolutional neural networks we use today. This was a powerful proof of concept, demonstrating that these neurally-inspired, trained architectures could solve real, practical problems.</p>
<p>This success spurred a small dedicated community of researchers throughout the 2000s to explore what was then beginning to be called “Deep Learning”. The central idea was to build networks that deeper and deeper, with the hypothesis that more layers would allow the learning of more complex and hierarchical features. However, this was not yet a mainstream topic. Training these very deep networks proved to be extremely difficult due to the optimization challenges like the vanishing gradient problem. Researchers like Hinton, Bengio, and others developed clever techniques, like the unsupervised pre-training shown here, to try to initialize these deep networks in a better way before fine-tuning them with backpropagation.</p>
</section>
<section id="the-dataset-that-changed-everything" class="level2">
<h2 class="anchored" data-anchor-id="the-dataset-that-changed-everything">The dataset that changed everything</h2>
<p>Alright. So, the Viola-Jones face detector in 2001 gave us a powerful glimpse into the future, showing what was possible when you replaced hand-engineered rules with data-driven machine learning. This trend toward learning-based approaches and the need to rigorously evaluate them, led to another critical development in the field.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./images/caltech.png" class="lightbox" data-gallery="quarto-lightbox-gallery-17" title="Left: The Caltech 101 images. Right: PASCAL Visual Object Challenge"><img src="https://bhdai.github.io/blog/posts/history-of-computer-vision-and-deep-learning/images/caltech.png" class="img-fluid figure-img" alt="Left: The Caltech 101 images. Right: PASCAL Visual Object Challenge"></a></p>
<figcaption>Left: The Caltech 101 images. Right: PASCAL Visual Object Challenge</figcaption>
</figure>
</div>
<p>And that was the creation of standardized, large-scale benchmark datasets. Before the 2000s, it was common for researcher to test their algorithms on their own private, often small, collections of images. This made a direct, quantitative comparison of different methods exceptionally difficult. The establishment of datasets like Caltech101 in 2004, and later the PASCAL Visual Object Challenge, which ran from 2005 to 2012, was a major step in transforming computer vision into a more rigorous empirical science. PASCAL was particularly influential because it went beyond simple image classification. It challenged algorithms to perform more complex tasks like object detection drawing a bounding box around an object and semantic segmentation. These shared benchmarks created a common ground, a competitive arena, where the entire community could measure progress.</p>
<p>Still, deep learning remained something of a niche topic within a broader machine learning and computer vision community. And there was a fundamental reason for this. Even with these new algorithm tricks, these deep high-capacity models were incredibly data-hungry. They require vast amounts of labeled data to learn meaningful representations and to avoid overfitting. And in the mid-2000s, there was simply no good dataset to work on. The existing benchmarks, like Caltech101, were orders of magnitude too small to truly unlock the potential of these models. The algorithms were simply ahead of the data. And that brings us to the final, critical ingredient that would ignite the deep learning explosion. The <strong>ImageNet</strong> dataset.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./images/imagenet.png" class="lightbox" data-gallery="quarto-lightbox-gallery-18" title="ImageNet Large Scale Visual Recognition Challenge"><img src="https://bhdai.github.io/blog/posts/history-of-computer-vision-and-deep-learning/images/imagenet.png" class="img-fluid figure-img" alt="ImageNet Large Scale Visual Recognition Challenge"></a></p>
<figcaption>ImageNet Large Scale Visual Recognition Challenge</figcaption>
</figure>
</div>
<p>Conceived and led by Fei-Fei Li, starting in 2007, the ImageNet project was an effort of unprecedented scale. The goal is to map out the entire noun hierarchy of WorldNet and populate it with millions of clean, annotated images. The result was a dataset with over 14 million images, spanning more than 20,000 categories. Crucially, in 2010, the project launched the ImageNet Large Scale Visual Recognition Challenge, or ILSVRC. This competition focused on a subset of the data: 1,000 object classes, with roughly 1.3 million training images. The task was straightforward: given an image, produce a list of five object labels, and you get credit if the correct label is in your list. This dataset and this annual challenge provide a perfect crucible. It was a dataset massive and complex enough to finally demonstrate the power of data-hungry deep learning models, and a competition that would pit them directly against the state-of-the-art classical computer vision system of the day. The stage was now set for a revolution.</p>
<p>So, ImageNet sets the stage. Now let’s look at the performance on this challenge over the years. The bar chart above shows the top-5 error rate. That means the model gets five guesses, and if the correct label isn’t in those five, it’s an error. In 2010, the winning entry from Lin et al.&nbsp;had an error of <strong>28.2%</strong>. In 2011, Sanchez &amp; Perronnin improved this to <strong>25.8%</strong>. These were typically system based on more traditional computer vision pipelines(hand-crafted features like SIFT or HoG), followed by machine learning classifiers like SVMs. Good progress, but still very high error rate. Then look at <strong>2012</strong>. A massive drop to <strong>16.4%</strong> with Krizhevsky et al.’s model, which we now famously know as <strong>AlexNet</strong>. We’ll talk a lot about AlexNet. The trend continues, 2013, Zeiler &amp; Fergus: <strong>11.7%</strong>, 2014, we see two big ones: VGG (Simonyan &amp; Zisserman) at <strong>7.3%</strong> and GoogLeNet (Szegedy et al.) at <strong>6.7%</strong>. And then, a really significant milestone in 2015: ResNet (He et al.) achieved 3.<strong>6%</strong> error. Now, why is that 3.6% so significant? Look over the far right. Andrej Karpathy, when he was a PhD at Stanford and several others including Fei-Fei, did a study (Russakovsky et al.&nbsp;IJCV 2015) to benchmark human performance on the subset of ImageNet. And a well-trained human annotator gets around <strong>5.1% top-5 error</strong>. So, by 2015, deep learning models were, for the first time, <strong>surpassing human-level performance</strong> on this specific, very challenging task! The progress didn’t stop there, 2016, 2017 saw even lower error rates with models like SENet</p>
<p>Now, let’s zero in on that pivotal moment, <strong>AlexNet, 2012</strong>. You see the red arrow pointing squarely at that 2012 bar. That 28% down to 16% was not an incremental improvement; it was a <strong>paradigm shift</strong>. This was the moment deep learning, specifically deep convolutional neural networks, truly announced its arrival and demonstrated its power to the broader computer vision community. AlexNet in 2012 right after Deep learning(2016) and ImageNet(2009), this isn’t a coincidence. The availability of a large dataset like ImageNet, coupled with the increasing computational power of GPUs, allowed deep learning models, which had been around conceptually for a while(you see LeNet from ’98, Neocognitron from ’80) , to finally be trained effectively at scale. AlexNet’s success fundamentally changed the direction of computer vision research. Almost overnight, people shifted from feature engineering to learning features directly from data using deep neural networks. And the rest, as they say, is history, as subsequent years on that chart. So, ImageNet provided the challenge, and AlexNet provided the breakthrough deep learning solution. The combination really superchanged the field, and it’s why we’re here talking about these powerful models.</p>
</section>
<section id="a-revolution-in-pixels" class="level2">
<h2 class="anchored" data-anchor-id="a-revolution-in-pixels">A revolution in pixels</h2>
<p>Okay, we’ve seen how AlexNet in 2012 was a watershed moment for deep learning in computer vision, dramatically improving performance on ImageNet. Now let’s look at what happened after 2012</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./images/deep-learning-explosion.png" class="lightbox" data-gallery="quarto-lightbox-gallery-19" title="Left: Publications at top Computer Vision conferences. Right: arXiv papers per month"><img src="https://bhdai.github.io/blog/posts/history-of-computer-vision-and-deep-learning/images/deep-learning-explosion.png" class="img-fluid figure-img" alt="Left: Publications at top Computer Vision conferences. Right: arXiv papers per month"></a></p>
<figcaption>Left: Publications at top Computer Vision conferences. Right: arXiv papers per month</figcaption>
</figure>
</div>
<p>The graph on the left shows the number of paper submissions and acceptances to CVPR, which is one of the, if not the, top computer vision conferences. You can see a steady growth from 1985 up to around 2010-2012. But then, look what happens after 2012, especially the submissions. It just takes off, almost exponentially! We’re talking about going from around 2000, submissions to over 7000-8000 in just a few years. Now, the graph on the right shows the number of <strong>Machine Learning and AI papers uploaded to arXiv per month</strong>. arXiv, for those who don’t know, is a preprint server where researchers can upload their papers before or alongside peer review. This allows for very rapid dissemination of ideas. Again, we have a relative modest until around 2012-2013, and then it just skyrockets. We’re looking at thousands of ML/AI paper per month now. This isn’t just computer vision; it’s the broader AI field, but computer vision and deep learning are huge drivers of this trend.</p>
<p>So, we have an explosion of papers and research. But what kind of research? What were people working on? Let’s look at the winner of the ImageNet challenge each year following AlexNet:</p>
<ul>
<li><strong>Year 2010(NEC-UIUC):</strong> Before the deep learning craze really hit ImageNet, this was what a state-of-the-art system looked like. You had a ‘Dense descriptor grid’ using features like HOG and LBP, then some ‘Coding’ (like local coordinate coding), ‘Pooling’ (Spatial Pyramid Matching - SPM), and finally a ‘Linear SVM’ for classification. This is a classic, handcrafted feature pipeline.</li>
<li><strong>Year 2012 (SuperVision, aka AlexNet):</strong> We’ve talked about this, Krizhevsky, Sutskever, and Hinton. It’s a stack of layers(convolutions, pooling, fully connected layers). This is a deep convolutional neural network, learning features directly from data.</li>
<li><strong>Year 2014 (GoogLeNet and VGG):</strong> Two years later, and we see even more sophisticated architectures.
<ul>
<li><strong>GoogleNet</strong> (from Google, Szegedy et al.) the idea was to have filters of different sizes operating in parallel. It was also very deep but computationally quite efficient.</li>
<li><strong>VGG</strong> (from Oxford, Simonyan &amp; Zisserman) took a different approach: very simple, uniform architecture, just stacking 3x2 convolutions and 2x2 pooling layers deeper and deeper.</li>
</ul></li>
<li><strong>Year 2015 (MSRA, aka ResNet):</strong> This was another huge leap, from Microsoft Research Asia(He et al.). This is ResNet, or Residual Network. They introduced ‘skip connections’ or ‘residual connections’ which allowed them to train networks that were incredibly deep, even over 100 or 1000 layers, which was previously impossible due to vanishing gradient problems. This architecture, or variants of it, became the backbone for many, many subsequent models.</li>
</ul>
<p>So, in just a few years, we went from handcrafted pipelines to relatively shallow (by today’s standards) CNNs, to very deep and complex architectures, each pushing the boundaries of performance and what we thought was possible.</p>
<p>Now let’s look at what these models can actually do.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./images/deep-learning-everywhere.png" class="lightbox" data-gallery="quarto-lightbox-gallery-20" title="Deep learning is now everywhere"><img src="https://bhdai.github.io/blog/posts/history-of-computer-vision-and-deep-learning/images/deep-learning-everywhere.png" class="img-fluid figure-img" alt="Deep learning is now everywhere"></a></p>
<figcaption>Deep learning is now everywhere</figcaption>
</figure>
</div>
<p>On the far left, we have examples of <strong>Image Classification</strong> from AlexNet back in 2012. For each image, the model outputs a list of probabilities for different classes, and here we see the top predictions. These aren’t just simple ‘cat’ or ‘dog’ classifications; the model is identifying specific types of objects, often in challenging, cluttered scenes. And these are real images, not just sanitized datasets. This was a clear demonstration of the power of these learned features. In the middle, we see an application called <strong>Image Retrieval</strong>. The idea here is: given a query image, can the system find visually and semantically similar images from a large database? These are just two fundamental computer vision tasks, classification and retrieval. But the success of deep learning, starting around 2012, has meant it’s now being applied to virtually every area of computer vision: object detection, segmentation, image captioning, image generation, video analysis, 3D reconstruction and so much more.</p>
<p>Continuing with the theme of understanding humans and dynamic scenes at the bottom, we have <strong>Pose Recognition</strong> also known as human pose estimation. The goal here is to identify the key joints of a person’s body like elbows, wrists, knees, ankles, head, shoulders. You can see in these examples (from Toshev and Szegedy, 2014, “DeepPose”, one of the first deep learning approaches for this) that the model can accurately locate these joints even with varied clothing, complex poses, and different backgrounds. This is fundamental for a deeper understanding of human actions, for animation, and augmented reality, and more.</p>
<p>And the reach of deep learning extends far beyond everyday scenes, videos, or games. It’s making significant impacts in highly specialized scientific and medical domains. On the far right, we have <strong>Whale Recognition</strong>. This might seem niche, but it’s important for ecological studies and conservation This particular image refers to a Kaggle challenge <a href="https://www.kaggle.com/c/whale-categorization-playground">here</a> is the link to the competition page, where participants build models to automatically identify individual whales from photograph. Deep learning is very good at these kinds of fine-grained visual recognition tasks</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./images/even-more.png" class="lightbox" data-gallery="quarto-lightbox-gallery-21" title="Top left: Image Captioning Vinyals et al, 2015 Karpathy and Fei-Fei, 2015. Top left: Krishna et al., ECCV 2016 the “Visual Genome” dataset and the work on generating scene graphs. Bottom: The Neural Style Transfer Algorithm (Gatys et al.&nbsp;2016), which stylizes a photograph in the style of a given artwork"><img src="https://bhdai.github.io/blog/posts/history-of-computer-vision-and-deep-learning/images/even-more.png" class="img-fluid figure-img" alt="Top left: Image Captioning Vinyals et al, 2015 Karpathy and Fei-Fei, 2015. Top left: Krishna et al., ECCV 2016 the “Visual Genome” dataset and the work on generating scene graphs. Bottom: The Neural Style Transfer Algorithm (Gatys et al.&nbsp;2016), which stylizes a photograph in the style of a given artwork"></a></p>
<figcaption>Top left: Image Captioning Vinyals et al, 2015 Karpathy and Fei-Fei, 2015. Top left: Krishna et al., ECCV 2016 the “Visual Genome” dataset and the work on generating scene graphs. Bottom: The Neural Style Transfer Algorithm (Gatys et al.&nbsp;2016), which stylizes a photograph in the style of a given artwork</figcaption>
</figure>
</div>
<p>Now, this is where things get really interesting. We’re moving beyond just recognizing objects or pixels, and into the realm of understanding and describing images using natural language. This is <strong>Image Captioning</strong>. The task is, given an image, to automatically generate a human-like sentence that describes what’s happening in the image. These captions are remarkably accurate and fluent. This typically involves a combination of a Convolutional Neural Network (RNN), often an LSTM, to ‘generate’ the sentence word by word, conditioned on those visual features. The work here is from Vinyals et al.&nbsp;(from Google) and Karpathy and Fei-Fei (from Stanford), both published around 2015, were seminal works in this area, showing how to effectively combine CNNs and RNNs for this task. This was huge step toward machines that can not only see but also communicate what they see.</p>
<p>Image captioning gives us a sentence. But can we get a even deeper understanding of the relationships and interactions within an image? On the right you see an image with objects detected, and blow it we see something more structured: a <strong>scene graph</strong>. This moves us towards a much more comprehensive understanding of visual scenes. The work from Krishna et al.&nbsp;ECCV 2016, refers to the “Visual Genome” dataset and the work on generating a scene graph, which provides a dense, structured annotation of images, capturing objects, attributes, and relationships. This is crucial for tasks like visual question answering, where the model answers questions about an image and more complex reasoning about visual content.</p>
<p>So far, we’ve mostly seen deep learning used for understanding or analyzing images. But what about creating them? Or manipulating the artistic ways? At the bottom we see something called Neural Style Transfer pioneered by Gatys et al.&nbsp;in 2016. Here, you take two images, a <strong>content image</strong> here is the houses on the street and a <strong>style image</strong> like a famous painting like Van Gogh’s The Starry Night. The algorithm then synthesizes a new image that has the content of the first image but is rendered in the style of the second. So you get the houses looking as if they were painted by Van Gogh, or in a stained-glass style. This is done by optimizing an image to match content features from one image and style features (correlations between activations in different layers) from another, using a pre-trained CNN.</p>
<p>Continuing with generative models, we’ve shown artistic generation. But what about generating entirely new, photorealistic images from scratch? And this points to that capability, especially referencing <strong>Generative Adversarial Networks</strong> or GANs.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./images/gans.png" class="lightbox" data-gallery="quarto-lightbox-gallery-22" title="Sliced Wasserstein distance (SWD) between the generated and training images (Section 5) and multi-scale structural similarity (MS-SSIM) among the generated images for several training setups at 128 × 128. And (a-g) CELEBA examples corresponding to rows in Table 1. (h) the converged result"><img src="https://bhdai.github.io/blog/posts/history-of-computer-vision-and-deep-learning/images/gans.png" class="img-fluid figure-img" alt="Sliced Wasserstein distance (SWD) between the generated and training images (Section 5) and multi-scale structural similarity (MS-SSIM) among the generated images for several training setups at 128 × 128. And (a-g) CELEBA examples corresponding to rows in Table 1. (h) the converged result"></a></p>
<figcaption>Sliced Wasserstein distance (SWD) between the generated and training images (Section 5) and multi-scale structural similarity (MS-SSIM) among the generated images for several training setups at 128 × 128. And (a-g) CELEBA examples corresponding to rows in Table 1. (h) the converged result</figcaption>
</figure>
</div>
<p>This is from Karras et al., for their work on <a href="https://arxiv.org/pdf/1710.10196">“Progressive Growing of GANs for improved Quality”</a> <strong>GANs</strong>, introduced by Ian Goodfellow and his colleagues in 2014, work by having two networks compete against each other. A <strong>Generator</strong> network tries to create realistic images for example from random noise. And a <strong>Discriminator</strong> network tried to distinguish between real images (from training set) and images created by the generator. Through this adversarial process, the generator gets better and better at creating images that can fool the discriminator, and the discriminator gets better at telling them apart. The <strong>“Progressive Growing of GANs”</strong> technique, developed by Karras and his team at NVIDIA was a major breakthrough. It allowed for the generation of much higher-resolution and more stable results than previously possible. They started by generating very small images like 4x4 pixels and then progressively added layers to both the generator and discriminator to produce larger and more detailed images like 8x8, 16x16, all the way up to 1024x1024. You can see from the image above, faces that are not real people, yet they look entirely plausible. This ability to synthesize photorealistic imagery has huge implications for art, design, entertainment, data augmentation, and of course, also raises important ethical considerations about ‘deepfakes’ and misinformation. But these generative capabilities truly underscore how far deep learning has come since 2012, from classifying images to creating entirely new visual realities.</p>
<p>We’ve seen some incredible generative capabilities, like GANs creating photorealistic faces. But what if we could guide that generation with more than just random noise or style images? What if we could tell the model exactly what we want it to create, using natural language?</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./images/dalle.png" class="lightbox" data-gallery="quarto-lightbox-gallery-23" title="Ramesh et al, “DALL·E: Creating Images from Text”, 2021, images from https://openai.com/blog/dall-e/"><img src="https://bhdai.github.io/blog/posts/history-of-computer-vision-and-deep-learning/images/dalle.png" class="img-fluid figure-img" alt="Ramesh et al, “DALL·E: Creating Images from Text”, 2021, images from https://openai.com/blog/dall-e/"></a></p>
<figcaption>Ramesh et al, “DALL·E: Creating Images from Text”, 2021, images from <a href="https://openai.com/blog/dall-e/">https://openai.com/blog/dall-e/</a></figcaption>
</figure>
</div>
<p>This brings us to one of the most mind-blowing developments in recent years: <strong>Text-to-image Generation</strong>. These are not images found on the internet these were created by an AI model based purely on that text description. And they are remarkably good! You see various interpretations, some look more like a cut avocado half turned into a chair, others are more abstract but clearly evoke both “armchair”and ‘avocado’. What’s so powerful about this (and models like DALL-E, Imagen, Stable Diffusion, etc.) is the <strong>compositionality</strong> and <strong>zero-shot generalization</strong>. The model has likely never seen an “armchair in the shape of an avocado” during its training. But it knows what armchairs are, it knows what avocados are, and it understands how to combine these concepts based on the textual relationships. The images above are from Ramesh et al, 2021, for <strong>DALL-E</strong>, a groundbreaking model from OpenAI. This kind of model is typically a very large transformer-based architecture, trained on massive datasets of image-text pairs. It learns to associate visual concepts with textual descriptions and can then generate novel images by combining these learned concepts in new ways. This ability to translate complex, even whimsical, textual prompts into coherent and creative visual outputs is a huge leap.</p>
<p>This isn’t just about fun images. It has profound implications for creative industries, design, content creation, and even help us understand how these large models represent and manipulate concepts. We’ve gone from classifying what’s in an image to generating entirely new visual realities from abstract textual descriptions. It’s truly an exciting time for AI and vision.</p>
</section>
<section id="the-spark-and-the-fuel" class="level2">
<h2 class="anchored" data-anchor-id="the-spark-and-the-fuel">The spark and the fuel</h2>
<p>So, we’ve spent a lot of time looking at this incredible explosion of deep learning applications from 2012 to the present. We’ve seen progress in classification, detection, segmentation, captioning, generation, and so much more. A natural question arises: Why now? What were the key ingredients that came together to make this revolution possible?</p>
<p><a href="./images/the-fuel.png" class="lightbox" data-gallery="quarto-lightbox-gallery-24"><img src="https://bhdai.github.io/blog/posts/history-of-computer-vision-and-deep-learning/images/the-fuel.png" class="img-fluid"></a></p>
<p>There are three main reasons. The <strong>Computation</strong>, Deep learning models, especially the large ones we’ve been discussing, are incredibly computationally intensive to train. They require billions or even trillions of calculations. The <strong>Algorithms</strong>, while many core ideas of neural networks have been around for decades, there have been significant algorithm innovations. These include new architectures like ResNet, Transformers, better optimization techniques, new activation functions, regularization methods, and so on, which have made it possible to train much deeper and more complex models effectively. And finally the <strong>Data</strong>, datasets like ImageNet, which we discussed, were crucial. Deep learning models are data-hungry, they learn by seeing millions of examples. The internet, social media, and large-scale data collection efforts have provided this fuel.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./images/gpus.png" class="lightbox" data-gallery="quarto-lightbox-gallery-25" title="the GFLOP per Dollar graph. ‘GFLOP’ stands for Giga Floating Point Operations Per Second. It’s a measure of computational power (how many billions of calculations a processor can do in one second). The graph shows this metric per dollar, so it’s a measure of cost-effectiveness"><img src="https://bhdai.github.io/blog/posts/history-of-computer-vision-and-deep-learning/images/gpus.png" class="img-fluid figure-img" alt="the GFLOP per Dollar graph. ‘GFLOP’ stands for Giga Floating Point Operations Per Second. It’s a measure of computational power (how many billions of calculations a processor can do in one second). The graph shows this metric per dollar, so it’s a measure of cost-effectiveness"></a></p>
<figcaption>the <strong>GFLOP per Dollar</strong> graph. ‘GFLOP’ stands for Giga Floating Point Operations Per Second. It’s a measure of computational power (how many billions of calculations a processor can do in one second). The graph shows this metric per dollar, so it’s a measure of cost-effectiveness</figcaption>
</figure>
</div>
<p>Let’s focus on the <strong>Computation</strong> aspect. Look at the dramatic difference between CPUs and GPUs starting around 2007-2008 with GPUs like the GeForce 8800 GTX which was one of the first to support general-purpose computing via CUDA. Around 2010-2012 we had GeForce GTX 580, this is the era when AlexNet was developed. Alex Krizhevsky trained AlexNet on NVIDIA GPUs, and their parallel processing capabilities were absolutely critical for training such a large network in a reasonable amount of time. Then we have the so-called “Deep learning Explosion” starting around 2012-2013, precisely when GPU performance and accessibility were taking off. Later GPUs like GTX 1080 Ti, RTX 2080 Ti, RTX 3090, and RTX 3080 continued this trend, offering massive parallel computation at increasingly better price points (or at least, significantly more power for a high-end card).</p>
<p>But the story doesn’t end there with the arrival of GPU (Tensor Core) which is a special hardware for deep learning. Starting with NVIDIA’s Volta architectures, GPUs began to include dedicated hardware nits specifically designed to accelerate the types of matrix multiplication and accumulation operations that are the heart of deep learning computations. These Tensor Cores can perform mixed-precision matrix math (e.g., multiplying FP16 matrices and accumulating in FP32) much, much faster than general-purpose FP32 units.</p>
<p>This is a fantastic example of a positive feedback loop:</p>
<ol type="1">
<li>Deep learning shows promise.</li>
<li>Researchers start using GPUs for their parallel processing capabilities.</li>
<li>The demand for deep learning computation grows.</li>
<li>Hardware manufacturers (like NVIDIA) see this massive market and start designing specialized hardware units like Tensor Cores to further accelerate deep learning workloads.</li>
<li>This new, even more powerful hardware enables researchers to train even larger, more complex models, pushing the boundaries of AI further.</li>
</ol>
<p>So it’s not just that the GPU happened to be good for deep learning; the hardware itself has evolved because of deep learning, making it even more powerful and efficient for these tasks. This co-evolution of algorithms, software, and hardware is a key characteristic of the current AI boom.</p>
<p>Now let’s zoom out and look at the broader AI’s explosive growth and impact. This isn’t just an academic phenomenon, it’s having a massive real-world impact.</p>
<div class="cell">
<div class="sourceCode cell-code hidden" id="cb1" data-startfrom="252" data-source-offset="-27" style="background: #f1f3f5;"><pre class="sourceCode js code-with-copy"><code class="sourceCode javascript" style="counter-reset: source-line 251;"><span id="cb1-252">data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">FileAttachment</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"./data/attendance-major-artificial-intelligence-conferences.csv"</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">csv</span>()</span>
<span id="cb1-253"></span>
<span id="cb1-254"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Group data by conference</span></span>
<span id="cb1-255">aaai_data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">filter</span>(d <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> d<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">Entity</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">===</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"AAAI"</span>)</span>
<span id="cb1-256">cvpr_data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">filter</span>(d <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> d<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">Entity</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">===</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"CVPR"</span>)</span>
<span id="cb1-257">iclr_data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">filter</span>(d <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> d<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">Entity</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">===</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"ICLR"</span>)</span>
<span id="cb1-258">icml_data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">filter</span>(d <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> d<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">Entity</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">===</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"ICML"</span>)</span>
<span id="cb1-259">neurips_data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">filter</span>(d <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> d<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">Entity</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">===</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"NeurIPS"</span>)</span>
<span id="cb1-260">total_data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">filter</span>(d <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> d<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">Entity</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">===</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Total"</span>)</span>
<span id="cb1-261"></span>
<span id="cb1-262"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Create the Plotly chart</span></span>
<span id="cb1-263">Plotly <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">require</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"plotly.js-dist@2"</span>)</span>
<span id="cb1-264"></span>
<span id="cb1-265">chart <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb1-266">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">const</span> traces <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [</span>
<span id="cb1-267">    {</span>
<span id="cb1-268">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">x</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> aaai_data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">map</span>(d <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> d<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">Year</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-269">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">y</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> aaai_data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">map</span>(d <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> d[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Number of attendees"</span>])<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-270">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">type</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'scatter'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-271">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">mode</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'lines+markers'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-272">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">name</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'AAAI'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-273">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">line</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> { <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">color</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'#9467bd'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">width</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span> }<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-274">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">marker</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> { <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">size</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span> }<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-275">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">hovertemplate</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'%{fullData.name}: %{y:,}&lt;extra&gt;&lt;/extra&gt;'</span></span>
<span id="cb1-276">    }<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-277">    {</span>
<span id="cb1-278">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">x</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> cvpr_data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">map</span>(d <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> d<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">Year</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-279">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">y</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> cvpr_data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">map</span>(d <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> d[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Number of attendees"</span>])<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-280">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">type</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'scatter'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-281">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">mode</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'lines+markers'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-282">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">name</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'CVPR'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-283">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">line</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> { <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">color</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'#1f77b4'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">width</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span> }<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-284">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">marker</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> { <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">size</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span> }<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-285">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">hovertemplate</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'%{fullData.name}: %{y:,}&lt;extra&gt;&lt;/extra&gt;'</span></span>
<span id="cb1-286">    }<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-287">    {</span>
<span id="cb1-288">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">x</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> iclr_data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">map</span>(d <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> d<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">Year</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-289">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">y</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> iclr_data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">map</span>(d <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> d[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Number of attendees"</span>])<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-290">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">type</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'scatter'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-291">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">mode</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'lines+markers'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-292">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">name</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ICLR'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-293">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">line</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> { <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">color</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'#ff7f0e'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">width</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span> }<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-294">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">marker</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> { <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">size</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span> }<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-295">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">hovertemplate</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'%{fullData.name}: %{y:,}&lt;extra&gt;&lt;/extra&gt;'</span></span>
<span id="cb1-296">    }<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-297">    {</span>
<span id="cb1-298">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">x</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> icml_data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">map</span>(d <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> d<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">Year</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-299">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">y</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> icml_data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">map</span>(d <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> d[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Number of attendees"</span>])<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-300">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">type</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'scatter'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-301">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">mode</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'lines+markers'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-302">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">name</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ICML'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-303">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">line</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> { <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">color</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'#8c564b'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">width</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span> }<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-304">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">marker</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> { <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">size</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span> }<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-305">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">hovertemplate</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'%{fullData.name}: %{y:,}&lt;extra&gt;&lt;/extra&gt;'</span></span>
<span id="cb1-306">    }<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-307">    {</span>
<span id="cb1-308">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">x</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> neurips_data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">map</span>(d <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> d<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">Year</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-309">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">y</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> neurips_data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">map</span>(d <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> d[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Number of attendees"</span>])<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-310">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">type</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'scatter'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-311">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">mode</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'lines+markers'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-312">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">name</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'NeurIPS'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-313">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">line</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> { <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">color</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'#2ca02c'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">width</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span> }<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-314">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">marker</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> { <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">size</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span> }<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-315">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">hovertemplate</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'%{fullData.name}: %{y:,}&lt;extra&gt;&lt;/extra&gt;'</span></span>
<span id="cb1-316">    }<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-317">    {</span>
<span id="cb1-318">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">x</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> total_data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">map</span>(d <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> d<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">Year</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-319">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">y</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> total_data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">map</span>(d <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> d[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Number of attendees"</span>])<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-320">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">type</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'scatter'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-321">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">mode</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'lines+markers'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-322">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">name</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Total'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-323">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">line</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> { <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">color</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'#d62728'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">width</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">dash</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'dash'</span> }<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-324">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">marker</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> { <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">size</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span> }<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-325">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">hovertemplate</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'%{fullData.name}: %{y:,}&lt;extra&gt;&lt;/extra&gt;'</span></span>
<span id="cb1-326">    }</span>
<span id="cb1-327">  ]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb1-328"></span>
<span id="cb1-329">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">const</span> layout <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb1-330">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">title</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> {</span>
<span id="cb1-331">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">text</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Conference Attendance Over Time'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-332">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">font</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> { <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">size</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span> }</span>
<span id="cb1-333">    }<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-334">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">xaxis</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> {</span>
<span id="cb1-335">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">title</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Year'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-336">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">showgrid</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">true</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-337">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">gridcolor</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'rgba(0,0,0,0.1)'</span></span>
<span id="cb1-338">    }<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-339">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">yaxis</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> {</span>
<span id="cb1-340">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">title</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Number of Attendees'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-341">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">showgrid</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">true</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-342">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">gridcolor</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'rgba(0,0,0,0.1)'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-343">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">tickformat</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">',d'</span></span>
<span id="cb1-344">    }<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-345">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">legend</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> {</span>
<span id="cb1-346">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">x</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.02</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-347">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">y</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.98</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-348">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">bgcolor</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'rgba(255,255,255,0.8)'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-349">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">bordercolor</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'rgba(0,0,0,0.2)'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-350">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">borderwidth</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb1-351">    }<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-352">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">hovermode</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'x unified'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-353">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">plot_bgcolor</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'white'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-354">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">paper_bgcolor</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'white'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-355">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">margin</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> { <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">l</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">80</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">r</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">40</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">t</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">80</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">b</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">80</span> }</span>
<span id="cb1-356">  }<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb1-357"></span>
<span id="cb1-358">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">const</span> config <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb1-359">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">displayModeBar</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">true</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-360">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">displaylogo</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">false</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-361">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">modeBarButtonsToRemove</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'pan2d'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'lasso2d'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'select2d'</span>]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-362">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">responsive</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">true</span></span>
<span id="cb1-363">  }<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb1-364"></span>
<span id="cb1-365">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">const</span> div <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> DOM<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">element</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"div"</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb1-366">  div<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">style</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">width</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"100%"</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb1-367">  div<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">style</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">height</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"500px"</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb1-368"></span>
<span id="cb1-369">  Plotly<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">newPlot</span>(div<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> traces<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> layout<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> config)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb1-370"></span>
<span id="cb1-371">  <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> div<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb1-372">}</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-1" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-2" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-3" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-4" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-5" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-6" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-7" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-8" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-9" data-nodetype="declaration">

</div>
</div>
</div>
</div>
<p>This chart show the number of attendance at major AI conferences like CVPR(Computer Vision), NeurIPS(Neural Information Processing System, a top ML conference), ICML (International Conference on Machine Learning), AAAI (Association for the Advancement of Artificial Intelligence), ICLR (International conference on Learning Representations), and others, from around 2010 to 2024. Look at what happens around 2012-2015 onwards. The attendance for many of these conferences, especially those focused on machine learning and computer vision (like CVPR, NeurlPS, ICML, ICLR) just explodes. We’re talking about conferences going from a few thousand attendees to over 20,000, sometimes even more, in just a few years. This signifies a huge influx of researchers, students, and industry practitioners into the field. The source is from <a href="https://ourworldindata.org/grapher/attendance-major-artificial-intelligence-conferences?country=NeurIPS~Total~CVPR~ICLR~ICML~AAAI">Our World in Data</a>.</p>
<div class="cell">
<div class="sourceCode cell-code hidden" id="cb2" data-startfrom="379" data-source-offset="-0" style="background: #f1f3f5;"><pre class="sourceCode js code-with-copy"><code class="sourceCode javascript" style="counter-reset: source-line 378;"><span id="cb2-379">aiData <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">FileAttachment</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"./data/enterprise-ai-revenue.csv"</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">csv</span>()</span>
<span id="cb2-380"></span>
<span id="cb2-381">aiChart <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb2-382">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">const</span> trace <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb2-383">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">x</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> aiData<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">map</span>(d <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> d<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">Year</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb2-384">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">y</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> aiData<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">map</span>(d <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> d<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">Revenue</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb2-385">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">type</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'bar'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb2-386">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">name</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'AI Market Revenue'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb2-387">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">marker</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> {</span>
<span id="cb2-388">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">color</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'#4285f4'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb2-389">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">opacity</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.8</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb2-390">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">line</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> {</span>
<span id="cb2-391">        <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">color</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'#1a73e8'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb2-392">        <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">width</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb2-393">      }</span>
<span id="cb2-394">    }<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb2-395">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">text</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> aiData<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">map</span>(d <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> <span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">`$</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">${</span>(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span>d<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">Revenue</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">toFixed</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">`</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb2-396">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">textposition</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'outside'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb2-397">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">textfont</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> {</span>
<span id="cb2-398">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">color</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'#333'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb2-399">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">size</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">11</span></span>
<span id="cb2-400">    }<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb2-401">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">hovertemplate</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'&lt;b&gt;%{x}&lt;/b&gt;&lt;br&gt;'</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb2-402">                   <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Revenue: $%{y:,.2f} million USD&lt;br&gt;'</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb2-403">                   <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'&lt;extra&gt;&lt;/extra&gt;'</span></span>
<span id="cb2-404">  }<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb2-405"></span>
<span id="cb2-406">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">const</span> layout <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb2-407">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">title</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> {</span>
<span id="cb2-408">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">text</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Enterprise Artificial Intelligence Market Revenue Worldwide 2016-2025'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb2-409">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">font</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> { <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">size</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">18</span> }<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb2-410">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">x</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span></span>
<span id="cb2-411">    }<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb2-412">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">xaxis</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> {</span>
<span id="cb2-413">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">title</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Year'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb2-414">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">showgrid</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">false</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb2-415">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">tickmode</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'array'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb2-416">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">tickvals</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> aiData<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">map</span>(d <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> d<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">Year</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb2-417">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">ticktext</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> aiData<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">map</span>(d <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> d<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">Year</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2025</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">?</span> <span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">`</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">${</span>d<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">Year</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">*`</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">`</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">${</span>d<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">Year</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">*`</span>)</span>
<span id="cb2-418">    }<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb2-419">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">yaxis</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> {</span>
<span id="cb2-420">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">title</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Revenue in million U.S. dollars'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb2-421">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">showgrid</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">true</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb2-422">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">gridcolor</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'rgba(0,0,0,0.1)'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb2-423">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">tickformat</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">',.0f'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb2-424">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">range</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">Math</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">max</span>(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">...</span>aiData<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">map</span>(d <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span>d<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">Revenue</span>)) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.1</span>]</span>
<span id="cb2-425">    }<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb2-426">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">plot_bgcolor</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'white'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb2-427">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">paper_bgcolor</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'white'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb2-428">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">margin</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> { <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">l</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">80</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">r</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">40</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">t</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">b</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">80</span> }<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb2-429">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">showlegend</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">false</span></span>
<span id="cb2-430">  }<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb2-431"></span>
<span id="cb2-432">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">const</span> config <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb2-433">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">displayModeBar</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">true</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb2-434">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">displaylogo</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">false</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb2-435">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">modeBarButtonsToRemove</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'pan2d'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'lasso2d'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'select2d'</span>]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb2-436">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">responsive</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">true</span></span>
<span id="cb2-437">  }<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb2-438"></span>
<span id="cb2-439">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">const</span> div <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> DOM<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">element</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"div"</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb2-440">  div<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">style</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">width</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"100%"</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb2-441">  div<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">style</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">height</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"500px"</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb2-442"></span>
<span id="cb2-443">  Plotly<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">newPlot</span>(div<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> [trace]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> layout<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> config)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb2-444"></span>
<span id="cb2-445">  <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> div<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb2-446">}</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-2-1" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-2-2" data-nodetype="declaration">

</div>
</div>
</div>
</div>
<p>Next is Enterprise application AI revenue, the bar chart shows the revenue generated from enterprise applications of AI, in billions of U.S. dollars, from 2016 projected out to 2025. Even starting in 2016, there’s already noticeable revenue. But the projected growth is staggering. It goes from a few hundred billion dollars, and then projected to over thirty trillion dollars by 2025. This shows that AI is not just research or startups, it’s being deployed in established businesses across various sectors generating significant economic value. The source is from <a href="https://cloudlevante.com/2023/04/25/the-booming-cloud-how-artificial-intelligence-is-transforming-businesses/">cloudlevante</a></p>
</section>
<section id="beyond-the-benchmark" class="level2">
<h2 class="anchored" data-anchor-id="beyond-the-benchmark">Beyond the Benchmark</h2>
<p>We’ve had a whirlwind tour through the incredible achievements of deep learning in computer vision since 2012. We’ve seen models classify, detect, segment, caption, and even generate incredibly realistic images. Now, it’s crucial to bring us back to reality and acknowledge that while the successes are profound, there’s still a lot of work to be done.</p>
<blockquote class="blockquote">
<p>Despite the successes, computer vision still has a long way to go</p>
</blockquote>
<p>We’ve achieved incredible feats on specific benchmarks, but true human-level visual intelligence, with common sense, robustness, and ethical considerations, is still a grand challenge. This isn’t to diminish the progress, but to inspire you for the future.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./images/harmful.png" class="lightbox" data-gallery="quarto-lightbox-gallery-26" title="Source: https://www.washingtonpos t.com/technology/2019/ 10/22/ai-hiring-face-scan ning-algorithm-increasingly-decides-whether-you-deserve-job/ and https://www.hirevue.com/platform/ online-video-interviewing-software"><img src="https://bhdai.github.io/blog/posts/history-of-computer-vision-and-deep-learning/images/harmful.png" class="img-fluid figure-img" alt="Source: https://www.washingtonpos t.com/technology/2019/ 10/22/ai-hiring-face-scan ning-algorithm-increasingly-decides-whether-you-deserve-job/ and https://www.hirevue.com/platform/ online-video-interviewing-software"></a></p>
<figcaption>Source: <a href="https://www.washingtonpos%20t.com/technology/2019/%2010/22/ai-hiring-face-scan%20ning-algorithm-increasingly-decides-whether-you-deserve-job/">https://www.washingtonpos t.com/technology/2019/ 10/22/ai-hiring-face-scan ning-algorithm-increasingly-decides-whether-you-deserve-job/</a> and <a href="https://www.hirevue.com/platform/%20online-video-interviewing-software">https://www.hirevue.com/platform/ online-video-interviewing-software</a></figcaption>
</figure>
</div>
<p>In fact, while computer vision can do immense good, it also has the potential to <strong>cause harm</strong> if not developed and deployed carefully. As future engineers and scientists in this field, it’s vital to be aware of those risks. Consider this concerned example <strong>Harmful Stereotypes</strong>, specifically related to gender classification. The table on the left shows the accuracy of gender classifiers from major tech companies like Microsoft, FACE++, IBM on different demographic groups. The largest gap column, while accuracy for lighter males and females is very high, it significantly drops for darker-skinned individuals, especially darker females. This means these systems are <strong>biased</strong>. Why does this happen? Often due to the lack of diverse and representative training data, or biases inherent in the data collection process. The averaged faces below visually represent these biased training sets. This is a critical issue, AI systems, if trained on biased data, will perpetuate and even amplify existing societal biases.</p>
<p>On the right, we see that can <strong>Affect people’s lives</strong>. The headline from The Washington Post: “A face-scanning algorithm increasingly decides whether”you deserve the job”. This refers to companies like HireVue, which use AI-powered video analysis in job interviews to assess candidates. The system analyzes facial expression, speech patterns, and other cues. While the intent might be to standardize hiring, outside experts call it “profoundly disturbing”. Imagine an algorithm, potentially biased, making decisions about your career prospects. This highlights that computer vision systems, when deployed in high-stakes environments like hiring, criminal justice, or healthcare, must be rigorously tested for fairness, transparency, and accuracy across all demographics. The ethical implications are enormous, and we, as a community, have a responsibility to address them.</p>
<p>But it’s not all about potential harm. We also need to recognize the immense potential for good. <strong>Computer vision can save lives</strong>. Consider the challenge of <strong>how to take care of seniors while keeping them safe?</strong> This is a growing societal problem with an aging global population. Computer vision offers a promising non-invasive solution. Imagine a camera system in a senior’s home, it can help early symptom detection of COVID-19 by monitoring cough, breathing changes, fever-like symptoms through thermal imaging. It can monitor patients with mild symptoms by reducing the need for frequent in-person visits. It can help manage chronic conditions like detecting changes in gait for mobility issues, monitoring sleep patterns, diet, or overall activity levels.</p>
<p>These systems are versatile and, crucially, scalable. They can be low-cost compared to continuous human care and can be burden-free for the seniors themselves, allowing them to maintain independence while providing peace of mind to their families and caregivers. This is a powerful example of how computer vision, when designed ethically and thoughtfully can be a force for immense societal benefit.</p>
<p>But even with these powerful applications, there are fundamental limitations in reasoning and common sense that remind us just how far we still have to go. This brings us to a classic, and still deeply relevant, thought experiment in computer vision.</p>
<p><a href="./images/karpathy.png" class="lightbox" data-gallery="quarto-lightbox-gallery-27"><img src="https://bhdai.github.io/blog/posts/history-of-computer-vision-and-deep-learning/images/karpathy.png" class="img-fluid"></a></p>
<p>Back in 2012, Andrej Karpathy (who you may know as a former Director of AI at Tesla and a key figure in the field) wrote a blog post called <a href="https://karpathy.github.io/2012/10/22/state-of-computer-vision/">“The state of Computer Vision and AI: we are really, really far away.”</a> about the image you see above. He argued that it perfectly illustrated challenge facing AI. He called the state of computer vision at the time “pathetic” in the face of what this image requires. To truly understand the humor and the story in this photo, a computer would need to go far beyond just identifying pixels. It would need to synthesize an incredible amount of world knowledge.</p>
<p>First it needs to understand the complex <strong>scene geometry</strong>. It has to recognize people, but also realize that some of them are reflections in a mirror, not separate individuals.</p>
<p>Second it needs to grasp <strong>physical interaction and object affordance</strong>. It has to identify the object as a weight scale, understand that the person is standing on it to measure their weight, and then notice that then President Obama has his foot slyly placed on the back of the scale. This requires understanding that applying force to a scale alters its measurement a basic concept of physics.</p>
<p>But the real challenge, the part that truly tests intelligence, is <strong>reasoning about minds</strong>. The system would need to infer that the person on the scale is unaware of Obama’s prank because of his pose and limited field of view. It would need to anticipate the person’s imminent confusion when he sees the inflated number. Add it’s a deeply social, psychological, and physical understanding, all from a single 2D image of RGB pixels.</p>
<p>So, that was 2012. Now, let’s fast forward to the present day, over a decade into the deep learning revolution. Did we solve it? This very question resurfaced in 2023. When asked about the original Obama image, Karpathy’s response was telling:</p>
<blockquote class="blockquote">
<p>We tried and it solves it :o.</p>
</blockquote>
<p>For a moment, it seems like the problem was solved. But the story gets more complex. Karpathy immediately followed up with his own skepticism:</p>
<blockquote class="blockquote">
<p>I still didn’t believe it could be true.</p>
</blockquote>
<p>The reason for his doubt is a critical concept in modern AI: <strong>data contamination</strong>. The Obama photo is famous. It, along with Karpathy’s original blog post and thousands of articles explain the joke, and almost certainly part of the massive datasets used to train today’s large vision-language models. So, when the model “explains” the joke, is it truly reasoning from first principles, or is it performing an act of incredibly sophisticated retrieval? Is it recreating an explanation it has already seen, or is it generating one from scratch? Maybe the image might be leaked into the training set. This ambiguity is perfectly captured by Karpathy’s own words:</p>
<blockquote class="blockquote">
<p>The waters are muddied…</p>
</blockquote>
<p>And this is where we stand today, truly beyond the benchmark. The lines are blurring. Our models have become so powerful that we are no longer just asking “Is it accurate?” but the much harder question: “Does it understand?” The challenge is no longer simply about building a better classifier, but about building a system with verifiable reasoning, untangling true intelligence from phenomenal memory.</p>
<p>The road ahead is still long, but the problems we face are no longer just about recognizing pixels. They are about navigating ambiguity, context, and common sense which is the very fabric of intelligence itself. The canvas is far from finished, but the picture we are beginning to paint is more intricate and fascinating than we could have ever imagined.</p>


</section>

 ]]></description>
  <category>Computer Vision</category>
  <category>Deep Learning</category>
  <guid>https://bhdai.github.io/blog/posts/history-of-computer-vision-and-deep-learning/</guid>
  <pubDate>Sat, 12 Jul 2025 17:00:00 GMT</pubDate>
  <media:content url="https://bhdai.github.io/blog/posts/history-of-computer-vision-and-deep-learning/images/cover_image.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>Paddy doctor</title>
  <dc:creator>Bui Huu Dai</dc:creator>
  <link>https://bhdai.github.io/blog/posts/paddy/</link>
  <description><![CDATA[ 






<p>Alright, we’ve been diving deep into tabular data lately, haven’t we? We played with it, had some fun, and now i think it’s time to go deeper to image classification problem. Yeah, we’ve touched on this before with the world simplest model <a href="https://bhdai.github.io/blog/posts/2024-06-30-your-deep-learning-journey/#exploring-the-is-it-a-bird-classifier">“Is it a Bird?”</a>, or better our simple model for <a href="https://bhdai.github.io/blog/posts/2024-07-07-from-notebook-to-web-app/">recognizing three types of bears</a> but that was mostly about deployment. This time we’re going all in.</p>
<blockquote class="blockquote">
<p>In an ideal world deep learning practitioners wouldn’t have to know every detail of how things work under the hood… But as yet, we don’t live in an ideal world. The truth is, to make your model really work, and work reliably, there are a lot of details you have to get right, and a lot of details that you have to check. This process requires being able to look inside your neural network as it trains, and as it makes predictions, find possible problems, and know how to fix them.</p>
</blockquote>
<p>From now on, we’ll be getting into the mechanics of deep learning, and exploring what a solid computer vision model architecture looks like.</p>
<p>Today we’re dealing with the <a href="https://www.kaggle.com/competitions/paddy-disease-classification/overview">Paddy Doctor: Paddy Disease Classification</a> competition on Kaggle. The goal? Predict paddy diseases based on images. Through this competitions we will go though, the general architecture, the presizing process, the loss, and improve our model further, alright let’s get right into it shall we?</p>
<div id="cell-3" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> iskaggle:</span>
<span id="cb1-2">  path <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> setup_comp(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'paddy-disease-classification'</span>, install<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"timm&gt;=0.6.2.dev0"</span>)</span>
<span id="cb1-3"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb1-4">  path <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Path(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"./data"</span>)</span></code></pre></div>
</div>
<p>First we need to understand how our data laid out</p>
<div id="cell-5" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">path.ls()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>(#4) [Path('data/test_images'),Path('data/train.csv'),Path('data/train_images'),Path('data/sample_submission.csv')]</code></pre>
</div>
</div>
<p>Data is usually provided in one of these two ways:</p>
<ul>
<li>Individual files representing items of data, like images, text, can be organized in folders or with file name representing information about the images</li>
<li>A table of data as we dealt with them before, where each row is an item which may include filenames providing a connection between the table and data in other format, such as text documents and images.</li>
</ul>
<div id="cell-7" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">(path<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"train_images/"</span>).ls()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>(#10) [Path('data/train_images/bacterial_leaf_blight'),Path('data/train_images/bacterial_leaf_streak'),Path('data/train_images/bacterial_panicle_blight'),Path('data/train_images/blast'),Path('data/train_images/brown_spot'),Path('data/train_images/dead_heart'),Path('data/train_images/downy_mildew'),Path('data/train_images/hispa'),Path('data/train_images/normal'),Path('data/train_images/tungro')]</code></pre>
</div>
</div>
<p>As you can see we have 10 folders each represent paddy diseases that we need to predict, in this case each folders will contain images of paddy disease correspond to the parent folder name</p>
<div id="cell-9" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">train_path <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> path<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'train_images'</span></span>
<span id="cb6-2">files <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> get_image_files(train_path)</span>
<span id="cb6-3">files</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>(#10407) [Path('data/train_images/bacterial_leaf_blight/100023.jpg'),Path('data/train_images/bacterial_leaf_blight/100049.jpg'),Path('data/train_images/bacterial_leaf_blight/100126.jpg'),Path('data/train_images/bacterial_leaf_blight/100133.jpg'),Path('data/train_images/bacterial_leaf_blight/100148.jpg'),Path('data/train_images/bacterial_leaf_blight/100162.jpg'),Path('data/train_images/bacterial_leaf_blight/100169.jpg'),Path('data/train_images/bacterial_leaf_blight/100234.jpg'),Path('data/train_images/bacterial_leaf_blight/100248.jpg'),Path('data/train_images/bacterial_leaf_blight/100268.jpg'),Path('data/train_images/bacterial_leaf_blight/100289.jpg'),Path('data/train_images/bacterial_leaf_blight/100330.jpg'),Path('data/train_images/bacterial_leaf_blight/100365.jpg'),Path('data/train_images/bacterial_leaf_blight/100382.jpg'),Path('data/train_images/bacterial_leaf_blight/100445.jpg'),Path('data/train_images/bacterial_leaf_blight/100447.jpg'),Path('data/train_images/bacterial_leaf_blight/100513.jpg'),Path('data/train_images/bacterial_leaf_blight/100516.jpg'),Path('data/train_images/bacterial_leaf_blight/100523.jpg'),Path('data/train_images/bacterial_leaf_blight/100541.jpg')...]</code></pre>
</div>
</div>
<p>Let’s take a look a one</p>
<div id="cell-11" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1">img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> PILImage.create(files[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])</span>
<span id="cb8-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(img.size)</span>
<span id="cb8-3">img.to_thumb(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(480, 640)</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="6">
<div>
<figure class="figure">
<p><img src="https://bhdai.github.io/blog/posts/paddy/index_files/figure-html/cell-7-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>This image has the size of 480x640, let’s check all their sizes. Looping though over 10.000 images is a pain right, so we will do it in parallel</p>
<div id="cell-14" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> f(o): <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> PILImage.create(o).size</span>
<span id="cb10-2">sizes <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> parallel(f, files, n_workers<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>)</span>
<span id="cb10-3">pd.Series(sizes).value_counts()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>(480, 640)    10403
(640, 480)        4
Name: count, dtype: int64</code></pre>
</div>
</div>
<p>Well, there’s almost have the same size except 4 of them with 640x480, we’ll need to resize all of them to the same size, we will talk about it later, but now let’s create a dataloader</p>
<div id="cell-16" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;execution&quot;,&quot;value&quot;:{&quot;iopub.execute_input&quot;:&quot;2025-01-02T10:39:16.888162Z&quot;,&quot;iopub.status.busy&quot;:&quot;2025-01-02T10:39:16.887867Z&quot;,&quot;iopub.status.idle&quot;:&quot;2025-01-02T10:39:20.073235Z&quot;,&quot;shell.execute_reply&quot;:&quot;2025-01-02T10:39:20.072378Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2025-01-02T10:39:16.888138Z&quot;}}" data-trusted="true" data-execution_count="15">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1">dls <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> DataBlock(blocks<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(ImageBlock, CategoryBlock),</span>
<span id="cb12-2">                get_items<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>get_image_files,</span>
<span id="cb12-3">                splitter<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>RandomSplitter(valid_pct<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>, seed<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">42</span>),</span>
<span id="cb12-4">                get_y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>parent_label,</span>
<span id="cb12-5">                item_tfms<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>Resize(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">480</span>, method<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'squish'</span>),</span>
<span id="cb12-6">                batch_tfms<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>aug_transforms(size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span>, min_scale<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.75</span>)).dataloaders(train_path)</span></code></pre></div>
</div>
<p>This is the principle of our computer vision model mostly, but notice here, we need to focus on these two lines:</p>
<div class="sourceCode" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1">item_tfms<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>Resize(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">480</span>, method<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'squish'</span>),</span>
<span id="cb13-2">batch_tfms<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>aug_transforms(size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span>, min_scale<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.75</span>)).dataloaders(train_path)</span></code></pre></div>
<p>These lines implement a fastai data augmentation strategy which they often call presizing.</p>
<section id="presizing" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="presizing">Presizing</h2>
<p>We need our images to have the same dimensions, so that they can collate into tensors to be passed into GPU. We also want to minimize the number of distinct augmentation computations we perform. If possible we should compose our augmentation transforms into fewer transform(to reduce number of computations and lossy operations) and transform images into uniform sizes(for more efficient processing in GPU)</p>
<p>However if we resize images to their final dimensions(the augmented size) and then apply various augmentation transforms it can lead to issues like creating empty zones (e.g., when rotating an image by 45 degrees) which will not teach the computer anything at all. Many rotation and zooming operations will require interpolating<sup>1</sup> to create pixel</p>
<p>To walk around these challenges, presizing adopts a two-step strategy.</p>
<ol type="1">
<li>Images are resized to dimensions significantly larger than the target training size as this will create a “buffer zone” around the image allowing for more flexibility in subsequent augmentation.</li>
<li>All common augmentation operations, including the final resize to target dimensions, are combined into a single step performed on GPU at the end of the processing, rather than performing the operations individually and interpolating multiple times.</li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://bhdai.github.io/blog/posts/paddy/presizing.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>Presizing explained: Showing the two step resize to a large size then apply random crop and augment at the same time</figcaption>
</figure>
</div>
<p>As you can see in the picture it demonstrate what i described earlier</p>
<ol type="1">
<li>First it crop full width or height this time it still do it sequentially before copied to GPU, it make sure that all our images are the same size. On the training set the crop area is chosen randomly<sup>2</sup> and on validation set the it always choose the center square of the image. This is in <code>item_tfms</code></li>
</ol>
<ol start="2" type="1">
<li>Then it uses <code>RandomResizedCrop</code> as a batch transform. It’s applied to a batch all at once on the GPU, making it fast. For the training set, this includes random cropping and other augmentations, and for validation set only resizing to the final size needed for the model is done. This is in <code>batch_tfms</code></li>
</ol>
<section id="resize" class="level3">
<h3 class="anchored" data-anchor-id="resize">Resize</h3>
<p>Use <code>Resize</code> as an item transform with a large size you can use <code>pad</code><sup>3</sup> or <code>squish</code><sup>4</sup> instead of <code>crop</code><sup>5</sup>(the default) for the initial <code>Resize</code> but what the diff between them? In fact let’s see the different in action shall we? Here’s the original image:</p>
<div id="cell-20" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1">tst_img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> PILImage.create(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'./test_image.jpg'</span>).resize((<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">600</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">400</span>))</span>
<span id="cb14-2">tst_img.to_thumb(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">224</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<div>
<figure class="figure">
<p><img src="https://bhdai.github.io/blog/posts/paddy/index_files/figure-html/cell-11-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-21" class="cell">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1">_, axs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplots(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>,figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>))</span>
<span id="cb15-2"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> ax,method <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">zip</span>(axs.flatten(), [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'squish'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'pad'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'crop'</span>]):</span>
<span id="cb15-3">  rsz <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Resize(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">256</span>, method<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>method)</span>
<span id="cb15-4">  show_image(rsz(tst_img, split_idx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>), ctx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ax, title<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>method)</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://bhdai.github.io/blog/posts/paddy/index_files/figure-html/cell-12-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>On the validation set, the crop is always a center crop (on the dimension that’s cropped).</p>
<div id="cell-23" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1">_, axs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplots(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>,figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>))</span>
<span id="cb16-2"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> ax,method <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">zip</span>(axs.flatten(), [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'squish'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'pad'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'crop'</span>]):</span>
<span id="cb16-3">  rsz <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Resize(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">256</span>, method<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>method)</span>
<span id="cb16-4">  show_image(rsz(tst_img, split_idx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>), ctx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ax, title<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>method)</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://bhdai.github.io/blog/posts/paddy/index_files/figure-html/cell-13-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>My recommendation:</p>
<ol type="1">
<li>Start with padding if you’re unsure. It preserve all information and aspect ratios.</li>
<li>If padding introduces too much background, try cropping.</li>
<li>Use squish only if you’re sure it won’t distort important features.</li>
<li>Always validate your choice by inspecting resized image and checking model performance</li>
</ol>
<p>The best method can vary depending on the dataset the original aspect ratio. Now let’s see what the <code>aug_transforms</code> does under the hood</p>
</section>
<section id="augmentation" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="augmentation">Augmentation</h3>

<div class="no-row-height column-margin column-container"><div id="cell-25" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1">torch.permute??</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">Docstring:</span>

permute(input, dims) -&gt; Tensor



Returns a view of the original tensor :attr:`input` with its dimensions permuted.



Args:

    input (Tensor): the input tensor.

    dims (tuple of int): The desired ordering of dimensions



Example:

    &gt;&gt;&gt; x = torch.randn(2, 3, 5)

    &gt;&gt;&gt; x.size()

    torch.Size([2, 3, 5])

    &gt;&gt;&gt; torch.permute(x, (2, 0, 1)).size()

    torch.Size([5, 2, 3])

<span class="ansi-red-fg">Type:</span>      builtin_function_or_method</pre>
</div>
</div>
</div><div class="">
<p>Most image processing libraries and formats (like PIL, OpenCV, matplotlib) use the format (Height, Width, Channels) or (H, W, C).</p>
<p>However, PyTorch expects images in the format (Channels, Height, Width) or (C, H, W).</p>
</div></div>
<div id="cell-27" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1">timg <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> TensorImage(array(tst_img)).permute(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>()<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">255.</span></span>
<span id="cb18-2"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> _batch_ex(bs): <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> TensorImage(timg[<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>].expand(bs, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>timg.shape).clone())</span>
<span id="cb18-3">tfms <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> aug_transforms(size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span>, min_scale<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.75</span>)</span>
<span id="cb18-4">y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> _batch_ex(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">9</span>)</span>
<span id="cb18-5"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> t <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> tfms: y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> t(y, split_idx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb18-6">_, axs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplots(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>))</span>
<span id="cb18-7"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i,ax <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(axs.flatten()): show_image(y[i], ctx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ax)</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://bhdai.github.io/blog/posts/paddy/index_files/figure-html/cell-15-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Choosing the correct size for augment transforms is also crucial too as the size parameter in <code>aug_transforms</code> determines the final size of the images that will be fed into the model. Picking the right one depends on the model architecture requirements, each pretrained models requires different input size (e.g., <code>ResNet</code> typically use 224x224), but in fact you can do some experiments here. But aware of this, larger sizes help computer learn more details, but of course require more resources, in other hand smaller sizes are faster to process but may lose some details, it’s a tradeoff</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li>Start with the standard size for your chosen model architecture(e.g., 224 for many standard models)</li>
<li>If computational resources are not a big deal with you and the images have fine details, try increasing the size (e.g., 299, 384, 521)</li>
<li>If using transfer learning, stick to the pretrained model’s original input size is the best option i’d say</li>
<li>For custom architectures, you got no choice but experiment with different sizes and choose based on performance and resource constraints.</li>
</ol>
</div>
</div>
</section>
<section id="checking-datablock" class="level3">
<h3 class="anchored" data-anchor-id="checking-datablock">Checking DataBlock</h3>
<p>Writing <code>DataBlock</code> is just like writing a blueprint, we will get an error if we have a syntax error some where in the code. So it’s a good practice to always check your data before doing anything further</p>
<div id="cell-30" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1">dls.show_batch(max_n<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://bhdai.github.io/blog/posts/paddy/index_files/figure-html/cell-16-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Take a look at the images and and check that each one seems to have correct label or not. In fact, we often have to deal with data with which is not as familiar as domain experts may be. Indeed, if you’re not a paddy doctor it will be hard to look at a random image and speak out the disease right? Since I’m not an expert on paddy diseases, I would use google to search to make sure the images look similar to what i see in the output. Also you can debug the DataBlock by using <code>DataBlock.summary</code><sup>6</sup></p>
<p>Once you think your data looks right, it’s a good practice to train a simple model, think about it, when you start trying to improve your model, how can you rate it? Compare it to the previous try you say, I mean what if it’s already worse, so that why we need to know what our baseline result looks like. Maybe you don’t need anything fancy - a basic model might do the job just fine. Or perhaps the data doesn’t seems to train the model at all. These are things that you want to know as soon as possible</p>
<div id="cell-32" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1">learn <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> vision_learner(dls, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'resnet26d'</span>, metrics<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>error_rate, path<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'.'</span>)</span></code></pre></div>
</div>
<div id="cell-33" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1">learn.fine_tune(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train_loss</th>
<th data-quarto-table-cell-role="th">valid_loss</th>
<th data-quarto-table-cell-role="th">error_rate</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>1.966901</td>
<td>1.133069</td>
<td>0.356079</td>
<td>01:03</td>
</tr>
</tbody>
</table>
</div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train_loss</th>
<th data-quarto-table-cell-role="th">valid_loss</th>
<th data-quarto-table-cell-role="th">error_rate</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>1.311612</td>
<td>0.885489</td>
<td>0.277751</td>
<td>01:03</td>
</tr>
<tr class="even">
<td>1</td>
<td>1.032764</td>
<td>0.691636</td>
<td>0.218645</td>
<td>01:04</td>
</tr>
<tr class="odd">
<td>2</td>
<td>0.871234</td>
<td>0.645610</td>
<td>0.203748</td>
<td>01:03</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>Remember that we’re in a competition right, so it’s nothing better than submit it and see how it will go and again this time we will use an pretty cool tool call fastkaggle in fact we used it earlier. Alright let’s see the submission layout</p>
<div id="cell-35" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1">ss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.read_csv(path<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sample_submission.csv'</span>)</span>
<span id="cb22-2">ss</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">image_id</th>
<th data-quarto-table-cell-role="th">label</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>200001.jpg</td>
<td>NaN</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>200002.jpg</td>
<td>NaN</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>200003.jpg</td>
<td>NaN</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>200004.jpg</td>
<td>NaN</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>200005.jpg</td>
<td>NaN</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">3464</td>
<td>203465.jpg</td>
<td>NaN</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3465</td>
<td>203466.jpg</td>
<td>NaN</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">3466</td>
<td>203467.jpg</td>
<td>NaN</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3467</td>
<td>203468.jpg</td>
<td>NaN</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">3468</td>
<td>203469.jpg</td>
<td>NaN</td>
</tr>
</tbody>
</table>

<p>3469 rows × 2 columns</p>
</div>
</div>
</div>
<p>alright seems like we need to sort the images in order before submitting it</p>
<div id="cell-37" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1">tst_files <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> get_image_files(path<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'test_images'</span>).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sorted</span>()</span>
<span id="cb23-2">tst_dl <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dls.test_dl(tst_files)</span></code></pre></div>
</div>
<p>Let’s make the prediction on the test set</p>
<div id="cell-39" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1">probs,_,idxs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> learn.get_preds(dl<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>tst_dl, with_decoded<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb24-2">idxs</span></code></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

</div>
<div class="cell-output cell-output-display" data-execution_count="19">
<pre><code>tensor([7, 8, 7,  ..., 8, 1, 5])</code></pre>
</div>
</div>
<p>Alright we got the indices of the diseases, we need to map the name to each diseases we can get the label by checking the <code>vocab</code></p>
<div id="cell-41" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb26" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1">dls.vocab</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="20">
<pre><code>['bacterial_leaf_blight', 'bacterial_leaf_streak', 'bacterial_panicle_blight', 'blast', 'brown_spot', 'dead_heart', 'downy_mildew', 'hispa', 'normal', 'tungro']</code></pre>
</div>
</div>
<div id="cell-42" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb28" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1">mapping <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">dict</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(dls.vocab))</span>
<span id="cb28-2">results <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.Series(idxs.numpy(), name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"idxs"</span>).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">map</span>(mapping)</span>
<span id="cb28-3">results</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="21">
<pre><code>0                       hispa
1                      normal
2                       hispa
3                       blast
4                       blast
                ...          
3464               dead_heart
3465                    hispa
3466                   normal
3467    bacterial_leaf_streak
3468               dead_heart
Name: idxs, Length: 3469, dtype: object</code></pre>
</div>
</div>
<p>Before submit let’s see if our file looks right</p>
<div id="cell-44" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb30" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1">ss[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'label'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> results</span>
<span id="cb30-2">ss.to_csv(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'subm.csv'</span>, index<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb30-3"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!</span>head subm.csv</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>image_id,label
200001.jpg,hispa
200002.jpg,normal
200003.jpg,hispa
200004.jpg,blast
200005.jpg,blast
200006.jpg,brown_spot
200007.jpg,dead_heart
200008.jpg,brown_spot
200009.jpg,hispa</code></pre>
</div>
</div>
<div id="cell-45" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb32" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> iskaggle:</span>
<span id="cb32-2">  <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> kaggle <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> api</span>
<span id="cb32-3">  api.competition_submit_cli(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'subm.csv'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'initial rn26d 128px'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'paddy-disease-classification'</span>)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>100%|██████████| 70.1k/70.1k [00:01&lt;00:00, 48.7kB/s]</code></pre>
</div>
</div>
<p>Alright i got 0.8917 score on the competitions, it’s not that good but it let’s us know what the base line is, then we can improve it later on.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Remember that loss is whatever function we’re decided to use to optimize the parameters of our models, here we’re actually not specific what loss to use, fastai will try to find the best loss to use here for us. In this case we’re using cross-entropy loss.</p>
</div>
</div>
</section>
</section>
<section id="cross-entropy" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="cross-entropy">Cross-Entropy</h2>
<p><em>Cross-Entropy loss</em> is of course a loss function - a function that used to optimized the parameter of our model. It work even if our dependent variable has more than two categories, and results in faster and reliable training.</p>
<p>Let’s look at the activation of our model, in fact let’s just look at one batch of our data</p>
<div id="cell-49" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb34" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1">x, y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dls.one_batch()</span></code></pre></div>
</div>
<p>It returns the dependent and independent variables as mini-batch</p>
<div id="cell-51" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb35" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1">y</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="19">
<pre><code>TensorCategory([8, 9, 5, 8, 2, 1, 8, 7, 5, 8, 3, 6, 8, 3, 7, 4, 8, 3, 8, 6, 5,
                6, 0, 5, 8, 8, 7, 5, 8, 9, 8, 8, 7, 7, 9, 4, 7, 3, 9, 7, 7, 5,
                7, 9, 1, 7, 3, 4, 9, 6, 8, 7, 9, 5, 9, 7, 9, 5, 5, 9, 3, 3, 5,
                8], device='cuda:0')</code></pre>
</div>
</div>
<p>We got 64 rows as our batch size is 64, and we get the values ranging from 0 to 9, representing our 10 possible diseases, alright we can even view the predictions in fact it is the activations of the final layer of our neural network by using <code>Learner.get_preds</code></p>
<div id="cell-53" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb37" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1">preds,_ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> learn.get_preds(dl<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[(x,y)])</span>
<span id="cb37-2">preds[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span></code></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

</div>
<div class="cell-output cell-output-display" data-execution_count="20">
<pre><code>tensor([0.2162, 0.0061, 0.0008, 0.0299, 0.0109, 0.0082, 0.0068, 0.0107, 0.5600,
        0.1503])</code></pre>
</div>
</div>
<p>The actual prediction are 10 probabilities between 0 and 1, which add up to 1.</p>
<div id="cell-55" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb39" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(preds[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]), preds[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="21">
<pre><code>(10, tensor(1.))</code></pre>
</div>
</div>
<section id="softmax" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="softmax">Softmax</h3>
<p>When a model runs, it’s last layer produces raw numbers, We call these <code>activations</code>, they are not probabilities yet, we need to change these raw number into probabilities, we want each number show how likely the model thinks each options is. So we use softmax activation in the final layer to ensure that the activations are all between 0 and 1. Softmax is similar to sigmoid function, below is what the sigmoid function look like in case you forget it</p>
<div id="cell-59" class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb41" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1">plot_function(torch.sigmoid, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">min</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>,<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://bhdai.github.io/blog/posts/paddy/index_files/figure-html/cell-31-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>When you apply a sigmoid function to a single column of activation it turns those numbers to be between 0 and 1, it’s pretty useful activations for our final layer. But hold on and think about it, what if we have more activations than just a single column, let’s say we need to create a neural network that predict wether that image is a image of 3 or 7 that returns 2 activations (one for each number). Alright let’s create 6 images and 2 categories(the first columns is 3, and other is 7)</p>
<div id="cell-61" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb42" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1">torch.random.manual_seed(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">42</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb42-2">acts <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.randn((<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>))<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span></span>
<span id="cb42-3">acts</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>tensor([[ 0.6734,  0.2576],
        [ 0.4689,  0.4607],
        [-2.2457, -0.3727],
        [ 4.4164, -1.2760],
        [ 0.9233,  0.5347],
        [ 1.0698,  1.6187]])</code></pre>
</div>
</div>
<p>We can’t pass this to a sigmoid function directly cuz we can not get rows that add up to 1</p>
<div id="cell-63" class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb44" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1">acts.sigmoid()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="36">
<pre><code>tensor([[0.6623, 0.5641],
        [0.6151, 0.6132],
        [0.0957, 0.4079],
        [0.9881, 0.2182],
        [0.7157, 0.6306],
        [0.7446, 0.8346]])</code></pre>
</div>
</div>
<p>We need to use softmax. Here how we can represent the softmax function</p>
<div class="sourceCode" id="cb46" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> softmax(x): <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> exp(x) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> exp(x).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, keepdim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span></code></pre></div>

<div class="no-row-height column-margin column-container"><div class="">
<p>Mathematically, here the softmax formula</p>
<p><img src="https://latex.codecogs.com/png.latex?%0As(x_%7Bi%7D)%20=%20%5Cfrac%20%7Be%5E%7Bx_i%7D%7D%20%7B%5Csum%20_%7Bj=1%7D%5EN%20e%5E%7Bx_j%7D%7D%0A"></p>
</div></div><div id="cell-66" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb47" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1">sm_acts <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.softmax(acts, dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb47-2">sm_acts</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>tensor([[0.6025, 0.3975],
        [0.5021, 0.4979],
        [0.1332, 0.8668],
        [0.9966, 0.0034],
        [0.5959, 0.4041],
        [0.3661, 0.6339]])</code></pre>
</div>
</div>
<p>In fact sofmax is the multi-category version of sigmoid, we need to use it anytime we have more that two categories and the probabilities must add up to 1</p>
<p>You might wondering why we have exponential the element here in the sofmax function, first of all, the obvious insight is that it helps to make the number to be positive, it also have a nice property: if one the numbers in our activations <code>x</code> is slightly bigger than the other the exponential will amplify it by make it closer to 1. That means the sofmax function really like to pick one class among others so that make sure that your each picture has definite labels</p>
<p>Softmax is just one part of the cross-entropy loss, we need to go through log likelihood</p>
</section>
<section id="log-likelihood" class="level3">
<h3 class="anchored" data-anchor-id="log-likelihood">Log likelihood</h3>
<p>In binary case we use <code>torch.where</code> to select between <code>inputs</code> and <code>1-inputs</code></p>
<div class="sourceCode" id="cb49" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> mnis_loss(inputs, targets):</span>
<span id="cb49-2">  inputs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> inputs.sigmoid()</span>
<span id="cb49-3">  <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> torch.where(targets<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>inputs, inputs)</span></code></pre></div>
<p>Let’s try to do this using pytorch, first we need to generate our label for 3s and 7s</p>
<div id="cell-69" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb50" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1">targ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tensor([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])</span></code></pre></div>
</div>
<p>Then each item of <code>targ</code> we can use that to select the appropriate column of <code>sm_acts</code> using tensor indexing, like this:</p>
<div id="cell-71" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb51" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1">idx <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>)</span>
<span id="cb51-2">sm_acts[idx, targ]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>tensor([0.6025, 0.4979, 0.1332, 0.0034, 0.4041, 0.3661])</code></pre>
</div>
</div>
<p>Pytorch provide a function which does just that (<code>sm_acts[range(6), targ]</code>) called <code>nll_loss</code> (NLL stands for negative log likelihood)</p>
<div id="cell-73" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb53" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>sm_acts[idx, targ]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>tensor([-0.6025, -0.4979, -0.1332, -0.0034, -0.4041, -0.3661])</code></pre>
</div>
</div>
<div id="cell-74" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb55" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1">F.nll_loss(sm_acts, targ, reduction<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"none"</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>tensor([-0.6025, -0.4979, -0.1332, -0.0034, -0.4041, -0.3661])</code></pre>
</div>
</div>
<p>You might wondering why we need the negative anyway? Well because we want to minimize the loss, the log likelihood of correct label should be maximized(closer to 0 is better, as <img src="https://latex.codecogs.com/png.latex?%5Clog(1)=0">). However optimization algorithms designed to minimize, not to maximize, by adding a negative sign we convert the maximization problem to minimization problem</p>
<p>The math behind it here is that let’s say if <em>p</em> is the probability of the correct class then the negative log likelihood is <img src="https://latex.codecogs.com/png.latex?-%5Clog(p)"> as <em>p</em> approaches 1(perfect prediction), the <img src="https://latex.codecogs.com/png.latex?-%5Clog(p)"> approaches 0(minimum loss), as <em>p</em> approaches 0(bad predictions) <img src="https://latex.codecogs.com/png.latex?-%5Clog(p)"> approaches infinity (maximum loss). Blow are plots demonstrate why we need a negative sign here</p>
<div id="cell-76" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb57" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1">fig, (ax1,ax2) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplots(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>))</span>
<span id="cb57-2">plt.sca(ax1)</span>
<span id="cb57-3">plot_function(torch.log, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">min</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, ty<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"log(x)"</span>, tx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'x'</span>)</span>
<span id="cb57-4">ax1.set_title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'log(x)'</span>)</span>
<span id="cb57-5">plt.sca(ax2)</span>
<span id="cb57-6">plot_function(<span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span> x: <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>torch.log(x), <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">min</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, ty<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"- log(x)"</span>, tx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'x'</span>)</span>
<span id="cb57-7">ax2.set_title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Negative Log Function'</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="14">
<pre><code>Text(0.5, 1.0, 'Negative Log Function')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://bhdai.github.io/blog/posts/paddy/index_files/figure-html/cell-39-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="i-love-logs" class="level3">
<h3 class="anchored" data-anchor-id="i-love-logs">I love Logs</h3>
<p>Why is that? what if we have a very very small probabilities or even when working with multi-label classification<sup>7</sup> it may involve the multiplication of many small numbers, as you may know it will lead to problems like numerical underflow<sup>8</sup> in computers. we want to transform these probabilities to a large values so we can perform mathematical operation on them. And there is a mathematical function that will help us doing that: <em>the logarithm</em> as you can see in the image above</p>
<p>Not stop there tho, we do want to ensure that our model is able to detect the differences between small numbers as our loss need to be sensitive enough to small changes in probabilities(especially when the model’s predictions are very wrong). Say, probabilities of 0.01 and 0.001 those number are very close together, but in probability, 0.001 is 10 times more confident compare to 0.01. Having said that, by taking the log out of our probabilities, we prevent these important different from being ignored.</p>
<p>One more thing that make log being amazing is this relationship:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Clog(a%5Ctimes%20b)%20=%20%5Clog(a)%20+%20%5Clog(b)%0A"></p>
<p>What this relationship tells us? Well this means the logarithm increases linearly when the argument increases exponentially or multiplicatively. Logarithms are awesome because when we do multiplication which can create really really large or really really small numbers, can be replaced by addition, which produce numbers that our computer can handle</p>
<p>We compute the loss on the column that contains the correct label, because there’s only one right answer per example, we don’t need to consider the others, because by the definition of softmax, the remain columns is indeed equal 1 minus the activation correspond to the correct label, then we’ll have a loss function that tells how well we are predicting each image. Therefore, making the activation of the correct label as high as possible will also decreasing the activations of the remaining columns</p>
</section>
<section id="negative-log-likelihood" class="level3">
<h3 class="anchored" data-anchor-id="negative-log-likelihood">Negative Log Likelihood</h3>
<p>Then what we do next? We will take the mean of negative log of our probabilities in other the word for each sample(image) we take the negative log of the predicted probability for the correct class as above this give us the loss for each individual sample we then calculate the mean of these individual losses across all samples, that give us the negative log likelihood or cross-entropy loss. One thing to note here that the Pytorch <code>nll_loss</code> assume that you already take the log of the softmax, even on it name have the word log but it dose not do it for you unfortunately</p>
<p>So that is cross-entropy loss in Pytorch, this is available as <code>F.cross_entropy</code> ofr <code>nn.CrossEntropyLoss</code> the <code>F</code> namespace version seems to be used more often by people</p>
<div id="cell-78" class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb59" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1">loss_func <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.CrossEntropyLoss()</span></code></pre></div>
</div>
<div id="cell-79" class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb60" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1">loss_func(acts, targ)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="45">
<pre><code>tensor(1.8045)</code></pre>
</div>
</div>
<div id="cell-80" class="cell" data-execution_count="46">
<div class="sourceCode cell-code" id="cb62" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1">F.cross_entropy(acts, targ)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="46">
<pre><code>tensor(1.8045)</code></pre>
</div>
</div>
<p>By default PyTorch loss functions take the mean of the loss of all items. You can use reduction=‘none’ to disable that:</p>
<div id="cell-82" class="cell" data-execution_count="47">
<div class="sourceCode cell-code" id="cb64" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1">nn.CrossEntropyLoss(reduction<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'none'</span>)(acts, targ)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="47">
<pre><code>tensor([0.5067, 0.6973, 2.0160, 5.6958, 0.9062, 1.0048])</code></pre>
</div>
</div>
<p>Let’s look at the loss above those are numbers that the computer can learn from, but with us human, it is hard to look at those number and and tell how good our model is, so that why we need metrics, Those number are not used in the optimization process but just to help us poor human understand what’s going on. We can also use the confusion matrix to see where our model perform good and not</p>
<div id="cell-84" class="cell" data-execution_count="48">
<div class="sourceCode cell-code" id="cb66" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1">interp <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ClassificationInterpretation.from_learner(learn)</span>
<span id="cb66-2">interp.plot_confusion_matrix(figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12</span>), dpi<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">60</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

</div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://bhdai.github.io/blog/posts/paddy/index_files/figure-html/cell-44-output-5.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="looking-for-a-learning-rate" class="level2">
<h2 class="anchored" data-anchor-id="looking-for-a-learning-rate">Looking for a learning rate</h2>
<p>One of the most important things we can do is to find the just right learning rate, if our learning rate is too high our optimizer will step too far from the minimal loss, and repeating this multiple time just make it jump around each side of the valley. Alright just make the learning rate really small to make sure it will never pass our minimal loss right? well of course it can take many epochs to train our model to go to the point, and it not only a waste of time but potentially causing overfitting because each epoch we go though our entire data one time, if we repeat it too much time we would give the computer a chance to memorize it.</p>
<p>So how can we find the perfect learning rate? not too low, not too high but just right? In 2015 the researcher Leslie Smith came up with a brilliant idea, called the <em>learning rate finder</em>. His idea was to start with a very, very small learning rate we use that for one mini-batch(not an epoch) and then look at the losses then increase the learning rate, say, x2, and we do it again and again until the loss get worse instead of better, then we know that we’ve gone too far it’s time to slower down by selecting a learning rate a bit smaller that the previous one.</p>
<p>Here’s the advice from fastai:</p>
<ul>
<li>One order of magnitude less than where the minimum loss was achieved (i.e., the minimum divided by 10)</li>
<li>The last point where the loss was clearly decreasing</li>
</ul>
<div id="cell-86" class="cell" data-execution_count="51">
<div class="sourceCode cell-code" id="cb67" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1">learn <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> vision_learner(dls, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'resnet26d'</span>, metrics<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>error_rate, path<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'.'</span>)</span>
<span id="cb67-2">learn.lr_find(suggest_funcs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(minimum, steep))</span></code></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

</div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/monarch/miniconda3/lib/python3.12/site-packages/fastai/learner.py:53: FutureWarning:

You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="51">
<pre><code>SuggestedLRs(minimum=0.010000000149011612, steep=0.0006918309954926372)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://bhdai.github.io/blog/posts/paddy/index_files/figure-html/cell-45-output-5.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>We can see in this plot that the range from 1e-6 to 1e-4 the model doesn’t seem to train at all, from 1e-3 the loss start to decrease until it reaches the minimum at 1e-1 then increasing rapidly, obviously we don’t want the learning rate after 1e-1 as i explained above. So choose 1e-1 then? While the loss minimum around the learning rate of 1e-1 it might give a good results, it’s right at the edge of stable as after this point there’s a sharp increase in the loos, in practice it often safer to choose a learning rate slightly lower than this threshold to ensure stability across different runs or datasets. Alright let’s choose 3e-3 to see</p>
<div id="cell-88" class="cell" data-execution_count="53">
<div class="sourceCode cell-code" id="cb70" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1">learn <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> vision_learner(dls, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'resnet26d'</span>, metrics<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>error_rate, path<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'.'</span>)</span>
<span id="cb70-2">learn.fine_tune(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, base_lr<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">3e-3</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train_loss</th>
<th data-quarto-table-cell-role="th">valid_loss</th>
<th data-quarto-table-cell-role="th">error_rate</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>1.861936</td>
<td>1.138522</td>
<td>0.362326</td>
<td>00:58</td>
</tr>
</tbody>
</table>
</div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train_loss</th>
<th data-quarto-table-cell-role="th">valid_loss</th>
<th data-quarto-table-cell-role="th">error_rate</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>1.208978</td>
<td>0.829224</td>
<td>0.271985</td>
<td>01:04</td>
</tr>
<tr class="even">
<td>1</td>
<td>0.943550</td>
<td>0.610545</td>
<td>0.196060</td>
<td>01:03</td>
</tr>
<tr class="odd">
<td>2</td>
<td>0.784365</td>
<td>0.570067</td>
<td>0.183085</td>
<td>01:03</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>Alright, we’ve got a pretty good learning rate, let’s look at how we can fine-tune the weights of a pretrained model.</p>
</section>
<section id="transfer-learning" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="transfer-learning">Transfer Learning</h2>
<p>We’ve used transfer learning a lot in fact but, what it really is, and how it work? Pretrained model is trained on millions of data points(such as ImageNet) then it is fine-tuned for other tasks.</p>
<p>We now know that a convolutional neural network consists of many layers with a nonlinear activation between each pairs of layers, followed by one or several final linear layers with an activation function like sofmax at the very end. The final linear layer uses a matrix that has number of columns(which determine the size of the outputs) should match the number of classes in our classification problem. This final layers is useless for us when we fine-tuning it because the number of class is likely different, the specific categories it was trained on to identify are different. So we throw it away and replace it with new linear layer with the correct number of outputs for our specific task.</p>
<p>When we add a new linear layer for our specific task it weights are indeed initialized randomly, despite that the entire pretrained model is not random at all, all the layers before the final layers still retain their pretrained weights, which encoded valuable information, such as finding gradient and edges, and later on layer can identify eyeballs, and fur. We want to train it in a way that make it still remember all of these generally useful ideas that it has trained on, and use that to solve our problem.</p>
<p>So our problem is replace the random weights in our added layers with weights that are correctly achieve our desire task without breaking the carefully pretrained weights. So what we can do is to tell the optimizer to only update the weights in those randomly added final layers, don’t change the weights in the rest of the neuron network at all in other word freezing those pretrained layers. When we use <code>fine-tune</code> fastai automatically freezes all the pretrained layers for us, in fact it does these two things:</p>
<ul>
<li>First it trains the randomly added layers for one epoch, with all other layers frozen</li>
<li>Then it unfreezes all other layers and train them all with the number of epoch we tell it</li>
</ul>
<p>That why when fine-tune we always have a table with one column above then the table with the number of epoch blow it.</p>
<p>In fact <code>fine_tune</code> first does <code>fit_one_cycle</code> then unfreeze and does <code>fit_one_cycle</code> again. Alright let’s do it manually this time</p>

<div class="no-row-height column-margin column-container"><div id="cell-90" class="cell" data-execution_count="54">
<div class="sourceCode cell-code" id="cb71" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1">learn.fine_tune??</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">Signature:</span>

learn<span class="ansi-blue-fg">.</span>fine_tune<span class="ansi-blue-fg">(</span>

    epochs<span class="ansi-blue-fg">,</span>

    base_lr<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">0.002</span><span class="ansi-blue-fg">,</span>

    freeze_epochs<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span>

    lr_mult<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">100</span><span class="ansi-blue-fg">,</span>

    pct_start<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">0.3</span><span class="ansi-blue-fg">,</span>

    div<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">5.0</span><span class="ansi-blue-fg">,</span>

    <span class="ansi-blue-fg">*</span><span class="ansi-blue-fg">,</span>

    lr_max<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>

    div_final<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">100000.0</span><span class="ansi-blue-fg">,</span>

    wd<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>

    moms<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>

    cbs<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>

    reset_opt<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">False</span><span class="ansi-blue-fg">,</span>

    start_epoch<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">,</span>

<span class="ansi-blue-fg">)</span>

<span class="ansi-red-fg">Source:</span>   

<span class="ansi-blue-fg">@</span>patch

<span class="ansi-blue-fg">@</span>delegates<span class="ansi-blue-fg">(</span>Learner<span class="ansi-blue-fg">.</span>fit_one_cycle<span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">def</span> fine_tune<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">:</span>Learner<span class="ansi-blue-fg">,</span> epochs<span class="ansi-blue-fg">,</span> base_lr<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">2e-3</span><span class="ansi-blue-fg">,</span> freeze_epochs<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span> lr_mult<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">100</span><span class="ansi-blue-fg">,</span>

              pct_start<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">0.3</span><span class="ansi-blue-fg">,</span> div<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">5.0</span><span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">**</span>kwargs<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>

    <span class="ansi-blue-fg">"Fine tune with `Learner.freeze` for `freeze_epochs`, then with `Learner.unfreeze` for `epochs`, using discriminative LR."</span>

    self<span class="ansi-blue-fg">.</span>freeze<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>

    self<span class="ansi-blue-fg">.</span>fit_one_cycle<span class="ansi-blue-fg">(</span>freeze_epochs<span class="ansi-blue-fg">,</span> slice<span class="ansi-blue-fg">(</span>base_lr<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">,</span> pct_start<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">0.99</span><span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">**</span>kwargs<span class="ansi-blue-fg">)</span>

    base_lr <span class="ansi-blue-fg">/=</span> <span class="ansi-cyan-fg">2</span>

    self<span class="ansi-blue-fg">.</span>unfreeze<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>

    self<span class="ansi-blue-fg">.</span>fit_one_cycle<span class="ansi-blue-fg">(</span>epochs<span class="ansi-blue-fg">,</span> slice<span class="ansi-blue-fg">(</span>base_lr<span class="ansi-blue-fg">/</span>lr_mult<span class="ansi-blue-fg">,</span> base_lr<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">,</span> pct_start<span class="ansi-blue-fg">=</span>pct_start<span class="ansi-blue-fg">,</span> div<span class="ansi-blue-fg">=</span>div<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">**</span>kwargs<span class="ansi-blue-fg">)</span>

<span class="ansi-red-fg">File:</span>      ~/miniconda3/lib/python3.12/site-packages/fastai/callback/schedule.py

<span class="ansi-red-fg">Type:</span>      method</pre>
</div>
</div>
</div></div><div id="cell-91" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb72" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1">learn <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> vision_learner(dls, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'resnet26d'</span>, metrics<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>error_rate, path<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'.'</span>)</span>
<span id="cb72-2">learn.fit_one_cycle(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">3e-3</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train_loss</th>
<th data-quarto-table-cell-role="th">valid_loss</th>
<th data-quarto-table-cell-role="th">error_rate</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>1.797806</td>
<td>1.022250</td>
<td>0.333974</td>
<td>01:01</td>
</tr>
<tr class="even">
<td>1</td>
<td>1.111010</td>
<td>0.680857</td>
<td>0.217203</td>
<td>01:02</td>
</tr>
<tr class="odd">
<td>2</td>
<td>0.841627</td>
<td>0.611456</td>
<td>0.190293</td>
<td>00:59</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>We unfreeze the model</p>
<div id="cell-93" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb73" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1">learn.unfreeze()</span></code></pre></div>
</div>
<p>We also need to fine a new learning rate because we trained it with more layers, and weights that already train for 3 epochs means our previous founded learning rate isn’t appropriate anymore</p>
<div id="cell-95" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb74" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1">learn.lr_find()</span></code></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

</div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/monarch/miniconda3/lib/python3.12/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(file, map_location=device, **torch_load_kwargs)</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>SuggestedLRs(valley=6.30957365501672e-05)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://bhdai.github.io/blog/posts/paddy/index_files/figure-html/cell-50-output-5.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>alright let’s pick 1e-4 this time</p>
<div id="cell-97" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb77" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1">learn.fit_one_cycle(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>, lr_max<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-4</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train_loss</th>
<th data-quarto-table-cell-role="th">valid_loss</th>
<th data-quarto-table-cell-role="th">error_rate</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0.719057</td>
<td>0.490316</td>
<td>0.155694</td>
<td>01:06</td>
</tr>
<tr class="even">
<td>1</td>
<td>0.512388</td>
<td>0.341793</td>
<td>0.097549</td>
<td>01:05</td>
</tr>
<tr class="odd">
<td>2</td>
<td>0.364478</td>
<td>0.241446</td>
<td>0.069678</td>
<td>01:04</td>
</tr>
<tr class="even">
<td>3</td>
<td>0.255537</td>
<td>0.206497</td>
<td>0.064392</td>
<td>01:00</td>
</tr>
<tr class="odd">
<td>4</td>
<td>0.207048</td>
<td>0.193223</td>
<td>0.059106</td>
<td>01:05</td>
</tr>
<tr class="even">
<td>5</td>
<td>0.171621</td>
<td>0.182925</td>
<td>0.057184</td>
<td>01:03</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>Well it improve our model a lot from 0.18 to 0.06, in fact when i submit the predictions from the model we’ve built i got a pretty good result, it’s 0.93778, which improves a lot. Alright let’s see how our model train</p>
<div id="cell-99" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb78" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1">learn.recorder.plot_loss()</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://bhdai.github.io/blog/posts/paddy/index_files/figure-html/cell-52-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="how-long-to-train" class="level2">
<h2 class="anchored" data-anchor-id="how-long-to-train">How Long to Train?</h2>
<p>Well the first approach to train should be simply pick a number of epochs that will train in the amount of time that you’re happy to wait for, maybe take a cup of water, scrolling reddit, reading stuff,… then look at the training and validation loss plot like above and if you see it getting better when it comes to your final epochs, then you know that you should train it longer. In other hand you may see that the metrics you chosen really getting worse at the end of the training(remember that it’s not just that we’re looking for the validation loss to get worse, but the actual metrics). While the loss function is essential for optimization, what truly matters are your chosen practical metrics, don’t be overly concerned with validation loss inconstancy if your metric are still improving.</p>
<p>If you find you’ve trained for too long (your metrics getting worse, loss getting worse), what you should do is retrain your model from scratch, really, and this time choose the number of epochs based on where your previous best result was found. One more thing if you have extra time available instead of just simply increasing epochs consider using that time to train more parameters or use deeper architecture, this can potentially yield better results than extended training of a simpler model.</p>
</section>
<section id="mixed-precision-training" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="mixed-precision-training">Mixed Precision Training</h2>
<p>Alright let’s train it for one more time. This time notice i use <code>to_fp16</code> here, it called <em>mixed-precision training</em> it used for speeding time up especially when we using a big architecture. I highly recommend you to read <a href="https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html">this post from NVIDIA</a>.</p>
<p>But basically you first need to understand what is half-precision? Well it’s a floating-point number format uses 16 bits to represent numbers it called half-precision as the more common are 32 bits (single precision) and 64 bits (double precision).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://bhdai.github.io/blog/posts/paddy/fp16.png" class="img-fluid figure-img"></p>
<figcaption>half float</figcaption>
</figure>
</div>
<p>As you can see we have one bit to represent sign 5 for exponent bits while 10 for fraction bits. For instance, between 1 and 2, it can only represents the number <img src="https://latex.codecogs.com/png.latex?1,%201+2%5E%7B-10%7D,%201+2%5Cast2%5E%7B-10%7D">,… which mean if we plus 1 with a number smaller than <img src="https://latex.codecogs.com/png.latex?2%5E%7B-10%7D">(approximately 0.0009765625) we will get 1 instead of a number slightly greater than 1. let’s say 1 + 0.0001 = 1 in half precision, that means it less precise than single or double precision, with only about 3 decimal digits of precision. So it helps reduce the memory usage by half compare to single precision, or we can double our batch, model size. Another very nice feature is that NVIDIA developed its latest GPUs (the Volta generation) to take fully advantage of half-precision tensors.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>When we talk about <img src="https://latex.codecogs.com/png.latex?2%5E%7B-10%7D">, we’re referring to the smallest positive value that can be represented in the fraction part of the number. We have 10 bits to represent fraction part which mean it can represent <img src="https://latex.codecogs.com/png.latex?2%5E%7B10%7D%20=%201024"> different values, these 1024 values are distributed eventually between 0 and 1. In fact, it’s the smallest step between these values so we divide the range(which is 1 here) by the possible value (<img src="https://latex.codecogs.com/png.latex?2%5E%7B10%7D">):</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B1%7D%7B2%5E%7B10%7D%7D%20=%202%5E%7B-10%7D%0A"></p>
</div></div><p>But there’re several problem with half precision when using it:</p>
<ol type="1">
<li>The weight update is imprecise: What your optimizer does under the hood is basically this equation w = w - lr * w.grad for each weights of your network. So the problem is the w.grad is several order of magnitude below w especially as the network starts to converge, these gradient often become very small as the network is making tiny and tiny adjustment, even smaller than <img src="https://latex.codecogs.com/png.latex?2%5E%7B-10%7D"> is very common, so when using half precision, obviously the update doesn’t do anything here as FP16 can’t represent the tiny difference between w and (w - lr * w.grad).</li>
<li>During the backpropagation of gradients, the gradients themselves become so small that they are rounded down to 0 in FP16.</li>
<li>Your activation or loss can be overflow, the opposite problem from the gradients as during forward propagation, activation function like RElU or exponential function like softmax can produce a large values therefore it also make loss result in large numbers (especially in early training), it’s more easier to hit nan(of infinity) in FP16 precision and your training my more likely diverge.</li>
</ol>
<p>So the solution for this is mixed precision training, instead of fully train in FP16 precision some of the operations will be done in FP16, others in FP32. The main idea is that we will do the forward pass and the gradient computation in half precision to go fast, but the update in single precision. So our training loop will look like this:</p>
<ol type="1">
<li>compute the output with FP16 model, and loss</li>
<li>back-propagate the gradients in half-precision</li>
<li>we copy the gradient in FP32 precision</li>
<li>do the update on the master model (in FP32 precision)</li>
<li>copy the master model in the FP16 model</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Note that we will lose precision during step 5, and that the 1.0001 in one of the weights will go back to 1. But if the next update corresponds to add 0.0001 again, since the optimizer step is done on the master model, the 1.0001 will become 1.0002 and if we eventually go like this up to 1.0005, the FP16 model will be able to tell the difference.</p>
</div>
</div>
<p>Alright that’s solve the first problem. For the second problem we use something call the gradient scaling. To avoid the gradient getting zeroed by FP16 precision we multiple the loss by the scale factor(often scale=512), by multiplying the loss with a large numbers all the gradients are effectively made larger. Of course we don’t want those 512-scaled gradients to be in the weight updates so that after converting them into FP32 we can divide them by this scale. So it change the loop to:</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>In fact the scaling factor that we multiply it with the loss can leads our gradients or our loss to be overflow. So there a way around this, which is just simple as this, first we will try with a very high scale factor and see if it cause overflow to our loss or gradients, if it does, we will try with the half of that big value and again until we get the largest loss scale possible that doesn’t make our gradient overflow.</p>
</div></div><ol type="1">
<li>compute the output with the FP16 model, then the loss</li>
<li>multiply the loss by scale and then back-propagate the gradients in half precision</li>
<li>copy the gradients in FP32 precision then divide them by the scale</li>
<li>do the update on the master model</li>
<li>copy the master model in FP16 model</li>
</ol>
<p>For the last problem, the tricks offered by NVIDIA are to leave the batchnorm layers in single precision (they don’t have many weights so it’s not a big memory challenge) and compute the loss in single precision (which means converting the last output of the model in single precision before passing it to the loss).</p>
<p>Alright let’s apply our mixed precision strategy to our model, when we create a learner we call <code>to_fp16()</code>.</p>
<div id="cell-106" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb79" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1">learn <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> vision_learner(dls, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'resnet26d'</span>, metrics<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>error_rate, path<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'.'</span>).to_fp16()</span>
<span id="cb79-2">learn.lr_find(suggest_funcs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[valley, slide])</span></code></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

</div>
<div class="cell-output cell-output-stderr">
<pre><code>/usr/local/lib/python3.10/dist-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(file, map_location=device, **torch_load_kwargs)</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre><code>SuggestedLRs(valley=0.0012022644514217973, slide=0.002511886414140463)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://bhdai.github.io/blog/posts/paddy/index_files/figure-html/cell-53-output-5.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-107" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb82" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1">learn.fine_tune(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">14</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-2</span>, freeze_epochs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train_loss</th>
<th data-quarto-table-cell-role="th">valid_loss</th>
<th data-quarto-table-cell-role="th">error_rate</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>1.907815</td>
<td>1.100638</td>
<td>0.353676</td>
<td>00:41</td>
</tr>
<tr class="even">
<td>1</td>
<td>1.442608</td>
<td>1.050639</td>
<td>0.346468</td>
<td>00:41</td>
</tr>
<tr class="odd">
<td>2</td>
<td>1.303783</td>
<td>0.873419</td>
<td>0.289284</td>
<td>00:42</td>
</tr>
</tbody>
</table>
</div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train_loss</th>
<th data-quarto-table-cell-role="th">valid_loss</th>
<th data-quarto-table-cell-role="th">error_rate</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0.892789</td>
<td>0.587087</td>
<td>0.187410</td>
<td>00:42</td>
</tr>
<tr class="even">
<td>1</td>
<td>0.721903</td>
<td>0.487567</td>
<td>0.159058</td>
<td>00:41</td>
</tr>
<tr class="odd">
<td>2</td>
<td>0.625562</td>
<td>0.414129</td>
<td>0.140317</td>
<td>00:42</td>
</tr>
<tr class="even">
<td>3</td>
<td>0.559306</td>
<td>0.321464</td>
<td>0.098510</td>
<td>00:41</td>
</tr>
<tr class="odd">
<td>4</td>
<td>0.437455</td>
<td>0.248258</td>
<td>0.069678</td>
<td>00:41</td>
</tr>
<tr class="even">
<td>5</td>
<td>0.332489</td>
<td>0.218854</td>
<td>0.059106</td>
<td>00:42</td>
</tr>
<tr class="odd">
<td>6</td>
<td>0.265161</td>
<td>0.221709</td>
<td>0.059106</td>
<td>00:41</td>
</tr>
<tr class="even">
<td>7</td>
<td>0.205299</td>
<td>0.193941</td>
<td>0.052859</td>
<td>00:41</td>
</tr>
<tr class="odd">
<td>8</td>
<td>0.189015</td>
<td>0.177993</td>
<td>0.044690</td>
<td>00:42</td>
</tr>
<tr class="even">
<td>9</td>
<td>0.141942</td>
<td>0.173127</td>
<td>0.044690</td>
<td>00:42</td>
</tr>
<tr class="odd">
<td>10</td>
<td>0.137524</td>
<td>0.150655</td>
<td>0.041326</td>
<td>00:41</td>
</tr>
<tr class="even">
<td>11</td>
<td>0.111285</td>
<td>0.146560</td>
<td>0.034118</td>
<td>00:41</td>
</tr>
<tr class="odd">
<td>12</td>
<td>0.106115</td>
<td>0.145193</td>
<td>0.034599</td>
<td>00:42</td>
</tr>
<tr class="even">
<td>13</td>
<td>0.105035</td>
<td>0.143570</td>
<td>0.033157</td>
<td>00:41</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>Alright let’s see if it’s better, the best way to check is to submit it to kaggle we just do the same thing as above, just copy and past it here.</p>
<div id="cell-109" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb83" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1">ss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.read_csv(path<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sample_submission.csv'</span>)</span>
<span id="cb83-2">tst_files <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> get_image_files(path<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'test_images'</span>).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sorted</span>()</span>
<span id="cb83-3">tst_dl <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dls.test_dl(tst_files)</span>
<span id="cb83-4">probs,_,idxs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> learn.get_preds(dl<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>tst_dl, with_decoded<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb83-5">mapping <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">dict</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(dls.vocab))</span>
<span id="cb83-6">results <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.Series(idxs.numpy(), name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"idxs"</span>).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">map</span>(mapping)</span>
<span id="cb83-7">ss[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'label'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> results</span>
<span id="cb83-8">ss.to_csv(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'subm.csv'</span>, index<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb83-9"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!</span>head subm.csv</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/usr/local/lib/python3.10/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()
/usr/local/lib/python3.10/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()</code></pre>
</div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

</div>
<div class="cell-output cell-output-stdout">
<pre><code>image_id,label
200001.jpg,hispa
200002.jpg,normal
200003.jpg,blast
200004.jpg,blast
200005.jpg,blast
200006.jpg,brown_spot
200007.jpg,dead_heart
200008.jpg,brown_spot
200009.jpg,hispa</code></pre>
</div>
</div>
<p>This time i got a little higher result, i got 0.95967 as score which is pretty understandable as we trained it with 14 epochs, but yeah that was pretty cool.</p>
</section>
<section id="scaling-up" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="scaling-up">Scaling up</h2>
<p>Alright, can we do better, let’s see how far we can go, let’s do some experimenting, we will use different architectures and image processing approaches, for the sake of convenient let’s put our steps together into a little function.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Note that we can use <code>ImageDataLoader.from_folder</code> for our dataloader for make it shorter, but in general it the same as DataBlock</p>
</div>
</div>
<div id="cell-113" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;execution&quot;,&quot;value&quot;:{&quot;iopub.execute_input&quot;:&quot;2025-01-02T10:58:38.001235Z&quot;,&quot;iopub.status.busy&quot;:&quot;2025-01-02T10:58:38.000944Z&quot;,&quot;iopub.status.idle&quot;:&quot;2025-01-02T10:58:38.006134Z&quot;,&quot;shell.execute_reply&quot;:&quot;2025-01-02T10:58:38.005191Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2025-01-02T10:58:38.001214Z&quot;}}" data-trusted="true" data-execution_count="3">
<div class="sourceCode cell-code" id="cb86" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb86-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> train(arch, item, batch, epochs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>):</span>
<span id="cb86-2">  dls <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ImageDataLoaders.from_folder(train_path, seed<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">42</span>, valid_pct<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>, item_tfms<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>item, batch_tfms<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>batch)</span>
<span id="cb86-3">  learn <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> vision_learner(dls, arch, metrics<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>error_rate).to_fp16()</span>
<span id="cb86-4">  learn.fine_tune(epochs, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-2</span>)</span>
<span id="cb86-5">  <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> learn</span></code></pre></div>
</div>
<p>To have a better result, one way to archive this is to use a better model right, but what to choose, well Jeremy Howard has a really good <a href="https://www.kaggle.com/code/jhoward/the-best-vision-models-for-fine-tuning">notebook</a> that helps us choosing a appropriate architecture for our model based on what kind of problem you’re dealing, the GPU memory, error rate, time,… But basically there are two key to choose the right one:</p>
<ol type="1">
<li>How similar between our dataset and the pretrained model’s dataset.</li>
<li>How large they are.</li>
</ol>
<p>Then it turned out that when it comes to computer vision model <code>convnext</code> model is one of the best, if not the best till now so let’s give it a try shall we?</p>
<div id="cell-115" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;execution&quot;,&quot;value&quot;:{&quot;iopub.execute_input&quot;:&quot;2025-01-02T10:58:39.060620Z&quot;,&quot;iopub.status.busy&quot;:&quot;2025-01-02T10:58:39.060322Z&quot;,&quot;iopub.status.idle&quot;:&quot;2025-01-02T10:58:39.064862Z&quot;,&quot;shell.execute_reply&quot;:&quot;2025-01-02T10:58:39.063867Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2025-01-02T10:58:39.060596Z&quot;}}" data-trusted="true" data-execution_count="4">
<div class="sourceCode cell-code" id="cb87" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1">arch <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"convnext_small_in22k"</span></span></code></pre></div>
</div>
<p>From now on, if you not sure what architecture to use, just use this, right. And of course we have different version of <code>convnext</code> we have tinny, small, large… it will take more time to train but of course lower error rate. alright let’s see how it will go</p>
<div id="cell-117" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb88" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1">learn <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train(arch, item<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>Resize(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">192</span>,method<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"squish"</span>), batch<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>aug_transforms(size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span>, min_scale<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.75</span>))</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/usr/local/lib/python3.10/dist-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name convnext_small_in22k to current convnext_small.fb_in22k.
  model = create_fn(</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"0edc311bb36441c09986cd539a71048e","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/usr/local/lib/python3.10/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()
/usr/local/lib/python3.10/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()</code></pre>
</div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train_loss</th>
<th data-quarto-table-cell-role="th">valid_loss</th>
<th data-quarto-table-cell-role="th">error_rate</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>1.257208</td>
<td>0.790532</td>
<td>0.246997</td>
<td>00:46</td>
</tr>
</tbody>
</table>
</div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train_loss</th>
<th data-quarto-table-cell-role="th">valid_loss</th>
<th data-quarto-table-cell-role="th">error_rate</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0.610890</td>
<td>0.450510</td>
<td>0.146564</td>
<td>00:44</td>
</tr>
<tr class="even">
<td>1</td>
<td>0.504428</td>
<td>0.302570</td>
<td>0.097069</td>
<td>00:44</td>
</tr>
<tr class="odd">
<td>2</td>
<td>0.299630</td>
<td>0.194396</td>
<td>0.061989</td>
<td>00:44</td>
</tr>
<tr class="even">
<td>3</td>
<td>0.186308</td>
<td>0.130406</td>
<td>0.036521</td>
<td>00:44</td>
</tr>
<tr class="odd">
<td>4</td>
<td>0.134839</td>
<td>0.115092</td>
<td>0.035079</td>
<td>00:43</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>Well it did a pretty good job isn’t it? we just do 5 epochs and we archive almost the same as the one we trained with 12 epochs, even our time to go through each epoch is the same(that’s because i reduce the presize to 192X192 for it to run faster but it still produce the same performance as the previous one but with fewer epochs).</p>
<p>So one thing we could try is instead of using squish as our pre-processing let’s try using padding, now we will use bigger presize so that when we use padding here we will get entire image but the downside is we also get few extra zero pixels which literally pointless, but whatever let’s see if it work better</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>padding is interesting because it’s the only way of pre-processing images which doesn’t distort them and doesn’t loose anything, if you crop you lose things, if you squish you distort things</p>
</div></div><div id="cell-120" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb91" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb91-1">dls <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ImageDataLoaders.from_folder(train_path, valid_pct<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>, seed<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">42</span>,</span>
<span id="cb91-2">    item_tfms<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>Resize(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">480</span>, method<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ResizeMethod.Pad, pad_mode<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>PadMode.Zeros))</span>
<span id="cb91-3">dls.show_batch(max_n<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://bhdai.github.io/blog/posts/paddy/index_files/figure-html/cell-59-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-121" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;execution&quot;,&quot;value&quot;:{&quot;iopub.execute_input&quot;:&quot;2025-01-02T09:53:36.244933Z&quot;,&quot;iopub.status.busy&quot;:&quot;2025-01-02T09:53:36.244542Z&quot;,&quot;iopub.status.idle&quot;:&quot;2025-01-02T10:03:20.563150Z&quot;,&quot;shell.execute_reply&quot;:&quot;2025-01-02T10:03:20.562113Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2025-01-02T09:53:36.244902Z&quot;}}" data-trusted="true" data-execution_count="7">
<div class="sourceCode cell-code" id="cb92" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb92-1">learn <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train(arch, item<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>Resize((<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">480</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">360</span>), method<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ResizeMethod.Pad, pad_mode<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>PadMode.Zeros), batch<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>aug_transforms(size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">256</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">192</span>), min_scale<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.75</span>))</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/usr/local/lib/python3.10/dist-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name convnext_small_in22k to current convnext_small.fb_in22k.
  model = create_fn(</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"4ca9382aab4d4b6ebfa4e3d3a37e7e18","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/usr/local/lib/python3.10/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()
/usr/local/lib/python3.10/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()</code></pre>
</div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train_loss</th>
<th data-quarto-table-cell-role="th">valid_loss</th>
<th data-quarto-table-cell-role="th">error_rate</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>1.091136</td>
<td>0.672584</td>
<td>0.214320</td>
<td>01:12</td>
</tr>
</tbody>
</table>
</div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train_loss</th>
<th data-quarto-table-cell-role="th">valid_loss</th>
<th data-quarto-table-cell-role="th">error_rate</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0.579350</td>
<td>0.418952</td>
<td>0.131187</td>
<td>01:36</td>
</tr>
<tr class="even">
<td>1</td>
<td>0.430034</td>
<td>0.259760</td>
<td>0.086497</td>
<td>01:37</td>
</tr>
<tr class="odd">
<td>2</td>
<td>0.275291</td>
<td>0.176225</td>
<td>0.046612</td>
<td>01:38</td>
</tr>
<tr class="even">
<td>3</td>
<td>0.147642</td>
<td>0.123821</td>
<td>0.037963</td>
<td>01:38</td>
</tr>
<tr class="odd">
<td>4</td>
<td>0.109251</td>
<td>0.107270</td>
<td>0.030274</td>
<td>01:38</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>As you can see it indeed did better, and its error_rate is the best we can get so far but not huge different yet!</p>
<section id="test-time-augmentation" class="level3">
<h3 class="anchored" data-anchor-id="test-time-augmentation">Test Time Augmentation</h3>
<p>Well first let’s look how can we calculate the error rate manually with our normal prediction</p>
<div id="cell-123" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;execution&quot;,&quot;value&quot;:{&quot;iopub.execute_input&quot;:&quot;2025-01-02T10:03:54.927689Z&quot;,&quot;iopub.status.busy&quot;:&quot;2025-01-02T10:03:54.927216Z&quot;,&quot;iopub.status.idle&quot;:&quot;2025-01-02T10:04:07.727666Z&quot;,&quot;shell.execute_reply&quot;:&quot;2025-01-02T10:04:07.726181Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2025-01-02T10:03:54.927654Z&quot;}}" data-trusted="true" data-execution_count="8">
<div class="sourceCode cell-code" id="cb95" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb95-1">valid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> learn.dls.valid</span>
<span id="cb95-2">preds,targs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> learn.get_preds(dl<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>valid)</span>
<span id="cb95-3">error_rate(preds, targs)</span></code></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

</div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>TensorBase(0.0303)</code></pre>
</div>
</div>
<p>Well that actually the previous error-rate we got above, so what i’m doing here? well let’s take a look at the images blow</p>
<div id="cell-125" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;execution&quot;,&quot;value&quot;:{&quot;iopub.execute_input&quot;:&quot;2025-01-02T10:04:10.629694Z&quot;,&quot;iopub.status.busy&quot;:&quot;2025-01-02T10:04:10.629391Z&quot;,&quot;iopub.status.idle&quot;:&quot;2025-01-02T10:04:11.977555Z&quot;,&quot;shell.execute_reply&quot;:&quot;2025-01-02T10:04:11.976421Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2025-01-02T10:04:10.629672Z&quot;}}" data-trusted="true" data-execution_count="9">
<div class="sourceCode cell-code" id="cb97" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb97-1">learn.dls.train.show_batch(max_n<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>, unique<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://bhdai.github.io/blog/posts/paddy/index_files/figure-html/cell-62-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Notice that, those are indeed the same picture but it gone through the data augmentation so sometimes it a bit darker, a bit lighter, sometimes it flipped horizontally, some times int zoom into a slightly different section, sometimes it rotate a little bit but those are all the same picture. So the idea of TTA(Test Time Augmentation) is maybe our model would like some of these version better than the others even the original image, so what we can do is we can pass all of these to our model get the prediction of all of them and take the average right, so if you read my previous blog, it’s indeed the mini version of bagging approach. In fastai you can archive this by using the <code>tta</code> in our <code>learn</code> object</p>
<div id="cell-127" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;execution&quot;,&quot;value&quot;:{&quot;iopub.execute_input&quot;:&quot;2025-01-02T10:04:12.249981Z&quot;,&quot;iopub.status.busy&quot;:&quot;2025-01-02T10:04:12.249642Z&quot;,&quot;iopub.status.idle&quot;:&quot;2025-01-02T10:05:16.237725Z&quot;,&quot;shell.execute_reply&quot;:&quot;2025-01-02T10:05:16.236670Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2025-01-02T10:04:12.249954Z&quot;}}" data-trusted="true" data-execution_count="10">
<div class="sourceCode cell-code" id="cb98" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb98-1">tta_preds,_ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> learn.tta(dl<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>valid)</span></code></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

    <div>
      <progress value="0" class="" max="5" style="width:300px; height:20px; vertical-align: middle;"></progress>
      
    </div>
    
</div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

</div>
</div>
<div id="cell-128" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;execution&quot;,&quot;value&quot;:{&quot;iopub.execute_input&quot;:&quot;2025-01-02T10:05:22.943221Z&quot;,&quot;iopub.status.busy&quot;:&quot;2025-01-02T10:05:22.942752Z&quot;,&quot;iopub.status.idle&quot;:&quot;2025-01-02T10:05:22.951969Z&quot;,&quot;shell.execute_reply&quot;:&quot;2025-01-02T10:05:22.951021Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2025-01-02T10:05:22.943184Z&quot;}}" data-trusted="true" data-execution_count="11">
<div class="sourceCode cell-code" id="cb99" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb99-1">error_rate(tta_preds, targs)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>TensorBase(0.0259)</code></pre>
</div>
</div>
<p>See, we got better result like 10% better our previous. Alright let’s train it with more epochs but this time let’s just make a bigger image and something really interesting is that our images don’t have to be square they just need to be in the same size right, it can be rectangular, having said that all of our original images are nearly 640x480, so we just need to pick one has the same aspect ratio for example 256x192 is good</p>
<div id="cell-130" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;execution&quot;,&quot;value&quot;:{&quot;iopub.execute_input&quot;:&quot;2025-01-02T10:58:44.236186Z&quot;,&quot;iopub.status.busy&quot;:&quot;2025-01-02T10:58:44.235876Z&quot;,&quot;iopub.status.idle&quot;:&quot;2025-01-02T11:17:32.223793Z&quot;,&quot;shell.execute_reply&quot;:&quot;2025-01-02T11:17:32.222832Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2025-01-02T10:58:44.236162Z&quot;}}" data-trusted="true" data-execution_count="5">
<div class="sourceCode cell-code" id="cb101" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb101-1">learn <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train(arch, epochs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12</span>, item<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>Resize((<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">480</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">360</span>), method<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ResizeMethod.Pad, pad_mode<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>PadMode.Zeros), batch<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>aug_transforms(size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">256</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">192</span>), min_scale<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.75</span>))</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/usr/local/lib/python3.10/dist-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name convnext_small_in22k to current convnext_small.fb_in22k.
  model = create_fn(</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"c6339ec349dc43059e0d43dba1b0454e","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/usr/local/lib/python3.10/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()
/usr/local/lib/python3.10/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()</code></pre>
</div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train_loss</th>
<th data-quarto-table-cell-role="th">valid_loss</th>
<th data-quarto-table-cell-role="th">error_rate</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>1.091136</td>
<td>0.672584</td>
<td>0.214320</td>
<td>01:09</td>
</tr>
</tbody>
</table>
</div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train_loss</th>
<th data-quarto-table-cell-role="th">valid_loss</th>
<th data-quarto-table-cell-role="th">error_rate</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0.534227</td>
<td>0.295779</td>
<td>0.100432</td>
<td>01:25</td>
</tr>
<tr class="even">
<td>1</td>
<td>0.406855</td>
<td>0.259834</td>
<td>0.077847</td>
<td>01:26</td>
</tr>
<tr class="odd">
<td>2</td>
<td>0.345489</td>
<td>0.217263</td>
<td>0.065353</td>
<td>01:26</td>
</tr>
<tr class="even">
<td>3</td>
<td>0.272754</td>
<td>0.186886</td>
<td>0.056704</td>
<td>01:26</td>
</tr>
<tr class="odd">
<td>4</td>
<td>0.220682</td>
<td>0.205609</td>
<td>0.054781</td>
<td>01:27</td>
</tr>
<tr class="even">
<td>5</td>
<td>0.182489</td>
<td>0.122831</td>
<td>0.037001</td>
<td>01:27</td>
</tr>
<tr class="odd">
<td>6</td>
<td>0.118704</td>
<td>0.119720</td>
<td>0.035560</td>
<td>01:26</td>
</tr>
<tr class="even">
<td>7</td>
<td>0.099475</td>
<td>0.117059</td>
<td>0.034118</td>
<td>01:26</td>
</tr>
<tr class="odd">
<td>8</td>
<td>0.071605</td>
<td>0.094223</td>
<td>0.025949</td>
<td>01:26</td>
</tr>
<tr class="even">
<td>9</td>
<td>0.055326</td>
<td>0.096391</td>
<td>0.025949</td>
<td>01:26</td>
</tr>
<tr class="odd">
<td>10</td>
<td>0.037327</td>
<td>0.096320</td>
<td>0.024507</td>
<td>01:26</td>
</tr>
<tr class="even">
<td>11</td>
<td>0.035568</td>
<td>0.094268</td>
<td>0.023066</td>
<td>01:27</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>Alright our error_rate down to 2.3% which is pretty good, now, let’s what our error_rate when using tta</p>
<div id="cell-132" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;execution&quot;,&quot;value&quot;:{&quot;iopub.execute_input&quot;:&quot;2025-01-02T11:18:00.693869Z&quot;,&quot;iopub.status.busy&quot;:&quot;2025-01-02T11:18:00.693464Z&quot;,&quot;iopub.status.idle&quot;:&quot;2025-01-02T11:19:00.635588Z&quot;,&quot;shell.execute_reply&quot;:&quot;2025-01-02T11:19:00.634596Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2025-01-02T11:18:00.693833Z&quot;}}" data-trusted="true" data-execution_count="6">
<div class="sourceCode cell-code" id="cb104" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb104-1">tta_preds,targs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> learn.tta(dl<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>learn.dls.valid)</span>
<span id="cb104-2">error_rate(tta_preds, targs)</span></code></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

    <div>
      <progress value="0" class="" max="12" style="width:300px; height:20px; vertical-align: middle;"></progress>
      
    </div>
    
</div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

</div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>TensorBase(0.0235)</code></pre>
</div>
</div>
<p>Oops! it’s worse this time, that’s strange, i think it won’t always produce a better error_rate, but maybe it will work well in practice i guess, alright forget about it, let’s submit it</p>
<div id="cell-134" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;execution&quot;,&quot;value&quot;:{&quot;iopub.execute_input&quot;:&quot;2025-01-02T11:19:00.637385Z&quot;,&quot;iopub.status.busy&quot;:&quot;2025-01-02T11:19:00.637036Z&quot;,&quot;iopub.status.idle&quot;:&quot;2025-01-02T11:20:43.617429Z&quot;,&quot;shell.execute_reply&quot;:&quot;2025-01-02T11:20:43.616028Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2025-01-02T11:19:00.637353Z&quot;}}" data-trusted="true" data-execution_count="7">
<div class="sourceCode cell-code" id="cb106" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb106-1">ss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.read_csv(path<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sample_submission.csv'</span>)</span>
<span id="cb106-2">tst_files <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> get_image_files(path<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'test_images'</span>).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sorted</span>()</span>
<span id="cb106-3">tst_dl <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> learn.dls.test_dl(tst_files)</span>
<span id="cb106-4">preds,_ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> learn.tta(dl<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>tst_dl)</span>
<span id="cb106-5">idxs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> preds.argmax(dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb106-6">vocab <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array(learn.dls.vocab)</span>
<span id="cb106-7">results <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.Series(vocab[idxs], name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"idxs"</span>)</span>
<span id="cb106-8">ss[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'label'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> results</span>
<span id="cb106-9">ss.to_csv(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'subm.csv'</span>, index<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb106-10"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!</span>head subm.csv</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/usr/local/lib/python3.10/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()
/usr/local/lib/python3.10/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()</code></pre>
</div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

    <div>
      <progress value="0" class="" max="12" style="width:300px; height:20px; vertical-align: middle;"></progress>
      
    </div>
    
</div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

</div>
<div class="cell-output cell-output-stdout">
<pre><code>image_id,label
200001.jpg,hispa
200002.jpg,normal
200003.jpg,blast
200004.jpg,blast
200005.jpg,blast
200006.jpg,brown_spot
200007.jpg,dead_heart
200008.jpg,brown_spot
200009.jpg,hispa</code></pre>
</div>
</div>
<p>This time i got a little bit higher result, around 0.98 which is quite impressive. So we’ve gone through all of the essential concepts that we need to get familiar with as later we will delve deeper and deeper into more amazing thing later on, understand these concepts is like a solid groundwork for us to exploring even more fascinating topics in the future.</p>


</section>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Interpolation in image processing is the technique of estimating new pixel values when resizing or transforming a digital image. It’s like filling in the blanks between known pixels to create a smooth transition when changing an image’s size, rotation or shape↩︎</p></li>
<li id="fn2"><p>While the crop is indeed random, but it’s not entirely arbitrary. The goal is to create diversity in the training data, which helps the model learn to recognize objects and features from various perspectives and scales. In practice, it often use multiple crops from a single image during training, this increase the chances of capturing important features at least some of the crops.↩︎</p></li>
<li id="fn3"><p>a resize method which resizes the image to fit within the target dimensions and adds padding (usually black) to fill the rest, it keeps the original aspect ratio and all image information, however it can lead to artificial background which might affect model performance. Use it when you have to keep the entire image and the its aspect ratio is important and of course be a wear of extra background↩︎</p></li>
<li id="fn4"><p>a resize method which resizes the image to fit the target dimensions, potentially distorting the aspect ratio. it helps preserves all information in the image, but at the same time it can distort the image, potentially altering important features. Use it when the aspect ratio is not crucial for your task, or when your imagees are already mostly square↩︎</p></li>
<li id="fn5"><p>Resizes the image and then crops it to fit the target dimensions. Help maintains aspect ratio of the visible part and doesn’t introduce distortion, however it may lose important information at the edges of the image. Use it when the main subject is typically centered in your images, or when edge information is less important.↩︎</p></li>
<li id="fn6"><p>It will attempt to create a batch from the source you give it, with a lot of details. Also, if it fails, you will see exactly at which point the error happens, and the library will try to give you some help. For instance, one common mistake is to forget to use a Resize transform, so you end up with pictures of different sizes and are not able to batch them.↩︎</p></li>
<li id="fn7"><p>in multi label classification, each instance can belong to multiple classes simultaneously, imagine working on a dog cat classification, where an image could contain both dog and cat at the same time so in this problem it requires us to do multiplications on probabilities which will lead to numerical underflow problem in computer science↩︎</p></li>
<li id="fn8"><p>Number underflow occurs when a computation results in a number too small for computer to represent accurately, often leading to be rounded to 0↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>kaggle</category>
  <category>competition</category>
  <category>deep learning</category>
  <guid>https://bhdai.github.io/blog/posts/paddy/</guid>
  <pubDate>Sun, 29 Dec 2024 17:00:00 GMT</pubDate>
  <media:content url="https://bhdai.github.io/blog/posts/paddy/paddy-disease-farmer.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Random Forest, Bagging, Boosting</title>
  <dc:creator>Bui Huu Dai</dc:creator>
  <link>https://bhdai.github.io/blog/posts/2024-11-12-random-forest/</link>
  <description><![CDATA[ 






<p>What’s up! It’s been a long time since the last post, i’m quite lazy recently, but from know i will try to write more blog post though. I’ve revisited the Titanic dataset, this time through the lens of ensemble learning techniques. Previously I wrote about this dataset in this <a href="https://bhdai.github.io/blog/posts/titanic_competition/">blog</a>, but now, let’s dive into why random forests and gradient boosting machine are particularly suitable for tabular data.</p>
<p>You might ask, “Why not just use logistic regression?” While it seems simple, logistic regression can be surprisingly difficult to get right especially with transformation, interactions, and outlier handling. Random forests, on the other hand, offers resilience and robustness that are hard to match, which I’ll explain today.</p>
<p>To start, building a random forest is insightful help demystify the intricacies of machine learning algorithm. I’ll also touch on bagging and boosting, giving a clear view of their strengths</p>
<p>On a practical note, a helpful tip I’ve stumbled upon is using fastai’s import to efficiently bringing in essential libraries like Numpy an pandas. Here’s the snippet to simplify your setup:</p>
<div id="cell-2" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> fastai.imports <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span></span>
<span id="cb1-2">np.set_printoptions(linewidth<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">130</span>)</span></code></pre></div>
</div>
<p>These tools and techniques have enhanced my learning journey, and I’m excited to share these insights with you. Alright without any further ado let’s get right into it.</p>
<section id="decision-tree" class="level2">
<h2 class="anchored" data-anchor-id="decision-tree">Decision Tree</h2>
<section id="data-processing" class="level3">
<h3 class="anchored" data-anchor-id="data-processing">Data Processing</h3>
<p>First off, ensure that you have the Titanic dataset downloaded, Here’s the quick setup:</p>
<div id="cell-7" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> zipfile, kaggle</span>
<span id="cb2-2"></span>
<span id="cb2-3">path <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Path(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'titanic'</span>)</span>
<span id="cb2-4">kaggle.api.competition_download_cli(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>(path))</span>
<span id="cb2-5">zipfile.ZipFile(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>path<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">.zip'</span>).extractall(path)</span>
<span id="cb2-6"></span>
<span id="cb2-7">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.read_csv(path<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'train.csv'</span>)</span>
<span id="cb2-8">tst_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.read_csv(path<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'test.csv'</span>)</span>
<span id="cb2-9">modes <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.mode().iloc[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Downloading titanic.zip to /home/monarch/workplace/random_forest</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>100%|██████████| 34.1k/34.1k [00:00&lt;00:00, 365kB/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
</div>
<p>I’ve previously detailed the intricacies of processing the Titanic dataset in a separate blog post which you might find useful. For now, let’s breeze through some basic data processing steps without going into too much detail:</p>
<div id="cell-9" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> proc_data(df):</span>
<span id="cb7-2">    df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Fare'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.Fare.fillna(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb7-3">    df.fillna(modes, inplace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb7-4">    df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'LogFare'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.log1p(df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Fare'</span>])</span>
<span id="cb7-5">    df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Embarked'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.Categorical(df.Embarked)</span>
<span id="cb7-6">    df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Sex'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.Categorical(df.Sex)</span>
<span id="cb7-7"></span>
<span id="cb7-8">proc_data(df)</span>
<span id="cb7-9">proc_data(tst_df)</span></code></pre></div>
</div>
<p>Our next task involves organizing the data by identifying continuous and categorical variables, along with dependent variable we’re predicting</p>
<div id="cell-11" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1">cats<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Sex"</span>,<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Embarked"</span>]</span>
<span id="cb8-2">conts<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Age'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'SibSp'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Parch'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'LogFare'</span>,<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Pclass"</span>]</span>
<span id="cb8-3">dep<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Survived"</span></span></code></pre></div>
</div>
<p>Now, a brief look at how <code>Pandas</code> handles categorical variables. Let’s consider the <code>Sex</code> column:</p>
<div id="cell-13" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1">df.Sex.head()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>0      male
1    female
2    female
3    female
4      male
Name: Sex, dtype: category
Categories (2, object): ['female', 'male']</code></pre>
</div>
</div>
<p>It’s fascinating, although it appears unchanged(still just <code>Male</code> and <code>Female</code>), it’s now a category with a predefine list. Behind the magic, Pandas cleverly assigns numerical codes for these categories for efficient processing:</p>
<div id="cell-15" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">df.Sex.cat.codes.head()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>0    1
1    0
2    0
3    0
4    1
dtype: int8</code></pre>
</div>
</div>
<p>It’s actually turned them into numbers. This transformation sets the stage for our decision tree modeling</p>
</section>
<section id="binary-split" class="level3">
<h3 class="anchored" data-anchor-id="binary-split">Binary Split</h3>
<p>A random forest is essentially an ensemble of decision trees, and each tree is constructed from a series of binary split. But what exactly is a binary split?</p>
<p>Imagine taking all the passengers on the Titanic and dividing them into males and females to examine their survival rates.</p>
<div id="cell-19" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> seaborn <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> sns</span>
<span id="cb13-2"></span>
<span id="cb13-3">fig,axs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplots(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">11</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>))</span>
<span id="cb13-4">sns.barplot(data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>df, y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dep, x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Sex"</span>, ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>axs[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">set</span>(title<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Survival rate"</span>)</span>
<span id="cb13-5">sns.countplot(data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>df, x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Sex"</span>, ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>axs[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">set</span>(title<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Histogram"</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://bhdai.github.io/blog/posts/2024-11-12-random-forest/index_files/figure-html/cell-8-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>we see a stark difference: about a 20% survival rate for males and 75% for females, there are roughly twice as many males as females. If you base a model solely on sex, predicting survival becomes surprisingly effective: men likely didn’t survive, while woman likely did this division by sex exemplifies a binary split - it simple divide the data into two distinct groups.</p>
<p>To test the efficacy of this basic model, we first split our data into training and test dataset and encode our categorical variables.</p>
<div id="cell-21" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> random</span>
<span id="cb14-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.model_selection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> train_test_split</span>
<span id="cb14-3"></span>
<span id="cb14-4">random.seed(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">42</span>)</span>
<span id="cb14-5">trn_df,val_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_test_split(df, test_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.25</span>)</span>
<span id="cb14-6">trn_df[cats] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> trn_df[cats].<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">apply</span>(<span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span> x: x.cat.codes)</span>
<span id="cb14-7">val_df[cats] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> val_df[cats].<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">apply</span>(<span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span> x: x.cat.codes)</span></code></pre></div>
</div>
<p>Next, let’s create function to to extract independent variables (<code>xs</code>) and the dependent variable (<code>y</code>).</p>
<div id="cell-23" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> xs_y(df):</span>
<span id="cb15-2">    xs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[cats<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span>conts].copy()</span>
<span id="cb15-3">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> xs,df[dep] <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> dep <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> df <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span></span>
<span id="cb15-4"></span>
<span id="cb15-5">trn_xs,trn_y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xs_y(trn_df)</span>
<span id="cb15-6">val_xs,val_y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xs_y(val_df)</span></code></pre></div>
</div>
<p>From here we make predictions:</p>
<div id="cell-25" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.metrics <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> mean_absolute_error</span>
<span id="cb16-2">preds <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> val_xs.Sex<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb16-3">mean_absolute_error(val_y, preds)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>0.21524663677130046</code></pre>
</div>
</div>
<p>A 21.5% error rate isn’t too shabby for such a simple model. Can we do better? Let’s try another variable such as <code>Fare</code> which is continuous.</p>
<div id="cell-27" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1">df_fare <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> trn_df[trn_df.LogFare<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb18-2">fig,axs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplots(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">11</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>))</span>
<span id="cb18-3">sns.boxenplot(data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>df_fare, x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dep, y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"LogFare"</span>, ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>axs[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])</span>
<span id="cb18-4">sns.kdeplot(data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>df_fare, x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"LogFare"</span>, ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>axs[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://bhdai.github.io/blog/posts/2024-11-12-random-forest/index_files/figure-html/cell-12-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The boxenplot shows that those who survived generally paid higher fares.</p>
<p>So here’s another model <code>LogFare</code> greater than 2.7:</p>
<div id="cell-29" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1">preds <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> val_xs.LogFare<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">2.7</span></span>
<span id="cb19-2">mean_absolute_error(val_y, preds)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre><code>0.336322869955157</code></pre>
</div>
</div>
<p>Oh, much worse</p>
<p>To evaluate binary split uniformly, regardless of the datatype, We measure how similar the dependent variable values are within each split. We aim for standard deviations within groups, multiplied by group sizes to account for impact differences.</p>
<div id="cell-31" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> _side_score(side, y):</span>
<span id="cb21-2">    tot <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> side.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>()</span>
<span id="cb21-3">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> tot<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>: <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb21-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> y[side].std()<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>tot</span>
<span id="cb21-5"></span>
<span id="cb21-6"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> score(col, y, split):</span>
<span id="cb21-7">    lhs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> col<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;=</span>split</span>
<span id="cb21-8">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> (_side_score(lhs,y) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> _side_score(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span>lhs,y))<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(y)</span></code></pre></div>
</div>
<p>So for example, if we split by Sex, is greater than or less than 0.5.That’ll create two groups, males and females, and that gives us this score.</p>
<div id="cell-33" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1">score(trn_xs[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Sex"</span>], trn_y, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="14">
<pre><code>0.4078753098206398</code></pre>
</div>
</div>
<p>And if we do LogFare greater than or less than 2.7, it gives us this score.</p>
<div id="cell-35" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1">score(trn_xs[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"LogFare"</span>], trn_y, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">2.7</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="15">
<pre><code>0.4718087395209973</code></pre>
</div>
</div>
<p>Lower scores indicates better splits, with <code>Sex</code> outperforming <code>LogFare</code>. But how can we find a best split point i mean we have to try ourself right? In every values and see if the score improve or not right, well that was pretty inefficient. It would be nice if we could find some automatic wway to do al that. Well, of course we can. If we want to find the best split point for <code>Age</code>, and try each one in turn, and see what score we get, if we made a binary split on that level of <code>Age</code>. So here’s a list of all the possible binary split threshold of <code>Age</code></p>
<div id="cell-37" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb26" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1">col <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> trn_xs[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Age"</span>]</span>
<span id="cb26-2">unq <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> col.unique()</span>
<span id="cb26-3">unq.sort()</span>
<span id="cb26-4">unq</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="16">
<pre><code>array([ 0.42,  0.67,  0.75,  0.83,  0.92,  1.  ,  2.  ,  3.  ,  4.  ,  5.  ,  6.  ,  7.  ,  8.  ,  9.  , 10.  , 11.  , 12.  ,
       13.  , 14.  , 14.5 , 15.  , 16.  , 17.  , 18.  , 19.  , 20.  , 21.  , 22.  , 23.  , 24.  , 24.5 , 25.  , 26.  , 27.  ,
       28.  , 28.5 , 29.  , 30.  , 31.  , 32.  , 32.5 , 33.  , 34.  , 34.5 , 35.  , 36.  , 36.5 , 37.  , 38.  , 39.  , 40.  ,
       40.5 , 41.  , 42.  , 43.  , 44.  , 45.  , 45.5 , 46.  , 47.  , 48.  , 49.  , 50.  , 51.  , 52.  , 53.  , 54.  , 55.  ,
       55.5 , 56.  , 57.  , 58.  , 59.  , 60.  , 61.  , 62.  , 64.  , 65.  , 70.  , 70.5 , 74.  , 80.  ])</code></pre>
</div>
</div>
<p>Let’s go through all of them. For each of them calculate the score and then <code>Numpy</code> and <code>Pytorch</code> have an <code>argmin()</code> function, which tells you what index into that list is the smallest.</p>
<div id="cell-39" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb28" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1">scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([score(col, trn_y, o) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> o <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> unq <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> np.isnan(o)])</span>
<span id="cb28-2">unq[scores.argmin()]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<pre><code>6.0</code></pre>
</div>
</div>
<p>Here’s the scores.</p>
<div id="cell-41" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb30" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1">scores</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="18">
<pre><code>array([0.48447755, 0.48351588, 0.48158676, 0.48061929, 0.47964987, 0.480937  , 0.48347294, 0.48171397, 0.47987776, 0.47884826,
       0.47831672, 0.47949847, 0.47957573, 0.48092137, 0.48130659, 0.48200571, 0.48163287, 0.48124801, 0.48151498, 0.48183316,
       0.48105614, 0.48202484, 0.48178211, 0.48337829, 0.48439618, 0.48501782, 0.48545475, 0.48556795, 0.48550856, 0.48554074,
       0.48550094, 0.48504976, 0.48480161, 0.48561331, 0.4852559 , 0.48513473, 0.48529147, 0.48530156, 0.48543741, 0.48569729,
       0.48571309, 0.48571467, 0.4856701 , 0.48563657, 0.48579877, 0.48579767, 0.4858019 , 0.48580095, 0.48580002, 0.48580178,
       0.48580211, 0.48579777, 0.4857996 , 0.48580236, 0.48579236, 0.48580043, 0.48580303, 0.4858034 , 0.4857613 , 0.4855666 ,
       0.48579394, 0.48580506, 0.48580434, 0.48580707, 0.48579364, 0.48580788, 0.48581017, 0.48580597, 0.48581077, 0.48576815,
       0.48580167, 0.48545792, 0.48567909, 0.48542059, 0.48557468, 0.48492654, 0.4852198 , 0.48548666, 0.48590271, 0.48601112,
       0.48447755, 0.48543732])</code></pre>
</div>
</div>
<p>Create a function to calculate this for any column:</p>
<div id="cell-43" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb32" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> min_col(df, nm):</span>
<span id="cb32-2">    col,y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[nm],df[dep]</span>
<span id="cb32-3">    unq <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> col.dropna().unique()</span>
<span id="cb32-4">    scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([score(col, y, o) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> o <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> unq <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> np.isnan(o)])</span>
<span id="cb32-5">    idx <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scores.argmin()</span>
<span id="cb32-6">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> unq[idx],scores[idx]</span>
<span id="cb32-7"></span>
<span id="cb32-8">min_col(trn_df, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Age"</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="19">
<pre><code>(6.0, 0.47831671750899085)</code></pre>
</div>
</div>
<p>Revealing that is at 6.0 for <code>Age</code>. So now we can just go through and calculates the score for the best split point for each column.</p>
<div id="cell-45" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb34" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1">cols <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> cats<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span>conts</span>
<span id="cb34-2">{o:min_col(trn_df, o) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> o <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> cols}</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="20">
<pre><code>{'Sex': (0, 0.4078753098206398),
 'Embarked': (0, 0.478833425731479),
 'Age': (6.0, 0.47831671750899085),
 'SibSp': (4, 0.4783740258817423),
 'Parch': (0, 0.4805296527841601),
 'LogFare': (2.4390808375825834, 0.4620823937736595),
 'Pclass': (2, 0.4604826188580666)}</code></pre>
</div>
</div>
<p>And if we do that, we find that the lowest score is Sex. So that is how to calculate the best binary split. So we now know that the model we created earlier with <code>Sex</code> is the best single binary split model we can find.</p>
<p>And this simple thing we just did which is finding a single binary split, actually is a type of model, it has a name too, it’s called OneR. And OneR model it turned out in a review of machine learning methods in the 90s is one of the best, if not the best. It’s not a bad idea to always start creating a baseline of OneR, a decision tree with a single binary split.</p>
</section>
<section id="creating-a-tree" class="level3">
<h3 class="anchored" data-anchor-id="creating-a-tree">Creating a Tree</h3>
<p>“OneR” is probably not going to cut it for a lot of things, though it’s surprisingly effective, but maybe we could go a step further. And the other step further we could go is by creating a maybe “TwoR”. What if we took each of those groups, males and females in the Titanic dataset, and split each of these into two other groups? So split the males into two groups and split the females into two groups. To do that, we can repeat the exact same piece of code we just did, but let’s remove sex from it:</p>
<div id="cell-49" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb36" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1">cols.remove(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Sex"</span>)</span>
<span id="cb36-2">ismale <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> trn_df.Sex<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb36-3">males,females <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> trn_df[ismale],trn_df[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span>ismale]</span></code></pre></div>
</div>
<p>Then, run the same piece of code that we just did before, but just for the males:</p>
<div id="cell-51" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb37" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1">{o:min_col(males, o) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> o <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> cols}</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="22">
<pre><code>{'Embarked': (0, 0.387558187041091),
 'Age': (6.0, 0.37398283710105873),
 'SibSp': (4, 0.38758642275862637),
 'Parch': (0, 0.3874704821461953),
 'LogFare': (2.803360380906535, 0.38048562317581447),
 'Pclass': (1, 0.3815544200436083)}</code></pre>
</div>
</div>
<p>This provides a “OneR” rule for how to predict which males survived the Titanic, Interestingly, age turns out to be the biggest predictor for males whether they were greater than or less than 6 determined their survival odds</p>
<p>Similarity, for females:</p>
<div id="cell-53" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb39" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1">{o:min_col(females, o) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> o <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> cols}</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="23">
<pre><code>{'Embarked': (0, 0.4295252982857326),
 'Age': (50.0, 0.4225927658431646),
 'SibSp': (4, 0.42319212059713585),
 'Parch': (3, 0.4193314500446157),
 'LogFare': (4.256321678298823, 0.413505983329114),
 'Pclass': (2, 0.3335388911567602)}</code></pre>
</div>
</div>
<p>The passenger class <code>Pclass</code>, or whether they were in first class or not, was the biggest predictor of survival.</p>
<p>This process generates a decision tree - a serries of binary splits that gradually categorize our data so that in the leaf nodes, we derive strong predictions about survival</p>
<p>We can continue these steps for each of the four groups manually with a couple of extra lines of code, or we can use a decision tree classifier. This class automates the process we just outlined:</p>
<div id="cell-55" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb41" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.tree <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> DecisionTreeClassifier, export_graphviz</span>
<span id="cb41-2"></span>
<span id="cb41-3">m <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> DecisionTreeClassifier(max_leaf_nodes<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>).fit(trn_xs, trn_y)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
</div>
<p>And one very nice thing it has is it can draw the tree for us. So here’s a tiny little draw_tree function:</p>
<div id="cell-57" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb42" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> graphviz</span>
<span id="cb42-2"></span>
<span id="cb42-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> draw_tree(t, df, size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, ratio<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.6</span>, precision<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span>kwargs):</span>
<span id="cb42-4">    s<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>export_graphviz(t, out_file<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, feature_names<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>df.columns, filled<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, rounded<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb42-5">                      special_characters<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, rotate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, precision<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>precision, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span>kwargs)</span>
<span id="cb42-6">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> graphviz.Source(re.sub(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Tree {'</span>, <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'Tree </span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">{{</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> size=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>size<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">; ratio=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>ratio<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>, s))</span>
<span id="cb42-7"></span>
<span id="cb42-8">draw_tree(m, trn_xs, size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="25">
<div>
<figure class="figure">
<p><img src="https://bhdai.github.io/blog/posts/2024-11-12-random-forest/index_files/figure-html/cell-26-output-1.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>And you can see here it’s going to first of all split on sex. Now, it looks a bit weird to say sex is less than or equal to 0.5, but remember our binary characteristics are coded as zero or one. This is just an easy way to denote males versus females.</p>
<p>For females, the next split is based on their class. For males, age is the dedicating factor. This creates our four leaf nodes. For instance, of the females in the first class, 116 survived, and only 4 didn’t showing that being a wealthy woman on the Titanic was quite advantageous. On the other hand, among adult males, 68 survived while 350 perished, illustrating the peril they faced.</p>
<p>This quick summary showcases why decision trees are favoured in exploratory data analysis; they provide a clear picture of key variables driving the dataset and their predictive power</p>
<p>One additional point is the <code>Gini</code> measure, a way of evaluating how good a split is, which i’ve illustrated in the code below:</p>
<div id="cell-59" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb43" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> gini(cond):</span>
<span id="cb43-2">    act <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.loc[cond, dep]</span>
<span id="cb43-3">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> act.mean()<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>act).mean()<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span></span></code></pre></div>
</div>
<p>To understand this mathematically: if <img src="https://latex.codecogs.com/png.latex?p"> is a probability of an instance being classified as a positive class, and <img src="https://latex.codecogs.com/png.latex?(1%20-%20p)"> for the negative class, <img src="https://latex.codecogs.com/png.latex?p%5E2"> denotes the chance of both randomly selected instances being positive and <img src="https://latex.codecogs.com/png.latex?(1-p)%5E2"> being negative. The term <img src="https://latex.codecogs.com/png.latex?(1-p%5E2%20-%20(1-p)%5E2)"> gives us the probability of misclassification, subtracting the chances of correctly classifying instances.</p>
<p>Here’s an example of <code>Gini</code> calculation for gender:</p>
<div id="cell-61" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb44" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1">gini(df.Sex<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'female'</span>), gini(df.Sex<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'male'</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="27">
<pre><code>(0.3828350034484158, 0.3064437162277842)</code></pre>
</div>
</div>
<p>Here, <code>act.mean()**2</code> is the probability that two randomly selected individual both survived, and <code>(1 - act.mean())**2</code> that both did not. Lower <code>Gini</code> impurity suggests a strong skew in survival outcomes, which can be insightful for decision making or predicting survival likelihood based on gender.</p>
<p>Decision trees thus provide not only visual insights but quantitative ways to discern what’s happening within your dataset.</p>
<div id="cell-63" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb46" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1">mean_absolute_error(val_y, m.predict(val_xs))</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="28">
<pre><code>0.2242152466367713</code></pre>
</div>
</div>
<p>So that was for the “OneR” version. For the decision tree with four leaf nodes, the mean absolute error was 0.224, which is actually a bit worse. This outcome suggest that due to the small size of the dataset, the “OneR” method was impressively effective, and enhancements weren’t substantial enough to be discerned among the randomness of such a small validation set.</p>
<p>To take it further, let’s implement a decision tree with a minimum of 50 samples per leaf node:</p>
<div id="cell-65" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb48" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1">m <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> DecisionTreeClassifier(min_samples_leaf<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>)</span>
<span id="cb48-2">m.fit(trn_xs, trn_y)</span>
<span id="cb48-3">draw_tree(m, trn_xs, size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="29">
<div>
<figure class="figure">
<p><img src="https://bhdai.github.io/blog/posts/2024-11-12-random-forest/index_files/figure-html/cell-30-output-1.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>This indicates that each leaf will contain at least 50 samples, in this context passengers on the Titanic. For example, suppose you’ve identified that 67 people were female, first-class, and under 28. That’s the point where the tree ceases splitting further</p>
<p>Let’s evaluate this decision tree:</p>
<div id="cell-67" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb49" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1">mean_absolute_error(val_y, m.predict(val_xs))</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="30">
<pre><code>0.18385650224215247</code></pre>
</div>
</div>
<p>With an absolute error of 0.183, this approach shows a bit of improvement.</p>
<p>An interesting aspect of decision trees is the minimal preprocessing required you may have noticed this advantage. There was no need for dummy variables for category features, and although you can create them, it isn’t necessary. Decision trees can manage without these adjustments. We only took the logarithm of the fare to enhance the visual appearance of our graph but the split would operate identically on the original scale, focusing only on data ordering</p>
<p>Moreover, decision trees are indifferent to outliers, long-tailed distributions, and categorical variables: they handle all these situations effectively.</p>
<p>The take away here is that for tabular data, starting with a decision tree-based approach is prudent. It helps create baselines because they are remarkably resilient and offer a robust performance without intricate tuning</p>
</section>
</section>
<section id="random-forest" class="level2">
<h2 class="anchored" data-anchor-id="random-forest">Random Forest</h2>
<p>Now, what if we wanted to make this more accurate? Could we grow the tree further? We could, but with only 50 samples in these leaves, further splitting would result in the leaf nodes having so little data that their predictions wouldn’t be very meaningful. Naturally, there are limitation to how accurate a decision tree can be. so, what we can do? Enter a fascinating strategy called <a href="https://www.stat.berkeley.edu/~breiman/bagging.pdf">bagging</a>.</p>
<p>Here’s the procedure of bagging:</p>
<ol type="1">
<li>Randomly choose a subset of data rows (a “bootstrap replicate” of the learning set).</li>
<li>Train a model using this subset.</li>
<li>Save that model, then go back to step 1 and repeat several times.</li>
<li>This will give you multiple trained models. Predict with all models, and then average their predictions to make the final prediction.</li>
</ol>
<p>The core insight of bagging is that although models trained on data subsets will make more errors than a model trained on the full dataset, these errors aren’t correlated across models. Different models will make different errors, and when averaged, those errors offset each other. Thus, average the predictions of all the model sharpens the final prediction with more models providing finer estimations.</p>
<p>In essence, a random forest averages the predictions of numerous decision trees, which are generated randomly varying parameters such as training dataset or tree parameters. Bagging is a particular approach to “ensembling” or combining results from multiple models.</p>
<p>Let’s create one in a few lines. Here’s a function to generate a decision tree:</p>
<div id="cell-71" class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb51" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> get_tree(prop<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.75</span>):</span>
<span id="cb51-2">    n <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(trn_y)</span>
<span id="cb51-3">    idxs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> random.choice(n, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(n<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>prop))</span>
<span id="cb51-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> DecisionTreeClassifier(min_samples_leaf<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>).fit(trn_xs.iloc[idxs], trn_y.iloc[idxs])</span></code></pre></div>
</div>
<p>Here, <code>prop</code> denotes the data proportion used, say 75% each time with <code>n</code> as the sample size. Random samples <code>idxs</code> are selected based on the specified proportion, and a decision tree is built from this subset.</p>
<p>Let’s get 100 trees and compile them into a list:</p>
<div id="cell-73" class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb52" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1">trees <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [get_tree() <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> t <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>)]</span>
<span id="cb52-2">all_probs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [t.predict(val_xs) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> t <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> trees]</span>
<span id="cb52-3">avg_probs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.stack(all_probs).mean(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb52-4"></span>
<span id="cb52-5">mean_absolute_error(val_y, avg_probs)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="32">
<pre><code>0.2272645739910314</code></pre>
</div>
</div>
<p>By collecting predictions from these trees, stacking them, and averaging their predictions, we have our random forest.</p>
<p>Random forests are remarkably simple yet powerful. A key feature is that they also randomly select subset of columns to build decision trees, changing the column subset with each node split. The idea is to maintain randomness, yet retain usefulness. For more efficient implementation, we use <code>RandomForestClassifier</code>:</p>
<div id="cell-75" class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb54" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.ensemble <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> RandomForestClassifier</span>
<span id="cb54-2"></span>
<span id="cb54-3">rf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> RandomForestClassifier(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, min_samples_leaf<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>)</span>
<span id="cb54-4">rf.fit(trn_xs, trn_y)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb54-5">mean_absolute_error(val_y, rf.predict(val_xs))</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="33">
<pre><code>0.18834080717488788</code></pre>
</div>
</div>
<p>Here, we specify the number of trees and samples per leaf, then fit the classifier. While our mean absolute error might not surpass a single decision tree due to dataset constraints, it remains robust</p>
<p>One can inspect the built decision trees to identify split columns. Monitoring column improvements in <code>Gini</code> across decision trees yields a <strong>feature importance plot</strong>:</p>
<div id="cell-77" class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb56" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1">pd.DataFrame(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">dict</span>(cols<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>trn_xs.columns, imp<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>m.feature_importances_)).plot(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cols'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'imp'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'barh'</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://bhdai.github.io/blog/posts/2024-11-12-random-forest/index_files/figure-html/cell-35-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Feature importance plots demonstrate a feature’s significance by indicating how frequently and effectively it was used for splits. The <code>Sex</code> variable emerges as most significant, follow by <code>Pclass</code>, with other variables less crucial. And this is another reason, by the way, why the random forest isn’t really particularly helpful, because it’s just a easy split to do, basically all the matter is what class you are in and whether you’re male of female.</p>
<p>Random Forests, due to their versatility with data distribution and categorical variable handling, allow immediate and insightful datasets analyses. For large datasets, they quickly reveal key features, facilitating further focused analysis.</p>
</section>
<section id="what-else-can-we-do-with-random-forest" class="level2">
<h2 class="anchored" data-anchor-id="what-else-can-we-do-with-random-forest">What else can we do with Random Forest</h2>
<p>There are other things that you can do with Random Forests and the Titanic dataset is a small one, so it doesn’t highlight the full power of Random Forests. For a bigger and more numerically interesting dataset, let’s consider the auction price of heavy industrial equipment. This dataset is from The <a href="https://www.kaggle.com/c/bluebook-for-bulldozers/overview">Blue Book for Bulldozers</a> Kaggle competition. I highly recommended taking a peek at the overview and the dataset on the competition page before we start.</p>
<section id="preparing-stuff" class="level3">
<h3 class="anchored" data-anchor-id="preparing-stuff">Preparing Stuff</h3>
<section id="downloading-the-dataset" class="level4">
<h4 class="anchored" data-anchor-id="downloading-the-dataset">Downloading the Dataset</h4>
<div id="cell-83" class="cell">
<details class="code-fold">
<summary>Import stuff click to show the code</summary>
<div class="sourceCode cell-code" id="cb57" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> fastbook <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span></span>
<span id="cb57-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> pandas.api.types <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> is_string_dtype, is_numeric_dtype, is_categorical_dtype</span>
<span id="cb57-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> fastai.tabular.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">all</span> <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span></span>
<span id="cb57-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.ensemble <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> RandomForestRegressor</span>
<span id="cb57-5"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.tree <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> DecisionTreeRegressor</span>
<span id="cb57-6"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> dtreeviz.trees <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span></span>
<span id="cb57-7"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> IPython.display <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Image, display_svg, SVG</span>
<span id="cb57-8"></span>
<span id="cb57-9"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> warnings</span>
<span id="cb57-10">warnings.simplefilter(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ignore'</span>, <span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">FutureWarning</span>)</span>
<span id="cb57-11"></span>
<span id="cb57-12">pd.options.display.max_rows <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span></span>
<span id="cb57-13">pd.options.display.max_columns <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span></span></code></pre></div>
</details>
</div>
<p>Pick a path to download the dataset:</p>
<div id="cell-86" class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb58" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1">comp <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'bluebook-for-bulldozers'</span></span>
<span id="cb58-2">path <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> URLs.path(comp)</span>
<span id="cb58-3">path</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="37">
<pre><code>Path('/home/monarch/.fastai/archive/bluebook-for-bulldozers')</code></pre>
</div>
</div>
<p>Use the Kaggle API to download the data to the specified path and extract it:</p>
<div id="cell-88" class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb60" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> kaggle <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> api</span>
<span id="cb60-2"></span>
<span id="cb60-3"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> path.exists():</span>
<span id="cb60-4">    path.mkdir(parents<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>true)</span>
<span id="cb60-5">    api.competition_download_cli(comp, path<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>path)</span>
<span id="cb60-6">    shutil.unpack_archive(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>(path<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>comp<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">.zip'</span>), <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>(path))</span>
<span id="cb60-7"></span>
<span id="cb60-8">path.ls(file_type<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'text'</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Downloading bluebook-for-bulldozers.zip to /home/monarch/.fastai/archive/bluebook-for-bulldozers</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>100%|██████████| 48.4M/48.4M [00:07&lt;00:00, 6.52MB/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="38">
<pre><code>(#7) [Path('/home/monarch/.fastai/archive/bluebook-for-bulldozers/Machine_Appendix.csv'),Path('/home/monarch/.fastai/archive/bluebook-for-bulldozers/Test.csv'),Path('/home/monarch/.fastai/archive/bluebook-for-bulldozers/TrainAndValid.csv'),Path('/home/monarch/.fastai/archive/bluebook-for-bulldozers/Valid.csv'),Path('/home/monarch/.fastai/archive/bluebook-for-bulldozers/ValidSolution.csv'),Path('/home/monarch/.fastai/archive/bluebook-for-bulldozers/median_benchmark.csv'),Path('/home/monarch/.fastai/archive/bluebook-for-bulldozers/random_forest_benchmark_test.csv')]</code></pre>
</div>
</div>
<p>I’ll now walk you through the dataset. If you examine the <a href="https://www.kaggle.com/c/bluebook-for-bulldozers/data">Data tab</a> on the competition page, here are the key fields found in train.csv:</p>
<ul>
<li><code>SalesID</code>: The unique identifier of the sale.</li>
<li><code>MachineID</code>: the unique identifier of the machine. A machine can be sold multiple times.</li>
<li><code>saleprice</code>: The auction sale price of the machine (only provided in train.csv)</li>
<li><code>saledate</code>: The date the sale occurred.</li>
</ul>
<p>We begin by reading the training set into Pandas <code>DataFrame</code>. It’s generally advisable to specify <code>low_memory=False</code> unless Pandas runs out of memory and throws an error. By default, <code>low_memory</code> is <code>True</code>, instructing Pandas to process data in chucks, which may lead to inconsistent column data types and subsequent data processing or modeling errors.</p>
<div id="cell-90" class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb65" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.read_csv(path<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'TrainAndValid.csv'</span>, low_memory<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb65-2">df.columns</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="39">
<pre><code>Index(['SalesID', 'SalePrice', 'MachineID', 'ModelID', 'datasource',
       'auctioneerID', 'YearMade', 'MachineHoursCurrentMeter', 'UsageBand',
       'saledate', 'fiModelDesc', 'fiBaseModel', 'fiSecondaryDesc',
       'fiModelSeries', 'fiModelDescriptor', 'ProductSize',
       'fiProductClassDesc', 'state', 'ProductGroup', 'ProductGroupDesc',
       'Drive_System', 'Enclosure', 'Forks', 'Pad_Type', 'Ride_Control',
       'Stick', 'Transmission', 'Turbocharged', 'Blade_Extension',
       'Blade_Width', 'Enclosure_Type', 'Engine_Horsepower', 'Hydraulics',
       'Pushblock', 'Ripper', 'Scarifier', 'Tip_Control', 'Tire_Size',
       'Coupler', 'Coupler_System', 'Grouser_Tracks', 'Hydraulics_Flow',
       'Track_Type', 'Undercarriage_Pad_Width', 'Stick_Length', 'Thumb',
       'Pattern_Changer', 'Grouser_Type', 'Backhoe_Mounting', 'Blade_Type',
       'Travel_Controls', 'Differential_Type', 'Steering_Controls'],
      dtype='object')</code></pre>
</div>
</div>
<p>That’s many columns to scour! Start by exploring the dataset to familiarize yourself with the data content in each column. Soon we’ll focus on the most compelling bits.</p>
<p>With ordinal columns, it’s beneficial to specify meaningful order. These columns contain strings with an inherent sequence. For example, check out the <code>ProducSize</code> levels:</p>
<div id="cell-92" class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb67" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ProductSize'</span>].unique()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="40">
<pre><code>array([nan, 'Medium', 'Small', 'Large / Medium', 'Mini', 'Large', 'Compact'], dtype=object)</code></pre>
</div>
</div>
<p>Instruct Pandas about the relevant order of these levels:</p>
<div id="cell-94" class="cell" data-execution_count="41">
<div class="sourceCode cell-code" id="cb69" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1">sizes <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Large'</span>,<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Large / Medium'</span>,<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Medium'</span>,<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Small'</span>,<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Mini'</span>,<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Compact'</span></span>
<span id="cb69-2"></span>
<span id="cb69-3">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ProductSize'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ProductSize'</span>].astype(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'category'</span>)</span>
<span id="cb69-4">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ProductSize'</span>].cat.set_categories(sizes, ordered<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="41">
<pre><code>0            NaN
1         Medium
2            NaN
3          Small
4            NaN
           ...  
412693      Mini
412694      Mini
412695      Mini
412696      Mini
412697      Mini
Name: ProductSize, Length: 412698, dtype: category
Categories (6, object): ['Large' &lt; 'Large / Medium' &lt; 'Medium' &lt; 'Small' &lt; 'Mini' &lt; 'Compact']</code></pre>
</div>
</div>
<p>In this dataset, Kaggle suggests using Root Mean Square Log Error (RMSLE) as the metric for comparing actual versus predicted auction prices.</p>
<div id="cell-96" class="cell" data-execution_count="42">
<div class="sourceCode cell-code" id="cb71" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1">dep_var <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'SalePrice'</span></span>
<span id="cb71-2">df[dep_var] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.log(df[dep_var])</span></code></pre></div>
</div>
<p>This transformation ensures that the target variables is in format suitable for modeling.</p>
</section>
<section id="data-preparation" class="level4">
<h4 class="anchored" data-anchor-id="data-preparation">Data Preparation</h4>
<p>The first piece of data preparation we need to to do is enrich our representation of dates. The fundamental basis of the decision tree that we just discussed is bisection (dividing a group into two). We look at the ordinal variables and divide the dataset based on whether the variable’s value is greater (ow lower) than a threshold, and we look at the categorical variables and divide the dataset based on whether the variable’s level is a particular level. This algorithm divides the dataset based on both original and categorical data</p>
<p>But how does this apply to a common data type, the date? You might want to tree at date as an ordinal value because it is meaningful to say that one date is greater than other. However, dates are a bit different from most ordinal values in that some dates are qualitatively different from others, which is often relevant to the systems we are modeling.</p>
<p>To help our algorithm handle dates intelligently, we’d like our model to know ore than whether a date is more recent or less recent than other. We might want our model to make decisions based on that date’s day of the week, on whether a day is holiday, on what month it is in, and so forth. To accomplish this, we replace every date column with a set of date metadata columns, such as holiday, day of the week, and month. These columns provide categorical data that we suspect will be useful.</p>
<p>Fastai comes with a function to do this for us that mean we only need to pass in a column name that contains dates:</p>
<div id="cell-100" class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb72" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> add_datepart(df, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'saledate'</span>)</span>
<span id="cb72-2"></span>
<span id="cb72-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># do the same for the test set</span></span>
<span id="cb72-4">df_test <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.read_csv(path<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Test.csv'</span>, low_memory<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb72-5">df_test <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> add_datepart(df_test, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'saledate'</span>)</span></code></pre></div>
</div>
<p>We can see that there are now many new columns in our <code>DataFrame</code>:</p>
<div id="cell-102" class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb73" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">' '</span>.join(o <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> o <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> df.columns <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> o.startswith(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sale'</span>))</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="44">
<pre><code>'saleYear saleMonth saleWeek saleDay saleDayofweek saleDayofyear saleIs_month_end saleIs_month_start saleIs_quarter_end saleIs_quarter_start saleIs_year_end saleIs_year_start saleElapsed'</code></pre>
</div>
</div>
<p>This a solid first step, but we need further data cleaning. For this, we will use fastai objects called <code>TabularPandas</code> and <code>TabularProc</code>.</p>
<p>Another aspect of preparatory processing is ensuring we can handle strings and missing data. We will use fastai’s class <code>TabularPandas</code>, which wraps a Pandas <code>DataFrame</code> and offers some conveniences. when we say it “wraps” a <code>DataFrame</code>, it means taking a Pandas <code>DataFrame</code> as input and adding additional specifically useful for machine-learning tasks with tabular data. To populate a <code>TabularPandas</code>, we will utilize two <code>TabularProcs</code>: <code>Categorify</code> and <code>FillMissing</code>.</p>
<p><code>TabularProcs</code> are unique data transformation process used in fastai designed to prepare you data to ML models. We introduce two specific <code>TabularProcs</code> here:</p>
<ul>
<li><code>Categorify</code>: convert categorical columns text or non numeric data into numeric categories. For instance, a column <code>Color</code> with values like “Red”, “Blue”, “Green” could be encoded as 1, 2, 3.</li>
<li><code>FillMissing</code>: Manages missing data in your dataset. it replaces missing values with the column’s median value and creates a new boolean column to flag rows that originally had missing values.</li>
</ul>
<p>How <code>TabularProc</code> differs from regular transforms:</p>
<ul>
<li>Returns the exact same object that’s passed to it, after modifying the object in place, which optimizes memory efficiency especially with large datasets.</li>
<li>Executes the transformation immediately when the data is first passed in rather than delaying until the data is accessed.</li>
</ul>
<p>In practical terms, when using <code>TabularPandas</code> with <code>TabularProcs</code>:</p>
<ol type="1">
<li>Start with your raw data in a Pandas <code>DataFrame</code>.</li>
<li>Wrap this <code>DataFrame</code> with <code>TabularPandas</code>.</li>
<li>Apply <code>TabularProcs</code> (<code>Categorify</code> and <code>FillMissing</code>)</li>
<li>These procs instantly process all your data, converting categories to numbers and filling in missing values.</li>
<li>The outcome is a dataset ready for machine learning models, with all categorical data converted and missing values addressed.</li>
</ol>
<p>This methodology streamlines the data preparation process, ensure consistent data processing ready for model training or inference.</p>
<div id="cell-104" class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb75" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1">procs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [Categorify, FillMissing]</span></code></pre></div>
</div>
<p><code>TabularPandas</code> will also manage the dataset split into training and validation sets for us.</p>
</section>
<section id="handling-a-time-series" class="level4">
<h4 class="anchored" data-anchor-id="handling-a-time-series">Handling a Time Series</h4>
<p>When dealing with time series data, randomly selecting a subset of data points for training and validation is not sufficient, as sequence of data is vital. The test set represents a future six-month period starting from May 2012, thus not overlapping with the training set. This setup is intentional because the competition sponsor aims to evaluate the model’s predictive capability selected from a later time than your training dataset.</p>
<p>The provided Kaggle training data concludes in April 2012. Therefore, we’ll construct to focused training dataset comprising data from before November 2011 and establish a validation set with data from after November 2011.</p>
<p>This is achieved using <code>np.where</code>, which helps in obtaining indices for specific conditions:</p>
<div id="cell-108" class="cell" data-execution_count="46">
<div class="sourceCode cell-code" id="cb76" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1">cond <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (df.saleYear<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2011</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> (df.saleMonth<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)</span>
<span id="cb76-2">train_idx <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.where( cond)[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb76-3">valid_idx <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.where(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span>cond)[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb76-4"></span>
<span id="cb76-5">splits <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>(train_idx),<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>(valid_idx))</span></code></pre></div>
</div>
<p>TabularPandas requires knowledge of which columns are continuous and which are categorical. We can simplify this with the <code>cont_cat_split</code> helper function:</p>
<div id="cell-110" class="cell" data-execution_count="47">
<div class="sourceCode cell-code" id="cb77" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1">cont,cat <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> cont_cat_split(df, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, dep_var<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dep_var)</span>
<span id="cb77-2">to <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> TabularPandas(df, procs, cat, cont, y_names<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dep_var, splits<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>splits)</span></code></pre></div>
</div>
<p>This setup turns <code>TabularPandas</code>into something akin to a fastai <code>Dataset</code> object, with accessible train and valid attributes:</p>
<div id="cell-112" class="cell" data-execution_count="48">
<div class="sourceCode cell-code" id="cb78" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(to.train),<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(to.valid)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="48">
<pre><code>(404710, 7988)</code></pre>
</div>
</div>
<p>It’s possible to view the dataset’s categorical variables still represented as strings:</p>
<div id="cell-114" class="cell">
<div class="sourceCode cell-code" id="cb80" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1">to.show(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">UsageBand</th>
<th data-quarto-table-cell-role="th">fiModelDesc</th>
<th data-quarto-table-cell-role="th">fiBaseModel</th>
<th data-quarto-table-cell-role="th">fiSecondaryDesc</th>
<th data-quarto-table-cell-role="th">fiModelSeries</th>
<th data-quarto-table-cell-role="th">fiModelDescriptor</th>
<th data-quarto-table-cell-role="th">ProductSize</th>
<th data-quarto-table-cell-role="th">fiProductClassDesc</th>
<th data-quarto-table-cell-role="th">state</th>
<th data-quarto-table-cell-role="th">ProductGroup</th>
<th data-quarto-table-cell-role="th">ProductGroupDesc</th>
<th data-quarto-table-cell-role="th">Drive_System</th>
<th data-quarto-table-cell-role="th">Enclosure</th>
<th data-quarto-table-cell-role="th">Forks</th>
<th data-quarto-table-cell-role="th">Pad_Type</th>
<th data-quarto-table-cell-role="th">Ride_Control</th>
<th data-quarto-table-cell-role="th">Stick</th>
<th data-quarto-table-cell-role="th">Transmission</th>
<th data-quarto-table-cell-role="th">Turbocharged</th>
<th data-quarto-table-cell-role="th">Blade_Extension</th>
<th data-quarto-table-cell-role="th">Blade_Width</th>
<th data-quarto-table-cell-role="th">Enclosure_Type</th>
<th data-quarto-table-cell-role="th">Engine_Horsepower</th>
<th data-quarto-table-cell-role="th">Hydraulics</th>
<th data-quarto-table-cell-role="th">Pushblock</th>
<th data-quarto-table-cell-role="th">Ripper</th>
<th data-quarto-table-cell-role="th">Scarifier</th>
<th data-quarto-table-cell-role="th">Tip_Control</th>
<th data-quarto-table-cell-role="th">Tire_Size</th>
<th data-quarto-table-cell-role="th">Coupler</th>
<th data-quarto-table-cell-role="th">Coupler_System</th>
<th data-quarto-table-cell-role="th">Grouser_Tracks</th>
<th data-quarto-table-cell-role="th">Hydraulics_Flow</th>
<th data-quarto-table-cell-role="th">Track_Type</th>
<th data-quarto-table-cell-role="th">Undercarriage_Pad_Width</th>
<th data-quarto-table-cell-role="th">Stick_Length</th>
<th data-quarto-table-cell-role="th">Thumb</th>
<th data-quarto-table-cell-role="th">Pattern_Changer</th>
<th data-quarto-table-cell-role="th">Grouser_Type</th>
<th data-quarto-table-cell-role="th">Backhoe_Mounting</th>
<th data-quarto-table-cell-role="th">Blade_Type</th>
<th data-quarto-table-cell-role="th">Travel_Controls</th>
<th data-quarto-table-cell-role="th">Differential_Type</th>
<th data-quarto-table-cell-role="th">Steering_Controls</th>
<th data-quarto-table-cell-role="th">saleIs_month_end</th>
<th data-quarto-table-cell-role="th">saleIs_month_start</th>
<th data-quarto-table-cell-role="th">saleIs_quarter_end</th>
<th data-quarto-table-cell-role="th">saleIs_quarter_start</th>
<th data-quarto-table-cell-role="th">saleIs_year_end</th>
<th data-quarto-table-cell-role="th">saleIs_year_start</th>
<th data-quarto-table-cell-role="th">auctioneerID_na</th>
<th data-quarto-table-cell-role="th">MachineHoursCurrentMeter_na</th>
<th data-quarto-table-cell-role="th">SalesID</th>
<th data-quarto-table-cell-role="th">MachineID</th>
<th data-quarto-table-cell-role="th">ModelID</th>
<th data-quarto-table-cell-role="th">datasource</th>
<th data-quarto-table-cell-role="th">auctioneerID</th>
<th data-quarto-table-cell-role="th">YearMade</th>
<th data-quarto-table-cell-role="th">MachineHoursCurrentMeter</th>
<th data-quarto-table-cell-role="th">saleYear</th>
<th data-quarto-table-cell-role="th">saleMonth</th>
<th data-quarto-table-cell-role="th">saleWeek</th>
<th data-quarto-table-cell-role="th">saleDay</th>
<th data-quarto-table-cell-role="th">saleDayofweek</th>
<th data-quarto-table-cell-role="th">saleDayofyear</th>
<th data-quarto-table-cell-role="th">saleElapsed</th>
<th data-quarto-table-cell-role="th">SalePrice</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>Low</td>
<td>521D</td>
<td>521</td>
<td>D</td>
<td>#na#</td>
<td>#na#</td>
<td>#na#</td>
<td>Wheel Loader - 110.0 to 120.0 Horsepower</td>
<td>Alabama</td>
<td>WL</td>
<td>Wheel Loader</td>
<td>#na#</td>
<td>EROPS w AC</td>
<td>None or Unspecified</td>
<td>#na#</td>
<td>None or Unspecified</td>
<td>#na#</td>
<td>#na#</td>
<td>#na#</td>
<td>#na#</td>
<td>#na#</td>
<td>#na#</td>
<td>#na#</td>
<td>2 Valve</td>
<td>#na#</td>
<td>#na#</td>
<td>#na#</td>
<td>#na#</td>
<td>None or Unspecified</td>
<td>None or Unspecified</td>
<td>#na#</td>
<td>#na#</td>
<td>#na#</td>
<td>#na#</td>
<td>#na#</td>
<td>#na#</td>
<td>#na#</td>
<td>#na#</td>
<td>#na#</td>
<td>#na#</td>
<td>#na#</td>
<td>#na#</td>
<td>Standard</td>
<td>Conventional</td>
<td>False</td>
<td>False</td>
<td>False</td>
<td>False</td>
<td>False</td>
<td>False</td>
<td>False</td>
<td>False</td>
<td>1139246</td>
<td>999089</td>
<td>3157</td>
<td>121</td>
<td>3.0</td>
<td>2004</td>
<td>68.0</td>
<td>2006</td>
<td>11</td>
<td>46</td>
<td>16</td>
<td>3</td>
<td>320</td>
<td>1.163635e+09</td>
<td>11.097410</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>Low</td>
<td>950FII</td>
<td>950</td>
<td>F</td>
<td>II</td>
<td>#na#</td>
<td>Medium</td>
<td>Wheel Loader - 150.0 to 175.0 Horsepower</td>
<td>North Carolina</td>
<td>WL</td>
<td>Wheel Loader</td>
<td>#na#</td>
<td>EROPS w AC</td>
<td>None or Unspecified</td>
<td>#na#</td>
<td>None or Unspecified</td>
<td>#na#</td>
<td>#na#</td>
<td>#na#</td>
<td>#na#</td>
<td>#na#</td>
<td>#na#</td>
<td>#na#</td>
<td>2 Valve</td>
<td>#na#</td>
<td>#na#</td>
<td>#na#</td>
<td>#na#</td>
<td>23.5</td>
<td>None or Unspecified</td>
<td>#na#</td>
<td>#na#</td>
<td>#na#</td>
<td>#na#</td>
<td>#na#</td>
<td>#na#</td>
<td>#na#</td>
<td>#na#</td>
<td>#na#</td>
<td>#na#</td>
<td>#na#</td>
<td>#na#</td>
<td>Standard</td>
<td>Conventional</td>
<td>False</td>
<td>False</td>
<td>False</td>
<td>False</td>
<td>False</td>
<td>False</td>
<td>False</td>
<td>False</td>
<td>1139248</td>
<td>117657</td>
<td>77</td>
<td>121</td>
<td>3.0</td>
<td>1996</td>
<td>4640.0</td>
<td>2004</td>
<td>3</td>
<td>13</td>
<td>26</td>
<td>4</td>
<td>86</td>
<td>1.080259e+09</td>
<td>10.950807</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>High</td>
<td>226</td>
<td>226</td>
<td>#na#</td>
<td>#na#</td>
<td>#na#</td>
<td>#na#</td>
<td>Skid Steer Loader - 1351.0 to 1601.0 Lb Operating Capacity</td>
<td>New York</td>
<td>SSL</td>
<td>Skid Steer Loaders</td>
<td>#na#</td>
<td>OROPS</td>
<td>None or Unspecified</td>
<td>#na#</td>
<td>#na#</td>
<td>#na#</td>
<td>#na#</td>
<td>#na#</td>
<td>#na#</td>
<td>#na#</td>
<td>#na#</td>
<td>#na#</td>
<td>Auxiliary</td>
<td>#na#</td>
<td>#na#</td>
<td>#na#</td>
<td>#na#</td>
<td>#na#</td>
<td>None or Unspecified</td>
<td>None or Unspecified</td>
<td>None or Unspecified</td>
<td>Standard</td>
<td>#na#</td>
<td>#na#</td>
<td>#na#</td>
<td>#na#</td>
<td>#na#</td>
<td>#na#</td>
<td>#na#</td>
<td>#na#</td>
<td>#na#</td>
<td>#na#</td>
<td>#na#</td>
<td>False</td>
<td>False</td>
<td>False</td>
<td>False</td>
<td>False</td>
<td>False</td>
<td>False</td>
<td>False</td>
<td>1139249</td>
<td>434808</td>
<td>7009</td>
<td>121</td>
<td>3.0</td>
<td>2001</td>
<td>2838.0</td>
<td>2004</td>
<td>2</td>
<td>9</td>
<td>26</td>
<td>3</td>
<td>57</td>
<td>1.077754e+09</td>
<td>9.210340</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>However, all underlying data has been converted to numeric form:</p>
<div id="cell-116" class="cell" data-execution_count="50">
<div class="sourceCode cell-code" id="cb81" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1">to.items.head(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="50">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">SalesID</th>
<th data-quarto-table-cell-role="th">SalePrice</th>
<th data-quarto-table-cell-role="th">MachineID</th>
<th data-quarto-table-cell-role="th">ModelID</th>
<th data-quarto-table-cell-role="th">...</th>
<th data-quarto-table-cell-role="th">saleIs_year_start</th>
<th data-quarto-table-cell-role="th">saleElapsed</th>
<th data-quarto-table-cell-role="th">auctioneerID_na</th>
<th data-quarto-table-cell-role="th">MachineHoursCurrentMeter_na</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>1139246</td>
<td>11.097410</td>
<td>999089</td>
<td>3157</td>
<td>...</td>
<td>1</td>
<td>1.163635e+09</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>1139248</td>
<td>10.950807</td>
<td>117657</td>
<td>77</td>
<td>...</td>
<td>1</td>
<td>1.080259e+09</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>1139249</td>
<td>9.210340</td>
<td>434808</td>
<td>7009</td>
<td>...</td>
<td>1</td>
<td>1.077754e+09</td>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>

<p>3 rows × 67 columns</p>
</div>
</div>
</div>
<p>Categorical columns undergo transformation by substituting each unique category with a number. These numbers are assigned consecutively as they first appear, implying no intrinsic value to these numbers, unless ordered categories (like <code>ProductSize</code>) pre-specify the sequence. You can check the mapping through the classes attribute:</p>
<div id="cell-118" class="cell" data-execution_count="51">
<div class="sourceCode cell-code" id="cb82" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1">to.classes[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ProductSize'</span>]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="51">
<pre><code>['#na#', 'Compact', 'Large', 'Large / Medium', 'Medium', 'Mini', 'Small']</code></pre>
</div>
</div>
<p>A neat feature in fastai is the ability to save processed data, which can be time-consuming. Saving the data allows you to resume further work without repeating the preprocessing steps. Fastai utilizes Python’s pickle system for this purpose:</p>
<div id="cell-120" class="cell" data-execution_count="52">
<div class="sourceCode cell-code" id="cb84" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb84-1">save_pickle(path<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'to.pkl'</span>,to)</span></code></pre></div>
</div>
<p>to retrieve it later you’ll simply do:</p>
<div id="cell-122" class="cell" data-execution_count="53">
<div class="sourceCode cell-code" id="cb85" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1">to <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> load_pickle(path<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'to.pkl'</span>)</span></code></pre></div>
</div>
<p>With preprocessing complete, we’re set to create a decision tree.</p>
</section>
</section>
<section id="decision-tree-ensembles" class="level3">
<h3 class="anchored" data-anchor-id="decision-tree-ensembles">Decision Tree Ensembles</h3>
<p>Let’s consider how we find the right questions to ask when creating decision trees. Fortunately we don’t have to do this manually computer are designed for this purpose! Here’s a simple overview of training a decision tree:</p>
<ol type="1">
<li>Loop through each column of the dataset in turn.</li>
<li>For each column, loop through each possible level of that column in turn.</li>
<li>Try splitting the data into two groups, based on whether they are greater than or less than that value (or if it is a categorical variable, based on whether they are equal to or not equal to that level of that categorical variable).</li>
<li>Find the average sale price for each of those two groups, and see how close that is to the actual sale price of each of the items of equipment in that group. That is, treat this as a very simple “model” where our predictions are simply the average sale price of the item’s group.</li>
<li>After looping through all of the columns and all the possible levels for each, pick the split point that gave the best predictions using that simple model.</li>
<li>We now have two different groups for our data, based on this selected split. Treat each of these as separate datasets, and find the best split for each by going back to step 1 for each group.</li>
<li>Continue this process recursively, until you have reached some stopping criterion for each group—for instance, stop splitting a group further when it has only 20 items in it.</li>
</ol>
<p>To implement this, start by defining your independent and dependent variables:</p>
<div id="cell-126" class="cell" data-execution_count="54">
<div class="sourceCode cell-code" id="cb86" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb86-1">xs,y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> to.train.xs,to.train.y</span>
<span id="cb86-2">valid_xs,valid_y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> to.valid.xs,to.valid.y</span></code></pre></div>
</div>
<div id="cell-127" class="cell">
<div class="sourceCode cell-code" id="cb87" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1">xs.head()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="55">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">UsageBand</th>
<th data-quarto-table-cell-role="th">fiModelDesc</th>
<th data-quarto-table-cell-role="th">fiBaseModel</th>
<th data-quarto-table-cell-role="th">fiSecondaryDesc</th>
<th data-quarto-table-cell-role="th">...</th>
<th data-quarto-table-cell-role="th">saleDay</th>
<th data-quarto-table-cell-role="th">saleDayofweek</th>
<th data-quarto-table-cell-role="th">saleDayofyear</th>
<th data-quarto-table-cell-role="th">saleElapsed</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>2</td>
<td>963</td>
<td>298</td>
<td>43</td>
<td>...</td>
<td>16</td>
<td>3</td>
<td>320</td>
<td>1.163635e+09</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>2</td>
<td>1745</td>
<td>529</td>
<td>57</td>
<td>...</td>
<td>26</td>
<td>4</td>
<td>86</td>
<td>1.080259e+09</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>1</td>
<td>336</td>
<td>111</td>
<td>0</td>
<td>...</td>
<td>26</td>
<td>3</td>
<td>57</td>
<td>1.077754e+09</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>1</td>
<td>3716</td>
<td>1381</td>
<td>0</td>
<td>...</td>
<td>19</td>
<td>3</td>
<td>139</td>
<td>1.305763e+09</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>3</td>
<td>4261</td>
<td>1538</td>
<td>0</td>
<td>...</td>
<td>23</td>
<td>3</td>
<td>204</td>
<td>1.248307e+09</td>
</tr>
</tbody>
</table>

<p>5 rows × 66 columns</p>
</div>
</div>
</div>
<p>Once your data is numeric and lacks missing values, you can create a decision tree:</p>
<div id="cell-129" class="cell" data-execution_count="56">
<div class="sourceCode cell-code" id="cb88" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1">m <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> DecisionTreeRegressor(max_leaf_nodes<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>)</span>
<span id="cb88-2">m.fit(xs, y)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
</div>
<p>Here, we’ve instructed sklearn to create four leaf nodes. To visualize what the model has learned, we can display the tree:</p>
<div id="cell-131" class="cell" data-execution_count="57">
<div class="sourceCode cell-code" id="cb89" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb89-1">draw_tree(m, xs, size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, leaves_parallel<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, precision<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="57">
<div>
<figure class="figure">
<p><img src="https://bhdai.github.io/blog/posts/2024-11-12-random-forest/index_files/figure-html/cell-58-output-1.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Understanding this visualization helps in graphing decision tree:</p>
<ol type="1">
<li><p>Top node: Represents the entire dataset before any splits. Average sale price (log) is 10.10, with a mean squared error of 0.48.</p></li>
<li><p>First split: Based on coupler_system.</p></li>
</ol>
<ul>
<li>Left branch: coupler_system &lt; 0.5 (360,847 records, avg. 10.21)</li>
<li>Right branch: coupler_system &gt; 0.5 (43,863 records, avg. 9.21)</li>
</ul>
<ol start="3" type="1">
<li>Second split (on left branch): Based on <code>YearMade</code>.</li>
</ol>
<ul>
<li>Left sub-branch: <code>YearMade</code> &lt;= 1991.5 (155,724 records, avg. 9.97)</li>
<li>Right sub-branch: <code>YearMade</code> &gt; 1991.5 (205,123 records, avg. 10.4)</li>
</ul>
<ol start="4" type="1">
<li>Leaf nodes: The bottom row, where no more splits occur.</li>
</ol>
<p>We can display this information using Terence Parr’s dtreeviz library to enhance visualization:</p>
<div id="cell-133" class="cell" data-execution_count="58">
<div class="sourceCode cell-code" id="cb90" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1">samp_idx <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.permutation(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(y))[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">500</span>]</span>
<span id="cb90-2">dtreeviz(m, xs.iloc[samp_idx], y.iloc[samp_idx], xs.columns, dep_var,</span>
<span id="cb90-3">        fontname<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'DejaVu Sans'</span>, scale<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.6</span>, label_fontsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>,</span>
<span id="cb90-4">        orientation<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'LR'</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="58">
<div>
<figure class="figure">
<p><img src="https://bhdai.github.io/blog/posts/2024-11-12-random-forest/index_files/figure-html/cell-59-output-1.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>This visualization illuminates data distribution, showcasing issues like bulldozers dated to the year 1000, likely placeholders for missing data. For modeling precision, these can be substituted with 1950 to improve visualization clarity without significantly influencing model results:</p>
<div id="cell-135" class="cell" data-execution_count="59">
<div class="sourceCode cell-code" id="cb91" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb91-1">xs.loc[xs[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'YearMade'</span>]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1900</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'YearMade'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1950</span></span>
<span id="cb91-2">valid_xs.loc[valid_xs[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'YearMade'</span>]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1900</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'YearMade'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1950</span></span></code></pre></div>
</div>
<p>This update clarifies the tree visualization while maintaining the models integrity. After making this change, re-evaluate the decision tree:</p>
<div id="cell-137" class="cell" data-execution_count="60">
<div class="sourceCode cell-code" id="cb92" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb92-1">m <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> DecisionTreeRegressor(max_leaf_nodes<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>).fit(xs, y)</span>
<span id="cb92-2"></span>
<span id="cb92-3">dtreeviz(m, xs.iloc[samp_idx], y.iloc[samp_idx], xs.columns, dep_var,</span>
<span id="cb92-4">        fontname<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'DejaVu Sans'</span>, scale<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.6</span>, label_fontsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>,</span>
<span id="cb92-5">        orientation<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'LR'</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="60">
<div>
<figure class="figure">
<p><img src="https://bhdai.github.io/blog/posts/2024-11-12-random-forest/index_files/figure-html/cell-61-output-1.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Now, let’s leverage the decision tree algorithm to generate a more complex model. This time, we’ll refrain from specifying any stopping criteria, such as <code>max_leaf_nodes</code>:</p>
<div id="cell-139" class="cell" data-execution_count="61">
<div class="sourceCode cell-code" id="cb93" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb93-1">m <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> DecisionTreeRegressor()</span>
<span id="cb93-2">m.fit(xs, y)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
</div>
<p>To evaluate our model’s performance, we’ll define a function to compute the root mean squared error(RMSE) which was the scoring criterion in this competition:</p>
<div id="cell-141" class="cell" data-execution_count="62">
<div class="sourceCode cell-code" id="cb94" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb94-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> r_mse(pred,y): <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">round</span>(math.sqrt(((pred<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>y)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>).mean()), <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>)</span>
<span id="cb94-2"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> m_rmse(m, xs, y): <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> r_mse(m.predict(xs), y)</span>
<span id="cb94-3">m_rmse(m, xs, y)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="62">
<pre><code>0.0</code></pre>
</div>
</div>
<p>The output is 0.0. At the first glance, it appears that our model is flawless. But hold on, we need to evalueate the validation set to check for overfitting:</p>
<div id="cell-143" class="cell" data-execution_count="63">
<div class="sourceCode cell-code" id="cb96" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb96-1">m_rmse(m, valid_xs, valid_y)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="63">
<pre><code>0.332239</code></pre>
</div>
</div>
<p>The validation set RMSE is 0.332239, indicating potential overfitting. Let’s investigating further</p>
<div id="cell-145" class="cell">
<div class="sourceCode cell-code" id="cb98" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb98-1">m.get_n_leaves(), <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(xs)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="64">
<pre><code>(324338, 404710)</code></pre>
</div>
</div>
<p>It turns out our model has nearly as many leaves as data point! This occurs because sklearn’s default setting allow continual splitting until there’s just one item per leaf node. We can address this by adjusting the stopping rule to require each leaf node to have at least 25 auction records:</p>
<div id="cell-147" class="cell" data-execution_count="65">
<div class="sourceCode cell-code" id="cb100" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb100-1">m <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> DecisionTreeRegressor(min_samples_leaf<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">25</span>)</span>
<span id="cb100-2">m.fit(to.train.xs, to.train.y)</span>
<span id="cb100-3">m_rmse(m, xs, y), m_rmse(m, valid_xs, valid_y)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="65">
<pre><code>(0.243049, 0.308857)</code></pre>
</div>
</div>
<p>This results in a more balanced model. Let’s verify the new number of leaves:</p>
<div id="cell-149" class="cell" data-execution_count="66">
<div class="sourceCode cell-code" id="cb102" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb102-1">m.get_n_leaves()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="66">
<pre><code>12432</code></pre>
</div>
</div>
<p>Decision trees are adept at modelling data due to their adaptability to nonlinear relationships and variable interactions. Nonetheless, a compromise exist between generallizability (achieved with smaller trees) and training accuracy (achieved with larger trees)</p>
<p>How do wee balance these strengths? We’ll explore further after covering essential aspect handling categorical variables.</p>
<p>In deep learning, categorical variables are often one-hot encoded and fed into embedding layers. However, decision trees lack embedding layers so how can we leverage untreated categorical variables efficiently? let’s consider a use-case with product codes.</p>
<p>Suppose we have an auction dataset with product codes (categorical variables) and sale prices. “Product X” for instance, consistently sells at a premium. Decision trees split data based on features optimally partition the target variable. A split distinguishing “Product X” from others creates:</p>
<ul>
<li>Group A: containing product X</li>
<li>Group B: containing all other products</li>
</ul>
<p>This chose arises because “Product X” is notably pricier, leading Group A to have a higher average price than Group B. This split provides valuable insights for price prediction, prompting the algorithm to prefer it. The decision tree isolates “Product X” quickly, allowing precise price predictions while evaluating other products’ prices.</p>
<p>One-hot encoding is another option; it transforms a single categorical column into multiple binary columns, each representing a category level. Pandas offers the <code>get_dummies</code> method which does just that.</p>
<p>However, there’s little evidence that one-hot encoding enhances results. Thus, we tend to avoid it when unnecessary, as it complicates data handling.</p>
</section>
<section id="creating-a-random-forest" class="level3">
<h3 class="anchored" data-anchor-id="creating-a-random-forest">Creating a Random Forest</h3>
<p>Creating a random forest involves a process similar to crafting a decision tree, but with added flexibility through parameters that determine the number of trees, data point subset size(rows), and field subset size(columns):</p>
<div id="cell-153" class="cell" data-execution_count="67">
<div class="sourceCode cell-code" id="cb104" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb104-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> rf(xs, y, n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">40</span>, max_samples<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">200_000</span>,</span>
<span id="cb104-2">       max_features<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>, min_samples_leaf<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span>kwargs):</span>
<span id="cb104-3">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> RandomForestRegressor(n_jobs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>n_estimators,</span>
<span id="cb104-4">        max_samples<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>max_samples, max_features<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>max_features,</span>
<span id="cb104-5">        min_samples_leaf<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>min_samples_leaf, oob_score<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>).fit(xs, y)</span></code></pre></div>
</div>
<p>Here’s an explanation of the parameters used in the function:</p>
<ul>
<li><code>n_estimators</code>: specifies the number of tree in the forest.</li>
<li><code>max_samples</code>: indicates how many rows to sample when training each tree.</li>
<li><code>max_features</code>: sets the number of columns to sample at each split (e.g., 0.5 means using half of the columns).</li>
<li><code>min_samples_leaf</code>: determines the minimum number of samples required in the leaf node, controlling the tree depth.</li>
</ul>
<p>Additionally, <code>n_jobs=-1</code> ensures that all available CPUs are utilized for parallel tree building. This function allows quick experimentation with different configurations.</p>
<p>Initiating the random forest model is straightforward:</p>
<div id="cell-155" class="cell" data-execution_count="68">
<div class="sourceCode cell-code" id="cb105" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb105-1">m <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> rf(xs, y)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
</div>
<p>By using multiple trees rather than a single <code>DecisionTreeRegressor</code>, the validation RMSE significantly improves:</p>
<div id="cell-157" class="cell" data-execution_count="69">
<div class="sourceCode cell-code" id="cb106" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb106-1">m_rmse(m, xs, y), m_rmse(m, valid_xs, valid_y)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="69">
<pre><code>(0.171371, 0.233223)</code></pre>
</div>
</div>
<p>A distinctive feature of random forests is the resilience hyperparameter configurations, particularly <code>max_features</code>.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>When we say random forests show resilience to hyperparameter configurations, it means that the algorithm performs well across a range of different hyperparameter settings. It doesn’t require very precise tuning to achieve good results, making it a flexible option in many applications.</p>
</div>
</div>
<p>The N_estimators parameter can be set to as high as value as feasible, the more trees, the greater the accuracy potential</p>
<p>For visualizing effects of varying max_features with increasing tree counts, refer to sklearn’s documentation which provides insightful plots.</p>
<p><img src="https://scikit-learn.org/stable/_images/sphx_glr_plot_ensemble_oob_001.png"></p>
<p>The image demonstrates:</p>
<ul>
<li>Blue line: represents minimal features usage.</li>
<li>Green line: represents maximal feature usage (full feature set). Subsets of features combined with numerous trees usualy yield the lowest error.</li>
</ul>
<p>To explore the impact of <code>n_estimators</code> analyze predictions from each individual tree within the forest (accessible via the <code>estimators_</code> attribute):</p>
<div id="cell-159" class="cell">
<div class="sourceCode cell-code" id="cb108" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb108-1">preds <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.stack([t.predict(valid_xs) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> t <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> m.estimators_])</span></code></pre></div>
</div>
<div id="cell-160" class="cell">
<div class="sourceCode cell-code" id="cb109" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb109-1">r_mse(preds.mean(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>), valid_y)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="71">
<pre><code>0.233223</code></pre>
</div>
</div>
<p>This calculation, <code>preds.mean(0)</code>, parallels the overall random forest prediction. Observe RMSE progression as trees are added:</p>
<div id="cell-162" class="cell" data-execution_count="72">
<div class="sourceCode cell-code" id="cb111" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb111-1">plt.plot([r_mse(preds[:i<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>].mean(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>), valid_y) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">40</span>)])<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://bhdai.github.io/blog/posts/2024-11-12-random-forest/index_files/figure-html/cell-73-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Despite improved RMSE in training, the validation set’s performance may deteriorate due to potential overfitting or time discrepancies. This challenge is addressable by leveraging the out-of-bag (OOB) error methodology in random forests, offering valuable insights.</p>
<p>In the next section, we’ll delve deeper into creating a random forest and optimizing it’s performance.</p>
</section>
<section id="out-of-bag-error" class="level3">
<h3 class="anchored" data-anchor-id="out-of-bag-error">Out of Bag Error</h3>
<p>In a random forest, each tree is trained on different subset of data. Consequently, there’s a unique opportunity: each tree has an implicit validation set composed of the data rows not selected for its training, know as out-of-bag (OOB) data.</p>
<p>OOB error is particularly useful when dealing with a limited dataset, as it offers a measure of model generalization without needing to withhold data for a separate validation set. These OOB predictions are stored in the <code>oob_prediction_</code> attribute. Remember, these are compared with training labels, as the OOB calculation involves the training set:</p>
<div id="cell-166" class="cell" data-execution_count="73">
<div class="sourceCode cell-code" id="cb112" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb112-1">r_mse(m.oob_prediction_, y)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="73">
<pre><code>0.211234</code></pre>
</div>
</div>
<p>The OOB error frequently appears lower than the validation set error, hinting that other factors might contribute to the validation error, hinting that other factors might contribute to the validation error outside mere generalization discrepancies. We’ll delve into these causes soon.</p>
</section>
<section id="model-interpretation" class="level3">
<h3 class="anchored" data-anchor-id="model-interpretation">Model Interpretation</h3>
<p>Interpreting models trained on tabular data presents valuable insights. Higher understanding can be sought in ares like:</p>
<ul>
<li>How confident are we in our predictions using a particular row of data?</li>
<li>For predicting with a particular row of data, what were the most important factors, and how did they influence that prediction?</li>
<li>Which columns are the strongest predictors, which can we ignore?</li>
<li>Which columns are effectively redundant with each other, for purposes of prediction?</li>
<li>How do predictions vary, as we vary these columns?</li>
</ul>
<p>Random forests are adept at addressing these questions. Let’s start with evaluating confidence in predictions!</p>
<p>Model predictions are an average of individual tree predictions, providing an estimated value. But how can we gauge the confidence of this estimate? One simplistic approach is using the standard deviations of tree predictions - higher deviations imply less confidence, suggesting that caution is needed, especially in scenarios where tree predictions are inconsistent.</p>
<p>In creating the random forest, predictions over the validations set were obtained using Python’s list comprehension:</p>
<div id="cell-170" class="cell">
<div class="sourceCode cell-code" id="cb114" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb114-1">preds <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.stack([t.predict(valid_xs) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> t <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> m.estimators_])</span></code></pre></div>
</div>
<div id="cell-171" class="cell">
<div class="sourceCode cell-code" id="cb115" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb115-1">preds.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="75">
<pre><code>(40, 7988)</code></pre>
</div>
</div>
<p>This results in a prediction for each tree across all validation set auctions (40 trees, 7,988 auctions). With this data, compute the standard deviation of predictions for each auction:</p>
<div id="cell-173" class="cell" data-execution_count="76">
<div class="sourceCode cell-code" id="cb117" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb117-1">preds_std <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> preds.std(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb117-2">preds_std[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="76">
<pre><code>array([0.2000169 , 0.08355874, 0.113672  , 0.2747    , 0.12065141])</code></pre>
</div>
</div>
<p>The standard deviations highlight varying levels of confidence across auctions. A lower deviation signals stronger agreement among trees, leading to higher confidence. Conversely, higher deviations indicate disagreement, pointing towards lower confidence. In practical applications like auction bidding, this information is useful; you might reconsider bidding when predictions show low certainty.</p>
<section id="feature-importance" class="level4">
<h4 class="anchored" data-anchor-id="feature-importance">Feature Importance</h4>
<p>Knowing a model’s predictive accuracy is critical, but equally important is understanding how those predictions are made. Feature importance offers valuable insight into this process. Sklearn’s random forest model provides feature importance scores via the <code>feature_importance_</code> attributes. Here’s a simple function load these scores into a DataFrame and sort them</p>
<div id="cell-177" class="cell" data-execution_count="77">
<div class="sourceCode cell-code" id="cb119" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb119-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> rf_feat_importance(m, df):</span>
<span id="cb119-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> pd.DataFrame({<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cols'</span>:df.columns, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'imp'</span>:m.feature_importances_}</span>
<span id="cb119-3">                       ).sort_values(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'imp'</span>, ascending<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb119-4">fi <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> rf_feat_importance(m, xs)</span>
<span id="cb119-5">fi[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="77">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">cols</th>
<th data-quarto-table-cell-role="th">imp</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">57</td>
<td>YearMade</td>
<td>0.166375</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">30</td>
<td>Coupler_System</td>
<td>0.113599</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">6</td>
<td>ProductSize</td>
<td>0.103802</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">7</td>
<td>fiProductClassDesc</td>
<td>0.078686</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">3</td>
<td>fiSecondaryDesc</td>
<td>0.054542</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">54</td>
<td>ModelID</td>
<td>0.052919</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">65</td>
<td>saleElapsed</td>
<td>0.050521</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">31</td>
<td>Grouser_Tracks</td>
<td>0.041514</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">12</td>
<td>Enclosure</td>
<td>0.039451</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">32</td>
<td>Hydraulics_Flow</td>
<td>0.035355</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>Evaluating the features importances reveals that a few columns significantly contribute to the model’s predictions, most notably, <code>YearMade</code> and <code>ProductSize</code>.</p>
<p>To visualize these importance, plotting them can clarify their relative value:</p>
<div id="cell-179" class="cell" data-execution_count="78">
<div class="sourceCode cell-code" id="cb120" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb120-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> plot_fi(fi):</span>
<span id="cb120-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> fi.plot(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cols'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'imp'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'barh'</span>, figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>), legend<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb120-3"></span>
<span id="cb120-4">plot_fi(fi[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>])<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://bhdai.github.io/blog/posts/2024-11-12-random-forest/index_files/figure-html/cell-79-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="removing-low-importance-variables" class="level4">
<h4 class="anchored" data-anchor-id="removing-low-importance-variables">Removing Low-Importance Variables</h4>
<p>A subset of columns might suffice to maintain accuracy while enhancing simplicity by discarding low-importance variables. Let’s retain only those with an importance score above 0.005:</p>
<div id="cell-182" class="cell" data-execution_count="79">
<div class="sourceCode cell-code" id="cb121" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb121-1">to_keep <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> fi[fi.imp<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.005</span>].cols</span>
<span id="cb121-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(to_keep)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="79">
<pre><code>22</code></pre>
</div>
</div>
<p>Retrain the model using this refined feature set:</p>
<div id="cell-184" class="cell" data-execution_count="80">
<div class="sourceCode cell-code" id="cb123" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb123-1">xs_imp <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xs[to_keep]</span>
<span id="cb123-2">valid_xs_imp <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> valid_xs[to_keep]</span>
<span id="cb123-3"></span>
<span id="cb123-4">m <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> rf(xs_imp, y)</span>
<span id="cb123-5">m_rmse(m, xs_imp, y), m_rmse(m, valid_xs_imp, valid_y)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="80">
<pre><code>(0.180965, 0.231633)</code></pre>
</div>
</div>
<p>The models accuracy remain consistent, yet fewer columns necessitate examination:</p>
<div id="cell-186" class="cell" data-execution_count="81">
<div class="sourceCode cell-code" id="cb125" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb125-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(xs.columns), <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(xs_imp.columns)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="81">
<pre><code>(66, 22)</code></pre>
</div>
</div>
<p>Simplifying a model is often the initial step in enhancing it having 78 columns can be overwhelming for deep analysis. Particularly, a learner, more interpretable model is simpler to deploy and manage.</p>
<p>Revisiting the feature importance plot provides clearer insights:</p>
<div id="cell-188" class="cell" data-execution_count="82">
<div class="sourceCode cell-code" id="cb127" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb127-1">plot_fi(rf_feat_importance(m, xs_imp))<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://bhdai.github.io/blog/posts/2024-11-12-random-forest/index_files/figure-html/cell-83-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>While interpreting, redundancy may arise as seen with <code>ProductGroup</code> and <code>ProductGroupDesc</code>. Attempting to remove such redundant features can further streamline interpretation.</p>
</section>
<section id="removing-redundant-variables" class="level4">
<h4 class="anchored" data-anchor-id="removing-redundant-variables">Removing Redundant Variables</h4>
<p>We’ll begin by clustering columns to identify pairs that are closely aligned often suggesting redundancy:</p>
<div id="cell-192" class="cell" data-execution_count="83">
<div class="sourceCode cell-code" id="cb128" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb128-1">cluster_columns(xs_imp)</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://bhdai.github.io/blog/posts/2024-11-12-random-forest/index_files/figure-html/cell-84-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The chart generated from clustering will reveal which columns were merged early on. Notably, pairs like <code>ProductGroup</code> with <code>ProductGroupDesc</code>, <code>saleYear</code> with <code>saleElapsed</code>, and <code>fiModelDesc</code> with <code>fiBaseModel</code> are likely correlated to the point of redundancy.</p>
<p>Next, we will attempt to simplify the model by removing these related features. We begin by defining a function to quickly train a random forest and capture the out-of-bag(OOB) score. This score, ranging from 1.0 for perfection to near-zero, provides a relative comparison metric as we remove redundant columns:</p>
<div id="cell-194" class="cell" data-execution_count="84">
<div class="sourceCode cell-code" id="cb129" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb129-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> get_oob(df):</span>
<span id="cb129-2">    m <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> RandomForestRegressor(n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">40</span>, min_samples_leaf<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">15</span>,</span>
<span id="cb129-3">        max_samples<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50000</span>, max_features<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>, n_jobs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, oob_score<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb129-4">    m.fit(df, y)</span>
<span id="cb129-5">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> m.oob_score_</span></code></pre></div>
</div>
<p>First, we’ll confirm our baseline score with all columns:</p>
<div id="cell-196" class="cell" data-execution_count="85">
<div class="sourceCode cell-code" id="cb130" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb130-1">get_oob(xs_imp)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="85">
<pre><code>0.8760739540611289</code></pre>
</div>
</div>
<p>Next, test the impact of removing each potentially redundant variable individually:</p>
<div id="cell-198" class="cell" data-execution_count="86">
<div class="sourceCode cell-code" id="cb132" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb132-1">{c:get_oob(xs_imp.drop(c, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> c <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> (</span>
<span id="cb132-2">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'saleYear'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'saleElapsed'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ProductGroupDesc'</span>,<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ProductGroup'</span>,</span>
<span id="cb132-3">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'fiModelDesc'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'fiBaseModel'</span>,</span>
<span id="cb132-4">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Hydraulics_Flow'</span>,<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Grouser_Tracks'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Coupler_System'</span>)}</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="86">
<pre><code>{'saleYear': 0.8742959821922331,
 'saleElapsed': 0.8698149904307536,
 'ProductGroupDesc': 0.8755334280543031,
 'ProductGroup': 0.8745495772129529,
 'fiModelDesc': 0.8743458666758965,
 'fiBaseModel': 0.8748827464781819,
 'Hydraulics_Flow': 0.8762012623754625,
 'Grouser_Tracks': 0.8755826405754699,
 'Coupler_System': 0.8758570604637711}</code></pre>
</div>
</div>
<p>We’ll also explore the effect of dropping one columns from each identified pair:</p>
<div id="cell-200" class="cell" data-execution_count="87">
<div class="sourceCode cell-code" id="cb134" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb134-1">to_drop <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'saleYear'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ProductGroupDesc'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'fiBaseModel'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Grouser_Tracks'</span>]</span>
<span id="cb134-2">get_oob(xs_imp.drop(to_drop, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="87">
<pre><code>0.8743053306321846</code></pre>
</div>
</div>
<p>Encouragingly, the model’s performance remains largely unchanged. We will now finalize this reduce dataset:</p>
<div id="cell-202" class="cell" data-execution_count="88">
<div class="sourceCode cell-code" id="cb136" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb136-1">xs_final <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xs_imp.drop(to_drop, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb136-2">valid_xs_final <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> valid_xs_imp.drop(to_drop, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb136-3"></span>
<span id="cb136-4">save_pickle(path<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'xs_final.pkl'</span>, xs_final)</span>
<span id="cb136-5">save_pickle(path<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'valid_xs_final.pkl'</span>, valid_xs_final)</span></code></pre></div>
</div>
<p>For later retrieval, you can load these condensed datasets with:</p>
<div id="cell-204" class="cell" data-execution_count="89">
<div class="sourceCode cell-code" id="cb137" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb137-1">xs_final <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> load_pickle(path<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'xs_final.pkl'</span>)</span>
<span id="cb137-2">valid_xs_final <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> load_pickle(path<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'valid_xs_final.pkl'</span>)</span></code></pre></div>
</div>
<p>Let’s verify that the RMSE remains consistent after this reduction:</p>
<div id="cell-206" class="cell" data-execution_count="90">
<div class="sourceCode cell-code" id="cb138" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb138-1">m <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> rf(xs_final, y)</span>
<span id="cb138-2">m_rmse(m, xs_final, y), m_rmse(m, valid_xs_final, valid_y)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="90">
<pre><code>(0.182663, 0.231313)</code></pre>
</div>
</div>
<p>By concentrating on key variables and eliminating redundancies, we’ve streamlined our model significantly. Now, let’s further explore how these influential variables affect predictions using partial dependence plots.</p>
</section>
<section id="partial-dependence" class="level4">
<h4 class="anchored" data-anchor-id="partial-dependence">Partial Dependence</h4>
<p>Alright, let’s get a feel for these predictions. Imagine checking out the menu at a restaurant. Before ordering, you’d want to know what’s popular, right? We do the same thing with our data. For <code>ProductSize</code>, we count how many times each size appears using something like Pandas’ <code>value_counts</code> method and then plot this on a bar chart. Here’s our code in action:</p>
<div id="cell-210" class="cell" data-execution_count="91">
<div class="sourceCode cell-code" id="cb140" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb140-1">p <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> valid_xs_final[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ProductSize'</span>].value_counts(sort<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>).plot.barh()</span>
<span id="cb140-2">c <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> to.classes[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ProductSize'</span>]</span>
<span id="cb140-3">plt.yticks(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(c)), c)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://bhdai.github.io/blog/posts/2024-11-12-random-forest/index_files/figure-html/cell-92-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Turns out, the biggest “dish” on our menu is labeled <code>Compact</code> but look at #na#, fastai’s way of showing missing values. No big surprise there!</p>
<p>What about YearMade? This time, instead of a bar chart, we whip out a histogram.</p>
<div id="cell-212" class="cell" data-execution_count="92">
<div class="sourceCode cell-code" id="cb141" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb141-1">ax <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> valid_xs_final[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'YearMade'</span>].hist()</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://bhdai.github.io/blog/posts/2024-11-12-random-forest/index_files/figure-html/cell-93-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Apart from 1950, which we used as placeholder for unknown years, most machines were crafted post-1990. Vintage anyone?</p>
<p>Partial dependence plots help us see what would happen to the sale price if one feature changed while everything else stayed the same.</p>
<p>For YearMade, we can’t just average sale prices by year because many things change over time. Instead, we replace every year value with a single year, like 1950, and calculate the average predicted sale price. We repeat this for each year, up to 2011, to see how YearMade alone affects price.</p>
<p>Then, we plot the results:</p>
<div id="cell-214" class="cell" data-execution_count="93">
<div class="sourceCode cell-code" id="cb142" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb142-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.inspection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> PartialDependenceDisplay</span>
<span id="cb142-2">fig,ax <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplots(figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>))</span>
<span id="cb142-3">PartialDependenceDisplay.from_estimator(m, valid_xs_final, [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'YearMade'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ProductSize'</span>],</span>
<span id="cb142-4">                                        grid_resolution<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span>, ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ax)</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://bhdai.github.io/blog/posts/2024-11-12-random-forest/index_files/figure-html/cell-94-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>For YearMade, after 1990, there’s a clear pattern: prices rise as the year increase. This make sense because older items depreciate.</p>
<p>The plot for ProductSize show that the group with missing values has the lowest prices. Understanding why these values are missing is crucial, as sometimes they can be good predictors, or they could indicate an issue like data leakage</p>
</section>
<section id="data-leakage" class="level4">
<h4 class="anchored" data-anchor-id="data-leakage">Data Leakage</h4>
<p>In the world of data mining, there’s a tricky issue known as data leakage, described in detail by Shachar Kaufman, Saharon Rosset, and Claudia Perlich in their paper, <a href="https://dl.acm.org/doi/10.1145/2020408.2020496">Leakage in Data Mining: Formulation, Detection, and Avoidance.</a> They define it as the unintentional introduction of information about the target of a data mining problem that shouldn’t be available to mine from. To put it simply, it’s like saying ‘it rains on rainy days,’ where the model mistakenly uses the target itself as an input.</p>
<p>Data leakage can be subtle, appearing in various forms, and one such form is through missing values. Here are the straightforward steps to spot data leakage:</p>
<ul>
<li>Assess whether your model’s accuracy seems too perfect. If it feels too good to be true, leakage might be playing a part.</li>
<li>Evaluate the significant predictors. If they don’t add up in a practical sense, then something might be off.</li>
<li>Analyze the partial dependence plots. If they yield nonsensical results, you could be facing a leakage issue.</li>
</ul>
<p>Additionally, tools like tree interpreters can aid in understanding which factors are influencing specific predictions.</p>
<p>Avoiding data leakage demands meticulous attention through all phases of data handling—from collection to preparation. The key is adopting a “learn-now, predict-later” approach, ensuring that models are built without any preview of the answers.</p>
</section>
<section id="tree-interpreter" class="level4">
<h4 class="anchored" data-anchor-id="tree-interpreter">Tree Interpreter</h4>
<p>Before we go in please make sure you’re already have <code>treeinterpreter</code> and <code>waterfallcharts</code> installed if not run this in your terminal</p>
<div class="sourceCode" id="cb143" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb143-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">pip</span> install treeinterpreter</span>
<span id="cb143-2"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">pip</span> install waterfallcharts</span></code></pre></div>
<p>At the start of this section, we said that we wanted to be able to answer five questions:</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>How confident are we in our predictions using particular row of data?</li>
<li>For predicting with a particular row of data, what were the most important factors, and how did they influence that predictions?</li>
<li>Which columns are the strongest predictors, which can we ignore?</li>
<li>Which columns are effectively redundant with each other, for purpose of prediction?</li>
<li>How do predictions vary, as we vary these columns?</li>
</ul>
</div>
</div>
<p>We’ve addressed four of these, leaving only the second question. To tackle this, we’ll use the <code>treeinterpreter</code> library, along with the <code>waterfallcharts</code> library for visualization.</p>
<div id="cell-220" class="cell" data-execution_count="94">
<div class="sourceCode cell-code" id="cb144" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb144-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> treeinterpreter <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> treeinterpreter</span>
<span id="cb144-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> waterfall_chart <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> plot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> waterfall</span></code></pre></div>
</div>
<p>While we’ve computed feature importances across entire random forest, we can apply a similar concept to a single row of data. This approach examines the contribution of each variable to improving the model at each branch of every tree, then sums these contributions per variables for a specific data point.</p>
<p>For example, if we’re analyzing a particular auction item predicted to be expensive, we can understand why by examining that single row of data. We’ll process it through each decision tree, observing the split used at each point and calculating the increase or decrease in addition compared to the parent node. This process is repeated for every tree, summing up the total change in importance by split variable.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>For example, if you’re predicting house prices:</p>
<ul>
<li>The bias might be the average house price in your dataset.</li>
<li>A positive contribution from the “number of bedrooms” feature would indicate that having more bedrooms increased the predicted price.</li>
<li>A negative contribution from the “distance from city center” feature might indicate that being further from the city center decreased the predicted price.</li>
</ul>
</div>
</div>
<p>Let’s select the first few rows of our validation set:</p>
<div id="cell-222" class="cell" data-execution_count="95">
<div class="sourceCode cell-code" id="cb145" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb145-1">row <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> valid_xs_final.iloc[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>]</span></code></pre></div>
</div>
<p>We can then use <code>treeinterpreter</code>:</p>
<div id="cell-224" class="cell" data-execution_count="96">
<div class="sourceCode cell-code" id="cb146" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb146-1">prediction,bias,contributions <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> treeinterpreter.predict(m, row.values)</span></code></pre></div>
</div>
<p>Here, <code>prediction</code> is the random forest’s prediction, <code>bias</code> is the prediction based on the mean of the dependent variable, and <code>contributions</code> shows how each feature (independent variable) in your input data contributed to moving the prediction away from the bias. The sum of <code>contributions</code> plus <code>bias</code> equals the <code>prediction</code> for each row</p>
<div id="cell-226" class="cell" data-execution_count="97">
<div class="sourceCode cell-code" id="cb147" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb147-1">prediction[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], bias[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], contributions[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="97">
<pre><code>(array([10.06313964]), 10.104746057831763, -0.04160642242374439)</code></pre>
</div>
</div>
<p>To visualize the contributions clearly, we can use waterfall plot:</p>
<div id="cell-228" class="cell" data-execution_count="98">
<div class="sourceCode cell-code" id="cb149" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb149-1">waterfall(valid_xs_final.columns, contributions[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], threshold<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.08</span>,</span>
<span id="cb149-2">          rotation_value<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">45</span>,formatting<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{:,.3f}</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://bhdai.github.io/blog/posts/2024-11-12-random-forest/index_files/figure-html/cell-99-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>This plot demonstrates how positive and negative contributes from all independent variables sum up to create the final prediction, show in the rightmost column labeled <code>net</code>.</p>
<p>This type of information is particularly valuable in production environments, rather than during model development. It can provide users of your data product with insightful information about the underlying reasoning behind the predictions.</p>
<p>Having explored these classic machine learning techniques, we’re now ready to see how deep learning can contribute to solving this problem</p>
</section>
</section>
</section>
<section id="extrapolation-and-neuron-networks" class="level2">
<h2 class="anchored" data-anchor-id="extrapolation-and-neuron-networks">Extrapolation and Neuron Networks</h2>
<p>Random forests, like all machine learning or deep learning algorithms, don’t always generalize well to new data. Lets explore this issue, particularly focusing on the extrapolation problem that random forests face.</p>
<section id="the-extrapolation-problem" class="level3">
<h3 class="anchored" data-anchor-id="the-extrapolation-problem">The Extrapolation Problem</h3>
<p>Consider a simple task: making prediction from 40 data points showing a slightly noisy linear relationship. We’ll create this data and visualize it:</p>
<div id="cell-234" class="cell" data-execution_count="99">
<div class="sourceCode cell-code" id="cb150" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb150-1">np.random.seed(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">42</span>)</span>
<span id="cb150-2">x_lin <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.linspace(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span>, steps<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">40</span>)</span>
<span id="cb150-3">y_lin <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> x_lin <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> torch.randn_like(x_lin)</span>
<span id="cb150-4">plt.scatter(x_lin, y_lin)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://bhdai.github.io/blog/posts/2024-11-12-random-forest/index_files/figure-html/cell-100-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>We need to reshape our data for sklearn, which expect a matrix of independent variables:</p>
<div id="cell-236" class="cell" data-execution_count="100">
<div class="sourceCode cell-code" id="cb151" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb151-1">xs_lin <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> x_lin.unsqueeze(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb151-2">x_lin.shape,xs_lin.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="100">
<pre><code>(torch.Size([40]), torch.Size([40, 1]))</code></pre>
</div>
</div>
<div id="cell-237" class="cell" data-execution_count="101">
<div class="sourceCode cell-code" id="cb153" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb153-1">x_lin[:,<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>].shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="101">
<pre><code>torch.Size([40, 1])</code></pre>
</div>
</div>
<p>Now, let’s create a random forest using the first 30 rows for training:</p>
<div id="cell-239" class="cell" data-execution_count="102">
<div class="sourceCode cell-code" id="cb155" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb155-1">m_lin <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> RandomForestRegressor().fit(xs_lin[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>],y_lin[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>])</span></code></pre></div>
</div>
<p>We’ll test the model on the full dataset and visualize the results:</p>
<div id="cell-241" class="cell" data-execution_count="103">
<div class="sourceCode cell-code" id="cb156" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb156-1">plt.scatter(x_lin, y_lin, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span>)</span>
<span id="cb156-2">plt.scatter(x_lin, m_lin.predict(xs_lin), color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'red'</span>, alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://bhdai.github.io/blog/posts/2024-11-12-random-forest/index_files/figure-html/cell-104-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Here’s where we encounter a significant issue: our predictions outside the training data domain are consistently too low. This happens because a random forest average value of the rows in a leaf. Consequently, a random forest can’t predict values outside the rage of its training data.</p>
<p>This limitation is particularly problematic for data with time-based trends, like inflation, where future predictions are needed. Your predictions will systematically be too low.</p>
<p>The problem isn’t limited to time variables, though. Random forest struggle to extrapolate beyond the types of data they’ve seen in a more general sense. That’s wy it’s crucial to ensure our validation set doesn’t contain out-of-domain data</p>
</section>
<section id="finding-out-of-domain-data" class="level3">
<h3 class="anchored" data-anchor-id="finding-out-of-domain-data">Finding Out-of-Domain Data</h3>
<p>Identifying whether your test set is distributed differently from your training data can be challenging. Interestingly, we can use a random forest to help us with this task. Here’s how:</p>
<p>Instead of predicting our actual dependent variable, we’ll try to predict whether a row belongs to the validation set or the training set. Let’s combine our training and validation sets, create a new dependent variable representing the dataset origin, and build a random forest:</p>
<div id="cell-245" class="cell" data-execution_count="104">
<div class="sourceCode cell-code" id="cb157" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb157-1">df_dom <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.concat([xs_final, valid_xs_final])</span>
<span id="cb157-2">is_valid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(xs_final) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(valid_xs_final))</span>
<span id="cb157-3"></span>
<span id="cb157-4">m <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> rf(df_dom, is_valid)</span>
<span id="cb157-5">rf_feat_importance(m, df_dom)[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="104">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">cols</th>
<th data-quarto-table-cell-role="th">imp</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">6</td>
<td>saleElapsed</td>
<td>0.910266</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">11</td>
<td>SalesID</td>
<td>0.073707</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">14</td>
<td>MachineID</td>
<td>0.012246</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">0</td>
<td>YearMade</td>
<td>0.000813</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">9</td>
<td>fiModelDesc</td>
<td>0.000535</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">5</td>
<td>ModelID</td>
<td>0.000471</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>This reveals three columns that differ significantly between the sets: <code>saleElapsed</code>, <code>SalesID</code> and <code>MachineID</code>. <code>saleElapsed</code> directly encoded the date, while <code>SalesID</code> and <code>MachineID</code> likely represent incrementing identifiers over time.</p>
<p>Let’s compare the RMSE of our original model with versions that exclude these columns:</p>
<div id="cell-247" class="cell" data-execution_count="105">
<div class="sourceCode cell-code" id="cb158" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb158-1">m <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> rf(xs_final, y)</span>
<span id="cb158-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'orig'</span>, m_rmse(m, valid_xs_final, valid_y))</span>
<span id="cb158-3"></span>
<span id="cb158-4"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> c <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> (<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'SalesID'</span>,<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'saleElapsed'</span>,<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'MachineID'</span>):</span>
<span id="cb158-5">    m <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> rf(xs_final.drop(c,axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>), y)</span>
<span id="cb158-6">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(c, m_rmse(m, valid_xs_final.drop(c,axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>), valid_y))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>orig 0.231001
SalesID 0.230214
saleElapsed 0.235865
MachineID 0.231447</code></pre>
</div>
</div>
<p>It appears that we can remove SalesID and MachineID without losing accuracy:</p>
<div id="cell-249" class="cell" data-execution_count="106">
<div class="sourceCode cell-code" id="cb160" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb160-1">time_vars <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'SalesID'</span>,<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'MachineID'</span>]</span>
<span id="cb160-2">xs_final_time <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xs_final.drop(time_vars, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb160-3">valid_xs_time <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> valid_xs_final.drop(time_vars, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb160-4"></span>
<span id="cb160-5">m <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> rf(xs_final_time, y)</span>
<span id="cb160-6">m_rmse(m, valid_xs_time, valid_y)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="106">
<pre><code>0.228264</code></pre>
</div>
</div>
<p>Removing these variables slightly improves the model’s accuracy and should make it more resilient over time, easier to maintain, and understand.</p>
<p>Sometimes, using only recent data can help. Let’s try using data from the most recent years:</p>
<div id="cell-251" class="cell" data-execution_count="107">
<div class="sourceCode cell-code" id="cb162" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb162-1">xs[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'saleYear'</span>].hist()<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://bhdai.github.io/blog/posts/2024-11-12-random-forest/index_files/figure-html/cell-108-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-252" class="cell" data-execution_count="108">
<div class="sourceCode cell-code" id="cb163" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb163-1">filt <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xs[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'saleYear'</span>]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2004</span></span>
<span id="cb163-2">xs_filt <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xs_final_time[filt]</span>
<span id="cb163-3">y_filt <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> y[filt]</span>
<span id="cb163-4"></span>
<span id="cb163-5"></span>
<span id="cb163-6">m <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> rf(xs_filt, y_filt)</span>
<span id="cb163-7">m_rmse(m, xs_filt, y_filt), m_rmse(m, valid_xs_time, valid_y)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="108">
<pre><code>(0.176448, 0.228537)</code></pre>
</div>
</div>
<p>This yields a slightly improvement, demonstrating that using your entire dataset isn’t always the best approach; sometimes subset can perform better.</p>
<p>I recommend building a model with <code>is_valid</code> as the dependent variable for all datasets. This can uncover subtle domain shift issues that might otherwise go unnoticed.</p>
<p>Next, we’ll explore whether using a neural network can further improve our results</p>
</section>
<section id="using-neural-networks" class="level3">
<h3 class="anchored" data-anchor-id="using-neural-networks">Using Neural Networks</h3>
<p>To build a neural network model, we’ll follow a similar approach to our random forest setup. First, let’s replicate the steps for creating the TabularPandas object:</p>
<div id="cell-256" class="cell">
<div class="sourceCode cell-code" id="cb165" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb165-1">df_nn <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.read_csv(path<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'TrainAndValid.csv'</span>, low_memory<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb165-2">df_nn[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ProductSize'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df_nn[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ProductSize'</span>].astype(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'category'</span>)</span>
<span id="cb165-3">df_nn[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ProductSize'</span>].cat.set_categories(sizes, ordered<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb165-4">df_nn[dep_var] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.log(df_nn[dep_var])</span>
<span id="cb165-5">df_nn <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> add_datepart(df_nn, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'saledate'</span>)</span></code></pre></div>
</div>
<p>We can utilize the column selection from our random forest model for the neural network:</p>
<div id="cell-258" class="cell" data-execution_count="110">
<div class="sourceCode cell-code" id="cb166" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb166-1">df_nn_final <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df_nn[<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>(xs_final_time.columns) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> [dep_var]]</span></code></pre></div>
</div>
<p>Neural networks handle categorical columns differently than decision trees. Embedding are an effective method for categorical variables in neural nets. Fastai determines which columns should be treated as categorical by comparing the number of distinct levels to the <code>max_card</code> parameter. We’ll use 9,000 as our <code>max_card</code> to avoid unnecessarily large embeddings:</p>
<div id="cell-260" class="cell" data-execution_count="111">
<div class="sourceCode cell-code" id="cb167" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb167-1">cont_nn,cat_nn <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> cont_cat_split(df_nn_final, max_card<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">9000</span>, dep_var<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dep_var)</span></code></pre></div>
</div>
<p>It’s crucial to ensure that <code>saleElapsed</code> isn’t treated as a categorical variable as we need to predict auction sale prices in the feature. Let’s verify the continuous variable</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>As a continuous variable, <code>saleElapsed</code> can capture trends over time. If it were treated as a categorical variable, you’d lose the ability to interpolate or extrapolate between known values, which is crucial for prediction.</p>
<p>When you’re predicting auction sale prices for future dates, you’ll be dealing with ‘saleElapsed’ values that weren’t in your training data. If ‘saleElapsed’ were categorical, your model wouldn’t know how to handle these new values.</p>
</div>
</div>
<div id="cell-262" class="cell" data-execution_count="112">
<div class="sourceCode cell-code" id="cb168" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb168-1">cont_nn</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="112">
<pre><code>['saleElapsed']</code></pre>
</div>
</div>
<p>Now, let’s examine the cardinality of our chosen categorical variables:</p>
<div id="cell-264" class="cell" data-execution_count="113">
<div class="sourceCode cell-code" id="cb170" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb170-1">df_nn_final[cat_nn].nunique()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="113">
<pre><code>YearMade                73
Coupler_System           2
ProductSize              6
fiProductClassDesc      74
fiSecondaryDesc        177
ModelID               5281
Enclosure                6
Hydraulics_Flow          3
fiModelDesc           5059
fiModelDescriptor      140
Hydraulics              12
ProductGroup             6
Drive_System             4
Tire_Size               17
Track_Type               2
dtype: int64</code></pre>
</div>
</div>
<p>We notice two “model” variables with similar high cardinalities, suggesting potential redundancy. To reduce the embedding matrix size. Let’s assess the impact of removing one of these model columns on our random forest:</p>
<div id="cell-266" class="cell" data-execution_count="114">
<div class="sourceCode cell-code" id="cb172" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb172-1">xs_filt2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xs_filt.drop(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'fiModelDescriptor'</span>, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb172-2">valid_xs_time2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> valid_xs_time.drop(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'fiModelDescriptor'</span>, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb172-3">m2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> rf(xs_filt2, y_filt)</span>
<span id="cb172-4">m_rmse(m2, xs_filt2, y_filt), m_rmse(m2, valid_xs_time2, valid_y)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="114">
<pre><code>(0.178386, 0.229505)</code></pre>
</div>
</div>
<p>given the minimal impact, We’ll remove <code>fiModelDescriptor</code> from our neural network predictors:</p>
<div id="cell-268" class="cell" data-execution_count="115">
<div class="sourceCode cell-code" id="cb174" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb174-1">cat_nn.remove(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'fiModelDescriptor'</span>)</span></code></pre></div>
</div>
<p>When creating our <code>TabularPandas</code> object for the neural network, we need to add normalization, which is crucial for neural networks but unnecessary for random forests:</p>
<div id="cell-270" class="cell" data-execution_count="116">
<div class="sourceCode cell-code" id="cb175" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb175-1">procs_nn <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [Categorify, FillMissing, Normalize]</span>
<span id="cb175-2">to_nn <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> TabularPandas(df_nn_final, procs_nn, cat_nn, cont_nn,</span>
<span id="cb175-3">                      splits<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>splits, y_names<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dep_var)</span></code></pre></div>
</div>
<p>Since tabular models and data generally don’t require much GPU RAM, we can use larger batch sizes:</p>
<div id="cell-272" class="cell" data-execution_count="117">
<div class="sourceCode cell-code" id="cb176" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb176-1">dls <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> to_nn.dataloaders(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1024</span>)</span></code></pre></div>
</div>
<p>For regression models, it’s advisable to set y_range. Let’s find the min and max of our dependent variable:</p>
<div id="cell-274" class="cell" data-execution_count="118">
<div class="sourceCode cell-code" id="cb177" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb177-1">y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> to_nn.train.y</span>
<span id="cb177-2">y.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">min</span>(),y.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="118">
<pre><code>(8.465899467468262, 11.863582611083984)</code></pre>
</div>
</div>
<p>Now we can create the <code>Learner</code> for our tabular model. We’ll use MSE as the loss function and increase the default layer sizes to 500 and 250 for our large dataset:</p>
<div id="cell-276" class="cell" data-execution_count="119">
<div class="sourceCode cell-code" id="cb179" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb179-1">learn <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tabular_learner(dls, y_range<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12</span>), layers<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">500</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">250</span>],</span>
<span id="cb179-2">                        n_out<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, loss_func<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>F.mse_loss)</span>
<span id="cb179-3">learn.lr_find()</span></code></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

</div>
<div class="cell-output cell-output-display" data-execution_count="119">
<pre><code>SuggestedLRs(valley=0.00013182566908653826)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://bhdai.github.io/blog/posts/2024-11-12-random-forest/index_files/figure-html/cell-120-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>We’ll train with <code>fit_one_cycle</code> for a few epochs:</p>
<div id="cell-278" class="cell" data-execution_count="120">
<div class="sourceCode cell-code" id="cb181" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb181-1">learn.fit_one_cycle(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-2</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train_loss</th>
<th data-quarto-table-cell-role="th">valid_loss</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0.061921</td>
<td>0.067224</td>
<td>00:05</td>
</tr>
<tr class="even">
<td>1</td>
<td>0.051130</td>
<td>0.056330</td>
<td>00:04</td>
</tr>
<tr class="odd">
<td>2</td>
<td>0.046388</td>
<td>0.054012</td>
<td>00:03</td>
</tr>
<tr class="even">
<td>3</td>
<td>0.041853</td>
<td>0.054157</td>
<td>00:03</td>
</tr>
<tr class="odd">
<td>4</td>
<td>0.040173</td>
<td>0.052207</td>
<td>00:03</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>Let’s compare the result to our earlier random forest using the <code>r_mse</code> function:</p>
<div id="cell-280" class="cell" data-execution_count="121">
<div class="sourceCode cell-code" id="cb182" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb182-1">preds,targs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> learn.get_preds()</span>
<span id="cb182-2">r_mse(preds,targs)</span></code></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

</div>
<div class="cell-output cell-output-display" data-execution_count="121">
<pre><code>0.228488</code></pre>
</div>
</div>
<p>The neural network performs better than the random forest, although it take longer to train and requires more careful hyprerparameter tuning</p>
<p>We’ll save our model for future use:</p>
<div id="cell-282" class="cell" data-execution_count="122">
<div class="sourceCode cell-code" id="cb184" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb184-1">learn.save(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'nn'</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="122">
<pre><code>Path('models/nn.pth')</code></pre>
</div>
</div>
<p>To further improve generalization, we can use ensemble learning, which evolves averaging predictions from several models.</p>
</section>
</section>
<section id="ensembling" class="level2">
<h2 class="anchored" data-anchor-id="ensembling">Ensembling</h2>
<p>The success of random forests is rooted in the principle that while individual trees have errors, these errors are not correlated. With enough trees, the average of these errors should approach zero. We can apply similar reasoning to combine predictions from different algorithms.</p>
<p>In our case, we have two distinct models: a random forest and a neural network. Their different approaches likely result in different types of errors. Therefore, averaging their predictions could potentially outperform either model individually.</p>
<p>It’s worth nothing that a random forest is itself an ensemble, By combining it with a neural network, we’re creating an ensemble of ensembles! While ensembling may not revolutionize your modeling process, it can provide a welcome boost to your exiting model.</p>
<p>One small challenge we face is the different output types from our Pytorch and sklearn models. Pytorch gives a rank-2 tensor (a column matrix), while sklearn produces a rank-1 array (a vector). We can address this using <code>squeeze</code> to remove unit axes and <code>to_np</code> to convert to Numpy array</p>
<div id="cell-286" class="cell" data-execution_count="123">
<div class="sourceCode cell-code" id="cb186" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb186-1">rf_preds <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> m.predict(valid_xs_time)</span>
<span id="cb186-2">ens_preds <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (to_np(preds.squeeze()) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> rf_preds) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span></span></code></pre></div>
</div>
<p>This ensemble approach yield better result than either model individually:</p>
<div id="cell-288" class="cell" data-execution_count="124">
<div class="sourceCode cell-code" id="cb187" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb187-1">r_mse(ens_preds,valid_y)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="124">
<pre><code>0.222895</code></pre>
</div>
</div>
</section>
<section id="boosting" class="level2">
<h2 class="anchored" data-anchor-id="boosting">Boosting</h2>
<p>While our previous ensembling approach used bagging (combination many models trained on different data subsets by averaging), another important technique is boosting, where models are added instead of averaged.</p>
<p>Boosting works as follow:</p>
<ol type="1">
<li>Train a small, underfitting model on you dataset.</li>
<li>Calculate this model predictions for the training set.</li>
<li>Subtract these predictions from the actual targets to get the “residuals”(the error for each training point).</li>
<li>Return to step 1, but use the residuals as the new training targets.</li>
<li>Repeat this process until reaching a stopping criterion(e.g., maximum number of trees or worsening validation set error).</li>
</ol>
<p>In this approach, each new tree attempts to fit the combined error of all previous trees. As we continually create new residuals by subtracting each new tree’s predictions from the previous residuals, these residuals progressively decrease.</p>
<p>To make predictions with a boosted tree ensemble, we calculate predictions from each tree and sum them. This approach has many variations and names, including Gradient Boosting Machines (GBMs) and Gradient Boosted Decision Trees (GBDTs). XGBoost is currently the most popular implementation.</p>
<p>Unlike random forests, boosting can lead to overfitting. In random forests, adding more trees doesn’t cause overfitting because each tree is independent. However, in a boosted ensemble, more trees continuously improve the training error, potentially leading to overfitting on the validation set.</p>
</section>
<section id="key-takeaway" class="level2">
<h2 class="anchored" data-anchor-id="key-takeaway">Key takeaway</h2>
<p>We have discussed two approaches to tabular modeling: decision tree ensembles and neural networks. We’ve also mentioned two different decision tree ensembles: random forests, and gradient boosting machines. Each is very effective, but each also has compromises:</p>
<ul>
<li><em>Random forests</em> are the easiest to train, because they are extremely resilient to hyperparameter choices and require very little preprocessing. They are very fast to train, and should not overfit if you have enough trees. But they can be a little less accurate, especially if extrapolation is required, such as predicting future time periods.</li>
<li><em>Gradient boosting</em> machines in theory are just as fast to train as random forests, but in practice you will have to try lots of different hyperparameters. They can overfit, but they are often a little more accurate than random forests.</li>
<li><em>Neural networks</em> take the longest time to train, and require extra preprocessing, such as normalization; this normalization needs to be used at inference time as well. They can provide great results and extrapolate well, but only if you are careful with your hyperparameters and take care to avoid overfitting.</li>
</ul>
<p>We suggest starting your analysis with a random forest. This will give you a strong baseline, and you can be confident that it’s a reasonable starting point. You can then use that model for feature selection and partial dependence analysis, to get a better understanding of your data.</p>
<p>From that foundation, you can try neural nets and GBMs, and if they give you significantly better results on your validation set in a reasonable amount of time, you can use them. If decision tree ensembles are working well for you, try adding the embeddings for the categorical variables to the data, and see if that helps your decision trees learn better.</p>
<p>Alright guys, it’s been a long post huh? Thanks for reading all of those, catch you on the flip side, and I’ll see you… next time!</p>


</section>

 ]]></description>
  <category>kaggle</category>
  <category>competition</category>
  <category>random forest</category>
  <category>bagging</category>
  <category>boosting</category>
  <guid>https://bhdai.github.io/blog/posts/2024-11-12-random-forest/</guid>
  <pubDate>Sat, 09 Nov 2024 17:00:00 GMT</pubDate>
  <media:content url="https://bhdai.github.io/blog/posts/2024-11-12-random-forest/random_forest.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Looking inside neural networks</title>
  <dc:creator>Bui Huu Dai</dc:creator>
  <link>https://bhdai.github.io/blog/posts/2024-07-28-from-neuron-to-gradient/</link>
  <description><![CDATA[ 






<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Hey there. It’s been a couple of week since my last post - blame exams and obsessive quest to tweak every configuration setting for my workflow (which is turned into a week-long habit hole - i regret nothing). But today, I’m excited to dive back into the world of AI and share my latest escapades from <a href="https://youtu.be/hBBOjCiFcuo?si=dfuUYTPelGOYCogb">Lesson 3</a> of the FastAI course taught by the indomitable Jeremy Horawd. Spoiler alert: it’s packed with enough neural wonders to make your brain do a happy dance.</p>
<p>In the coming post, I’ll guide you through:</p>
<ul>
<li>Picking of right AI model that’s just right for you</li>
<li>Dissecting the anatomy of these models (paramedics not required)</li>
<li>The inner workings of neuron networks</li>
<li>The <a href="https://www.kaggle.com/competitions/titanic">Titanic competition</a></li>
</ul>
<p>So, hold onto your neural nets and let’s jump right into it, shall we?</p>
</section>
<section id="section" class="level2">
<h2 class="anchored" data-anchor-id="section"></h2>
</section>
<section id="choosing-the-right-model" class="level2">
<h2 class="anchored" data-anchor-id="choosing-the-right-model">Choosing the Right Model</h2>
<p>We’ll explore how to choose an image model that’s efficient, reliable, and cost-effective—much like selecting the perfect gadget. I’ll walk you through a practical example comparing two popular image models by training a pet detector model.</p>
<p>Let’s start by setting up our environment.</p>
<div id="cell-4" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;execution&quot;,&quot;value&quot;:{&quot;iopub.execute_input&quot;:&quot;2024-07-27T12:50:55.115169Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-07-27T12:50:55.114829Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-07-27T12:51:04.408985Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-07-27T12:51:04.408185Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-07-27T12:50:55.115141Z&quot;}}" data-trusted="true" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> fastai.vision.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">all</span> <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span></span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> timm</span></code></pre></div>
</div>
<div id="cell-5" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;execution&quot;,&quot;value&quot;:{&quot;iopub.execute_input&quot;:&quot;2024-07-27T12:51:04.410784Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-07-27T12:51:04.410469Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-07-27T12:52:28.319857Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-07-27T12:52:28.318947Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-07-27T12:51:04.410759Z&quot;}}" data-trusted="true" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">path <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> untar_data(URLs.PETS)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'images'</span></span>
<span id="cb2-2"></span>
<span id="cb2-3"></span>
<span id="cb2-4">dls <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ImageDataLoaders.from_name_func(</span>
<span id="cb2-5">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"."</span>,</span>
<span id="cb2-6">    get_image_files(path),</span>
<span id="cb2-7">    valid_pct<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>,</span>
<span id="cb2-8">    seed<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">42</span>,</span>
<span id="cb2-9">    label_func<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>RegexLabeller(pat<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">r'</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">^</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">(</span><span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">[^/]</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">)</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">_</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">\d</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>),</span>
<span id="cb2-10">    item_tfms<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>Resize(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">224</span>)</span>
<span id="cb2-11">)</span></code></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

    <div>
      <progress value="811712512" class="" max="811706944" style="width:300px; height:20px; vertical-align: middle;"></progress>
      100.00% [811712512/811706944 01:13&lt;00:00]
    </div>
    
</div>
</div>
<p>Let’s break down what’s happening here. We’re using <a href="https://academictorrents.com/details/b18bbd9ba03d50b0f7f479acc9f4228a408cecc1">The Oxford-IIIT Pet dataset</a>, fetched with a nifty little URL constant provide by FastAI. If you’re staring at the pattern <code>pat=r'^([^/]+)\_\d+'</code> like it’s some alien script, fear not! It’s just a regular expression used to extract label from filenames using fastai <code>RegexLabeller</code></p>
<p>Here’s the cheat sheet for the pattern:</p>
<ul>
<li><code>^</code> asserts the start of a string.</li>
<li><code>([^/]+)</code> matches one or more characters that are not forward slash and captures them as a group.</li>
<li><code>_</code> matches an underscore.</li>
<li><code>\d+</code> matches one ore more digits.</li>
</ul>
<p>Now, let’s visualize our data:</p>
<div id="cell-7" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">dls.show_batch(max_n<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://bhdai.github.io/blog/posts/2024-07-28-from-neuron-to-gradient/index_files/figure-html/cell-4-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>And, it’s training time! We start with a ResNet34 architecture:</p>
<div id="cell-9" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">learn <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> vision_learner(dls, resnet34, metrics<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>error_rate)</span>
<span id="cb4-2">learn.fine_tune(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Downloading: "https://download.pytorch.org/models/resnet34-b627a593.pth" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth

100%|██████████| 83.3M/83.3M [00:00&lt;00:00, 147MB/s] </code></pre>
</div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train_loss</th>
<th data-quarto-table-cell-role="th">valid_loss</th>
<th data-quarto-table-cell-role="th">error_rate</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>1.491942</td>
<td>0.334319</td>
<td>0.105548</td>
<td>00:26</td>
</tr>
</tbody>
</table>
</div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train_loss</th>
<th data-quarto-table-cell-role="th">valid_loss</th>
<th data-quarto-table-cell-role="th">error_rate</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0.454661</td>
<td>0.367568</td>
<td>0.112991</td>
<td>00:32</td>
</tr>
<tr class="even">
<td>1</td>
<td>0.272869</td>
<td>0.274704</td>
<td>0.081867</td>
<td>00:33</td>
</tr>
<tr class="odd">
<td>2</td>
<td>0.144361</td>
<td>0.246424</td>
<td>0.073072</td>
<td>00:33</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>After about two minutes, we reached a 7% error rate—not too shabby! However, there’s one catch: while ResNet34 is dependable like a classic family car, it isn’t the fastest option out there. To really amp things up, we need to find a more advanced, high-performance model.</p>
<section id="exploring-the-model-landscape" class="level3">
<h3 class="anchored" data-anchor-id="exploring-the-model-landscape">Exploring the Model Landscape</h3>
<p>The PyTorch image model library offers a wide range of architectures—not quite a zillion, but enough to give you plenty of options. Many of these models are built on mathematical functions like ReLUs (Rectified Linear Units), which we’ll discuss in more detail later. Ultimately, choosing the right model comes down to three key factors:</p>
<ol type="1">
<li>Speed<br>
</li>
<li>Memory Usage<br>
</li>
<li>Accuracy</li>
</ol>
</section>
<section id="the-which-image-model-is-best-notebook" class="level3">
<h3 class="anchored" data-anchor-id="the-which-image-model-is-best-notebook">The “Which Image Model is Best?” Notebook</h3>
<p>I highly recommend taking a look at Jeremy Howard’s excellent notebook, <a href="https://www.kaggle.com/code/jhoward/which-image-models-are-best/">“Which image models are best?”</a>. It’s a valuable resource for finding the best architecture for your needs. If you find it helpful, do check it out and consider giving it an upvote—Jeremy’s insights are solid.</p>
<p>I’ve also included a copy of the plot below for quick reference. Enjoy exploring the model landscape!</p>
<div class="cell">
<div class="sourceCode cell-code hidden" id="cb6" data-startfrom="1" data-source-offset="-0" style="background: #f1f3f5;"><pre class="sourceCode js code-with-copy"><code class="sourceCode javascript"><span id="cb6-1">Plotly <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">require</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'https://cdn.plot.ly/plotly-latest.min.js'</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb6-2">df_results <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">FileAttachment</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"results-imagenet.csv"</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">csv</span>()</span>
<span id="cb6-3">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">FileAttachment</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"benchmark-infer-amp-nhwc-pt111-cu113-rtx3090.csv"</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">csv</span>()</span>
<span id="cb6-4"></span>
<span id="cb6-5">df_merged <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb6-6">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">let</span> df_results_processed <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df_results<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">map</span>(r <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> ({ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">...</span>r<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">model_org</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> r<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">model</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">model</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> r<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">model</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">split</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'.'</span>)[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>] }))<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb6-7"></span>
<span id="cb6-8">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">const</span> dfColumns <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">Object</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">keys</span>(df[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb6-9">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">const</span> dfResultsColumns <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">Object</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">keys</span>(df_results_processed[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb6-10"></span>
<span id="cb6-11">  <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> df<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">flatMap</span>(d <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> {</span>
<span id="cb6-12">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">const</span> matches <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df_results_processed<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">filter</span>(r <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> r<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">model</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">===</span> d<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">model</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb6-13">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> matches<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">map</span>(match <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> {</span>
<span id="cb6-14">      <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">let</span> mergedRow <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {}<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb6-15">      dfColumns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">forEach</span>(col <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> {</span>
<span id="cb6-16">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> (dfResultsColumns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">includes</span>(col) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&amp;&amp;</span> col <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'model'</span>) { mergedRow[<span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">`</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">${</span>col<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">_x`</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> d[col]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span> } <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span> { mergedRow[col] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> d[col]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span> }</span>
<span id="cb6-17">      })<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb6-18">      dfResultsColumns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">forEach</span>(col <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> {</span>
<span id="cb6-19">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> (dfColumns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">includes</span>(col) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&amp;&amp;</span> col <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'model'</span>) { mergedRow[<span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">`</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">${</span>col<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">_y`</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> match[col]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span> } <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span> { mergedRow[col] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> match[col]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span> }</span>
<span id="cb6-20">      })<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb6-21">      <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> mergedRow<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb6-22">    })<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb6-23">  })<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb6-24">}</span>
<span id="cb6-25"></span>
<span id="cb6-26">df_final <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df_merged</span>
<span id="cb6-27">  <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">map</span>(d <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> ({<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">...</span>d<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">secs</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">Number</span>(d<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">infer_samples_per_sec</span>)}))</span>
<span id="cb6-28">  <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">map</span>(d <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> {</span>
<span id="cb6-29">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">const</span> familyMatch <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> d<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">model</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">match</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">/</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">^([a-z]+?(?</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">:v2</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">)?)(?</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">:</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">\d|</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">_</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">$</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">)</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">/</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb6-30">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">let</span> family <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> familyMatch <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">?</span> familyMatch[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">''</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb6-31">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> (d<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">model</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">includes</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'in22'</span>)) family <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'_in22'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb6-32">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> (d<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">model</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">match</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">/resnet.</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">d/</span>)) family <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'d'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb6-33">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> {<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">...</span>d<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">family</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> family}<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb6-34">  })</span>
<span id="cb6-35">  <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">filter</span>(d <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!</span>d<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">model</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">endsWith</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'gn'</span>))</span>
<span id="cb6-36">  <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">filter</span>(d <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">/</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">^</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">re</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">[sg]</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">netd</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">?|</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">beit</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">convnext</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">levit</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">efficient</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">vit</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">vgg</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">swin/</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">test</span>(d<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">family</span>))<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb6-37">{</span>
<span id="cb6-38">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">const</span> uniqueFamilies <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">...</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">new</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">Set</span>(df_final<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">map</span>(d <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> d<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">family</span>))]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb6-39">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">const</span> colorScale <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> uniqueFamilies<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">map</span>((family<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> index) <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> { <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">`hsl(</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">${</span>index <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">360</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> uniqueFamilies<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">length</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">, 70%, 50%)`</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span> })<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb6-40">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">const</span> traces <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> uniqueFamilies<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">map</span>((family<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> index) <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> {</span>
<span id="cb6-41">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">const</span> familyData <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df_final<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">filter</span>(d <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> d<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">family</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">===</span> family)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb6-42">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> {</span>
<span id="cb6-43">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">name</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> family<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb6-44">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">x</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> familyData<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">map</span>(d <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> d<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">secs</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb6-45">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">y</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> familyData<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">map</span>(d <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">Number</span>(d<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">top1</span>))<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb6-46">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">mode</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'markers'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb6-47">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">type</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'scatter'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb6-48">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">marker</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> { <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">size</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> familyData<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">map</span>(d <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">Math</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">pow</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">Number</span>(d<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">infer_img_size</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5700</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">color</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> colorScale[index]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> }<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb6-49">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">text</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> familyData<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">map</span>(d <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> <span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">`</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">${</span>d<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">model</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">&lt;br&gt; family=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">${</span>d<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">family</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">&lt;br&gt; secs=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">${</span>d<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">secs</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">toFixed</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">&lt;br&gt; top1=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">${</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">Number</span>(d<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">top1</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">toFixed</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">&lt;br&gt; size=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">${</span>d<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">param_count_x</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">&lt;br&gt; infer_img_size=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">${</span>d<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">infer_img_size</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">`</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb6-50">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">hoverinfo</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'text'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb6-51">      <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">hoverlabel</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> { <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">bgcolor</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> colorScale[index] }</span>
<span id="cb6-52">    }<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb6-53">  })<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb6-54">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">const</span> layout <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb6-55">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">title</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Inference'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb6-56">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">width</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">795</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb6-57">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">height</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">750</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb6-58">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">autosize</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">true</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb6-59">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">xaxis</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> { <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">title</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'secs'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">type</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'log'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">autorange</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">true</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">gridcolor</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'rgb(233,233,233)'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> }<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb6-60">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">yaxis</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> { <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">title</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'top1'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">range</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">65</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">90</span>]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">gridcolor</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'rgb(233,233,233)'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> }<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb6-61">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">plot_bgcolor</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'rgb(240,240,255)'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb6-62">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">showlegend</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">true</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb6-63">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">legend</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> { <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">title</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> {<span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">text</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'family'</span>}<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">itemclick</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'toggle'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">itemdoubleclick</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'toggleothers'</span> }<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb6-64">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">hovermode</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'closest'</span></span>
<span id="cb6-65">  }<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb6-66">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">const</span> config <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {<span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">responsive</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">true</span>}<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb6-67">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">const</span> plot <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> DOM<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">element</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'div'</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb6-68">  Plotly<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">newPlot</span>(plot<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> traces<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> layout<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> config)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb6-69">  <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> plot<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb6-70">}</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-1" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-2" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-3" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-4" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-5" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-6" data-nodetype="expression">

</div>
</div>
</div>
</div>
<p>Here’s a breakdown of the plot from the notebook:</p>
<ul>
<li>The X-axis represents seconds per sample (the lower, the better performance).</li>
<li>The Y-axis reflects the accuracy (higher is preferable).</li>
</ul>
<p>In an ideal scenario, you would choose models that are located in the upper left corner of the plot. Although ResNet34 is a reliable choice—like a pair of trusty jeans—it’s no longer considered state-of-the-art. It’s time to explore the ConvNeXT models!</p>
<p>Before you get started, ensure that you have the timm package installed. You can install it using pip or conda:</p>
<div class="sourceCode" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb7-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">pip</span> install timm</span></code></pre></div>
<p>or</p>
<div class="sourceCode" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb8-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">conda</span> install timm</span></code></pre></div>
<p>After that, let’s search for all available ConvNeXT models.</p>
<div id="cell-13" class="cell">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1">timm.list_models(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"convnext*"</span>)</span></code></pre></div>
</div>
<div id="cell-14" class="cell output" data-execution_count="7">
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>['convnext_atto',
 'convnext_atto_ols',
 'convnext_base',
 'convnext_femto',
 'convnext_femto_ols',
 'convnext_large',
 'convnext_large_mlp',
 'convnext_nano',
 'convnext_nano_ols',
 'convnext_pico',
 'convnext_pico_ols',
 'convnext_small',
 'convnext_tiny',
 'convnext_tiny_hnf',
 'convnext_xlarge',
 'convnext_xxlarge',
 'convnextv2_atto',
 'convnextv2_base',
 'convnextv2_femto',
 'convnextv2_huge',
 'convnextv2_large',
 'convnextv2_nano',
 'convnextv2_pico',
 'convnextv2_small',
 'convnextv2_tiny']</code></pre>
</div>
</div>
<p>Found one? Awesome! Now, let’s put it to the test. We’ll specify the architecture as a string when we call <code>vision_learner</code>, Why previous time when we use ResNet34 we don’t need to pass it as string? you say! That’s because ResNet34 was built in fastai library so you just need to call it but with ConvNext you have to pass the arch as a string for it to work, alright let’s see what it look like:</p>
<div id="cell-16" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">arch <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'convnext_tiny.fb_in22k'</span></span>
<span id="cb11-2">learn <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> vision_learner(dls, arch, metrics<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>error_rate).to_fp16()</span>
<span id="cb11-3">learn.fine_tune(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"4f6fe7d85b6d4f86bd7cbc3a6de8e93c","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train_loss</th>
<th data-quarto-table-cell-role="th">valid_loss</th>
<th data-quarto-table-cell-role="th">error_rate</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>1.123377</td>
<td>0.240116</td>
<td>0.081191</td>
<td>00:27</td>
</tr>
</tbody>
</table>
</div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train_loss</th>
<th data-quarto-table-cell-role="th">valid_loss</th>
<th data-quarto-table-cell-role="th">error_rate</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0.260218</td>
<td>0.225793</td>
<td>0.071719</td>
<td>00:34</td>
</tr>
<tr class="even">
<td>1</td>
<td>0.199426</td>
<td>0.169573</td>
<td>0.059540</td>
<td>00:33</td>
</tr>
<tr class="odd">
<td>2</td>
<td>0.132157</td>
<td>0.166686</td>
<td>0.056834</td>
<td>00:33</td>
</tr>
</tbody>
</table>
</div>
</div>
</section>
<section id="results-are-in" class="level3">
<h3 class="anchored" data-anchor-id="results-are-in">Results Are In!</h3>
<p>The training time increased slightly—by about 3 to 4 seconds—but here’s the exciting part: the error rate dropped from 7.3% to 5.6%!</p>
<p>Now, those model names might seem a bit cryptic at first glance. Here’s a quick guide to help you decode them:</p>
<ul>
<li>Names like Tiny, Small, Large, etc.: These indicate the model’s size and resource requirements.</li>
<li>fb_in22k: This means the model was trained on the ImageNet dataset with 22,000 image categories by <a href="https://ai.meta.com/research/">Facebook AI Research (FAIR)</a>.</li>
</ul>
<p>In general, ConvNeXT models tend to outperform others in accuracy for standard photographs of natural objects. In summary, we’ve seen how choosing the right architecture can make a significant difference by balancing speed, memory usage, and accuracy. Stay tuned as we dive even deeper into the intricacies of neural networks next!</p>
</section>
</section>
<section id="whats-in-the-model" class="level2">
<h2 class="anchored" data-anchor-id="whats-in-the-model">What’s in the Model?</h2>
<p>Alright, you see? Our model did better, right? Now, you’ve probably wondering, how do we turn this awesome piece of neural magic into an actual application? They key is to save the trained model so that users won’t have to wait for the training time.</p>
<p>To do that, we export our learner with the following command, creating a magical file called <code>model.pkl</code>:</p>
<div id="cell-19" class="cell">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1">learn.export(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'model.pkl'</span>)</span></code></pre></div>
</div>
<p>For those of you who’ve followed my previous blog posts, you’ll recall that when I deploy an application on HuggingFace Spaces, I simply load the <code>model.pkl</code> file. This way, the learner functions almost identically to the trained <code>learn</code> object—and the best part is, you no longer have to wait forever!</p>
<p>Now, you might be wondering, “What exactly did we do here? What’s inside this <code>model.pkl</code> file?”</p>
<section id="dissecting-the-model.pkl-file" class="level3">
<h3 class="anchored" data-anchor-id="dissecting-the-model.pkl-file">Dissecting the <code>model.pkl</code> File</h3>
<p>Let’s take a closer look. The <code>model.pkl</code> file is essentially a saved learner, and it contains two main components:</p>
<ol type="1">
<li><strong>Pre-processing Steps</strong>: These include all the procedures needed to transform your raw images into a format that the model can understand. In other words, it stores the information from your <code>DataLoaders</code> (<code>dls</code>), DataBlock, or any other pre-processing pipeline you’ve set up.</li>
<li><strong>The Trained Model</strong>: This is the core component—a trained model that’s ready to make predictions.</li>
</ol>
<p>To inspect its contents, we can load the model back up and examine it.</p>
<div id="cell-21" class="cell">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1">m <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> learn.model</span>
<span id="cb13-2">m</span></code></pre></div>
</div>
<div id="cell-22" class="cell output" data-quarto-private-1="{&quot;key&quot;:&quot;execution&quot;,&quot;value&quot;:{&quot;iopub.execute_input&quot;:&quot;2024-07-27T13:00:24.084613Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-07-27T13:00:24.084212Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-07-27T13:00:24.096155Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-07-27T13:00:24.095135Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-07-27T13:00:24.084580Z&quot;}}" data-trusted="true" data-execution_count="4">
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>Sequential(
  (0): TimmBody(
    (model): ConvNeXt(
      (stem): Sequential(
        (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
        (1): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)
      )
      (stages): Sequential(
        (0): ConvNeXtStage(
          (downsample): Identity()
          (blocks): Sequential(
            (0): ConvNeXtBlock(
              (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
              (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=96, out_features=384, bias=True)
                (act): GELU()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=384, out_features=96, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (shortcut): Identity()
              (drop_path): Identity()
            )
            (1): ConvNeXtBlock(
              (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
              (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=96, out_features=384, bias=True)
                (act): GELU()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=384, out_features=96, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (shortcut): Identity()
              (drop_path): Identity()
            )
            (2): ConvNeXtBlock(
              (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
              (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=96, out_features=384, bias=True)
                (act): GELU()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=384, out_features=96, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (shortcut): Identity()
              (drop_path): Identity()
            )
          )
        )
        (1): ConvNeXtStage(
          (downsample): Sequential(
            (0): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)
            (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))
          )
          (blocks): Sequential(
            (0): ConvNeXtBlock(
              (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)
              (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=192, out_features=768, bias=True)
                (act): GELU()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=768, out_features=192, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (shortcut): Identity()
              (drop_path): Identity()
            )
            (1): ConvNeXtBlock(
              (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)
              (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=192, out_features=768, bias=True)
                (act): GELU()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=768, out_features=192, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (shortcut): Identity()
              (drop_path): Identity()
            )
            (2): ConvNeXtBlock(
              (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)
              (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=192, out_features=768, bias=True)
                (act): GELU()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=768, out_features=192, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (shortcut): Identity()
              (drop_path): Identity()
            )
          )
        )
        (2): ConvNeXtStage(
          (downsample): Sequential(
            (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)
            (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))
          )
          (blocks): Sequential(
            (0): ConvNeXtBlock(
              (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
              (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=384, out_features=1536, bias=True)
                (act): GELU()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=1536, out_features=384, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (shortcut): Identity()
              (drop_path): Identity()
            )
            (1): ConvNeXtBlock(
              (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
              (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=384, out_features=1536, bias=True)
                (act): GELU()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=1536, out_features=384, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (shortcut): Identity()
              (drop_path): Identity()
            )
            (2): ConvNeXtBlock(
              (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
              (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=384, out_features=1536, bias=True)
                (act): GELU()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=1536, out_features=384, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (shortcut): Identity()
              (drop_path): Identity()
            )
            (3): ConvNeXtBlock(
              (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
              (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=384, out_features=1536, bias=True)
                (act): GELU()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=1536, out_features=384, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (shortcut): Identity()
              (drop_path): Identity()
            )
            (4): ConvNeXtBlock(
              (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
              (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=384, out_features=1536, bias=True)
                (act): GELU()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=1536, out_features=384, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (shortcut): Identity()
              (drop_path): Identity()
            )
            (5): ConvNeXtBlock(
              (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
              (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=384, out_features=1536, bias=True)
                (act): GELU()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=1536, out_features=384, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (shortcut): Identity()
              (drop_path): Identity()
            )
            (6): ConvNeXtBlock(
              (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
              (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=384, out_features=1536, bias=True)
                (act): GELU()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=1536, out_features=384, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (shortcut): Identity()
              (drop_path): Identity()
            )
            (7): ConvNeXtBlock(
              (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
              (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=384, out_features=1536, bias=True)
                (act): GELU()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=1536, out_features=384, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (shortcut): Identity()
              (drop_path): Identity()
            )
            (8): ConvNeXtBlock(
              (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
              (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=384, out_features=1536, bias=True)
                (act): GELU()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=1536, out_features=384, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (shortcut): Identity()
              (drop_path): Identity()
            )
          )
        )
        (3): ConvNeXtStage(
          (downsample): Sequential(
            (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)
            (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))
          )
          (blocks): Sequential(
            (0): ConvNeXtBlock(
              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (shortcut): Identity()
              (drop_path): Identity()
            )
            (1): ConvNeXtBlock(
              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (shortcut): Identity()
              (drop_path): Identity()
            )
            (2): ConvNeXtBlock(
              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU()
                (drop1): Dropout(p=0.0, inplace=False)
                (norm): Identity()
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop2): Dropout(p=0.0, inplace=False)
              )
              (shortcut): Identity()
              (drop_path): Identity()
            )
          )
        )
      )
      (norm_pre): Identity()
      (head): NormMlpClassifierHead(
        (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())
        (norm): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (pre_logits): Identity()
        (drop): Dropout(p=0.0, inplace=False)
        (fc): Identity()
      )
    )
  )
  (1): Sequential(
    (0): AdaptiveConcatPool2d(
      (ap): AdaptiveAvgPool2d(output_size=1)
      (mp): AdaptiveMaxPool2d(output_size=1)
    )
    (1): fastai.layers.Flatten(full=False)
    (2): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (3): Dropout(p=0.25, inplace=False)
    (4): Linear(in_features=1536, out_features=512, bias=False)
    (5): ReLU(inplace=True)
    (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (7): Dropout(p=0.5, inplace=False)
    (8): Linear(in_features=512, out_features=37, bias=False)
  )
)</code></pre>
</div>
</div>
</section>
<section id="whats-all-this-stuff" class="level3">
<h3 class="anchored" data-anchor-id="whats-all-this-stuff">What’s All This Stuff?</h3>
<p>Alright, there’s a lot to digest here. Basically, the model is structured in layers upon layers. Here’s the breakdown:</p>
<p><strong>TimmBody</strong>: this contains most of the model architecture. Inside the TimmBody. You’ll find:</p>
<ul>
<li><strong>Model</strong>: The main model components.</li>
<li><strong>Stem</strong>: The initial layers that process the raw input.</li>
<li><strong>Stages</strong>: There are further broken down into multiple blocks, each packed with convolutional layers. normalization layers, and more.</li>
</ul>
</section>
<section id="lets-peek-inside-a-layer" class="level3">
<h3 class="anchored" data-anchor-id="lets-peek-inside-a-layer">Let’s Peek Inside a Layer</h3>
<p>To dig deeper into what these layers contain, you can use a really convenient Pytorch method called <code>get_submodule</code>:</p>
<div id="cell-24" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;execution&quot;,&quot;value&quot;:{&quot;iopub.execute_input&quot;:&quot;2024-07-27T13:00:32.501020Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-07-27T13:00:32.500642Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-07-27T13:00:32.507609Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-07-27T13:00:32.506670Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-07-27T13:00:32.500991Z&quot;}}" data-trusted="true" data-execution_count="5">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1">l <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> m.get_submodule(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'0.model.stem.1'</span>)</span>
<span id="cb15-2">l</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)</code></pre>
</div>
</div>
<p>As you can see it return a <code>LayerNorm2d</code> layer. Wondering what this <code>LayerNorm2d</code> thing is all about? It comprises a mathematical function for normalization and bunch of parameters:</p>
<div id="cell-26" class="cell">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>(l.parameters()))</span></code></pre></div>
</div>
<div id="cell-27" class="cell output" data-quarto-private-1="{&quot;key&quot;:&quot;execution&quot;,&quot;value&quot;:{&quot;iopub.execute_input&quot;:&quot;2024-07-27T13:00:37.022159Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-07-27T13:00:37.021690Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-07-27T13:00:37.161180Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-07-27T13:00:37.160223Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-07-27T13:00:37.022106Z&quot;}}" data-trusted="true" data-execution_count="6">
<div class="cell-output cell-output-stdout">
<pre><code>[Parameter containing:
tensor([ 1.2546e+00,  1.9191e+00,  1.2191e+00,  1.0385e+00, -3.7148e-04,
         7.6571e-01,  8.8668e-01,  1.6324e+00,  7.0477e-01,  3.2892e+00,
         7.8641e-01, -1.7453e-03,  1.0006e+00, -2.0514e-03,  3.2976e+00,
        -1.2112e-03,  1.9842e+00,  1.0206e+00,  4.4522e+00,  2.5476e-01,
         2.7248e+00,  9.2616e-01,  1.2374e+00,  4.3668e-03,  1.7875e+00,
         5.4292e-01,  4.6268e+00,  1.1599e-02, -5.4437e-04,  3.4510e+00,
         1.3520e+00,  4.1267e+00,  2.6876e+00,  4.1197e+00,  3.4007e+00,
         8.5053e-01,  7.3569e-01,  3.9801e+00,  1.2851e+00,  6.3985e-01,
         2.6897e+00,  1.1181e+00,  1.1699e+00,  5.5318e-01,  2.3341e+00,
        -3.0504e-04,  9.7000e-01,  2.3409e-03,  1.1984e+00,  1.7897e+00,
         4.0138e-01,  4.5116e-01,  9.7186e-01,  3.9881e+00,  6.5935e-01,
         6.8778e-01,  9.8614e-01,  2.7053e+00,  1.2169e+00,  7.6268e-01,
         3.3019e+00,  1.6200e+00,  9.5547e-01,  2.1216e+00,  6.2951e-01,
         4.0349e+00,  8.9246e-01, -2.9147e-03,  4.0874e+00,  1.0639e+00,
         1.3963e+00,  1.6683e+00,  4.6571e-04,  7.6833e-01,  8.8542e-01,
         6.4305e-01,  1.3443e+00,  7.1566e-01,  5.4763e-01,  2.0902e+00,
         1.1952e+00,  3.0668e-01,  2.9682e-01,  1.4709e+00,  4.0830e+00,
        -7.8233e-04,  1.1455e+00,  3.8835e+00,  3.5997e+00,  4.8206e-01,
         2.1703e-01, -1.6550e-04,  6.4791e-01,  3.0069e+00,  3.0463e+00,
         4.6374e-03], device='cuda:0', requires_grad=True), Parameter containing:
tensor([-9.8183e-02, -4.0191e-02,  4.1647e+00, -8.9313e-03,  3.7929e-03,
        -2.7139e-02, -3.1174e-02, -7.9865e-02, -1.4053e-01, -6.3492e-02,
         3.2160e-01, -3.3837e-01, -5.6851e-02, -4.0384e-03, -4.7630e-02,
        -2.6376e-02, -4.0858e-02, -4.0886e-02,  8.7548e-03, -2.4149e-02,
         8.5088e-03, -1.6333e-01, -4.0154e+00,  5.2989e-01, -5.3410e-01,
         2.8046e+00,  3.5663e-02, -1.0321e-02, -1.1255e-03, -1.1721e-01,
        -1.3768e-01,  1.8840e-02, -9.5614e-02, -1.3149e-01, -1.9291e-01,
        -6.8939e-02, -3.6672e-02, -1.2902e-01,  1.5387e-01,  3.6398e-03,
        -6.6185e-02,  5.8841e-02, -9.1987e-02, -1.1453e+00, -5.4502e-02,
        -5.3649e-03, -1.8238e-01,  2.3167e-02,  3.8862e-02, -5.9394e-02,
        -4.1380e-02, -5.6917e-02, -4.3903e-02, -1.2954e-02, -1.1092e-01,
         7.0337e-03, -3.9300e-02, -1.5816e-01, -9.8132e-02, -1.8553e-01,
        -1.1112e-01, -1.8186e-01, -3.4278e-02, -2.6474e-02,  1.4192e+00,
        -3.1935e-02, -4.3245e-02, -2.7030e-01, -4.6695e-02, -6.4756e-04,
         2.6561e-01,  1.8779e-01,  6.9716e-01, -3.0647e-01,  8.1973e-02,
        -1.0845e+00,  1.4999e-02, -4.4244e-02, -8.0861e-02, -6.8972e-02,
        -1.3070e-01, -1.7093e-02, -1.9623e-02, -3.9345e-02, -6.9878e-02,
         1.2335e-02, -5.9947e-02, -3.5691e-02, -7.9831e-02, -7.4387e-02,
        -9.5232e-03, -3.7763e-01, -1.1987e-02, -2.5113e-02, -6.2690e-02,
        -3.0666e-04], device='cuda:0', requires_grad=True)]</code></pre>
</div>
</div>
<p>Another example: Let’s inspect a layer deeper inside:</p>
<div id="cell-29" class="cell">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1">l <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> m.get_submodule(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'0.model.stages.0.blocks.1.mlp.fc1'</span>)</span>
<span id="cb19-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(l)</span>
<span id="cb19-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>(l.parameters()))</span></code></pre></div>
</div>
<div id="cell-30" class="cell output" data-quarto-private-1="{&quot;key&quot;:&quot;execution&quot;,&quot;value&quot;:{&quot;iopub.execute_input&quot;:&quot;2024-07-27T13:00:40.913666Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-07-27T13:00:40.912949Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-07-27T13:00:40.929991Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-07-27T13:00:40.929119Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-07-27T13:00:40.913632Z&quot;}}" data-trusted="true" data-execution_count="7">
<div class="cell-output cell-output-stdout">
<pre><code>Linear(in_features=96, out_features=384, bias=True)
[Parameter containing:
tensor([[ 0.0227, -0.0014,  0.0404,  ...,  0.0016, -0.0453,  0.0083],
        [-0.1439,  0.0169,  0.0261,  ...,  0.0126, -0.1044,  0.0565],
        [-0.0655, -0.0327,  0.0056,  ..., -0.0414,  0.0659, -0.0401],
        ...,
        [-0.0089,  0.0699,  0.0003,  ...,  0.0040,  0.0415, -0.0191],
        [ 0.0019,  0.0321,  0.0297,  ..., -0.0299, -0.0304,  0.0555],
        [ 0.1211, -0.0355, -0.0045,  ..., -0.0062,  0.0240, -0.0114]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([-0.4049, -0.7419, -0.4234, -0.1651, -0.3027, -0.1899, -0.5534, -0.6270,
        -0.3008, -0.4253, -0.5996, -0.4107, -0.2173, -1.7935, -0.3170, -0.1163,
        -0.4483, -0.2847, -0.4343, -0.4945, -0.4064, -1.1403, -0.6754, -1.7236,
        -0.2954, -0.2655, -0.2188, -0.3913, -0.4148, -0.4771,  0.2366, -0.7542,
        -0.5851, -0.1821, -1.5273, -0.3625, -2.4688, -2.3461, -0.6110, -0.4114,
        -0.6963, -0.5764, -0.5878, -0.0318, -2.0354, -0.2859, -0.3954, -0.8404,
        -2.2399, -1.0874, -0.2296, -0.9002, -0.7585, -0.8834, -0.3753, -0.4548,
        -0.3836, -0.4048, -2.0231, -1.0264, -0.4106, -1.1566, -0.2225, -0.4251,
        -0.2496, -0.4224, -0.0975, -1.4017, -0.6887, -0.4370, -0.2931, -0.4643,
        -0.4959, -1.2535, -1.0720, -1.2966, -0.6276, -1.4162, -2.3081, -2.4540,
        -0.4258, -0.9987, -0.4638, -0.3147, -0.2417, -0.8744, -0.2828, -1.4208,
        -0.3257, -0.3202, -0.0603, -0.1894, -0.2496, -0.6130, -0.2975, -2.1466,
        -0.4129, -0.3677, -1.9813, -0.3814, -0.3785, -0.2294, -0.3698, -0.3256,
        -0.5585, -2.4192, -0.4589, -1.7748, -0.3995, -0.4092, -0.3517, -0.5331,
        -1.6535, -1.8190,  0.6264, -0.4059,  0.5873, -2.2074, -0.2438, -2.4539,
        -0.2283, -0.6865,  0.6988,  0.6476, -0.6445, -0.3452, -0.3276, -0.5700,
        -0.5173, -0.2775, -0.4089, -0.3020, -0.4872, -0.4952, -0.4072, -0.4356,
        -0.5102, -0.4128, -2.0918, -0.2826, -0.5830, -1.5835,  0.6139, -0.8504,
        -0.4669, -2.1358, -0.3418, -0.3767, -0.3345, -0.3960, -0.3886, -0.5667,
        -0.2225, -1.3059, -0.4600, -0.3927, -0.4667, -0.4214, -0.4755, -0.2866,
        -1.5805, -0.1787, -0.4367, -0.3172,  1.5731, -0.4046, -0.4838, -0.2576,
        -0.5612, -0.4264, -0.2578, -0.3175, -0.4620, -1.9552, -1.9145, -0.3960,
         0.3988, -2.3519, -0.9688, -0.2831, -1.9001, -0.4180,  0.0159, -1.1109,
        -0.4921, -0.3177, -1.8909, -0.3101, -0.8136, -2.3345, -0.3845, -0.3847,
        -0.1974, -0.4445, -1.6233, -2.5485, -0.3176, -1.2715, -1.1479,  0.6149,
        -0.3748, -0.3949, -2.0747, -0.4657, -0.3780, -0.4957, -0.3282, -1.9219,
        -2.0019, -0.5307, -0.2554, -1.1160, -0.3517, -2.2185, -1.1393,  0.5364,
        -0.3217, -2.0389, -0.4655,  0.1850, -0.5830, -0.3128,  0.6180, -0.2125,
        -2.3538, -0.9699, -0.9785, -0.3667, -0.4502, -1.9564, -0.2662, -1.1755,
        -0.4198, -0.9024, -0.3605, -0.5172, -1.1879, -0.4190, -0.4770, -1.5560,
        -0.4011, -0.6518, -0.4818, -0.2423,  0.6909, -0.5081, -0.4304, -0.6068,
        -0.4000, -0.3329, -0.3596, -1.6108, -0.2371, -0.2467, -0.4545,  0.1807,
        -0.3227, -0.3918, -0.3515, -0.3755, -1.2178, -0.3999, -0.3578, -0.2882,
        -1.7483, -0.2363, -0.1599, -0.2640, -0.9769, -1.3065, -0.4148, -0.2663,
        -0.3933, -0.4627, -0.2174,  0.2140, -0.5733, -0.2766, -0.3659, -0.5172,
        -0.3484, -0.3362, -0.6445,  0.6866, -0.3738, -0.2902, -2.0863, -0.4882,
        -0.2597, -1.0496, -1.6616, -0.3398, -0.5111, -0.5659, -0.3027, -0.5048,
        -0.2877, -0.2841, -0.1982, -0.6910, -0.2873, -2.1121, -0.8927, -0.2301,
        -1.5013, -0.4734, -2.2292, -0.4022, -0.2926, -0.4199,  0.6646, -0.3047,
        -0.1688, -0.3749, -0.6433, -2.3348, -0.3101, -1.2730, -0.8193, -1.0593,
        -0.0934, -1.6387,  0.3426, -0.8484, -0.4910, -0.5001, -1.0631, -0.3534,
        -1.1564, -0.3842, -0.3172, -0.6432, -0.9083, -0.6567, -0.6490,  0.6337,
        -0.2662, -1.3202, -1.1623, -1.2032, -2.0577, -0.3001, -1.3596, -0.4612,
        -0.5024, -0.4950, -0.3156, -0.3272, -0.2669, -0.4279, -0.3296, -0.3011,
        -1.6635,  0.6434, -0.9455,  0.6099, -0.4234,  0.3917, -0.4944, -0.4284,
        -0.2587, -0.4952, -2.1991, -0.2601, -0.3934, -0.4565, -0.5816, -0.3487,
        -0.7372, -0.3589, -0.4894, -2.0105,  0.4557, -0.8055, -1.7748, -0.3512,
        -0.5359, -0.2101, -0.3955, -0.4782, -1.1457, -0.3974, -2.2115, -0.2838],
       device='cuda:0', requires_grad=True)]</code></pre>
</div>
</div>
<p>What do these numbers mean, you ask? Essentially, they represent the learned parameters of the model— the weights that have been fine-tuned during training. These weights form the “secret sauce” that enables the model to distinguish between, say, a basset hound and a tabby cat.</p>
<p>Next, we’ll dive into how neural networks function behind the scenes, exploring the mechanisms that transform these parameters into powerful predictions.</p>
</section>
</section>
<section id="how-neural-networks-really-work" class="level2">
<h2 class="anchored" data-anchor-id="how-neural-networks-really-work">How Neural Networks Really Work</h2>
<p>To answer the burning question from before, let’s dive into the marvels of neural networks. Yes, Jeremy Howard has an amazing notebook called <a href="https://www.kaggle.com/code/jhoward/how-does-a-neural-net-really-work">“How does a neural net really work?”</a> that’s perfect for beginners. But, I’m here to give you a walkthrough with a dash of humor!</p>
<p>Machine learning models are like very smart shape-fitting artists. They find pattern in data and learn to recognize them. We’ll start simple - with a quadratic function. Let’s see how it all works:</p>
<div id="cell-33" class="cell" data-execution_count="16">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> plotly</span>
<span id="cb21-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> plotly.express <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> px</span>
<span id="cb21-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> torch</span>
<span id="cb21-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb21-5"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> IPython.display <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> display, HTML</span>
<span id="cb21-6"></span>
<span id="cb21-7"> <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Tomas Mazak's workaround for MathJax in VSCode</span></span>
<span id="cb21-8">plotly.offline.init_notebook_mode()</span>
<span id="cb21-9">display(HTML(</span>
<span id="cb21-10">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'&lt;script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_SVG"&gt;&lt;/script&gt;'</span></span>
<span id="cb21-11">)) </span>
<span id="cb21-12"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> plot_function(f, title<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">min</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">2.1</span>, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">2.1</span>):</span>
<span id="cb21-13">    x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.linspace(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">min</span>, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>, steps<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>)</span>
<span id="cb21-14">    y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> f(x)</span>
<span id="cb21-15">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> px.line(x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>x, y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>y, title<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>title)</span></code></pre></div>
</details>
</div>
<div id="cell-34" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> f(x): <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb22-2">plot_function(f, title<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">r"</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">$</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">3x</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">^</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">2 </span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;"> 2x </span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;"> 1</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">$</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code hidden" id="cb23" data-startfrom="1" data-source-offset="-0" style="background: #f1f3f5;"><pre class="sourceCode js code-with-copy"><code class="sourceCode javascript"><span id="cb23-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">function</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">f</span>(x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> a<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> b<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> c) { <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> a<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> b<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> c }</span>
<span id="cb23-2">x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> { <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">Array</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">from</span>({ <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">length</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">40</span> }<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> (_<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> i) <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> (<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">40</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)))<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span> }</span>
<span id="cb23-3">y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">map</span>(element <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">f</span>(element<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> a<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> b<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> c))</span>
<span id="cb23-4">{</span>
<span id="cb23-5">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">var</span> trace1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> { <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">x</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">y</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">mode</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'lines'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">name</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'quadratic'</span>}<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb23-6">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">var</span> data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [trace1]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb23-7">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">var</span> layout <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> { <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">title</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"3x²+ 2x + 1"</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">xaxis</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> { <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">title</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'x'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">zeroline</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">false</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> }<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">yaxis</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> { <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">title</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"y"</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> } }<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb23-8">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">const</span> div <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> DOM<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">element</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'div'</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb23-9">  Plotly<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">newPlot</span>(div<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> layout)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb23-10">  <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> div<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb23-11">}</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-2-1" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-2-2" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-2-3" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-2-4" data-nodetype="expression">

</div>
</div>
</div>
</div>
<p>What we want to do here is straightforward: suppose we don’t know the exact mathematical function, and we’re trying to reconstruct it from some data. Here’s the actual function, and our goal is to approximate it using a variety of quadratic equations.</p>
<p><strong>Creating Quadratics on Demand</strong></p>
<p>In Python, the <code>partial</code> function lets us fix certain parameters of a function to generate different variations. It’s like having a playlist of your favorite songs with the flexibility to change the lyrics whenever you want!</p>
<div id="cell-37" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> functools <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> partial</span>
<span id="cb24-2"></span>
<span id="cb24-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> quad(a, b, c, x): <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> a <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> b <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> c</span>
<span id="cb24-4"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> mkquad(a, b, c): <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> partial(quad, a, b, c)</span></code></pre></div>
</div>
<section id="introducing-noise" class="level3">
<h3 class="anchored" data-anchor-id="introducing-noise">Introducing Noise</h3>
<p>In real life, data never fits perfectly to a function. There’s always some noise, it’s often as messy and unpredictable as a doctor’s illegible handwriting. Let’s add some noise to our data:</p>
<div id="cell-39" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb25" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> noise(x, scale): <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> np.random.normal(scale<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>scale, size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>x.shape)</span>
<span id="cb25-2"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> add_noise(x, mult, add): <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> noise(x, mult)) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> noise(x, add)</span></code></pre></div>
</div>
<div id="cell-40" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb26" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1">np.random.seed(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">42</span>)</span>
<span id="cb26-2">x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.linspace(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, steps<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">40</span>)</span>
<span id="cb26-3">y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> add_noise(f(x), <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.15</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.5</span>)</span>
<span id="cb26-4">px.scatter(x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>x, y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>y)</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code hidden" id="cb27" data-startfrom="1" data-source-offset="-0" style="background: #f1f3f5;"><pre class="sourceCode js code-with-copy"><code class="sourceCode javascript"><span id="cb27-1">data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">FileAttachment</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"dataponts.csv"</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">csv</span>()</span>
<span id="cb27-2">x_data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">map</span>(item <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> item<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb27-3">y_data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">map</span>(item <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> item<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">y</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb27-4">{</span>
<span id="cb27-5">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">var</span> trace1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> { <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">x</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span>  x_data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">y</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span>  y_data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">mode</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'markers'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">name</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'data ponts'</span>}<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb27-6">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">var</span> data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [trace1]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb27-7">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">var</span> layout <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> { <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">title</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">""</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">xaxis</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> { <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">title</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'x'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> }<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">yaxis</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> { <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">title</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"y"</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> } }<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb27-8">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">const</span> div <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> DOM<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">element</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'div'</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb27-9">  Plotly<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">newPlot</span>(div<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> layout)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb27-10">  <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> div<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb27-11">}</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-3-1" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-3-2" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-3-3" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-3-4" data-nodetype="expression">

</div>
</div>
</div>
</div>
<p>This noisy data is inspired by the quadratic function but comes with a sprinkle of randomness.</p>
<p><strong>Plot Quadratics with Sliders: Interactive Fun</strong></p>
<p>Ever played with sliders to adjust stuff? Here’s your chance to do the same with quadratics. You can tweak the coefficients a, b, and c to fit the noisy data manually.</p>
<div class="cell">
<div class="sourceCode cell-code hidden" id="cb28" data-startfrom="1" data-source-offset="-0" style="background: #f1f3f5;"><pre class="sourceCode js code-with-copy"><code class="sourceCode javascript"><span id="cb28-1">viewof a <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Inputs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">range</span>([<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> {<span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">label</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"a"</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">step</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>})</span>
<span id="cb28-2">viewof b <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Inputs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">range</span>([<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> {<span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">label</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"b"</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">step</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>})</span>
<span id="cb28-3">viewof c <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Inputs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">range</span>([<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> {<span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">label</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"c"</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">step</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>})</span>
<span id="cb28-4">{</span>
<span id="cb28-5">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">var</span> trace1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> { <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">x</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">y</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">mode</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'lines'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">name</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'quadratic'</span>}<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb28-6">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">var</span> trace2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> { <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">x</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span>  x_data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">y</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span>  y_data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">mode</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'markers'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">name</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'data ponts'</span>}<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb28-7">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">var</span> data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [trace1<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> trace2]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb28-8">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">var</span> layout <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> { <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">title</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">`Interactive Quadratics`</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">xaxis</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> { <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">title</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'x'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">zeroline</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">false</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> }<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">yaxis</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> { <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">title</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">`</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">${</span>a<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">x² + </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">${</span>b<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">x + </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">${</span>c<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">`</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> } }<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb28-9">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">const</span> div <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> DOM<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">element</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'div'</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb28-10">  Plotly<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">newPlot</span>(div<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> layout)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb28-11">  <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> div<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb28-12">}</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-4-1" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-4-2" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-4-3" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-4-4" data-nodetype="expression">

</div>
</div>
</div>
</div>
<p>But who wants to be a human slider forever? We need a more scientific approach to measure how well our function fits the data. Enter loss functions - the unsung heroes of machine learning.</p>
</section>
<section id="meet-the-mean-squared-error-mse" class="level3">
<h3 class="anchored" data-anchor-id="meet-the-mean-squared-error-mse">Meet the Mean Squared Error (MSE)</h3>
<p>MSE stands for Mean Squared Error. It’s a way to measure how far off our predictions are from the actual values. Here’s how you define it:</p>
<div id="cell-45" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb29" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> mse(preds, acts): <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> ((preds <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> acts)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>).mean()</span></code></pre></div>
</div>
<p>Now, let’s use MSE to evaluate our quadratics. This function will calculate the loss (how bad our predictions are) and give us a number we can use to improve our model.</p>
<div class="cell">
<div class="sourceCode cell-code hidden" id="cb30" data-startfrom="1" data-source-offset="-0" style="background: #f1f3f5;"><pre class="sourceCode js code-with-copy"><code class="sourceCode javascript"><span id="cb30-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">function</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mse</span>(preds<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> acts) {</span>
<span id="cb30-2">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">const</span> squared_error <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb30-3">  <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> (<span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">let</span> i<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span> i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> preds<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">length</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span> i<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">++</span>) {</span>
<span id="cb30-4">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">const</span> error <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> preds[i] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> acts[i]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb30-5">    squared_error<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">push</span>(error<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb30-6">  }</span>
<span id="cb30-7">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">const</span> mse <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> squared_error<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">reduce</span>((acc<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> curr) <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> acc<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span>curr<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> preds<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">length</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb30-8">  <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> mse<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb30-9">}</span>
<span id="cb30-10">_y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">map</span>(element <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">f</span>(element<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> _a<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> _b<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> _c))</span>
<span id="cb30-11">viewof _a <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Inputs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">range</span>([<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> {<span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">label</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"a"</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">step</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>})</span>
<span id="cb30-12">viewof _b <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Inputs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">range</span>([<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> {<span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">label</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"b"</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">step</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>})</span>
<span id="cb30-13">viewof _c <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Inputs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">range</span>([<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> {<span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">label</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"c"</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">step</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>})</span>
<span id="cb30-14">{</span>
<span id="cb30-15">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">const</span> loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mse</span>(_y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> y_data)</span>
<span id="cb30-16">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">var</span> trace1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> { <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">x</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">y</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> _y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">mode</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'lines'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">name</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'quadratic'</span>}<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb30-17">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">var</span> trace2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> { <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">x</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span>  x_data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">y</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span>  y_data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">mode</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'markers'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">name</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'data ponts'</span>}<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb30-18">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">var</span> data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [trace1<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> trace2]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb30-19">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">var</span> layout <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> { <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">title</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">`Loss: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">${</span>loss<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">toFixed</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">`</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">xaxis</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> { <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">title</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'x'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">zeroline</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">false</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> }<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">yaxis</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> { <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">title</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">`</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">${</span>_a<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">x² + </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">${</span>_b<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">x + </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">${</span>_c<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">`</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> } }<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb30-20">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">const</span> div <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> DOM<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">element</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'div'</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb30-21">  Plotly<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">newPlot</span>(div<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> layout)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb30-22">  <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> div<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb30-23">}</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-5-1" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-5-2" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-5-3" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-5-4" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-5-5" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-5-6" data-nodetype="expression">

</div>
</div>
</div>
</div>
<p>With Mean Squared Error (MSE), you can objectively assess whether a model’s fit is improving without relying solely on visual inspection. Instead of manually adjusting parameters—which can be tedious and inefficient—we can automate the process using calculus.</p>
</section>
<section id="the-power-of-derivatives" class="level3">
<h3 class="anchored" data-anchor-id="the-power-of-derivatives">The Power of Derivatives</h3>
<p>One straightforward approach might be to manually tweak each parameter and observe how the loss, which quantifies the model’s prediction error, changes. However, there’s a far more efficient method: by computing the derivative of the loss function with respect to the parameters. These derivatives, also known as gradients, indicate the direction and rate at which the loss changes. This information is crucial for guiding the optimization process.</p>
</section>
<section id="leveraging-pytorch" class="level3">
<h3 class="anchored" data-anchor-id="leveraging-pytorch">Leveraging PyTorch</h3>
<p>Fortunately, PyTorch automates the calculation of these derivatives, greatly simplifying the optimization process. For example, consider a function called <code>quad_mse</code>, which computes the Mean Squared Error between our observed noisy data and a quadratic model defined by parameters [a, b, c]. This function serves as a foundation for adjusting the model parameters in an informed and efficient way.</p>
<div id="cell-49" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb31" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> quad_mse(params):</span>
<span id="cb31-2">    f <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> mkquad(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>params)</span>
<span id="cb31-3">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> mse(f(x), y)</span></code></pre></div>
</div>
<p>This function takes the coefficients (a, b, c), creates a quadratic function, and then returns the MSE of the predicted values against the actual noisy data.</p>
<div id="cell-51" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb32" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1">quad_mse([<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.5</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.5</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.5</span>])</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="23">
<pre><code>tensor(6.7798, dtype=torch.float64)</code></pre>
</div>
</div>
<p>We get a MSE of 6.78, and yes, it’s a tenser (just a fancy array with some extra Pytorch powers). Let’s make it easier to hand:</p>
<div id="cell-53" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb34" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1">abc <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.tensor([<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.5</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.5</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.5</span>])</span>
<span id="cb34-2">abc.requires_grad_()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="24">
<pre><code>tensor([1.5000, 1.5000, 1.5000], requires_grad=True)</code></pre>
</div>
</div>
<p>Now, our tensor is ready to calculate gradients for these coefficients whenever used in computations. Pass this to <code>quad_mse</code> to verify:</p>
<div id="cell-55" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb36" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1">loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> quad_mse(abc)</span>
<span id="cb36-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(loss)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor(6.7798, dtype=torch.float64, grad_fn=&lt;MeanBackward0&gt;)</code></pre>
</div>
</div>
<p>As expected, we get that magical tensor value 6.78. Nothing fancy yet? Hold on. We now tell Pytorch to store the gradients:</p>
<div id="cell-57" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb38" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1">loss.backward()</span></code></pre></div>
</div>
<p>No fireworks, but something profound just happened. Run this:</p>
<div id="cell-59" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb39" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(abc.grad)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([-7.6934, -0.4701, -2.8031])</code></pre>
</div>
</div>
<p>Voila! You’ve got the gradients or slopes. They tell us how much the loss changes if you tweak each parameter-perfect for finding the optimal values.</p>
</section>
<section id="updating-parameters-using-gradients" class="level3">
<h3 class="anchored" data-anchor-id="updating-parameters-using-gradients">Updating Parameters Using Gradients</h3>
<p>To bring our loss down, we adjust the parameters in the direction that reduces the loss. Essentially, we descend down the gradient:</p>
<div id="cell-61" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb41" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> torch.no_grad():</span>
<span id="cb41-2">    abc <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-=</span> abc.grad <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span></span>
<span id="cb41-3">    loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> quad_mse(abc)</span>
<span id="cb41-4"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(loss)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor(6.1349, dtype=torch.float64)</code></pre>
</div>
</div>
<p>This operation subtracts a small proportion of the gradient from each parameter, resulting in an updated set of parameters and a reduction of the loss from 6.78 to 6.13.</p>
<p>Note that using the context manager <code>with torch.no_grad()</code> disables gradient computation for the weight and bias update step, as this update does not require gradient tracking.</p>
</section>
<section id="automating-gradient-descent" class="level3">
<h3 class="anchored" data-anchor-id="automating-gradient-descent">Automating Gradient Descent</h3>
<p>Instead of performing updates manually, you can automate the process using a loop to handle multiple iterations of gradient descent.</p>
<div id="cell-63" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb43" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>):</span>
<span id="cb43-2">    loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> quad_mse(abc)</span>
<span id="cb43-3">    loss.backward()</span>
<span id="cb43-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> torch.no_grad():</span>
<span id="cb43-5">        abc <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-=</span> abc.grad <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span></span>
<span id="cb43-6">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Step </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">; loss: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>loss<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.2f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> "</span>)</span>
<span id="cb43-7">        abc.grad.zero_()  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Clear the gradient after each step</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Step 0; loss: 6.13 
Step 1; loss: 5.05 
Step 2; loss: 4.68 
Step 3; loss: 4.37 
Step 4; loss: 4.10 </code></pre>
</div>
</div>
<div id="cell-64" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb45" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1">abc</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="30">
<pre><code>tensor([1.9329, 1.5305, 1.6502], requires_grad=True)</code></pre>
</div>
</div>
<p>After about five gradient descent iterations, the parameters have adjusted incrementally toward their optimal values. These parameters continuously update to minimize the loss and capture the underlying patterns in your data.</p>
</section>
<section id="welcome-to-optimization-the-role-of-gradient-descent" class="level3">
<h3 class="anchored" data-anchor-id="welcome-to-optimization-the-role-of-gradient-descent">Welcome to Optimization: The Role of Gradient Descent</h3>
<p>The process of fine-tuning parameters to reduce prediction error is known as optimization, with gradient descent being one of the most widely used methods. Nearly all machine learning models—including complex neural networks—rely on some variant of this technique.</p>
</section>
<section id="the-importance-of-relus" class="level3">
<h3 class="anchored" data-anchor-id="the-importance-of-relus">The Importance of ReLUs</h3>
<p>Simple quadratic functions are often insufficient for modeling real-world data, which tends to exhibit far greater complexity. When distinguishing subtle visual features in images, for example, a more sophisticated approach is required.</p>
<p>This is where the Rectified Linear Unit (ReLU) comes in. As an activation function, ReLU serves as a fundamental building block for constructing highly flexible models capable of capturing intricate patterns.</p>
<div id="cell-66" class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb47" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> rectified_linear(m, b, x):</span>
<span id="cb47-2">    y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> m <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> b</span>
<span id="cb47-3">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> torch.clip(y, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">min</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.0</span>)</span></code></pre></div>
</div>
<p>This function is a simple line <code>y = mx + b</code>. The <code>torch.clip()</code> function takes anything blow zero and flatlines it at zero. Essentially, this turns any negative output into zero, while keeping positive values unchanged.</p>
<p>Here’s what the ReLU looks like:</p>
<div id="cell-68" class="cell">
<div class="sourceCode cell-code" id="cb48" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1">plot_function(partial(rectified_linear, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code hidden" id="cb49" data-startfrom="1" data-source-offset="-0" style="background: #f1f3f5;"><pre class="sourceCode js code-with-copy"><code class="sourceCode javascript"><span id="cb49-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">function</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rectified_linear</span>(m<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> b<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> x) {</span>
<span id="cb49-2">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">const</span> y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> m<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> b<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb49-3">  <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">Math</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">max</span>(y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb49-4">}</span>
<span id="cb49-5">viewof m <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Inputs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">range</span>([<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> {<span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">label</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"m"</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">step</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>})</span>
<span id="cb49-6">viewof b_ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Inputs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">range</span>([<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> {<span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">label</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"b"</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">step</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>})</span>
<span id="cb49-7">{</span>
<span id="cb49-8">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">const</span> _y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">map</span>(element <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rectified_linear</span>(m<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> b_<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> element))<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb49-9">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">var</span> trace1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb49-10">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">x</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb49-11">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">y</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> _y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb49-12">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">mode</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'lines'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb49-13">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">name</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ReLU'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb49-14">  }<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb49-15">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">var</span> data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [trace1]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb49-16">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">var</span> layout <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb49-17">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">title</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Rectified Linear Unit"</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb49-18">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">xaxis</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> { <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">title</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"x"</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> }<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb49-19">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">yaxis</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> { <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">title</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"y"</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">range</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> [<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>] }</span>
<span id="cb49-20">  }<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb49-21">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">const</span> div <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> DOM<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">element</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'div'</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb49-22">  Plotly<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">newPlot</span>(div<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> layout)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb49-23">  <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> div<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb49-24">}</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-6-1" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-6-2" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-6-3" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-6-4" data-nodetype="expression">

</div>
</div>
</div>
</div>
<p>Imagine a line rising up at a 45-degree angle until it hits zero-at which point it surrenders to the great oblivion blow it. Now, you can adjust the coefficients <code>m</code> (slope) and <code>b</code> (intercept) and watch the magic happen.</p>
</section>
<section id="the-power-of-double-relu-fun-with-functions" class="level3">
<h3 class="anchored" data-anchor-id="the-power-of-double-relu-fun-with-functions">The Power of Double ReLU: Fun With Functions</h3>
<p>Why stop at one ReLU when you can have double the fun with two?</p>
<div id="cell-71" class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb50" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> double_relu(m1, b1, m2, b2, x):</span>
<span id="cb50-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> rectified_linear(m1, b1, x) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> rectified_linear(m2, b2, x)</span></code></pre></div>
</div>
<p>This function combines two ReLUs. Let’s plot this end see what unfolds:</p>
<div id="cell-73" class="cell">
<div class="sourceCode cell-code" id="cb51" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1">plot_function(partial(double_relu, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code hidden" id="cb52" data-startfrom="1" data-source-offset="-0" style="background: #f1f3f5;"><pre class="sourceCode js code-with-copy"><code class="sourceCode javascript"><span id="cb52-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">function</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">double_relu</span>(m1<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span>b1<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span>m2<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span>b2<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span>x) {</span>
<span id="cb52-2">  <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rectified_linear</span>(m1<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span>b1<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span>x) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rectified_linear</span>(m2<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span>b2<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span>x)</span>
<span id="cb52-3">}</span>
<span id="cb52-4">viewof m1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Inputs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">range</span>([<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> {<span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">label</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"m1"</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">step</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>})</span>
<span id="cb52-5">viewof b1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Inputs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">range</span>([<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> {<span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">label</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"b1"</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">step</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>})</span>
<span id="cb52-6">viewof m2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Inputs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">range</span>([<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> {<span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">label</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"m2"</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">step</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>})</span>
<span id="cb52-7">viewof b2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Inputs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">range</span>([<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> {<span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">label</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"b2"</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">step</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>})</span>
<span id="cb52-8">{</span>
<span id="cb52-9">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">const</span> _y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">map</span>(element <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">double_relu</span>(m1<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span>b1<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span>m2<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span>b2<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> element))<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb52-10">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">var</span> trace1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb52-11">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">x</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb52-12">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">y</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> _y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb52-13">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">mode</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'lines'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb52-14">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">name</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Double ReLU'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb52-15">  }<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb52-16">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">var</span> data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [trace1]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb52-17">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">var</span> layout <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb52-18">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">title</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Double Rectified Linear Unit"</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb52-19">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">xaxis</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> { <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">title</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"x"</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> }<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb52-20">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">yaxis</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> { <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">title</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"y"</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">range</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> [<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>] }</span>
<span id="cb52-21">  }<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb52-22">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">const</span> div <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> DOM<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">element</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'div'</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb52-23">  Plotly<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">newPlot</span>(div<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> layout)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb52-24">  <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> div<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb52-25">}</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-7-1" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-7-2" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-7-3" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-7-4" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-7-5" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-7-6" data-nodetype="expression">

</div>
</div>
</div>
</div>
<p>You’ll notice a downward slope that hooks upward into another slope. Tweak the coefficients <code>m1</code>, <code>b1</code>, <code>m2</code>, and <code>b2</code>, and watch the slopes and hooks dance around!</p>
</section>
<section id="infinity-flexible-relus" class="level3">
<h3 class="anchored" data-anchor-id="infinity-flexible-relus">Infinity Flexible ReLUs</h3>
<p>Think this is fun? Imagine adding a million <strong>ReLUs</strong> together. In face, you can add as many as you want to create function as wiggly and complex as you desire.</p>
<p>Behold the power of ReLUs! With enough ReLUs, you can match any data pattern with incredible precision. you want a function that isn’t just 2D but spreads across multiply dimensions? You got it! ReLUs can do 3D, 4D, 5D…, nD.</p>
</section>
<section id="need-parameters-weve-got-gradient-descent" class="level3">
<h3 class="anchored" data-anchor-id="need-parameters-weve-got-gradient-descent">Need Parameters? We’ve got Gradient Descent</h3>
<p>But we need parameters to make magic happen, right? Here’s where <strong>gradient descent</strong> swoops in to save the day. By continuously tweaking these coefficients based on our loss calculations, we gradually descend towards the perfect parameter set.</p>
</section>
<section id="the-big-picture-adding-relus-and-gradient-descent-deep-learning" class="level3">
<h3 class="anchored" data-anchor-id="the-big-picture-adding-relus-and-gradient-descent-deep-learning">The Big Picture: Adding ReLus and Gradient Descent === Deep Learning</h3>
<p>Believe it or not, this is the essence of <strong>deep learning</strong>. Everything else-every other tweak is just about making this process faster and more efficient, sparking those “a-ha!” moments.</p>
<p>Quoting Jeremy Howard:</p>
<p><img src="https://bhdai.github.io/blog/posts/2024-07-28-from-neuron-to-gradient/how_to_draw_an_owl.jpg" style="width:40%;"></p>
<blockquote class="blockquote">
<p>“Now I remember a few years ago when I said something like this in a class, somebody on the forum was like, this reminds me of that thing about how to draw an owl. Okay, step one: draw two circles; step two: daw the rest of the owl”</p>
</blockquote>
<p>This explanation highlights the fundamental components of deep learning. At its core, deep learning involves leveraging activation functions like ReLUs, optimizing parameters through gradient descent, and processing data samples to generate predictions. Essentially, when you stack layers of ReLUs and systematically adjust parameters using gradient descent, the network learns to map inputs to outputs—much like an image of an owl being recognized by the computer.</p>
<p>Whenever the concepts feel overwhelming, remember that the process boils down to these basics: using gradient descent to fine-tune parameters and combining numerous activation functions to capture complex patterns in your data.</p>
<p>And that’s the essence of deep learning—simple building blocks working together to create sophisticated models. Stay curious and explore how each component contributes to the overall process.</p>


</section>
</section>

 ]]></description>
  <category>blogging</category>
  <category>fastai</category>
  <category>torch</category>
  <guid>https://bhdai.github.io/blog/posts/2024-07-28-from-neuron-to-gradient/</guid>
  <pubDate>Sat, 27 Jul 2024 17:00:00 GMT</pubDate>
  <media:content url="https://bhdai.github.io/blog/posts/2024-07-28-from-neuron-to-gradient/xkcd_trained_a_neural_net.png" medium="image" type="image/png" height="212" width="144"/>
</item>
</channel>
</rss>

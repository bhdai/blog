<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Bui Huu Dai</title>
<link>https://buidai123.github.io/blog/</link>
<atom:link href="https://buidai123.github.io/blog/index.xml" rel="self" type="application/rss+xml"/>
<description>Dai&#39;s blog.</description>
<generator>quarto-1.4.557</generator>
<lastBuildDate>Sat, 06 Jul 2024 17:00:00 GMT</lastBuildDate>
<item>
  <title>From Notebook to Web App: Deploying Your Models with fastai Lesson 2</title>
  <dc:creator>Bui Huu Dai</dc:creator>
  <link>https://buidai123.github.io/blog/posts/2024-07-07-from-notebook-to-web-app/</link>
  <description><![CDATA[ 





<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Welcome back to our deep learning adventure with fastai! In <a href="https://course.fast.ai/Lessons/lesson2.html">Lesson 2</a>, we dive into the exciting world of putting model into production. Whether you’re a beginner looking to get your feet wet or an experienced practitioner wanting to brush up on your deployment skills, this lesson is packed with practial tips and hands-on technquies to take your models from the notebook to the real world.</p>
<p>In this blog post, we’ll cover everything from gathering images to training and deploying models, using tools like Jupyter Notebooks, Gradio, and Hugging Face Spaces. Get ready to explore essential concepts like how to clean your data and see how different deployment platforms stack up against each other.</p>
<p>Buckle up and let’s get started on this journey and bring your deep learning models to life!</p>
</section>
<section id="gathering-and-cleaning-data" class="level2">
<h2 class="anchored" data-anchor-id="gathering-and-cleaning-data">Gathering and Cleaning Data</h2>
<p>In this section we’ll walk through the process of gathering and cleaning data, leveraging some handy tools and methods introduced in Lesson 2.</p>
<section id="importing-and-setting-up" class="level3">
<h3 class="anchored" data-anchor-id="importing-and-setting-up">Importing and Setting Up</h3>
<p>First ensure that you have all necessary libraries and modules in place. If you haven’t already, run the following command to install the fastbook module:</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">conda</span> install <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-y</span> <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-c</span> fastai fastbook</span></code></pre></div>
<p>Now you can import the required functions from fastbook:</p>
<div id="cell-3" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> fastbook <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span></span>
<span id="cb2-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> fastai.vision.widgets <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span></span></code></pre></div>
</div>
</section>
<section id="gathering-images-with-duckduckgo" class="level3">
<h3 class="anchored" data-anchor-id="gathering-images-with-duckduckgo">Gathering Images with DuckDuckGo</h3>
<p>Using DuckDuckGo(ddg) for image searches simplifies the process, as it doesn’t require an API key. Here’s the code to create our dataset of bear images:</p>
<div id="cell-5" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">bear_types <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'grizzly'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'black'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'teddy'</span></span>
<span id="cb3-2">path <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Path(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'bear'</span>)</span>
<span id="cb3-3"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">not</span> path.exists():</span>
<span id="cb3-4">    path.mkdir()</span>
<span id="cb3-5">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> o <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> bear_types:</span>
<span id="cb3-6">        dest <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (path<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>o)</span>
<span id="cb3-7">        dest.mkdir(exist_ok<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb3-8">        results <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> search_images_ddg(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>o<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> bear"</span>)</span>
<span id="cb3-9">        download_images(dest, urls<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>results)</span></code></pre></div>
</div>
<p>This code snippet sets up directories for different bear types and download images into respective folders.</p>
<p>Next, we verify and clean the downloaded images:</p>
<div id="cell-7" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">failed <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> verify_images(get_image_files(path))</span>
<span id="cb4-2">failed</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<pre><code>(#24) [Path('bear/black/b8d71ddf-a84d-4054-8088-bb08e8cbd814.jpg'),Path('bear/black/bbe19bf5-3d28-4d7e-b8bf-3e7f8afff5af.jpg'),Path('bear/teddy/ee833f9f-ff26-4435-a4cc-89208236c442.jpg'),Path('bear/teddy/f6f4f901-c873-46a6-8ef7-bbd65da2c910.jpg'),Path('bear/teddy/e3c4fc0d-e494-4c8a-9dc5-8510f8407cac.jpg'),Path('bear/teddy/3309df18-2a7f-4d67-b2e8-9f08eea06025.jpg'),Path('bear/teddy/99377b04-a870-4798-9e6a-02543a495395.JPG'),Path('bear/teddy/fbf4430d-0643-444b-a0c5-7d13c43d92b6.jpg'),Path('bear/teddy/4c576f46-fa7e-4800-bb89-9eea7662ab10.jpg'),Path('bear/teddy/50d2e017-49b9-49a1-9538-5e402932e463.jpg')...]</code></pre>
</div>
</div>
<div id="cell-8" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">failed.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">map</span>(Path.unlink)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
</div>
<p>This step ensures that any currupt images are identified and removed.</p>
</section>
<section id="structuring-data-with-datablock-api" class="level3">
<h3 class="anchored" data-anchor-id="structuring-data-with-datablock-api">Structuring Data with DataBlock API</h3>
<p>We use <code>DataBlock</code> API to structure our data, making it ready for training:</p>
<div id="cell-11" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">bears <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> DataBlock(</span>
<span id="cb7-2">    blocks<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(ImageBlock, CategoryBlock), </span>
<span id="cb7-3">    get_items<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>get_image_files, </span>
<span id="cb7-4">    splitter<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>RandomSplitter(valid_pct<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>, seed<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">42</span>),</span>
<span id="cb7-5">    get_y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>parent_label,</span>
<span id="cb7-6">    item_tfms<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>Resize(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span>)</span>
<span id="cb7-7">)</span>
<span id="cb7-8"></span>
<span id="cb7-9">dls <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> bears.dataloaders(path)</span>
<span id="cb7-10">dls.valid.show_batch(max_n<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, nrows<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://buidai123.github.io/blog/posts/2024-07-07-from-notebook-to-web-app/index_files/figure-html/cell-6-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>This show a batch of images using the default resizing method. Different resizing startegies can impact the dataset in various ways.</p>
</section>
</section>
<section id="exploring-resizing-methods" class="level2">
<h2 class="anchored" data-anchor-id="exploring-resizing-methods">Exploring Resizing Methods</h2>
<p>Resizing plays a crucial role in preparing your images for model training. Let’s explore three different resizing methods:</p>
<section id="standard-resize" class="level3">
<h3 class="anchored" data-anchor-id="standard-resize">Standard Resize</h3>
<p>The standard resize method adjust the image size for model while maintaining a specific aspect rato. Here, we pad the images with zeros(black) to ensure the entire image is included:</p>
<div id="cell-14" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1">bears <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> bears.new(item_tfms<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>Resize(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span>, ResizeMethod.Pad, pad_mode<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'zeros'</span>))</span>
<span id="cb8-2">dls <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> bears.dataloaders(path)</span>
<span id="cb8-3">dls.valid.show_batch(max_n<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, nrows<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://buidai123.github.io/blog/posts/2024-07-07-from-notebook-to-web-app/index_files/figure-html/cell-7-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>This approach, padding with zeros maintains the aspect rato and ensures that the entire image fit within the frame.</p>
</section>
<section id="randomresizedcrop" class="level3">
<h3 class="anchored" data-anchor-id="randomresizedcrop">RandomResizedCrop</h3>
<p>Another effective method is <code>RandomResizedCrop</code>, which crops different parts of an image each time, providing varied views:</p>
<div id="cell-17" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1">bears <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> bears.new(item_tfms<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>RandomResizedCrop(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span>, min_scale<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>))</span>
<span id="cb9-2">dls <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> bears.dataloaders(path)</span>
<span id="cb9-3">dls.train.show_batch(max_n<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, nrows<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, unique<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://buidai123.github.io/blog/posts/2024-07-07-from-notebook-to-web-app/index_files/figure-html/cell-8-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p><code>RandomResizedCrop</code> is excellent for generating diverse training data. It explores different regions of the same image, enhancing the robustless of your model.</p>
</section>
</section>
<section id="applying-data-augmentation" class="level2">
<h2 class="anchored" data-anchor-id="applying-data-augmentation">Applying Data Augmentation</h2>
<p>Data Augmentation increases the diversity of your training data by applying various transformations, such as rotation and flipping:</p>
<div id="cell-20" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1">bears <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> bears.new(</span>
<span id="cb10-2">    item_tfms<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>Resize(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span>),</span>
<span id="cb10-3">    batch_tfms<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>aug_transforms(mult<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb10-4">)</span>
<span id="cb10-5">dls <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> bears.dataloaders(path)</span>
<span id="cb10-6">dls.train.show_batch(max_n<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>, nrows<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, unique<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://buidai123.github.io/blog/posts/2024-07-07-from-notebook-to-web-app/index_files/figure-html/cell-9-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Using <code>aug_transforms</code>, we can dynamically modify images during training. This process, called data augmentation, helps the model generalize better by exposing it to various versions of the same iamge. The <code>mult=2</code> parameter exaggerates the transformations for better visualization.</p>
</section>
<section id="traning-the-model" class="level2">
<h2 class="anchored" data-anchor-id="traning-the-model">Traning the Model</h2>
<p>With our data ready, we can proceed to train a model using a pre-trained <code>resnet18</code>:</p>
<div id="cell-23" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">bears <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> bears.new(</span>
<span id="cb11-2">    item_tfms<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>RandomResizedCrop(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">224</span>, min_scale<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>),</span>
<span id="cb11-3">    batch_tfms<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>aug_transforms()</span>
<span id="cb11-4">)</span>
<span id="cb11-5">dls <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> bears.dataloaders(path)</span>
<span id="cb11-6">learn <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> vision_learner(dls, resnet18, metrics<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>error_rate)</span>
<span id="cb11-7">learn.fine_tune(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Downloading: "https://download.pytorch.org/models/resnet18-f37072fd.pth" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth
100%|██████████| 44.7M/44.7M [00:00&lt;00:00, 131MB/s] </code></pre>
</div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train_loss</th>
<th data-quarto-table-cell-role="th">valid_loss</th>
<th data-quarto-table-cell-role="th">error_rate</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>1.010127</td>
<td>0.147814</td>
<td>0.066038</td>
<td>00:15</td>
</tr>
</tbody>
</table>
</div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train_loss</th>
<th data-quarto-table-cell-role="th">valid_loss</th>
<th data-quarto-table-cell-role="th">error_rate</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0.187663</td>
<td>0.105568</td>
<td>0.037736</td>
<td>00:13</td>
</tr>
<tr class="even">
<td>1</td>
<td>0.165576</td>
<td>0.146279</td>
<td>0.037736</td>
<td>00:13</td>
</tr>
<tr class="odd">
<td>2</td>
<td>0.140621</td>
<td>0.171902</td>
<td>0.047170</td>
<td>00:14</td>
</tr>
<tr class="even">
<td>3</td>
<td>0.117243</td>
<td>0.166212</td>
<td>0.047170</td>
<td>00:13</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>In this case we use <code>RandomSizedCrop</code> and <code>aug_transforms</code> to create robust data loaders. Traning a model for four epochs results in an error rate of under five percent-quite impressive!</p>
</section>
<section id="evaludating-the-model" class="level2">
<h2 class="anchored" data-anchor-id="evaludating-the-model">Evaludating the Model</h2>
<p>Evaluating the trained model is crucial for understanding its performance and identifying areas for improvement. Lesson 2 introduces several important techniques for model evaluation.</p>
<section id="confusion-matrix-explaination" class="level3">
<h3 class="anchored" data-anchor-id="confusion-matrix-explaination">Confusion Matrix Explaination</h3>
<p>The confusion matrix is powerful tool for examining the perfomance of classification models. It provide insights into which catetgories are commonly confused by the model:</p>
<div id="cell-26" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1">interp <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ClassificationInterpretation.from_learner(learn)</span>
<span id="cb13-2">interp.plot_confusion_matrix()</span></code></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

</div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://buidai123.github.io/blog/posts/2024-07-07-from-notebook-to-web-app/index_files/figure-html/cell-11-output-5.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The confusion matrix shows the model’s predictions against the actual labels. Here’s what it tells us:</p>
<ul>
<li>The diagonal represents correct predictions (e.g., 29 black bears correctly predicted as black bears).</li>
<li>Off-diagonal elements reveal misclassifications (e.g., 2 black bears predicted as grizzly bears)</li>
</ul>
<p>For instance, if our bear classifier mislabels a grizzly bear as a black bear, the coressponding cell in the matrix indicates how often this mistaken occurs. It’s a visual representation of “where did we go wrong?” and is essential for refining the model.</p>
<p>Jeremy pointed out that such insight help:</p>
<ul>
<li>Identify which categories are inherently difficult to distinguish(e.g., black bears and grizzly bears).</li>
<li>Understand if certain errors systematics and need targeted improvements.</li>
</ul>
<p>Here’s my model shows:</p>
<ul>
<li><strong>High accuracy in identifying grizzly bears and teddy bears</strong>: There are only a couple of misclassification.</li>
<li><strong>Some confusion between black bears and grizzly bears</strong>: This is evident from the few off-diagonal elements</li>
</ul>
<p>Understand these errors hepls us focus on areas that need more training data or better distigushing features.</p>
</section>
<section id="plot_top_losses-explaination" class="level3">
<h3 class="anchored" data-anchor-id="plot_top_losses-explaination"><code>plot_top_losses</code> Explaination</h3>
<p>The <code>plot_top_losses</code> function highlights the individual images where your model made the worst predictions:</p>
<div id="cell-29" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1">interp.plot_top_losses(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, nrows<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">17</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>))</span></code></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://buidai123.github.io/blog/posts/2024-07-07-from-notebook-to-web-app/index_files/figure-html/cell-12-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The results show specific cases where the model was either:</p>
<ul>
<li><strong>Highly confident but wrong</strong>: E.g., the model predicted “teddy” with high confidence when it actually “black”.</li>
<li><strong>Correct but not confident</strong>: E.g., predicting the right class but with low confidence.</li>
</ul>
<p>By examining these top losses, you gain insights into why the model might be confused. Here, my model misclassified a black bear as a grizzly bear with high confidence which might indicate that the features used to distigush between these classes are not prominent enough.</p>
<p>These insights can help in refining your data and possibly augmenting it to address these specific weaknesses.</p>
</section>
</section>
<section id="clean-the-data-with-imageclassifiercleaner" class="level2">
<h2 class="anchored" data-anchor-id="clean-the-data-with-imageclassifiercleaner">Clean the Data with <code>ImageClassifierCleaner</code></h2>
<p>Once we’ve evaluated our model, the next important step is data cleaning. Surprisingly, Jeremy suggests cleaning the data <strong>after</strong> training the initial model. This counterintuitive approach allows the model to highlight problematic data points</p>
<section id="imageclassifiercleaner-demonstration" class="level3">
<h3 class="anchored" data-anchor-id="imageclassifiercleaner-demonstration"><code>ImageClassifierCleaner</code> Demonstration</h3>
<p>The <code>ImageClassifierCleaner</code> widget is a fantastic tool for this purpose. It helps you manually review and clean your dataset based on the model’s predictions:</p>
<div id="cell-32" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1">cleaner <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ImageClassifierCleaner(learn)</span>
<span id="cb15-2">cleaner</span></code></pre></div>
</div>
<p><img src="https://buidai123.github.io/blog/posts/2024-07-07-from-notebook-to-web-app/cleaner_widget.png" style="width:100%;"></p>
<p>When you run this widget, it launches an interactive interface where you can:</p>
<ul>
<li><strong>Sort Image by Loss</strong>: Images are ordered by the model’s confidence, making it easy to identify incorrect or ambiguous labels.</li>
<li><strong>Correct Labels</strong>: Reassign images to the correct categories if they were mislabeled.</li>
<li><strong>Delete Incorrect Images</strong>: Remove images that don’t belong in any category.</li>
</ul>
<p>Jeremy explained how he used it to clean the bear dataset:</p>
<ul>
<li>By seleting “teddy bears”, the widget displayed all images classified as teddy bears.</li>
<li>He manually review the images, reassigning or deleting those that were incorrecly labeled.</li>
</ul>
<p>Here’s how you can apply the changes:</p>
<div id="cell-34" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1">fns <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> get_image_files(path)</span>
<span id="cb16-2"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> idx <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> cleaner.delete(): cleaner.fns[idx].unlink()</span>
<span id="cb16-3"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> idx, cat <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> cleaner.change(): shutil.move(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>(cleaner.fns[idx]), path<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>cat)</span></code></pre></div>
</div>
<p>This code snippet updates the dataset based on your interaction with the <code>ImageClassifierCleaner</code>: - <strong>Delete</strong>: Removes files marked for deletion. - <strong>Move</strong>: Reassigns files to the correct categories.</p>
</section>
<section id="why-clean-after-training" class="level3">
<h3 class="anchored" data-anchor-id="why-clean-after-training">Why Clean After Training?</h3>
<p>Cleaning the data after training might seems backward, but it has significant advantages:</p>
<ul>
<li><strong>Model-Assisted Cleaning</strong>: The initial model helps identify problematic data points that might be hard to spot manually.</li>
<li><strong>Focus on Hard Cases</strong>: The confusion matrix and top losses highlight the hight areas that need most attention, making your cleaning effort more efficient.</li>
</ul>
<p>This process ensures a high quality dataset for subsequent training iterations, leading to better model performance.</p>
<p>By incorporating thorough evaluation and cleaning steps, you refine you dataset and improve your model’s accuracy and reliability. These insights are invaluable for building robust deep learning models that perform well on real-world data.</p>
</section>
</section>
<section id="deployment-building-and-deploying-a-model" class="level2">
<h2 class="anchored" data-anchor-id="deployment-building-and-deploying-a-model">Deployment: Building and Deploying a Model</h2>
<p>After cleaning our data, the next exciting step is to put our model into production. While <a href="https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527">the book</a> introduces Voilà for creating interactive web application using Jupyter Notebooks, there’s another powerful tool that’s becoming increasingly popular: <strong>HuggingFace Spaces</strong>. Together with <strong>Gradio</strong>, they offer an intuitive and powerful way to deploy machine learning models as web applications.</p>
<section id="introducing-huggingface-spaces-and-gradio." class="level3">
<h3 class="anchored" data-anchor-id="introducing-huggingface-spaces-and-gradio.">Introducing HuggingFace Spaces and Gradio.</h3>
<p>HuggingFace Spaces is a platform that allows you to host machine learning model and their interfaces for free. On the other hand, Gradio make it easy to create customizable web interfaces with a few lines of Python code.</p>
</section>
<section id="a-shortout-of-tanishq-abraham" class="level3">
<h3 class="anchored" data-anchor-id="a-shortout-of-tanishq-abraham">A shortout of Tanishq Abraham</h3>
<p>Before diving into techinical details, let’s give a shortout to Tanishq Abraham, one of the most remarkable individuals in the fastai community. Known as a child prodigy, Tanishq has contributed immensely to the community, making complex topics accessable to everyone. I’ve learned a lot from his work and highly recommend checking out his <a href="https://www.tanishq.ai/">website</a> and his <a href="https://x.com/iScienceLuvr">Twitter</a> for more insightful resources.</p>
<p>Tanshiq has also written an exellent blog post the cover everything you need to know about using Gradio and HuggingFace Spaces. You can read his detail guide <a href="https://www.tanishq.ai/blog/posts/2021-11-16-gradio-huggingface.html">here</a></p>
</section>
<section id="setting-up-and-deploying-your-model-using-gradio-and-huggingface-spaces" class="level3">
<h3 class="anchored" data-anchor-id="setting-up-and-deploying-your-model-using-gradio-and-huggingface-spaces">Setting Up and Deploying Your Model using Gradio and HuggingFace Spaces</h3>
<p>To deploy our model, we’ll use HuggingFace Spaces. The set up process is straightforward and free of charge. Follow these step to get started:</p>
<p><strong>Step 1: Sign Up and Create a New Space</strong></p>
<ol type="1">
<li><p>Go to the <a href="https://huggingface.co/spaces">HuggingFace Spaces page</a> and sign up for an account if you haven’t already.</p></li>
<li><p>Click “Create a new space”.</p></li>
<li><p>Give you space a name and chose a template (you can start with Gradio template).</p></li>
</ol>
<p>Congrats! You’ve created a new space. Now, what’s next?</p>
<p><strong>Step 2: Getting Familiar with Git</strong></p>
<p>HuggingFace Spaces works through Git, which many developers are already familiar with. Using Git is also a good practice, and Jeremy recommends using <a href="https://desktop.github.com/download/">Github Desktop</a> and WSL2. Refer to the guide for <a href="https://learn.microsoft.com/en-us/windows/wsl/install">WSL2 installation</a> to get started.</p>
<p><strong>Cloning the Repository</strong></p>
<p>To start working on HuggingFace Spaces you need to clone the repository locally, you have two options for cloning: <strong>HTTPs</strong> and <strong>SSH</strong></p>
<table class="table-striped table-hover caption-top table">
<colgroup>
<col style="width: 17%">
<col style="width: 41%">
<col style="width: 41%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Pros / Cons</th>
<th style="text-align: center;">HTTPS</th>
<th style="text-align: center;">SSH</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Pros</strong></td>
<td style="text-align: center;">Easier for beginners, no SSH key setup required</td>
<td style="text-align: center;">More secure, no need to enter credentials each time</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Cons</strong></td>
<td style="text-align: center;">Requires authentication each time you push</td>
<td style="text-align: center;">Requires SSH key setup</td>
</tr>
</tbody>
</table>
<p>Since I’m using SSH, if you follow along please make sure your SSH key is properly set up in your HuggingFace Spaces user setting</p>
<div class="sourceCode" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb17-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Clone the repository using SSH</span></span>
<span id="cb17-2"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">git</span> clone git@huggingface.co:USERNAME/YOUR_REPO_NAME.git</span>
<span id="cb17-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">cd</span> YOUR_REPO_NAME</span></code></pre></div>
<p><strong>Step 3: Prepare Your Model</strong></p>
<p>Make sure to export your trained model from the notebook:</p>
<div id="cell-37" class="cell">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># in your bear classifier notebook</span></span>
<span id="cb18-2">learn.export(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"model.pkl"</span>)</span></code></pre></div>
</div>
<p>This saves the trained model as <code>model.pkl</code>, which you’ll need for the deployment.</p>
<p><strong>Step 4: Building the Gradio Interface</strong></p>
<p>Gradio makes it easy to build an interactive interface. You can use Jupyter Notebook for experimentation and then use <code>nbdev.export.nb_export</code> to convert the notebook into Python script. This tool is very handly for such conversions. Alright, but first make sure you run <code>pip install gradio</code> in your terminal if you haven’t already.</p>
<p><strong>1. Import Required Libraries</strong>:</p>
<div id="cell-39" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> gradio <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> gr</span>
<span id="cb19-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> fastai.vision.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">all</span> <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span></span></code></pre></div>
</div>
<p>And so we can create a python image library image from that black bear</p>
<div id="cell-41" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1">im <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> PILImage.create(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'black.jpg'</span>)</span>
<span id="cb20-2">im.thumbnail((<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">192</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">192</span>))</span>
<span id="cb20-3">im</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<div>
<figure class="figure">
<p><img src="https://buidai123.github.io/blog/posts/2024-07-07-from-notebook-to-web-app/index_files/figure-html/cell-17-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Turn it into silghtly smaller one so it doesn’t overwhelm my whole screen and there’s is a picture of a black bear so we will use it for experimenting</p>
<p><strong>2. Load the Model</strong>:</p>
<div id="cell-43" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1">learn <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> load_learner(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'model.pkl'</span>)</span></code></pre></div>
</div>
<p>One of the methods that the learner has is a <code>predict</code> method</p>
<div id="cell-45" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1">learn.predict(im)</span></code></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

</div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>('black', tensor(0), tensor([9.9995e-01, 4.9545e-05, 3.8379e-06]))</code></pre>
</div>
</div>
<p>So if you run it, you can see, even on a laptop, it’s basically instant. It took a really short time to figure out this is a black bear</p>
<p><strong>3. Define the Prediction Function</strong>:</p>
<p>Gradio requires us to give it a function that it’s going to call but first we need to know what labels do we have?</p>
<div id="cell-48" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1">learn.dls.vocab</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>['black', 'grizzly', 'teddy']</code></pre>
</div>
</div>
<div id="cell-49" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb26" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#create our categories</span></span>
<span id="cb26-2">categories <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> learn.dls.vocab</span></code></pre></div>
</div>
<p>So here’s our function:</p>
<div id="cell-51" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb27" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> classify_image(img):</span>
<span id="cb27-2">    img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> PILImage.create(img)</span>
<span id="cb27-3">    pred, idx, probs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> learn.predict(img)</span>
<span id="cb27-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">dict</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">zip</span>(categories, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">map</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>, probs)))</span></code></pre></div>
</div>
<p>So we called <code>predict</code> and that returns three things: the prediction as a string, the index of that, and the probabilities of whether it’s black or grizzly or teddy bear. And what Gradio wants is it wants to get back a dictionary containing each of the possible categories-which is in this case grizzly, black and teddy bear-and the probabilities of each one.</p>
<p><strong>4. Create the Gradio Interface</strong>:</p>
<div id="cell-54" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb28" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1">image <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> gr.Image(height<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">512</span>, width<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">512</span>)</span>
<span id="cb28-2">labels <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> gr.Label()</span>
<span id="cb28-3">examples <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"grizzly.jpg"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"black.jpg"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"teddy.jpg"</span>]</span>
<span id="cb28-4"></span>
<span id="cb28-5">intf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> gr.Interface(fn<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>classify_image, inputs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>image, outputs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>labels, examples<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>examples)</span>
<span id="cb28-6">intf.launch(inline<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span></code></pre></div>
</div>
<pre><code>Running on local URL:  http://127.0.0.1:7860
To create a public link, set `share=True` in `launch()`.</code></pre>
<p><img src="https://buidai123.github.io/blog/posts/2024-07-07-from-notebook-to-web-app/huggingfacespaces.png" class="img-fluid"></p>
<p>This code creates a simple Gradio interface where user can upload images, and the model will predict whether it’s a grizzly, black or teddy bear. you can run the interface inline in you Jupyter Notebook for testing.</p>
<p><strong>Step 5: Export the Notebook to a Python script</strong>:</p>
<p>We will use nbdev to convert the Jupyter Notebook to a Python script.</p>
<p><strong>1. Add Metadata and Export Tags</strong>:</p>
<pre><code>- Add `#| default_exp app` to the first cell
- Add `#| export` to every cell you want to convert</code></pre>
<p><strong>2. Run the Conversion</strong>:</p>
<div id="cell-57" class="cell">
<div class="sourceCode cell-code" id="cb31" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> nbdev.export</span>
<span id="cb31-2">nbdev.export.nb_export(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'app.ipynb'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'.'</span>)</span></code></pre></div>
</div>
<p>This command converts the notebook <code>app.ipynb</code> to a python script <code>app.py</code>.</p>
<p><strong>Step 6: Push Your Changes to HuggingFace Spaces</strong>:</p>
<p>Handle large files like <code>model.pkl</code> uising <a href="https://git-lfs.com/">Git LFS</a> (Large File Storage).</p>
<p><strong>1. Set up Git LFS</strong>:</p>
<div id="cell-59" class="cell" data-vscode="{&quot;languageId&quot;:&quot;shellscript&quot;}">
<div class="sourceCode cell-code" id="cb32" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1">git lfs install</span>
<span id="cb32-2">git lfs track <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"*.pkl"</span></span>
<span id="cb32-3">git add .gitattributes</span>
<span id="cb32-4">git commit <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>m <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Track .pkl files with Git LFS"</span> <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># To be honest, when initializing git lfs, the .gitattributes already supports .pkl files, and in my repo when initialized, it already had .gitattributes file. I don't know why, but I didn't need to commit it anyway, but I still write it here for the sake of completeness. 😉</span></span></code></pre></div>
</div>
<p><strong>2. Commit and Push Your Changes</strong>:</p>
<div id="cell-61" class="cell" data-vscode="{&quot;languageId&quot;:&quot;shellscript&quot;}">
<div class="sourceCode cell-code" id="cb33" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1">git add .</span>
<span id="cb33-2">git commit <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>m <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Deploy bear classifier with Gradio interface🐻🎉"</span></span>
<span id="cb33-3">git push</span></code></pre></div>
</div>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>By following this guide, you’ve successfully built and deployed your bear classifier using Gradio and HuggingFace Spaces. This powerful combination not only make you model accessible for everyone through the user-friendly web interface but also leverages cutting-edge tool to ensure it easy to maintain and extend.</p>
<p>Deploying machine learning models in real-world applications is an exciting milestone. It transform your hard work and complex algorithms into actionable insights and tools that can be used by anyone, anywhere. Whether you’re a beginner or an experienced partitioner, the ability to take a model from Jupyter Noteook to a live web app is an invaluable skill in today’s AI driven world.</p>
<p>In case you want to explore more about Gradio and HuggingFace, here are some valuable resources:</p>
<ul>
<li><strong>Gradio Documentation</strong>: For more on Gradio, refer to the <a href="https://www.gradio.app/docs/python-client/introduction">Gradio documentation</a>.</li>
<li><strong>My HuggingFace Spaces Bear Classifier</strong>: Checkout my deployed bear classifier on HuggingFace Spaces <a href="https://huggingface.co/spaces/TheMonarch/bear_classifier">here</a>.</li>
<li><strong>Tanishq Abraham’s Blog</strong>: For an in-depth look at deploying model using Gradio and HuggingFace Spaces, make sure to read Tanishq’s excellent <a href="https://www.tanishq.ai/blog/posts/2021-11-16-gradio-huggingface.html">blog post</a>.</li>
</ul>
<p>Although I initially planned to look into the HuggingFace Spaces API and deploying your own web app via JavasScript, sometimes technical hitches happend. Whether it’s a client-side issue or just part of the learning curve, don’t let it discourage you. Every challenge is learning opportunity, and with the fast-placed advencements in AI and deployment tools, there’s always something new and exciting around the corner.</p>
<p>Thank you for joining me on this journey to bring deep learning model to life. Embrace the power of open-source tools, keep experimenting, and never stop learing. Happy coding, and may your models alwasy be accurate!</p>


</section>

 ]]></description>
  <category>blogging</category>
  <category>fastai</category>
  <category>huggingface spaces</category>
  <category>gradio</category>
  <guid>https://buidai123.github.io/blog/posts/2024-07-07-from-notebook-to-web-app/</guid>
  <pubDate>Sat, 06 Jul 2024 17:00:00 GMT</pubDate>
  <media:content url="https://buidai123.github.io/blog/posts/2024-07-07-from-notebook-to-web-app/huggingfacespaces.png" medium="image" type="image/png" height="80" width="144"/>
</item>
<item>
  <title>First Step in AI: My Experience with fast.ai Lesson 1</title>
  <dc:creator>Bui Huu Dai</dc:creator>
  <link>https://buidai123.github.io/blog/posts/2024-06-30-your-deep-learning-journey/</link>
  <description><![CDATA[ 





<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Welcome to my deep dive in to the world of deep learning! In this blog post, I’ll be sharing my journey through <a href="https://course.fast.ai/Lessons/lesson1.html">the first lession</a> of fast.ai course an acclaimed program that makes learning AI accessible and enjoyable.</p>
<p>Fast.ai was created with the goal of making deep learning understandable for everyone, no matter their background, and Lesson 1 accomplishes that by having us build a simple yet fascinating model: a bird classifier. this exciting task not just introduces me to the basics of deep learning but also alow me to experience firsthand the power and simplicity of modern AI tools.</p>
<p>Join me as I walk you though key conccept covered in the Lesson 1, from understanding how images are processed by computers to trainning and validating our model. I will also share some personal insights and reflections on the learning process, aiming to make this technicial journey both infomative and relatable.</p>
<p>Whether you are a bigginer in AI or someone looking for refresh your knowledge, I hope this post inspires and guides you in your own deep learning</p>
</section>
<section id="the-xkcd-joke-and-debunking-deep-learning-myths" class="level2">
<h2 class="anchored" data-anchor-id="the-xkcd-joke-and-debunking-deep-learning-myths">The XKCD Joke and Debunking Deep Learning Myths</h2>
<style>
    figure {
        display: block;
        margin-left: auto;
        margin-right: auto;
        text-align: center;
    }
</style>
<figure class="figure">
<img src="https://buidai123.github.io/blog/posts/2024-06-30-your-deep-learning-journey/xkcd.png" alt="XKCD joke" style="width:50%;" class="figure-img">
<figcaption>
XKCD Joke
</figcaption>
</figure>
<p>Jeremy Howard kicked off the lesson with relatable XKCD Joke about how in 2015, detecting a bird in a photo was seen as a challenging task, almost a joke. Fast forward to today, and we can build such as system in mere minutes, showcasing how far deep learning has come.</p>
<p>Many people believe that diving into deep learning requires extensive mathematical knowledge, huge datasets, and expensive hardware. However, these myths are far from the truth.</p>
<table class="table-striped table-hover caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 66%">
</colgroup>
<thead>
<tr class="header">
<th>Myth(Don’t need)</th>
<th>Truth</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Lots of math</td>
<td>Just high school math is sufficient</td>
</tr>
<tr class="even">
<td>Lots of data</td>
<td>We’ve seen record-breaking results with fewer than 50 items of data</td>
</tr>
<tr class="odd">
<td>Lots of expensive computer</td>
<td>You can perform state-of-the-art work with hardwere available for free of minimal cost</td>
</tr>
</tbody>
</table>
</section>
<section id="top-down-learning-approach" class="level2">
<h2 class="anchored" data-anchor-id="top-down-learning-approach">Top-Down Learning Approach</h2>
<p>One of the most refreshing aspects of fastai course is its top-down teaching approach. Traditional education often starts with the basics and slowly builds up to more complex topics. However, Jeremy Howard and Rachel Thomas believe that learning is more effective when you see the big picture first.</p>
<p>In the fastai course, we start by building practicall applications from lesson one, allowing us to see immediate results and understanding the relevance of what we are doing. This approach mirrors how we learn many real-word skills, such as sport or cooking, where we start by trying out the activity and learn the details as needed.</p>
<p>By diving straight into creating a deep learning model, we get hands-on experience early on, which helps solidify our understanding and maintain our interest. As we process though the course, we gradually delve deeper into the underlying principles and theories, bulding a robust foundation along the way</p>
</section>
<section id="understanding-deep-learning" class="level2">
<h2 class="anchored" data-anchor-id="understanding-deep-learning">Understanding Deep learning</h2>
<p>Deep learning is a technique for extracting and transforming data, with application ranging from speech recognition to image classification. It uses multiple layer of neural networks, where each layer refines the data received from the previous one. These layers are trained using the algorithms that minimize the errors and improve accuracy, enabling the network to learn specific tasks.</p>
<p>Deep learning’s power, flexibility, and simplicity make it applicable across various field, including social science, medicine, finance, and more. For instance, despite lacking of medical background, Jeremy Howard founded <a href="">Enlitic</a>, a company leveraging deep learning to diagnose illnesses. Within months, their algorithm was more effective at identifying malignant tumors than radiologists.</p>
<p>Here are some areas where deep learing excels:</p>
<ul>
<li><strong>Natural Language Processing (NLP)</strong>: Answering question, speech recognition, document summarization, and more.</li>
<li><strong>Computer Vision</strong>: Interpreting satellite images, face recognition, and automous vehicle navigation.</li>
<li><strong>Medicine</strong>: Analyzing radiology images, measuring features and medical scans, and diagnosing diseases.</li>
<li><strong>Biology</strong>: Protein folding, genomics tasks, and cell classification.</li>
<li><strong>Image Generation</strong>: Colorizing images, enhancing resolution, and converting images to artistic style.</li>
<li><strong>Recommendation System</strong>: Web search optimization, product recommendations, and personalized content layout.</li>
<li><strong>Gaming</strong>: Mastering games like Chess, Go, and various video games.</li>
<li><strong>Robotics</strong>: Handling challenging objects and complex manipulation tasks.</li>
<li><strong>Other</strong>: Financial forecasting, text-to-speech conversion, and much more.</li>
</ul>
<p>The versatility of deep learning lies in its foundation: neuron networks.</p>
</section>
<section id="a-brief-history-of-deep-learning" class="level2">
<h2 class="anchored" data-anchor-id="a-brief-history-of-deep-learning">A Brief History of Deep Learning</h2>
<style>
    figure {
        display: block;
        mergin-left: auto;
        mergin-right: auto;
        text-align: center;
    }
</style>
<figure class="figure">
<img src="https://buidai123.github.io/blog/posts/2024-06-30-your-deep-learning-journey/neurral_net.png" alt="Biological Neurons vs. Artificial Neural Network" style="width:50%;" class="figure-img">
<figcaption>
Biological Neurons vs.&nbsp;Artificial Neural Network
</figcaption>
</figure>
<p>Deep learning draws inspiration from human brain’s neural network. The concept of neural network isn’t new; it dates back to 1957 with the creation of the first neural network. The fundamental ideas remain the same today, but advances in hardware and data availability have significantly propelled the field forward.</p>
</section>
<section id="the-sofware-pytorch-fastai-and-jupyter" class="level2">
<h2 class="anchored" data-anchor-id="the-sofware-pytorch-fastai-and-jupyter">The Sofware: Pytorch, Fastai, and Jupyter</h2>
<p>At fastai, after extensive testing of various machine learning packages and languages, they decided to adopt Pytorch in 2017 for their course, software development, and research. Pytorch has become the fastest-growing deep learning library and is widely used in academic research and industry. Its flexibiligy and expressiveness make it an excellent foundation for deep learning.</p>
<p>The fastai library builds on top of Pytorch, provide high-level functionality for deep learning. This layered architecture allows for a seemless learning experience, make it easier to understand both high-level concepts and low-level operations.</p>
<p>However, the specific software you use a less important than understanding the core principles and techniques of deep learning. Learning to trasition between the libraries is relatively quick, but mastering deep learning foundation is crucial.</p>
<p>Jupyter notebook, a powerful and reflexible tool for data science, will be our primary platform for experimentation. Its interation with fastai and Pytorch makes it ideal for developing and testing deep learning model.</p>
<p>Ready to see it in action? Let’s train our first model!</p>
</section>
<section id="exploring-the-is-it-a-bird-classifier" class="level2">
<h2 class="anchored" data-anchor-id="exploring-the-is-it-a-bird-classifier">Exploring the “Is it a Bird?” Classifier</h2>
<p>One of the most exciting part of Lesson 1 was building our own image classifier to determine whether the a given image contains a bird. For this project, we used the fastai libray along with pre-trained model to quickly and efficiently create our classifier. Let’s dive into the code walkthrouh.</p>
<p>The basic steps we’ll need to do:</p>
<ol type="1">
<li>Use DuckDuckGo for search images of “bird photos”</li>
<li>Use DuckDuckGo to search for images of “forest photos”</li>
<li>Fine-tune a pretrained neural network to recognise these two groups</li>
<li>Try running this model on a picture of bird and see if it works.</li>
</ol>
<section id="searching-for-images-duckduckgo-search" class="level3">
<h3 class="anchored" data-anchor-id="searching-for-images-duckduckgo-search">Searching for images: DuckDuckGo Search</h3>
<p>Instead of using a big search that reqires an API key, we opted to DuckDuckGo, which doesn’t reqire an API key for image searches. This make the setup simpler and faster.</p>
<p>But make sure you run this command in your terminal before run the code to update duckduckgo</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">pip</span> install <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-Uqq</span> fastai duckduckgo_search</span></code></pre></div>
<div id="cell-9" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> duckduckgo_search <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> DDGS</span>
<span id="cb2-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> fastcore.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">all</span> <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span></span>
<span id="cb2-3"></span>
<span id="cb2-4">ddgs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> DDGS()</span>
<span id="cb2-5"></span>
<span id="cb2-6"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> search_images(term, max_images<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>):</span>
<span id="cb2-7">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Searching for '</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>term<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'"</span>)</span>
<span id="cb2-8">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> L(ddgs.images(keywords<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>term, max_results<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>max_images)).itemgot(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'image'</span>)</span></code></pre></div>
</div>
<div id="cell-10" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">urls <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> search_images(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'bird photos'</span>, max_images<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb3-2">urls[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Searching for 'bird photos'</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>'https://images.pexels.com/photos/326900/pexels-photo-326900.jpeg?cs=srgb&amp;dl=wood-flight-bird-326900.jpg&amp;fm=jpg'</code></pre>
</div>
</div>
<p>Jeremy Howard mentioned that using <code>import *</code> in Jupyter notebooks is not the big deal because Jupyter only import what we use. This approach simplifies the code and keeps it clean.</p>
<p>Here’s the quick explaination of the functions and libraries used in this snippet:</p>
<p><code>DDGS</code> from <code>duckduckgo_search</code>:</p>
<ul>
<li><code>duckduckgo_search</code>: This library allows us to search for iamges using DuckDuckGo without the need for an API key. So no more begging Google for an API key.</li>
<li><code>DDGS</code>: The class that does the heavy lifting of searching for images.</li>
</ul>
<p><code>fastcore</code>: - <code>fastcore</code>: A foundattional library that make Python feel like a Lamborghini-sleek, powerfull, and fast.</p>
<p><code>L</code>:</p>
<ul>
<li><code>L</code>: A magical list from <code>fastcore</code> that does way more than the regular Python list. Think of it as a list on steroids.</li>
</ul>
<p>In our example, <code>search_images</code> is a function that performs an image search using DuckDuckGo. It’s print out the search term being used and return a list of images URLs retrieved from the search results.</p>
<p>for more details on the tools, you can refer to the <a href="https://fastcore.fast.ai/">fastcore documentation</a> and the <a href="https://pypi.org/project/duckduckgo-search/">duckduckgo_search documentation</a>.</p>
<div id="cell-12" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> fastdownload <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> download_url</span>
<span id="cb6-2">dest <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'bird.jpg'</span></span>
<span id="cb6-3">download_url(urls[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], dest, show_progress<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb6-4"></span>
<span id="cb6-5"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> fastai.vision.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">all</span> <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span></span>
<span id="cb6-6">im <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Image.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">open</span>(dest)</span>
<span id="cb6-7">im.to_thumb(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">256</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">256</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<div>
<figure class="figure">
<p><img src="https://buidai123.github.io/blog/posts/2024-06-30-your-deep-learning-journey/index_files/figure-html/cell-4-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-13" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">download_url(search_images(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'forest photos'</span>, max_images<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'forest.jpg'</span>, show_progress<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb7-2">Image.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">open</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'forest.jpg'</span>).to_thumb(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">256</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">256</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Searching for 'forest photos'</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="6">
<div>
<figure class="figure">
<p><img src="https://buidai123.github.io/blog/posts/2024-06-30-your-deep-learning-journey/index_files/figure-html/cell-5-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p><code>fastdownload</code> and <code>download_url</code>:</p>
<ul>
<li><code>fastdownload</code>: Think of this as your friendly neighborhood delivery service, but for files. It’s help with downloading files and datasets easier.</li>
<li><code>download_url</code>: A function that fetches the file you need from a URL. In our case, it says “Hey URL, gimme that picture!” and save it as <code>bird.png</code></li>
</ul>
<p><code>fastai.vision.all</code>:</p>
<ul>
<li>TThis module from the fastai library is like a Swiss Army knife for vision tasks, providing all the tools you need, from data loaders to model training utilities.</li>
</ul>
<p><code>to_thumb</code>: - A method from the <code>PIL.Image</code> class, which is quite handy it resizes an image to a thumbnail while maintaining the aspect rato. Kind of like shrinking your favourate sweater but in a good way</p>
<p>These libraries and function streamline the process of getting and preparing the images for our model. For more detailed documentation, you can refer to the <a href="https://fastdownload.fast.ai/">fastdownload</a>, <a href="https://docs.fast.ai/tutorial.vision.html">fastai vision</a>, and <a href="https://pillow.readthedocs.io/en/stable/">Pillow</a> documentation.</p>
</section>
<section id="downloading-and-preparing-images" class="level3">
<h3 class="anchored" data-anchor-id="downloading-and-preparing-images">Downloading and Preparing Images</h3>
<p>To build our dataset, we need to download images for the categories we are interested in (‘forest’ and ‘bird’). Here’s how we did it:</p>
<div id="cell-16" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1">searches <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'forest'</span>,<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'bird'</span></span>
<span id="cb9-2">path <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Path(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'bird_or_not'</span>)</span>
<span id="cb9-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> time <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> sleep</span>
<span id="cb9-4"></span>
<span id="cb9-5"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> o <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> searches:</span>
<span id="cb9-6">    dest <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (path<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>o)</span>
<span id="cb9-7">    dest.mkdir(exist_ok<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, parents<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb9-8">    download_images(dest, urls<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>search_images(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>o<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> photo'</span>))</span>
<span id="cb9-9">    sleep(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Pause between searches to avoid over-loading server</span></span>
<span id="cb9-10">    download_images(dest, urls<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>search_images(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>o<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> sun photo'</span>))</span>
<span id="cb9-11">    sleep(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)</span>
<span id="cb9-12">    download_images(dest, urls<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>search_images(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>o<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> shade photo'</span>))</span>
<span id="cb9-13">    sleep(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)</span>
<span id="cb9-14">    resize_images(path<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>o, max_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">400</span>, dest<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>path<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>o)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Searching for 'forest photo'
Searching for 'forest sun photo'
Searching for 'forest shade photo'
Searching for 'bird photo'
Searching for 'bird sun photo'
Searching for 'bird shade photo'</code></pre>
</div>
</div>
<p><code>Path</code>:</p>
<ul>
<li><code>Path</code>: An object-oriented way to work with filesystem paths. It makes handling files and directories as easy as pie.</li>
</ul>
<p><code>download_images</code>:</p>
<ul>
<li><code>download_images</code>: This function fetches a bunch of images from the internet and saves them in a specified directory. Like ordering a pizza, but instead of pizza, you get pictures.</li>
</ul>
<p><em>Pausing Between Searches</em>:</p>
<ul>
<li>Pausing between searches (<code>sleep(10)</code>) is important to avoid overloading the server. Think of it as giving the server a coffee break between each request.</li>
</ul>
<p><code>resize_images</code>:</p>
<ul>
<li><code>resize_images</code>: A function from fastai that resizes images to a maximum specified size. This is useful for ensuring all images are of a consistent size before training the model.</li>
</ul>
<p>For more details on these tools, you can refer to the <a href="https://docs.python.org/3/library/pathlib.html">pathlib</a>, <a href="https://docs.fast.ai/vision.utils.html">Vision utils</a> documentation.</p>
</section>
<section id="verifying-and-leaning-images" class="level3">
<h3 class="anchored" data-anchor-id="verifying-and-leaning-images">Verifying and Leaning Images</h3>
<p>After download images, it’s enssential to verify them and remove corrupt or invalid images.</p>
<div id="cell-19" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">failed <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> verify_images(get_image_files(path))</span>
<span id="cb11-2">failed.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">map</span>(Path.unlink)</span>
<span id="cb11-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(failed)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>0</code></pre>
</div>
</div>
<p><code>verify_images</code>:</p>
<ul>
<li><code>verify_images</code>: Think of this as the bouncer for your image dataset, checking IDs to make sure no bad images get through.</li>
</ul>
<p><code>get_image_file</code>:</p>
<ul>
<li><code>get_image_file</code>: This function grabs all image paths in a directory. It’s like having someone fetch all your misplaced socks in the laundry room.</li>
</ul>
<p><code>Path.unlink</code>:</p>
<ul>
<li><code>Path.unlink</code>: A method to delete files. This is how we get rid of the bad apples in the bunch.</li>
</ul>
<p>Fortunately, in my case, all downloaded images were valid, so <code>len(failed)</code> return <code>0</code>–no bad apples in our dataset!</p>
</section>
<section id="the-datablock-api" class="level3">
<h3 class="anchored" data-anchor-id="the-datablock-api">The DataBlock API</h3>
<p>Creating our data loarder is a critical step. The <code>DataBlock</code> API in fastai allows us to define how to transform and manage our data easily.</p>
<div id="cell-22" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1">dls <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> DataBlock(</span>
<span id="cb13-2">    blocks<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(ImageBlock, CategoryBlock), </span>
<span id="cb13-3">    get_items<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>get_image_files, </span>
<span id="cb13-4">    splitter<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>RandomSplitter(valid_pct<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>, seed<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">42</span>),</span>
<span id="cb13-5">    get_y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>parent_label,</span>
<span id="cb13-6">    item_tfms<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[Resize(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">192</span>, method<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'squish'</span>)]</span>
<span id="cb13-7">).dataloaders(path, bs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32</span>)</span>
<span id="cb13-8">    </span>
<span id="cb13-9">dls.show_batch(max_n<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://buidai123.github.io/blog/posts/2024-06-30-your-deep-learning-journey/index_files/figure-html/cell-8-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Here’s the breakdown of the arguments in <code>DataBlock</code>:</p>
<p><code>blocks</code>:</p>
<ul>
<li>Specifies the type of inputs and targets. In our case, we have images (<code>ImageBlock</code>) and categories (<code>CategoryBlock</code>). It’s like saying, “I have pictures of cats and dogs”</li>
</ul>
<p><code>get_items</code>:</p>
<ul>
<li>Function to get the list of items. Here we’re using <code>get_image_file</code> to retrieve all our iamge files.</li>
</ul>
<p><code>splitter</code>:</p>
<ul>
<li>Defines how to split the dataset into training and validation sets. <code>RandomSplitter(valid_pct=0.2, seed=42)</code> means 20% of the data will be used for validation. The <code>seed</code> ensures that every time we run the code we get the same split. Think of like setting your DVR to record your favourate show at the same time everyweek.</li>
</ul>
<p><code>get_y</code>:</p>
<ul>
<li>Function to get the target label from each item. We use <code>parent_label</code> to get the label from parent directory name (e.g., ‘forest’ or ‘bird’)</li>
</ul>
<p><code>item_tfms</code>:</p>
<ul>
<li>item transformation to apply. We use <code>Resize(129, method='squish')</code> to resize images to 129x129 pixels by squishing them if necessary.</li>
</ul>
<p><code>dataloaders</code>:</p>
<ul>
<li>Creates the data loaders for our dataset, with a batch size of 32. Data loaders are like conveyor belt that feed the data into your model in manageable chunks.</li>
</ul>
<p>The <code>show_batch</code> method is handy way to visialize a batch of data items. It’s like a quick preview to make sure everything looks good.</p>
<p>For more details, checkout the fastai <a href="https://docs.fast.ai/data.block.html">DataBlock API documentation</a>.</p>
</section>
<section id="training-the-model-welcome-to-the-learner-world" class="level3">
<h3 class="anchored" data-anchor-id="training-the-model-welcome-to-the-learner-world">Training the Model: Welcome to the Learner World</h3>
<p>After preparing our dataset, it’s time to train our model. We use the <code>vision_learner</code> function to setup a learner and the powerful <code>fine_tune</code> method to train the model.</p>
<div id="cell-25" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1">learn <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> vision_learner(dls, resnet18, metrics<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>error_rate)</span>
<span id="cb14-2">learn.fine_tune(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Downloading: "https://download.pytorch.org/models/resnet18-f37072fd.pth" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth
  0%|          | 0.00/44.7M [00:00&lt;?, ?B/s]100%|██████████| 44.7M/44.7M [00:00&lt;00:00, 145MB/s] </code></pre>
</div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train_loss</th>
<th data-quarto-table-cell-role="th">valid_loss</th>
<th data-quarto-table-cell-role="th">error_rate</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>1.120399</td>
<td>1.209828</td>
<td>0.411765</td>
<td>00:01</td>
</tr>
</tbody>
</table>
</div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train_loss</th>
<th data-quarto-table-cell-role="th">valid_loss</th>
<th data-quarto-table-cell-role="th">error_rate</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0.185352</td>
<td>0.054729</td>
<td>0.029412</td>
<td>00:01</td>
</tr>
<tr class="even">
<td>1</td>
<td>0.102830</td>
<td>0.023147</td>
<td>0.000000</td>
<td>00:01</td>
</tr>
<tr class="odd">
<td>2</td>
<td>0.072183</td>
<td>0.049310</td>
<td>0.029412</td>
<td>00:01</td>
</tr>
</tbody>
</table>
</div>
</div>
<p><code>vision_learner</code>:</p>
<ul>
<li>This create a learner object that combines our data loaders(<code>dls</code>) and a pre-trained model(<code>resnet18</code>). We basically saying, “Hey, take this data and use this model to learn from it.”</li>
</ul>
<p><code>resnet18</code>:</p>
<ul>
<li>A specific architecture of a Convolutional Neuron Network that’s been pre-trained on a large dataset. Think of it as seasoned detective who’s seen it all and just need to be briefed on this specific case.</li>
</ul>
<p><code>metrics=error_rate</code>:</p>
<ul>
<li>This specifies that we want to use the error rate as a metric to evaluate our model’s performance. It’s like having a scoreboard to keep track of who’s winning.<br>
</li>
</ul>
<p><code>fine_tune(3)</code>:</p>
<ul>
<li>Here’s where the magic happens. Unlike the traditional <code>fit</code> mothod, <code>fine_tune</code> starts by refining the pre-trained model with our specific data. It’s like taking your detective and train them on a nuances of this particular mystery. The <code>3</code> indicates the number of epochs (full cycles throught the training data).</li>
</ul>
<p>The <code>fine_tune</code> method is particularly powerful because it starts with a model that already knows a lot (thanks to pre-training) and fine-tune it to specific task. This approach often yields better results, faster and with less data, compared to tranning a model from scratch.</p>
</section>
<section id="making-predictions" class="level3">
<h3 class="anchored" data-anchor-id="making-predictions">Making Predictions</h3>
<p>Finally, let’s make our bird classifier predict whether or not an image contain a bird.</p>
<div id="cell-28" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1">is_bird,_,probs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> learn.predict(PILImage.create(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'bird.jpg'</span>))</span>
<span id="cb16-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"This is a: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>is_bird<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">."</span>)</span>
<span id="cb16-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Probability it's a bird: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>probs[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.4f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

</div>
<div class="cell-output cell-output-stdout">
<pre><code>This is a: bird.
Probability it's a bird: 0.9988</code></pre>
</div>
</div>
<p><code>PILImage.create</code>:</p>
<ul>
<li>This function create a image object from a file. It’s like saying “Hey, look at this picture I just took.”</li>
</ul>
<p><code>learn.predict</code>:</p>
<ul>
<li><p>This method uses our train model to predict what’s in a image. It’s like asking your well-trained detective, “What do you see in this picture?”</p></li>
<li><p>The method returns three values:</p>
<ul>
<li><code>is_bird</code>: The predicted label(whether it’s a bird or not).</li>
<li><code>probs</code>: The probabilites associated with each class.</li>
</ul></li>
</ul>
<p>When we print out the predicted label and the probability. If the model says it’s a bird with a high probability, you can feel pretty confident your model knows its bird!</p>
<p>Building the “Is it a Bird?” classifier was hands-on way to introduce the principles of deep learning. By leveraging fastai and Pytorch, we could quickly create an effective model with minimal code. This approach of starting with practical, top-down learning ensures that we see immediately results and understand the real world applicability of deep learning from the get-go.</p>
</section>
</section>
<section id="what-is-machine-learning" class="level2">
<h2 class="anchored" data-anchor-id="what-is-machine-learning">What Is Machine Learning</h2>
<p>Ah, the age-old question: What is the machine learning? Well, imagine if your computer was a child, and you were its teacher. Instead of giving it a a strict set of rules to follow(which, let’s be honest, kids hate), you give it examples from which it can learn. In enssence, machine learning is about enabling computer to learn from data rather than being explicitly programmed. It’s like teaching your computer how to ride a bike by letting it practice, fall and get up again, rather than reading it a manual</p>
<p>Let’s take a closser look at this with a series of visualizations:</p>
<section id="traditional-programming" class="level3">
<h3 class="anchored" data-anchor-id="traditional-programming">Traditional Programming</h3>
<p>In traditional Programming we write explicit instructions-a program-that processes input to procude results.</p>
<div id="cell-33" class="cell" data-execution_count="2">
<div class="cell-output cell-output-display" data-execution_count="2">
<div>
<figure class="figure">
<p><img src="https://buidai123.github.io/blog/posts/2024-06-30-your-deep-learning-journey/index_files/figure-html/cell-11-output-1.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Think of it as following a recipe step-by-step: preheat the oven, mix the ingredients, bake for 30 minutes, and volià, you have a cake.</p>
</section>
<section id="program-using-weight-and-assignment" class="level3">
<h3 class="anchored" data-anchor-id="program-using-weight-and-assignment">Program Using Weight And Assignment</h3>
<p>In machine learning, we use model with weights(parameters) that processes inputs to generates result.</p>
<div id="cell-35" class="cell" data-execution_count="4">
<div class="cell-output cell-output-display" data-execution_count="4">
<div>
<figure class="figure">
<p><img src="https://buidai123.github.io/blog/posts/2024-06-30-your-deep-learning-journey/index_files/figure-html/cell-12-output-1.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Here, the model is like a reflexible recipe that can adjust itself. The ingredients(inputs) are mixed differently depending on the weights, and the ouput is a delicious result that varies based on those adjustments.</p>
</section>
<section id="training-a-machine-learning-model" class="level3">
<h3 class="anchored" data-anchor-id="training-a-machine-learning-model">Training a Machine Learning Model</h3>
<p>Training a model involves feeding inputs through the model to produce results, measuring performance and updating the weights to improve accuracy.</p>
<div id="cell-38" class="cell" data-execution_count="10">
<div class="cell-output cell-output-display" data-execution_count="10">
<div>
<figure class="figure">
<p><img src="https://buidai123.github.io/blog/posts/2024-06-30-your-deep-learning-journey/index_files/figure-html/cell-13-output-1.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Think of it as trial and error. The model tries to bake a cake, and if it’s to salty, it adjusts the recipe (update the weights). Over time, it learns the perfect proportions.</p>
</section>
<section id="using-a-trained-model" class="level3">
<h3 class="anchored" data-anchor-id="using-a-trained-model">Using a Trained Model</h3>
<p>Once the model is trained, it can be used just like a tranditional program, taking inputs and producing results predictably.</p>
<div id="cell-41" class="cell" data-execution_count="11">
<div class="cell-output cell-output-display" data-execution_count="11">
<div>
<figure class="figure">
<p><img src="https://buidai123.github.io/blog/posts/2024-06-30-your-deep-learning-journey/index_files/figure-html/cell-14-output-1.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Now, you have reliable recipe that consistently makes the perfect cake. The model processes new inputs(ingredients) and produces outputs(cakes) with the learned adjustments.</p>
</section>
</section>
<section id="what-our-image-recognizer-learned" class="level2">
<h2 class="anchored" data-anchor-id="what-our-image-recognizer-learned">What Our Image Recognizer Learned</h2>
<p>At this stage, we have an image recognizer that works very well. But what is it actually doing? Although many people believe that deep learning results in inpenetrable “black box” models (where predictions are given, but no one understand why), this isn’t entirely true. There is a vast body of reseach showing how to inspect deep learning model deeply and gain rich insights for them. However, all kind of machine learning model (including machine learning and traditional statistical models) can be challenging to fully understand, especially when dealing with new data that differs significantly from the training data.</p>
<p>When we fine-tuned our pre-trained model, we adapted the last layers(originally trained on general features like flowers, humans, animals) to specialize in a birds versus non-birds problem. Imagine our model initialy knew how to recognize the entire zoo, but now we’ve trained it to focus solely on recognizing birds. More generally, we could specialize such a pre-trained model on many different tasks.</p>
</section>
<section id="section" class="level1">
<h1></h1>
<section id="beyond-image-classification-other-application-of-deep-learning" class="level2">
<h2 class="anchored" data-anchor-id="beyond-image-classification-other-application-of-deep-learning">Beyond Image Classification: Other Application of Deep Learning</h2>
<p>Deep learning isn’t just about figuring out whether there’s bird in your photo. It’s way more powerful than that! Let’s explore a couple of areas where deep learning make significant strides:</p>
<ol type="1">
<li><p><strong>Image Segmentation</strong>:</p>
<p>Segmenation is a process of identifying and labling pixles in an image belonging to the same object. This is critically important for application like autonomous vehicles where the car needs to recognize and localize object such as pedestrians, other vehicles, and road signs. Instead of just saying, “Hey, there’s a cat in a picture”, segmentation says, “Here’s the outline of the cat in this picture”.</p></li>
<li><p><strong>Natural Language Processing (NLP)</strong>: Deep learning has drammatically improved Natural Language Processing over the last few years. Now computers can:</p>
<ul>
<li><strong>Generate text</strong>: Write conherent and context-aware essays (but don’t trust them with your love letters just yet).</li>
<li><strong>Translate languages</strong>: Turn English into Spanish, French, or Klingon (okay, maybe not Klingon…yet)</li>
<li><strong>Analize comments</strong>: Understand sentiments, detect sarcasm, and probably tell when you’re being a bit snarky.</li>
<li><strong>Label words in sentences</strong>: Identify parts of speech (nouns, verbs, adjectives, etc.), entities (like names and places), and more.</li>
</ul></li>
</ol>
<p>Here’s some cool code to classify the sentiment of a movie review better than anything availible just a few years ago:</p>
<div id="cell-46" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> fastai.text.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">all</span> <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span></span>
<span id="cb18-2">dls <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'test'</span>)</span>
<span id="cb18-3">learn <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> text_classifier_learner(dls, AWD_LSTM, drop_mult<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>, metrics<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>accuracy)</span>
<span id="cb18-4">learn.fine_tune(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-2</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

    <div>
      <progress value="144441344" class="" max="144440600" style="width:300px; height:20px; vertical-align: middle;"></progress>
      100.00% [144441344/144440600 00:03&lt;00:00]
    </div>
    
</div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

</div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

    <div>
      <progress value="105070592" class="" max="105067061" style="width:300px; height:20px; vertical-align: middle;"></progress>
      100.00% [105070592/105067061 00:01&lt;00:00]
    </div>
    
</div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train_loss</th>
<th data-quarto-table-cell-role="th">valid_loss</th>
<th data-quarto-table-cell-role="th">accuracy</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0.462561</td>
<td>0.395122</td>
<td>0.822320</td>
<td>03:08</td>
</tr>
</tbody>
</table>
</div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train_loss</th>
<th data-quarto-table-cell-role="th">valid_loss</th>
<th data-quarto-table-cell-role="th">accuracy</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0.301779</td>
<td>0.248262</td>
<td>0.899480</td>
<td>06:38</td>
</tr>
<tr class="even">
<td>1</td>
<td>0.244484</td>
<td>0.202708</td>
<td>0.921480</td>
<td>06:38</td>
</tr>
<tr class="odd">
<td>2</td>
<td>0.189148</td>
<td>0.194167</td>
<td>0.926160</td>
<td>06:37</td>
</tr>
<tr class="even">
<td>3</td>
<td>0.148741</td>
<td>0.191470</td>
<td>0.929720</td>
<td>06:38</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="cell-47" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1">learn.predict(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"I really liked that movie!"</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

</div>
<div class="cell-output cell-output-display" data-execution_count="2">
<pre><code>('pos', tensor(1), tensor([7.8042e-04, 9.9922e-01]))</code></pre>
</div>
</div>
<p>And boom! You have a state-of-art sentiment analyzer.</p>
</section>
<section id="the-important-of-validation-and-test-sets" class="level2">
<h2 class="anchored" data-anchor-id="the-important-of-validation-and-test-sets">The Important of Validation and Test Sets</h2>
<p>We’ve trained our model and it’s looking pretty smart, but know how do we know it’s actually learned something useful? This is where validation and test sets come in.</p>
<section id="why-do-we-need-a-validation-set" class="level3">
<h3 class="anchored" data-anchor-id="why-do-we-need-a-validation-set">Why Do We Need a Validation set?</h3>
<p>The goal of a model is to make predictions about unseen data. If we trained a model with all our data and evaluated it using the same data, we wouldn’t realy know how well it performs on new, unseen data. It could just memorize the training data(cheating basically). The model could get great results on your training data but bomb when given the data to analyze. To avoid this, we: - <strong>We split dataset</strong>: We divide our data into traning and validation sets. The trainning set is used to teach the model, and the validation set is used to see how well it’s learning</p>
</section>
<section id="preventing-overfitting-with-a-test-set" class="level3">
<h3 class="anchored" data-anchor-id="preventing-overfitting-with-a-test-set">Preventing Overfitting with a Test set</h3>
<p>Overfitting is a common issue where the model preform exceptionally well on the traning set but poorly on the validation set, meaning it has memorized the training data rather than learing the generalizable pattern.</p>
<p>Even when your model hasn’t fully memorized all your data, it might memorized certain parts of it during earlier traning stages. The longer you train, the better the accuracy on the traning set, but eventually, the validation accuracy will start to decline. This is because your model is begins memorizing the traning data instead of learning the parttern that generalize well. When this happens, we say the model is overfitting.</p>
<p>Here’s an example to visualize overfitting:</p>
<style>
    figure {
        display: block;
        mergin-left: auto;
        mergin-right: auto;
        text-align: center;
    }
</style>
<figure class="figure">
<img src="https://buidai123.github.io/blog/posts/2024-06-30-your-deep-learning-journey/att_00000.png" alt="Example of overfitting" style="width:90%;" class="figure-img">
<figcaption>
Example of overfitting
</figcaption>
</figure>
<p>The Image shows what happends when you overfit, using a simplified example where we have just one parameter and some randomly generated data. Although the overfitted model’s prediction are accurate for the data near the observed data points, they are way off when outside of that range.</p>
<p>Overfitting is the single most important and challenging issue when training machine learning models. It’s easy to create a model that does the great job at making predictions on the data it’s been trained on, but making accurate predictions on new data is much harder.</p>
<p>For instance, if you writting a handwriteen digit classifier (as we will very soon) and use it to recognize numbers on checks, you won’t see the same numbers the model was trained on–checks will have different variations of handwriting to deal with.</p>
</section>
</section>
<section id="wrapping-up" class="level2">
<h2 class="anchored" data-anchor-id="wrapping-up">Wrapping Up</h2>
<p>Deep learning is an exciting field that extends far beyond simple image classification. From understand speech to translate langugaes and detecting malware, it’s applications are vast. Through this blog post, we’ve seen how to build a bird classifier using the fastai library-an accessible, powrful tool that simplifies the complexities of machine learning.</p>
<p>By spllitting our data into traning and validation sets, we ensure our model doesn’t cheat and genuinely learns the task at hand. With powerful tools like fastai and the aibility to handle the deverse tasks, deep learning truly has potential to transform numerous industries.</p>
<p>I hope you enjoyed this journey as much as I did. Remember, the key to mastering deep learing is to keep experimenting and learning. So go ahead, build that next big thing, and maybe teach your computer to recognize your pet fish or translate cat’s meows!</p>
</section>
<section id="final-thoughts" class="level2">
<h2 class="anchored" data-anchor-id="final-thoughts">Final Thoughts</h2>
<p>Thank you for joining me on this deep learning adventure! if you find this blog helpful or inspiring, please share it with others who might also be interested. Deep learning is a continuously envolving field with endless possibilities. Stay curious, keep learning, and don’t hasitate to dive deeper into the world of AI.</p>
<p>Feel free to leave your comments, questions, or insights below. I’d love to hear your experiences, projects, and what you’re learning. Together, we can continue to explore and push the boundaries of what’s possible with deep learning.</p>
<p>Happy coding, and may your models always be accurate!</p>


</section>
</section>

 ]]></description>
  <category>blogging</category>
  <category>fastai</category>
  <guid>https://buidai123.github.io/blog/posts/2024-06-30-your-deep-learning-journey/</guid>
  <pubDate>Wed, 03 Jul 2024 17:00:00 GMT</pubDate>
  <media:content url="https://buidai123.github.io/blog/posts/2024-06-30-your-deep-learning-journey/xkcd.png" medium="image" type="image/png" height="127" width="144"/>
</item>
<item>
  <title>My First Blog Post</title>
  <dc:creator>Bui Huu Dai</dc:creator>
  <link>https://buidai123.github.io/blog/posts/first-post/</link>
  <description><![CDATA[ 





<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://buidai123.github.io/blog/posts/first-post/index_files/figure-html/cell-1-1-image.png" class="img-fluid figure-img"></p>
<figcaption>image.png</figcaption>
</figure>
</div>
<section id="welcome-to-my-blog" class="level1">
<h1>Welcome to My Blog</h1>
<p>This is my first blog post using Quarto and Jupyter Notebook. I’m excited to start sharing my thoughts and experiences here.</p>
<section id="why-i-started-this-blog" class="level2">
<h2 class="anchored" data-anchor-id="why-i-started-this-blog">Why I Started This Blog</h2>
<p>I decided to create this blog for a few reasons:</p>
<ul>
<li>To document my learning journey</li>
<li>To share interesting insights and ideas</li>
<li>To connect with like-minded individuals</li>
</ul>
</section>
<section id="what-to-expect" class="level2">
<h2 class="anchored" data-anchor-id="what-to-expect">What to Expect</h2>
<p>In this blog, you can expect to find:</p>
<ul>
<li>Tutorials and guides on various topics</li>
<li>Personal reflections and experiences</li>
<li>Interesting projects and experiments</li>
</ul>
<p>Stay tuned for more content coming soon!</p>


</section>
</section>

 ]]></description>
  <category>test</category>
  <category>quarto</category>
  <guid>https://buidai123.github.io/blog/posts/first-post/</guid>
  <pubDate>Sat, 29 Jun 2024 17:00:00 GMT</pubDate>
</item>
</channel>
</rss>
